{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8872caae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b34ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afd9ae9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
      "[[2], [2], [6], [6], [0], [0], [6], [5], [5], [4], [4], [3], [3], [2], [6], [6], [5], [5], [4], [4], [3], [6], [6], [5], [5], [4], [4], [3], [2], [2], [6], [6], [0], [0], [6], [5], [5], [4], [4], [3], [3], [2]]\n",
      "[[0], [1], [2], [3], [4], [5], [0], [1], [2], [3], [4], [5], [0], [1], [2], [3], [4], [5], [0], [1], [2], [3], [4], [5], [0], [1], [2], [3], [4], [5], [0], [1], [2], [3], [4], [5], [0], [1], [2], [3], [4], [5]]\n",
      "[[0], [1], [0], [2], [0], [3], [0], [4], [0], [5], [0], [1], [4], [5], [0], [3], [4], [2], [1], [0], [1], [2], [5], [4], [3], [4], [5], [0], [1], [2], [0], [3], [4], [1], [0], [2], [0], [3], [5], [0], [1], [4]]\n",
      "[[3], [1], [2], [0], [2], [1], [2], [5], [5], [3], [2], [4], [5], [5], [4], [5], [2], [3], [3], [4], [5], [4], [1], [4], [0], [2], [5], [4], [2], [1], [1], [1], [2], [1], [4], [2], [1], [5], [3], [0], [5], [1]]\n",
      "[[0], [1], [4], [1], [2], [0], [4], [5], [2], [3], [5], [5], [1], [2], [1], [3], [1], [1], [1], [2], [4], [3], [2], [1], [5], [1], [5], [5], [4], [2], [1], [2], [4], [1], [2], [0], [0], [5], [5], [0], [3], [1]]\n",
      "[[1], [4], [4], [5], [1], [0], [5], [3], [0], [4], [0], [0], [4], [5], [3], [1], [3], [5], [3], [4], [5], [2], [0], [2], [4], [1], [2], [2], [3], [0], [2], [4], [0], [2], [0], [2], [4], [4], [3], [1], [0], [0]]\n",
      "[[1], [5], [4], [1], [5], [4], [4], [1], [3], [1], [2], [5], [4], [0], [0], [2], [0], [0], [0], [5], [3], [5], [2], [1], [5], [1], [5], [4], [0], [0], [2], [5], [5], [2], [0], [0], [1], [2], [4], [3], [3], [2]]\n",
      "[[1], [0], [3], [3], [5], [5], [4], [0], [3], [1], [4], [3], [5], [3], [5], [1], [4], [1], [2], [2], [0], [3], [4], [5], [3], [4], [0], [1], [1], [5], [3], [4], [5], [5], [4], [1], [4], [4], [5], [3], [4], [5]]\n",
      "[[0], [1], [5], [5], [4], [3], [1], [3], [1], [5], [4], [2], [4], [3], [4], [0], [4], [1], [1], [4], [4], [2], [5], [3], [3], [0], [4], [3], [2], [3], [1], [1], [5], [2], [0], [3], [0], [3], [1], [1], [2], [5]]\n",
      "[[3], [5], [1], [2], [4], [1], [3], [0], [0], [3], [0], [0], [5], [2], [3], [0], [2], [0], [3], [3], [0], [5], [5], [0], [2], [3], [2], [5], [2], [2], [3], [3], [3], [2], [5], [1], [4], [1], [1], [3], [4], [3]]\n",
      "[[2], [2], [5], [1], [4], [5], [3], [3], [2], [1], [5], [3], [0], [3], [3], [1], [5], [1], [2], [2], [4], [4], [0], [1], [0], [5], [0], [0], [0], [4], [3], [2], [3], [2], [4], [0], [4], [5], [1], [5], [2], [3]]\n",
      "[[4], [1], [0], [3], [5], [5], [0], [0], [5], [0], [3], [3], [3], [0], [1], [4], [0], [1], [1], [3], [5], [3], [2], [0], [5], [1], [2], [3], [4], [4], [1], [1], [1], [4], [2], [3], [3], [5], [4], [4], [0], [4]]\n",
      "[[0], [5], [0], [3], [3], [5], [4], [5], [0], [3], [3], [1], [2], [3], [2], [5], [4], [4], [5], [2], [0], [4], [4], [4], [3], [5], [5], [4], [3], [1], [2], [0], [3], [1], [1], [3], [1], [0], [4], [5], [2], [3]]\n"
     ]
    }
   ],
   "source": [
    "songStrings = numpy.array([\n",
    "    \"CCGGAAGFFEEDDCGGFFEEDGGFFEEDCCGGAAGFFEEDDC\",\n",
    "    \"ABCDEFABCDEFABCDEFABCDEFABCDEFABCDEFABCDEF\",\n",
    "    \"ABACADAEAFABEFADECBABCFEDEFABCADEBACADFABE\",\n",
    "    \"DBCACBCFFDCEFFEFCDDEFEBEACFECBBBCBECBFDAFB\",\n",
    "    \"ABEBCAEFCDFFBCBDBBBCEDCBFBFFECBCEBCAAFFADB\",\n",
    "    \"BEEFBAFDAEAAEFDBDFDEFCACEBCCDACEACACEEDBAA\",\n",
    "    \"BFEBFEEBDBCFEAACAAAFDFCBFBFEAACFFCAABCEDDC\",\n",
    "    \"BADDFFEADBEDFDFBEBCCADEFDEABBFDEFFEBEEFDEF\",\n",
    "    \"ABFFEDBDBFECEDEAEBBEECFDDAEDCDBBFCADADBBCF\",\n",
    "    \"DFBCEBDAADAAFCDACADDAFFACDCFCCDDDCFBEBBDED\",\n",
    "    \"CCFBEFDDCBFDADDBFBCCEEABAFAAAEDCDCEAEFBFCD\",\n",
    "    \"EBADFFAAFADDDABEABBDFDCAFBCDEEBBBECDDFEEAE\",\n",
    "    \"AFADDFEFADDBCDCFEEFCAEEEDFFEDBCADBBDBAEFCD\"])\n",
    "notes=list(\"ABCDEFGH\")\n",
    "print(notes)\n",
    "chord=[[0],\n",
    "       [1],\n",
    "       [2],\n",
    "       [3],\n",
    "       [4],\n",
    "       [5],\n",
    "       [6],\n",
    "       [7]]\n",
    "allSongs=list()\n",
    "for songString in songStrings:\n",
    "    song=[]\n",
    "    for note in list(songString):\n",
    "        row=[0]*8\n",
    "        row[notes.index(note)]=1\n",
    "        song.append(chord[notes.index(note)])\n",
    "    allSongs.append(song)\n",
    "for song in allSongs:\n",
    "    print(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41104ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6], [5], [5], [4], [4], [3], [3], [2], [6], [6], [5], [5], [4], [4], [3], [6], [6], [5], [5], [4], [4], [3], [2], [2], [6], [6], [0], [0], [6], [5], [5], [4], [4], [3], [3], [2], [2], [2], [6], [6], [0], [0]]\n",
      "[5, 5, 4, 4, 3, 3, 2, 6, 6, 5, 5, 4, 4, 3, 6, 6, 5, 5, 4, 4, 3, 2, 2, 6, 6, 0, 0, 6, 5, 5, 4, 4, 3, 3, 2, 2, 2, 6, 6, 0, 0, 6]\n"
     ]
    }
   ],
   "source": [
    "def get_batch(source, i,songLength=42):# -> Tuple[Tensor, Tensor]\n",
    "    song=numpy.floor(i/songLength).astype(int)\n",
    "    start=i%songLength\n",
    "    #print(song,start)\n",
    "    data=[]\n",
    "    target=[]\n",
    "    for pos in range(start,start+songLength):\n",
    "        data.append(source[song][pos%songLength])\n",
    "        target+=source[song][(pos+1)%songLength]\n",
    "    return data, target\n",
    "\n",
    "d,t=get_batch(allSongs,6)\n",
    "print(d)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad59714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "n= 5\n",
    "length= 42\n",
    "\n",
    "song=numpy.floor(87/(length)).astype(int)\n",
    "print(song)\n",
    "\n",
    "print(%length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcbaf8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor, verbose=False) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        verbose=False\n",
    "        if verbose:\n",
    "            print(src.shape)\n",
    "            figure()\n",
    "            imshow(src.detach().numpy().reshape((42,20)))\n",
    "        src = self.pos_encoder(src)\n",
    "        if verbose:\n",
    "            print(src.shape)\n",
    "            figure()\n",
    "            imshow(src.detach().numpy().reshape((42,20)))\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        if verbose:\n",
    "            print(output.shape)\n",
    "            figure()\n",
    "            imshow(output.detach().numpy().reshape((42,20)))\n",
    "        output = self.decoder(output)\n",
    "        if verbose:\n",
    "            print(output.shape)\n",
    "            figure()\n",
    "            imshow(output.detach().numpy().reshape((42,6)))\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dec7a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fde5307b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df5cb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module,train_data) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    #for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "    who=numpy.array(list(range(bptt)))\n",
    "    numpy.random.shuffle(who)\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, who[i])\n",
    "        seq_len = len(data)\n",
    "        if seq_len != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:seq_len, :seq_len]\n",
    "        output = model(torch.tensor(data,dtype=torch.long), src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), torch.tensor(targets,dtype=torch.long))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(eval_data) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = len(data)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output = model(torch.tensor(data,dtype=torch.long), src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += seq_len * criterion(output_flat, torch.tensor(targets,dtype=torch.long)).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0cf2b4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 12, 210)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "x = range(0, len(allSongs) - 1, bptt)\n",
    "print(x)\n",
    "\n",
    "for batch, i in enumerate(range(0, 12, 210)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7756c8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The shape of the 2D attn_mask is torch.Size([210, 210]), but should be (42, 42).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m data,target\u001b[38;5;241m=\u001b[39mget_batch(allSongs,\u001b[38;5;241m42\u001b[39m\u001b[38;5;241m*\u001b[39msong)\n\u001b[1;32m     40\u001b[0m src_mask \u001b[38;5;241m=\u001b[39m generate_square_subsequent_mask(bptt)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 41\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#torch.save(model,\"models/DB_tf_epoch_{0}.model\".format(epoch))\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m42\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 42\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, src_mask, verbose)\u001b[0m\n\u001b[1;32m     40\u001b[0m     figure()\n\u001b[1;32m     41\u001b[0m     imshow(src\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m42\u001b[39m,\u001b[38;5;241m20\u001b[39m)))\n\u001b[0;32m---> 42\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:391\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    388\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 391\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    394\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:714\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    712\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 714\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:722\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    721\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 722\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:5354\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5352\u001b[0m     correct_2d_size \u001b[38;5;241m=\u001b[39m (tgt_len, src_len)\n\u001b[1;32m   5353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m correct_2d_size:\n\u001b[0;32m-> 5354\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe shape of the 2D attn_mask is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_2d_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5355\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   5356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attn_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The shape of the 2D attn_mask is torch.Size([210, 210]), but should be (42, 42)."
     ]
    }
   ],
   "source": [
    "nrOfSongs=5\n",
    "bptt = 42*nrOfSongs\n",
    "\n",
    "#original\n",
    "#ntokens = 8  # size of vocabulary\n",
    "#emsize = 20  # embedding dimension\n",
    "#d_hid = 20  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "#nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "#nhead = 4  # number of heads in nn.MultiheadAttention\n",
    "#dropout = 0.03  # dropout probability\n",
    "\n",
    "ntokens = 8  # size of vocabulary\n",
    "emsize = 100  # embedding dimension\n",
    "d_hid = 20  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 10  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.03  # dropout probability\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.999)\n",
    "\n",
    "\n",
    "epochs=10000\n",
    "W=[]\n",
    "Os=[]\n",
    "phi=[]\n",
    "phiSamples=[]\n",
    "Wsamples=[]\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model,allSongs)\n",
    "    \n",
    "    model.eval()\n",
    "    A=[]\n",
    "    B=[]\n",
    "    for song in range(nrOfSongs):\n",
    "        data,target=get_batch(allSongs,42*song)\n",
    "        src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "        output = model(torch.tensor(data,dtype=torch.long),src_mask,verbose=True)\n",
    "        #torch.save(model,\"models/DB_tf_epoch_{0}.model\".format(epoch))\n",
    "\n",
    "        \n",
    "        for i in range(42):\n",
    "            A.append(numpy.argmax(output.detach().numpy()[i]))\n",
    "            B.append(target[i])\n",
    "    score=(numpy.array(A)==numpy.array(B)).mean()\n",
    "    W.append(score)\n",
    "    if epoch%10==0:\n",
    "        print(epoch,W[-1])\n",
    "    #val_loss = evaluate(model, DaisyBell)\n",
    "    #val_ppl = math.exp(val_loss)\n",
    "    #print(f'| end of epoch {epoch:3d} | ' f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "\n",
    "    #if val_loss < best_val_loss:\n",
    "        #best_val_loss = val_loss\n",
    "    scheduler.step()\n",
    "plot(W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a8d4747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[2],\n",
       "  [2],\n",
       "  [6],\n",
       "  [6],\n",
       "  [0],\n",
       "  [0],\n",
       "  [6],\n",
       "  [5],\n",
       "  [5],\n",
       "  [4],\n",
       "  [4],\n",
       "  [3],\n",
       "  [3],\n",
       "  [2],\n",
       "  [6],\n",
       "  [6],\n",
       "  [5],\n",
       "  [5],\n",
       "  [4],\n",
       "  [4],\n",
       "  [3],\n",
       "  [6],\n",
       "  [6],\n",
       "  [5],\n",
       "  [5],\n",
       "  [4],\n",
       "  [4],\n",
       "  [3],\n",
       "  [2],\n",
       "  [2],\n",
       "  [6],\n",
       "  [6],\n",
       "  [0],\n",
       "  [0],\n",
       "  [6],\n",
       "  [5],\n",
       "  [5],\n",
       "  [4],\n",
       "  [4],\n",
       "  [3],\n",
       "  [3],\n",
       "  [2]],\n",
       " [[0],\n",
       "  [1],\n",
       "  [2],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5],\n",
       "  [0],\n",
       "  [1],\n",
       "  [2],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5],\n",
       "  [0],\n",
       "  [1],\n",
       "  [2],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5],\n",
       "  [0],\n",
       "  [1],\n",
       "  [2],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5],\n",
       "  [0],\n",
       "  [1],\n",
       "  [2],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5],\n",
       "  [0],\n",
       "  [1],\n",
       "  [2],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5],\n",
       "  [0],\n",
       "  [1],\n",
       "  [2],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5]],\n",
       " [[0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [2],\n",
       "  [0],\n",
       "  [3],\n",
       "  [0],\n",
       "  [4],\n",
       "  [0],\n",
       "  [5],\n",
       "  [0],\n",
       "  [1],\n",
       "  [4],\n",
       "  [5],\n",
       "  [0],\n",
       "  [3],\n",
       "  [4],\n",
       "  [2],\n",
       "  [1],\n",
       "  [0],\n",
       "  [1],\n",
       "  [2],\n",
       "  [5],\n",
       "  [4],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5],\n",
       "  [0],\n",
       "  [1],\n",
       "  [2],\n",
       "  [0],\n",
       "  [3],\n",
       "  [4],\n",
       "  [1],\n",
       "  [0],\n",
       "  [2],\n",
       "  [0],\n",
       "  [3],\n",
       "  [5],\n",
       "  [0],\n",
       "  [1],\n",
       "  [4]],\n",
       " [[3],\n",
       "  [1],\n",
       "  [2],\n",
       "  [0],\n",
       "  [2],\n",
       "  [1],\n",
       "  [2],\n",
       "  [5],\n",
       "  [5],\n",
       "  [3],\n",
       "  [2],\n",
       "  [4],\n",
       "  [5],\n",
       "  [5],\n",
       "  [4],\n",
       "  [5],\n",
       "  [2],\n",
       "  [3],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5],\n",
       "  [4],\n",
       "  [1],\n",
       "  [4],\n",
       "  [0],\n",
       "  [2],\n",
       "  [5],\n",
       "  [4],\n",
       "  [2],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [2],\n",
       "  [1],\n",
       "  [4],\n",
       "  [2],\n",
       "  [1],\n",
       "  [5],\n",
       "  [3],\n",
       "  [0],\n",
       "  [5],\n",
       "  [1]],\n",
       " [[0],\n",
       "  [1],\n",
       "  [4],\n",
       "  [1],\n",
       "  [2],\n",
       "  [0],\n",
       "  [4],\n",
       "  [5],\n",
       "  [2],\n",
       "  [3],\n",
       "  [5],\n",
       "  [5],\n",
       "  [1],\n",
       "  [2],\n",
       "  [1],\n",
       "  [3],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [2],\n",
       "  [4],\n",
       "  [3],\n",
       "  [2],\n",
       "  [1],\n",
       "  [5],\n",
       "  [1],\n",
       "  [5],\n",
       "  [5],\n",
       "  [4],\n",
       "  [2],\n",
       "  [1],\n",
       "  [2],\n",
       "  [4],\n",
       "  [1],\n",
       "  [2],\n",
       "  [0],\n",
       "  [0],\n",
       "  [5],\n",
       "  [5],\n",
       "  [0],\n",
       "  [3],\n",
       "  [1]],\n",
       " [[1],\n",
       "  [4],\n",
       "  [4],\n",
       "  [5],\n",
       "  [1],\n",
       "  [0],\n",
       "  [5],\n",
       "  [3],\n",
       "  [0],\n",
       "  [4],\n",
       "  [0],\n",
       "  [0],\n",
       "  [4],\n",
       "  [5],\n",
       "  [3],\n",
       "  [1],\n",
       "  [3],\n",
       "  [5],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5],\n",
       "  [2],\n",
       "  [0],\n",
       "  [2],\n",
       "  [4],\n",
       "  [1],\n",
       "  [2],\n",
       "  [2],\n",
       "  [3],\n",
       "  [0],\n",
       "  [2],\n",
       "  [4],\n",
       "  [0],\n",
       "  [2],\n",
       "  [0],\n",
       "  [2],\n",
       "  [4],\n",
       "  [4],\n",
       "  [3],\n",
       "  [1],\n",
       "  [0],\n",
       "  [0]],\n",
       " [[1],\n",
       "  [5],\n",
       "  [4],\n",
       "  [1],\n",
       "  [5],\n",
       "  [4],\n",
       "  [4],\n",
       "  [1],\n",
       "  [3],\n",
       "  [1],\n",
       "  [2],\n",
       "  [5],\n",
       "  [4],\n",
       "  [0],\n",
       "  [0],\n",
       "  [2],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [5],\n",
       "  [3],\n",
       "  [5],\n",
       "  [2],\n",
       "  [1],\n",
       "  [5],\n",
       "  [1],\n",
       "  [5],\n",
       "  [4],\n",
       "  [0],\n",
       "  [0],\n",
       "  [2],\n",
       "  [5],\n",
       "  [5],\n",
       "  [2],\n",
       "  [0],\n",
       "  [0],\n",
       "  [1],\n",
       "  [2],\n",
       "  [4],\n",
       "  [3],\n",
       "  [3],\n",
       "  [2]],\n",
       " [[1],\n",
       "  [0],\n",
       "  [3],\n",
       "  [3],\n",
       "  [5],\n",
       "  [5],\n",
       "  [4],\n",
       "  [0],\n",
       "  [3],\n",
       "  [1],\n",
       "  [4],\n",
       "  [3],\n",
       "  [5],\n",
       "  [3],\n",
       "  [5],\n",
       "  [1],\n",
       "  [4],\n",
       "  [1],\n",
       "  [2],\n",
       "  [2],\n",
       "  [0],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5],\n",
       "  [3],\n",
       "  [4],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [5],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5],\n",
       "  [5],\n",
       "  [4],\n",
       "  [1],\n",
       "  [4],\n",
       "  [4],\n",
       "  [5],\n",
       "  [3],\n",
       "  [4],\n",
       "  [5]],\n",
       " [[0],\n",
       "  [1],\n",
       "  [5],\n",
       "  [5],\n",
       "  [4],\n",
       "  [3],\n",
       "  [1],\n",
       "  [3],\n",
       "  [1],\n",
       "  [5],\n",
       "  [4],\n",
       "  [2],\n",
       "  [4],\n",
       "  [3],\n",
       "  [4],\n",
       "  [0],\n",
       "  [4],\n",
       "  [1],\n",
       "  [1],\n",
       "  [4],\n",
       "  [4],\n",
       "  [2],\n",
       "  [5],\n",
       "  [3],\n",
       "  [3],\n",
       "  [0],\n",
       "  [4],\n",
       "  [3],\n",
       "  [2],\n",
       "  [3],\n",
       "  [1],\n",
       "  [1],\n",
       "  [5],\n",
       "  [2],\n",
       "  [0],\n",
       "  [3],\n",
       "  [0],\n",
       "  [3],\n",
       "  [1],\n",
       "  [1],\n",
       "  [2],\n",
       "  [5]],\n",
       " [[3],\n",
       "  [5],\n",
       "  [1],\n",
       "  [2],\n",
       "  [4],\n",
       "  [1],\n",
       "  [3],\n",
       "  [0],\n",
       "  [0],\n",
       "  [3],\n",
       "  [0],\n",
       "  [0],\n",
       "  [5],\n",
       "  [2],\n",
       "  [3],\n",
       "  [0],\n",
       "  [2],\n",
       "  [0],\n",
       "  [3],\n",
       "  [3],\n",
       "  [0],\n",
       "  [5],\n",
       "  [5],\n",
       "  [0],\n",
       "  [2],\n",
       "  [3],\n",
       "  [2],\n",
       "  [5],\n",
       "  [2],\n",
       "  [2],\n",
       "  [3],\n",
       "  [3],\n",
       "  [3],\n",
       "  [2],\n",
       "  [5],\n",
       "  [1],\n",
       "  [4],\n",
       "  [1],\n",
       "  [1],\n",
       "  [3],\n",
       "  [4],\n",
       "  [3]],\n",
       " [[2],\n",
       "  [2],\n",
       "  [5],\n",
       "  [1],\n",
       "  [4],\n",
       "  [5],\n",
       "  [3],\n",
       "  [3],\n",
       "  [2],\n",
       "  [1],\n",
       "  [5],\n",
       "  [3],\n",
       "  [0],\n",
       "  [3],\n",
       "  [3],\n",
       "  [1],\n",
       "  [5],\n",
       "  [1],\n",
       "  [2],\n",
       "  [2],\n",
       "  [4],\n",
       "  [4],\n",
       "  [0],\n",
       "  [1],\n",
       "  [0],\n",
       "  [5],\n",
       "  [0],\n",
       "  [0],\n",
       "  [0],\n",
       "  [4],\n",
       "  [3],\n",
       "  [2],\n",
       "  [3],\n",
       "  [2],\n",
       "  [4],\n",
       "  [0],\n",
       "  [4],\n",
       "  [5],\n",
       "  [1],\n",
       "  [5],\n",
       "  [2],\n",
       "  [3]],\n",
       " [[4],\n",
       "  [1],\n",
       "  [0],\n",
       "  [3],\n",
       "  [5],\n",
       "  [5],\n",
       "  [0],\n",
       "  [0],\n",
       "  [5],\n",
       "  [0],\n",
       "  [3],\n",
       "  [3],\n",
       "  [3],\n",
       "  [0],\n",
       "  [1],\n",
       "  [4],\n",
       "  [0],\n",
       "  [1],\n",
       "  [1],\n",
       "  [3],\n",
       "  [5],\n",
       "  [3],\n",
       "  [2],\n",
       "  [0],\n",
       "  [5],\n",
       "  [1],\n",
       "  [2],\n",
       "  [3],\n",
       "  [4],\n",
       "  [4],\n",
       "  [1],\n",
       "  [1],\n",
       "  [1],\n",
       "  [4],\n",
       "  [2],\n",
       "  [3],\n",
       "  [3],\n",
       "  [5],\n",
       "  [4],\n",
       "  [4],\n",
       "  [0],\n",
       "  [4]],\n",
       " [[0],\n",
       "  [5],\n",
       "  [0],\n",
       "  [3],\n",
       "  [3],\n",
       "  [5],\n",
       "  [4],\n",
       "  [5],\n",
       "  [0],\n",
       "  [3],\n",
       "  [3],\n",
       "  [1],\n",
       "  [2],\n",
       "  [3],\n",
       "  [2],\n",
       "  [5],\n",
       "  [4],\n",
       "  [4],\n",
       "  [5],\n",
       "  [2],\n",
       "  [0],\n",
       "  [4],\n",
       "  [4],\n",
       "  [4],\n",
       "  [3],\n",
       "  [5],\n",
       "  [5],\n",
       "  [4],\n",
       "  [3],\n",
       "  [1],\n",
       "  [2],\n",
       "  [0],\n",
       "  [3],\n",
       "  [1],\n",
       "  [1],\n",
       "  [3],\n",
       "  [1],\n",
       "  [0],\n",
       "  [4],\n",
       "  [5],\n",
       "  [2],\n",
       "  [3]]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allSongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51e73522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 3]\n"
     ]
    }
   ],
   "source": [
    "who=numpy.array([1,2,3])\n",
    "numpy.random.shuffle(who)\n",
    "print(who)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18030539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26785714285714285"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade60316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
