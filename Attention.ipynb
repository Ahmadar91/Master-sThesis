{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:31.277897100Z",
     "start_time": "2023-12-06T17:00:30.656125100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:39.317492Z",
     "start_time": "2023-12-06T17:00:34.720993Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import numpy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib.pyplot import figure, subplots, imshow, xticks, yticks, title\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.cluster import mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "from statistics import mean\n",
    "from scipy.stats import entropy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim**0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:41.605650400Z",
     "start_time": "2023-12-06T17:00:41.594151100Z"
    }
   },
   "outputs": [],
   "source": [
    "class NetRNN(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Expansion layer to match CustomRNN\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        self.rnnLayer = nn.RNN(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        x = self.expand_layer(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        self.h0 = torch.Tensor(numpy.zeros((1, x.shape[0], self.hidden_dim)))\n",
    "        out, self.h0 = self.rnnLayer(x, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "        out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.Tensor(numpy.zeros((1, 1, self.hidden_dim)))\n",
    "            for i in range(x.shape[1]):\n",
    "                # Apply the expansion layer to each step\n",
    "                step_input = self.expand_layer(x[l][i].reshape((1, 1, self.inp)))\n",
    "                step_input = torch.tanh(step_input)\n",
    "\n",
    "                out, h0 = self.rnnLayer(step_input, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "            out = torch.tanh(out)\n",
    "            out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "            for i in range(x.shape[1]):\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "        return numpy.array(O), numpy.array(H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN With Attention then Expansion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:40.939651100Z",
     "start_time": "2023-12-06T17:00:40.913651900Z"
    }
   },
   "outputs": [],
   "source": [
    "class NetRNNWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetRNNWithAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = SelfAttention(inp)  # Assuming SelfAttention is defined elsewhere\n",
    "\n",
    "        # Expansion layer to match CustomRNN\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        self.rnnLayer = nn.RNN(self.hidden_dim, self.hidden_dim, batch_first=True, nonlinearity='tanh')\n",
    "        \n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(x)\n",
    "        print(attn_out.shape)\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        expanded_attn_out = self.expand_layer(attn_out)\n",
    "        print(expanded_attn_out.shape)\n",
    "        expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "        # RNN processing\n",
    "        h0 = torch.zeros(1, x.shape[0], self.hidden_dim)\n",
    "        rnn_out, _ = self.rnnLayer(expanded_attn_out, h0)\n",
    "        rnn_out = torch.tanh(rnn_out)\n",
    "\n",
    "        # Final output layer\n",
    "        out = torch.tanh(self.outputLayer(rnn_out[:, -1, :])).squeeze()\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                # Apply the expansion layer with tanh activation\n",
    "                expanded_attn_out = self.expand_layer(attn_out)\n",
    "                expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "                # RNN processing\n",
    "                out, h0 = self.rnnLayer(expanded_attn_out, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetRNNWithAttention()\n",
    "print(model)\n",
    "print(model(torch.Tensor(np.random.random((1,30,3)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Expansion first then Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRNNWithAttentionExpFirst(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetRNNWithAttentionExpFirst, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Expansion layer\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # Attention layer applied after expansion\n",
    "        self.attention = SelfAttention(self.hidden_dim)  # Assuming SelfAttention is defined elsewhere\n",
    "\n",
    "        # RNN layer\n",
    "        self.rnnLayer = nn.RNN(self.hidden_dim, self.hidden_dim, batch_first=True, nonlinearity='tanh')\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        expanded_x = self.expand_layer(x)\n",
    "        expanded_x = torch.tanh(expanded_x)\n",
    "\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(expanded_x)\n",
    "\n",
    "        # RNN processing\n",
    "        h0 = torch.zeros(1, attn_out.shape[0], self.hidden_dim)\n",
    "        rnn_out, _ = self.rnnLayer(attn_out, h0)\n",
    "        rnn_out = torch.tanh(rnn_out)\n",
    "\n",
    "        # Final output layer\n",
    "        out = torch.tanh(self.outputLayer(rnn_out[:, -1, :])).squeeze()\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Apply the expansion layer with tanh activation\n",
    "                expanded_input = self.expand_layer(x[l][i].reshape((1, 1, self.inp)))\n",
    "                expanded_input = torch.tanh(expanded_input)\n",
    "\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(expanded_input)\n",
    "\n",
    "                # RNN processing\n",
    "                out, h0 = self.rnnLayer(attn_out, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "\n",
    "model = NetRNNWithAttention(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Expansion layer\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstmLayer = nn.LSTM(self.hidden_dim, int(self.hidden_dim/2), 1, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(int(self.hidden_dim/2), 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        x = self.expand_layer(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        # Initialize hidden and cell states\n",
    "        self.h0 = (torch.zeros(1, x.shape[0], int(self.hidden_dim/2)),\n",
    "                   torch.zeros(1, x.shape[0], int(self.hidden_dim/2)))\n",
    "\n",
    "        # LSTM processing\n",
    "        out, self.h0 = self.lstmLayer(x, self.h0)\n",
    "        out = torch.tanh(out)  # Apply tanh to the LSTM output if needed\n",
    "\n",
    "        # Concatenate hidden and cell states\n",
    "        hh = numpy.concatenate((self.h0[0].detach().numpy(), self.h0[1].detach().numpy()), 2)\n",
    "        self.hidden.append(hh)\n",
    "\n",
    "        # Final output layer with tanh activation\n",
    "        out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = (torch.zeros(1, 1, int(self.hidden_dim/2)),\n",
    "                  torch.zeros(1, 1, int(self.hidden_dim/2)))\n",
    "            for i in range(x.shape[1]):\n",
    "                # Apply the expansion layer to each step\n",
    "                step_input = self.expand_layer(x[l][i].reshape((1, 1, self.inp)))\n",
    "                step_input = torch.tanh(step_input)\n",
    "\n",
    "                out, h0 = self.lstmLayer(step_input, h0)\n",
    "                hh = numpy.concatenate((h0[0].detach().numpy().flatten(), h0[1].detach().numpy().flatten()))\n",
    "                H.append(hh.flatten())\n",
    "\n",
    "            out = torch.tanh(out)  # Apply tanh to the LSTM output if needed\n",
    "            out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "            for i in range(x.shape[1]):\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "        return numpy.array(O), numpy.array(H)\n",
    "    \n",
    "model = NetLSTM(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Attention then Expansion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetLSTMWithAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = SelfAttention(inp)  # Assuming SelfAttention is defined elsewhere\n",
    "\n",
    "        # Expansion layer to match CustomRNN\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstmLayer = nn.LSTM(self.hidden_dim, int(self.hidden_dim/2), batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(int(self.hidden_dim/2), 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(x)\n",
    "\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        expanded_attn_out = self.expand_layer(attn_out)\n",
    "        expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "        # LSTM processing\n",
    "        h0 = (torch.zeros(1, x.shape[0], int(self.hidden_dim/2)),\n",
    "              torch.zeros(1, x.shape[0], int(self.hidden_dim/2)))\n",
    "        lstm_out, _ = self.lstmLayer(expanded_attn_out, h0)\n",
    "        lstm_out = torch.tanh(lstm_out)\n",
    "\n",
    "        # Final output layer\n",
    "        out = torch.tanh(self.outputLayer(lstm_out[:, -1, :])).squeeze()\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = (torch.zeros(1, 1, int(self.hidden_dim/2)),\n",
    "                  torch.zeros(1, 1, int(self.hidden_dim/2)))\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                # Apply the expansion layer with tanh activation\n",
    "                expanded_attn_out = self.expand_layer(attn_out)\n",
    "                expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "                # LSTM processing\n",
    "                out, h0 = self.lstmLayer(expanded_attn_out, h0)\n",
    "                H.append(torch.cat((h0[0].detach(), h0[1].detach()), 2).numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "    \n",
    "model = NetLSTMWithAttention(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Expansion first then Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLSTMWithAttentionExpFirst(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetLSTMWithAttentionExpFirst, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Expansion layer first\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # Attention layer applied after expansion\n",
    "        self.attention = SelfAttention(self.hidden_dim)  # Assuming SelfAttention is defined elsewhere\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstmLayer = nn.LSTM(self.hidden_dim, int(self.hidden_dim / 2), batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(int(self.hidden_dim / 2), 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        expanded_x = self.expand_layer(x)\n",
    "        expanded_x = torch.tanh(expanded_x)\n",
    "\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(expanded_x)\n",
    "\n",
    "        # LSTM processing\n",
    "        h0 = (torch.zeros(1, x.shape[0], int(self.hidden_dim / 2)),\n",
    "              torch.zeros(1, x.shape[0], int(self.hidden_dim / 2)))\n",
    "        lstm_out, _ = self.lstmLayer(attn_out, h0)\n",
    "        lstm_out = torch.tanh(lstm_out)\n",
    "\n",
    "        # Final output layer with tanh activation\n",
    "        out = torch.tanh(self.outputLayer(lstm_out[:, -1, :])).squeeze()\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = (torch.zeros(1, 1, int(self.hidden_dim / 2)),\n",
    "                  torch.zeros(1, 1, int(self.hidden_dim / 2)))\n",
    "            for i in range(x.shape[1]):\n",
    "                # Apply the expansion layer with tanh activation\n",
    "                expanded_input = self.expand_layer(x[l][i].reshape((1, 1, self.inp)))\n",
    "                expanded_input = torch.tanh(expanded_input)\n",
    "\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(expanded_input)\n",
    "\n",
    "                # LSTM processing\n",
    "                out, h0 = self.lstmLayer(attn_out, h0)\n",
    "                H.append(torch.cat((h0[0].detach(), h0[1].detach()), 2).numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "\n",
    "model = NetLSTMWithAttention(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetGRU(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetGRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Expansion layer\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # GRU layer\n",
    "        self.gruLayer = nn.GRU(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        x = self.expand_layer(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        # GRU processing\n",
    "        self.h0 = torch.zeros(1, x.shape[0], self.hidden_dim)\n",
    "        out, self.h0 = self.gruLayer(x, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "\n",
    "        # Final output layer with tanh activation\n",
    "        out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Apply the expansion layer to each step\n",
    "                step_input = self.expand_layer(x[l][i].reshape((1, 1, self.inp)))\n",
    "                step_input = torch.tanh(step_input)\n",
    "\n",
    "                # GRU processing\n",
    "                out, h0 = self.gruLayer(step_input, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)  # Apply tanh to the GRU output if needed\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "    \n",
    "model = NetGRU(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU with Attention then Expansion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetGRUMWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetGRUMWithAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = SelfAttention(inp)  # Assuming SelfAttention is defined elsewhere\n",
    "\n",
    "        # Expansion layer\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # GRU layer\n",
    "        self.gruLayer = nn.GRU(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(x)\n",
    "\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        expanded_attn_out = self.expand_layer(attn_out)\n",
    "        expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "        # GRU processing\n",
    "        self.h0 = torch.zeros(1, x.shape[0], self.hidden_dim)\n",
    "        out, self.h0 = self.gruLayer(expanded_attn_out, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "\n",
    "        # Final output layer with tanh activation\n",
    "        out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                # Apply the expansion layer with tanh activation\n",
    "                expanded_attn_out = self.expand_layer(attn_out)\n",
    "                expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "                # GRU processing\n",
    "                out, h0 = self.gruLayer(expanded_attn_out, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)  # Apply tanh to the GRU output if needed\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "\n",
    "model = NetGRUMWithAttention(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Expansion first then Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetGRUMWithAttentionExpFirst(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetGRUMWithAttentionExpFirst, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Expansion layer first\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # Attention layer applied after expansion\n",
    "        self.attention = SelfAttention(self.hidden_dim)  # Assuming SelfAttention is defined elsewhere\n",
    "\n",
    "        # GRU layer\n",
    "        self.gruLayer = nn.GRU(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        expanded_x = self.expand_layer(x)\n",
    "        expanded_x = torch.tanh(expanded_x)\n",
    "\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(expanded_x)\n",
    "\n",
    "        # GRU processing\n",
    "        self.h0 = torch.zeros(1, x.shape[0], self.hidden_dim)\n",
    "        out, self.h0 = self.gruLayer(attn_out, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "\n",
    "        # Final output layer with tanh activation\n",
    "        out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Apply the expansion layer with tanh activation\n",
    "                expanded_input = self.expand_layer(x[l][i].reshape((1, 1, self.inp)))\n",
    "                expanded_input = torch.tanh(expanded_input)\n",
    "\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(expanded_input)\n",
    "\n",
    "                # GRU processing\n",
    "                out, h0 = self.gruLayer(attn_out, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)  # Apply tanh to the GRU output if needed\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "\n",
    "model = NetGRUMWithAttention(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:23.990361100Z",
     "start_time": "2023-12-06T18:19:23.969358500Z"
    }
   },
   "outputs": [],
   "source": [
    "def generateTrainData(num_samples, min_max):\n",
    "    s = []  # Sequences\n",
    "    t = []  # Labels\n",
    "    params = {\n",
    "    \"min_length\": min_max[0],\n",
    "    \"max_length\": min_max[1],\n",
    "    \"fill\": 0,\n",
    "    \"value_1\": -1,\n",
    "    \"value_2\": 1,\n",
    "}\n",
    "    for _ in range(num_samples):\n",
    "        common_length = np.random.randint(params[\"min_length\"], params[\"max_length\"] + 1)\n",
    "\n",
    "        array_A = np.full(common_length, params[\"fill\"])\n",
    "        array_B = np.full(common_length, params[\"fill\"])\n",
    "        array_C = np.full(common_length, params[\"fill\"])\n",
    "\n",
    "        # Exclude the last two indices\n",
    "        possible_indices = np.arange(common_length - 2)\n",
    "\n",
    "        index_A = np.random.choice(possible_indices)\n",
    "        value_A = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_A[index_A] = value_A\n",
    "\n",
    "        # Update possible indices for array B to also exclude index_A\n",
    "        possible_indices_B = np.delete(possible_indices, np.where(possible_indices == index_A))\n",
    "        index_B = np.random.choice(possible_indices_B)\n",
    "        value_B = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_B[index_B] = value_B\n",
    "\n",
    "        value_C = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_C[-1] = value_C\n",
    "        array_C[-2] = value_C\n",
    "\n",
    "        mapped_value_A = 1 if value_A == params[\"value_2\"] else 0\n",
    "        mapped_value_B = 1 if value_B == params[\"value_2\"] else 0\n",
    "        #value_1 = -1\n",
    "        #value_2 = 1\n",
    "        if value_C == params[\"value_1\"]: # XOR\n",
    "            result = int(mapped_value_A != mapped_value_B) \n",
    "        else : # XNOR\n",
    "            result =  int(mapped_value_A == mapped_value_B)\n",
    "\n",
    "        # Mapping back to original value_1 and value_2 for the label\n",
    "        label_value_A = params[\"value_2\"] if mapped_value_A == 1 else params[\"value_1\"]\n",
    "        label_value_B = params[\"value_2\"] if mapped_value_B == 1 else params[\"value_1\"]\n",
    "        results_XORNOR= params[\"value_2\"] if result == 1 else params[\"value_1\"]\n",
    "\n",
    "        #label_arr = [mapped_value_A, mapped_value_B, result]  # Label array with value_A, value_B, and result\n",
    "        label_arr = [label_value_A, label_value_B, results_XORNOR]  # Label array with value_A, value_B, and result\n",
    "\n",
    "        combined_array = np.vstack([array_A, array_B, array_C]).T\n",
    "        s.append(combined_array)\n",
    "        t.append(label_arr)\n",
    "\n",
    "    return s, np.array(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:26.878858500Z",
     "start_time": "2023-12-06T18:19:26.864361100Z"
    }
   },
   "outputs": [],
   "source": [
    "num_seq = 1\n",
    "# Example dictionary with parameters\n",
    "sequences, labels = generateTrainData(num_seq, [5,5])\n",
    "print(sequences)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:28.905358900Z",
     "start_time": "2023-12-06T18:19:28.879361600Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sequences, labels = generateTrainData(1, [10,10])\n",
    "model = NetRNN()\n",
    "output=model(torch.Tensor(sequences))\n",
    "print(torch.Tensor(sequences))\n",
    "print(output.shape)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:03:38.917270600Z",
     "start_time": "2023-12-06T18:03:36.540272600Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences, labels = generateTrainData(5, [10,15])\n",
    "\n",
    "def plot_sequences(sequences, labels):\n",
    "    num_samples = len(sequences)  # Number of samples to display\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(3 * num_samples, 15))\n",
    "\n",
    "    for i, (seq, label) in enumerate(zip(sequences, labels)):\n",
    "        reshaped_sequence = seq  # Use the sequence as it is\n",
    "\n",
    "        ax = plt.subplot(1, num_samples, i + 1)\n",
    "        img = ax.imshow(\n",
    "            reshaped_sequence, cmap=\"gray\", vmin=-1.0, vmax=1.0\n",
    "        )  # Adjusted vmin and vmax\n",
    "        operation_title = \"XOR\" if seq[-1][2] == -1 else \"XNOR\"\n",
    "        ax.set_title(f\"Operation: {operation_title}, {label}\")\n",
    "\n",
    "        ax.set_xlabel(\"Arrays (A, B, C)\")\n",
    "        ax.set_ylabel(\"Time Points\")\n",
    "        ax.set_xticks(range(3))\n",
    "        ax.set_xticklabels([\"A\", \"B\", \"C\"])\n",
    "        ax.set_yticks(range(reshaped_sequence.shape[0]))\n",
    "        ax.set_yticklabels([f\"{j+1}\" for j in range(reshaped_sequence.shape[0])])\n",
    "\n",
    "    # Adjusted positioning of colorbar\n",
    "    cbar_ax = plt.gcf().add_axes([0.93, 0.15, 0.02, 0.7])\n",
    "    cbar = plt.colorbar(img, cax=cbar_ax)\n",
    "    cbar.set_ticks([-1, 0, 1])\n",
    "    cbar.set_ticklabels([\"-1\", \"0\", \"1\"])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming sequences and labels are already generated using generateTrainData\n",
    "plot_sequences(sequences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:01:02.144838900Z",
     "start_time": "2023-12-06T17:01:02.130460400Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters_list = []\n",
    "\n",
    "# min_lengths = [5, 10, 10, 20, 20, 40, 40, 70,100]\n",
    "# max_lengths = [5, 10, 15, 20, 25, 40, 45,75,140]\n",
    "min_lengths = [10]\n",
    "max_lengths = [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:44.759873Z",
     "start_time": "2023-12-06T18:19:42.281374100Z"
    }
   },
   "outputs": [],
   "source": [
    "collectorA = dict()\n",
    "num_samples = 100\n",
    "for rep in range(3):  # Number of repetitions\n",
    "    for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "        # Select the model based on 'kind'\n",
    "        for kind in [\"RNN\", \"RNNA\", \"RNNE\",\"LSTM\", \"LSTMA\",\"LSTME\", \"GRU\", \"GRUA\",\"GRUE\"]:\n",
    "        # Select the model based on 'kind'\n",
    "            if kind == \"RNN\":\n",
    "                model = NetRNN(hidden_dim=12, inp=3)\n",
    "            elif kind == \"RNNA\":\n",
    "                model = NetRNNWithAttention(hidden_dim=12, inp=3)\n",
    "            elif kind == \"RNNE\":\n",
    "                model = NetRNNWithAttentionExpFirst(hidden_dim=12, inp=3)\n",
    "            elif kind == \"LSTM\":\n",
    "                model = NetLSTM(hidden_dim=12, inp=3)\n",
    "            elif kind == \"LSTMA\":\n",
    "                model = NetLSTMWithAttention(hidden_dim=12, inp=3)\n",
    "            elif kind == \"LSTME\":\n",
    "                model = NetLSTMWithAttentionExpFirst(hidden_dim=12, inp=3)\n",
    "            elif kind == \"GRU\":\n",
    "                model = NetGRU(hidden_dim=12, inp=3)\n",
    "            elif kind == \"GRUA\":\n",
    "                model = NetGRUMWithAttention(hidden_dim=12, inp=3)\n",
    "            elif kind == \"GRUE\":\n",
    "                model = NetGRUMWithAttentionExpFirst(hidden_dim=12, inp=3)\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters())\n",
    "            criterion = nn.MSELoss()\n",
    "            acc = 0.0\n",
    "            W = []\n",
    "            AC = []\n",
    "            start_time = time.time()  # Start time of the epoch\n",
    "\n",
    "            while True:\n",
    "                sequences, targets = generateTrainData(num_samples, [min_len,max_len])\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                count = 0\n",
    "\n",
    "                for seq, target in zip(sequences, targets):\n",
    "                    optimizer.zero_grad()\n",
    "                    seq_tensor = torch.Tensor([seq])  # Add an extra dimension for batch\n",
    "                    target_tensor = torch.Tensor([target])\n",
    "\n",
    "                    output = model(seq_tensor)\n",
    "                    loss = criterion(output, target_tensor)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    A = 1.0 * (target_tensor.detach() > 0.0)\n",
    "                    B = 1.0 * (output.detach() > 0.0)\n",
    "                    acc = (1.0 * (A.flatten() == B.flatten())).mean()\n",
    "                    total_acc += acc\n",
    "                    count += 1\n",
    "\n",
    "                avg_loss = total_loss / count\n",
    "                avg_acc = total_acc / count\n",
    "                W.append(avg_loss)\n",
    "                AC.append(avg_acc)\n",
    "                print(f\"{kind}, rep: {rep}, epoch: {len(AC)}, acc: {avg_acc}, Loss {avg_loss}\")\n",
    "\n",
    "                # Check for stopping condition\n",
    "                if avg_acc >= 0.97 or len(W)>2000:\n",
    "                    break\n",
    "\n",
    "                end_time = time.time()  # End time of the epoch\n",
    "                epoch_duration = end_time - start_time  # Calculate duration\n",
    "                collectorA[f\"{kind}_rep_{rep}_min_{min_len} max_{max_len}\"] = AC\n",
    "                torch.save(model, f'model_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.model')\n",
    "                print(f\"{kind:<20} Rep: {rep:<3} Epoch: {len(A):<5} Acc: {avg_acc:.4f} \" f\"_min_{min_len}_max_{max_len} Time: {epoch_duration:.2f} sec\")\n",
    "                df=pd.DataFrame()\n",
    "                df[\"accuracy\"]=AC\n",
    "                df[\"loss\"]=W\n",
    "                df.to_csv(f'score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collectorA.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_str_to_float(tensor_str):\n",
    "    # Convert a string like 'tensor(0.4533)' to a float\n",
    "    return float(tensor_str.strip('tensor()'))\n",
    "\n",
    "def pad_data(data, target_length):\n",
    "    # Truncate or extend the data to the target length\n",
    "    if len(data) > target_length:\n",
    "        return data[:target_length]\n",
    "    else:\n",
    "        pad_value = data[-1] if data else 0  # Use last value or 0 if data is empty\n",
    "        x = data + [pad_value] * (target_length - len(data))\n",
    "        return x[:target_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kinds = [\"RNN\", \"RNNA\", \"RNNE\", \"LSTM\", \"LSTMA\", \"LSTME\", \"GRU\", \"GRUA\", \"GRUE\"]\n",
    "kinds = [\"RNN\", \"RNNA\", \"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]\n",
    "min_lengths = [10, 10 ,20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15 ,20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31\n",
    "target_epoch = 2000\n",
    "\n",
    "# Plotting for each kind\n",
    "for kind in kinds:\n",
    "    plt.figure(figsize=(10, 20))  # Adjust the figure size for 2x5 layout\n",
    "\n",
    "    for i, (min_len, max_len) in enumerate(zip(min_lengths, max_lengths)):\n",
    "        ax = plt.subplot(5, 2, i + 1)  # Create subplots in a 2x5 grid\n",
    "        rep_data = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            #filename = f'./data/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                padded_data = pad_data(data['accuracy'].tolist(), target_epoch)\n",
    "                rep_data.append(padded_data)\n",
    "                ax.plot(padded_data, label=f\"Rep {rep+1}\", alpha=0.5,linewidth=0.8, color='gray')\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if not rep_data:\n",
    "            continue\n",
    "\n",
    "        avg_data = np.mean(np.array(rep_data), axis=0)\n",
    "        ax.plot(avg_data, label=\"Average\", linewidth=1, color='black')\n",
    "\n",
    "        ax.set_title(f\"{kind} (Min: {min_len} - Max: {max_len})\")\n",
    "        #if i == 0:  # Add legend only to the first subplot\n",
    "        #    ax.legend()\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds = [\"RNN\", \"RNNA\", \"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]\n",
    "min_lengths = [10, 10 ,20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15 ,20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31\n",
    "target_epoch = 2000\n",
    "\n",
    "# Adjusting the plot to grayscale and using fill_between for replicates\n",
    "def plot_with_grayscale_fill_between(kind, min_lengths, max_lengths, num_reps, target_epoch):\n",
    "    plt.figure(figsize=(10, 10))  # Adjust the figure size for layout\n",
    "\n",
    "    for i, (min_len, max_len) in enumerate(zip(min_lengths, max_lengths)):\n",
    "        ax = plt.subplot(5, 2, i + 1)  # Create subplots in a 2x5 grid\n",
    "\n",
    "        rep_data = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                padded_data = pad_data(data['accuracy'].tolist(), target_epoch)\n",
    "                rep_data.append(padded_data)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "        if not rep_data:\n",
    "            continue\n",
    "\n",
    "        avg_data = np.mean(rep_data, axis=0)\n",
    "        std_dev = np.std(rep_data, axis=0)\n",
    "\n",
    "        # Plot replicates as fill_between\n",
    "        ax.fill_between(range(target_epoch), avg_data - std_dev, avg_data + std_dev, color='gray', alpha=0.3)\n",
    "        \n",
    "        # Plot average as a black line\n",
    "        ax.plot(avg_data, label=\"Average\", linewidth=2, color='black')\n",
    "\n",
    "        ax.set_title(f\"{kind} (Min: {min_len} - Max: {max_len})\")\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting for each kind\n",
    "for kind in kinds:\n",
    "    plot_with_grayscale_fill_between(kind, min_lengths, max_lengths, num_reps, target_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds = [\"RNN\", \"RNNA\", \"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]\n",
    "min_lengths = [10, 20, 30, 40, 50, 10, 20, 30, 40, 50]\n",
    "max_lengths = [10, 20, 30, 40, 50, 15, 25, 35, 45, 55]\n",
    "num_reps = 31\n",
    "target_epoch = 2000\n",
    "\n",
    "# Create a single figure for the plots\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, kind in enumerate(kinds):\n",
    "    ax = plt.subplot(3, 2, i + 1)  # Create subplots in a 3x2 grid\n",
    "\n",
    "    for j, (min_len, max_len) in enumerate(zip(min_lengths, max_lengths)):\n",
    "        combination_avg_data = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                padded_data = pad_data(data['accuracy'].tolist(), target_epoch)\n",
    "                combination_avg_data.append(padded_data)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "        if combination_avg_data:\n",
    "            avg_data = np.mean(np.array(combination_avg_data), axis=0)\n",
    "            line_style = '--' if min_len == max_len else '-'\n",
    "            ax.plot(avg_data, label=f\"Min {min_len}, Max {max_len}\", linewidth=0.8, linestyle=line_style)\n",
    "\n",
    "    ax.set_title(f\"{kind}\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    if i == 0:  # Add legend only to the first subplot\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kinds = [\"RNN\", \"RNNA\", \"RNNE\", \"LSTM\", \"LSTMA\", \"LSTME\", \"GRU\", \"GRUA\", \"GRUE\"]\n",
    "kinds = [\"RNN\", \"RNNA\", \"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]\n",
    "min_lengths = [10, 20, 30, 40, 50]  # Adjusted values\n",
    "max_lengths = [15, 25, 35, 45, 55]  # Adjusted values\n",
    "num_reps = 31  # Number of repetitions\n",
    "target_epoch = 2000  # Target number of epochs\n",
    "\n",
    "# Create a single figure for the 3x3 grid\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, kind in enumerate(kinds):\n",
    "    ax = plt.subplot(3,2, i + 1)  # Create subplots in a 3x3 grid\n",
    "\n",
    "    for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "        combination_avg_data = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            #filename = f'./data/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                padded_data = pad_data(data['accuracy'].tolist(), target_epoch)\n",
    "                combination_avg_data.append(padded_data)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if combination_avg_data:\n",
    "            # Calculate the average for this min/max length combination\n",
    "            avg_data = np.mean(np.array(combination_avg_data), axis=0)\n",
    "            ax.plot(avg_data, label=f\"Min {min_len}, Max {max_len}\")\n",
    "\n",
    "    ax.set_title(f\"{kind}\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kinds = [\"RNN\", \"RNNA\", \"RNNE\", \"LSTM\", \"LSTMA\", \"LSTME\", \"GRU\", \"GRUA\", \"GRUE\"]\n",
    "kinds = [\"RNN\", \"RNNA\", \"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]\n",
    "min_lengths = [10, 20, 30, 40, 50, 10, 20, 30, 40, 50]  # Adjusted values\n",
    "max_lengths = [10, 20, 30, 40, 50, 15, 25, 35, 45, 55]  # Adjusted values\n",
    "num_reps = 31  # Number of repetitions\n",
    "target_epoch = 2000  # Target number of epochs\n",
    "\n",
    "# Create a single figure for the 3x3 grid\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, kind in enumerate(kinds):\n",
    "    ax = plt.subplot(3, 2, i + 1)  # Create subplots in a 3x3 grid\n",
    "\n",
    "    for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "        combination_avg_data = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            #filename = f'./data/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                padded_data = pad_data(data['accuracy'].tolist(), target_epoch)\n",
    "                combination_avg_data.append(padded_data)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if combination_avg_data:\n",
    "            avg_data = np.mean(np.array(combination_avg_data), axis=0)\n",
    "            line_style = '--' if min_len == max_len else '-'\n",
    "            ax.plot(avg_data, label=f\"Min {min_len}, Max {max_len}\", linewidth=0.8, linestyle=line_style)\n",
    "\n",
    "    ax.set_title(f\"{kind}\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds = [\"RNN\", \"LSTM\",\"GRU\"]\n",
    "min_lengths = [10, 10 ,20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15 ,20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31\n",
    "target_epoch = 2000\n",
    "\n",
    "# Create a single figure for the 2x5 grid\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "for i, (min_len, max_len) in enumerate(zip(min_lengths, max_lengths)):\n",
    "    ax = plt.subplot(5, 2, i + 1)  # Create subplots in a 2x5 grid\n",
    "\n",
    "    for kind in kinds:\n",
    "        model_avg_data = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            #filename = f'./data/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                padded_data = pad_data(data['accuracy'].tolist(), target_epoch)\n",
    "                model_avg_data.append(padded_data)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if model_avg_data:\n",
    "            # Calculate the average for this model kind\n",
    "            avg_data = np.mean(np.array(model_avg_data), axis=0)\n",
    "            ax.plot(avg_data, label=f\"{kind}\")\n",
    "\n",
    "    ax.set_title(f\"Min {min_len}, Max {max_len}\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds = [\"RNNA\", \"LSTMA\",  \"GRUA\"]\n",
    "min_lengths = [10, 10 ,20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15 ,20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31\n",
    "target_epoch = 2000\n",
    "\n",
    "# Create a single figure for the 2x5 grid\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "for i, (min_len, max_len) in enumerate(zip(min_lengths, max_lengths)):\n",
    "    ax = plt.subplot(5, 2, i + 1)  # Create subplots in a 2x5 grid\n",
    "\n",
    "    for kind in kinds:\n",
    "        model_avg_data = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            #filename = f'./data/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                padded_data = pad_data(data['accuracy'].tolist(), target_epoch)\n",
    "                model_avg_data.append(padded_data)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if model_avg_data:\n",
    "            # Calculate the average for this model kind\n",
    "            avg_data = np.mean(np.array(model_avg_data), axis=0)\n",
    "            ax.plot(avg_data, label=f\"{kind}\")\n",
    "\n",
    "    ax.set_title(f\"Min {min_len}, Max {max_len}\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds = [\"RNNE\", \"LSTME\", \"GRUE\"]\n",
    "min_lengths = [10, 10 ,20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15 ,20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31\n",
    "target_epoch = 2000\n",
    "\n",
    "# Create a single figure for the 2x5 grid\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "for i, (min_len, max_len) in enumerate(zip(min_lengths, max_lengths)):\n",
    "    ax = plt.subplot(5, 2, i + 1)  # Create subplots in a 2x5 grid\n",
    "\n",
    "    for kind in kinds:\n",
    "        model_avg_data = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            filename = f'./data/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                padded_data = pad_data(data['accuracy'].tolist(), target_epoch)\n",
    "                model_avg_data.append(padded_data)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if model_avg_data:\n",
    "            # Calculate the average for this model kind\n",
    "            avg_data = np.mean(np.array(model_avg_data), axis=0)\n",
    "            ax.plot(avg_data, label=f\"{kind}\")\n",
    "\n",
    "    ax.set_title(f\"Min {min_len}, Max {max_len}\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kinds = [\"RNN\", \"RNNA\", \"RNNE\", \"LSTM\", \"LSTMA\", \"LSTME\", \"GRU\", \"GRUA\", \"GRUE\"]\n",
    "kinds = [\"RNN\", \"RNNA\", \"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]\n",
    "min_lengths = [10, 10 ,20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15 ,20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31\n",
    "target_epoch = 2000\n",
    "\n",
    "# Create a single figure for the 2x5 grid\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "for i, (min_len, max_len) in enumerate(zip(min_lengths, max_lengths)):\n",
    "    ax = plt.subplot(5, 2, i + 1)  # Create subplots in a 2x5 grid\n",
    "\n",
    "    for kind in kinds:\n",
    "        model_avg_data = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            #filename = f'./data/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                padded_data = pad_data(data['accuracy'].tolist(), target_epoch)\n",
    "                model_avg_data.append(padded_data)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if model_avg_data:\n",
    "            # Calculate the average for this model kind\n",
    "            avg_data = np.mean(np.array(model_avg_data), axis=0)\n",
    "            ax.plot(avg_data, label=f\"{kind}\", linewidth= 0.8)\n",
    "\n",
    "    ax.set_title(f\"Min {min_len}, Max {max_len}\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_groups = {\n",
    "#    \"RNN\": [\"RNN\", \"RNNA\", \"RNNE\"],\n",
    "#    \"LSTM\": [\"LSTM\", \"LSTMA\", \"LSTME\"],\n",
    "#    \"GRU\": [\"GRU\", \"GRUA\", \"GRUE\"]\n",
    "#}\n",
    "\n",
    "model_groups = {\n",
    "    \"RNN\": [\"RNN\", \"RNNA\"],\n",
    "    \"LSTM\": [\"LSTM\", \"LSTMA\"],\n",
    "    \"GRU\": [\"GRU\", \"GRUA\"]\n",
    "}\n",
    "\n",
    "min_lengths = [10, 10 ,20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15 ,20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31\n",
    "target_epoch = 2000\n",
    "\n",
    "# Plotting for each model group\n",
    "for group_name, kinds in model_groups.items():\n",
    "    plt.figure(figsize=(10, 20))  # Create a single figure for the 2x5 grid\n",
    "\n",
    "    for i, (min_len, max_len) in enumerate(zip(min_lengths, max_lengths)):\n",
    "        ax = plt.subplot(5, 2, i + 1)  # Create subplots in a 2x5 grid\n",
    "\n",
    "        for kind in kinds:\n",
    "            kind_avg_data = []\n",
    "\n",
    "            for rep in range(num_reps):\n",
    "                #filename = f'./data/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "                filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "                try:\n",
    "                    data = pd.read_csv(filename)\n",
    "                    data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                    padded_data = pad_data(data['accuracy'].tolist(), target_epoch)\n",
    "                    kind_avg_data.append(padded_data)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"File not found: {filename}\")\n",
    "                    continue\n",
    "\n",
    "            if kind_avg_data:\n",
    "                # Calculate the average for this kind and length combination\n",
    "                avg_data = np.mean(np.array(kind_avg_data), axis=0)\n",
    "                ax.plot(avg_data, label=f\"{kind}\")\n",
    "\n",
    "        ax.set_title(f\"Min {min_len}, Max {max_len}\")\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.suptitle(f\"Average Accuracy for {group_name} Models\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the first epoch where accuracy reaches 80%\n",
    "def find_epoch_at_80_percent(accuracy_list):  \n",
    "    for i, acc in enumerate(accuracy_list):\n",
    "        if acc >= 0.80:\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds = [\"RNN\", \"RNNA\", \"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]\n",
    "min_lengths = [10, 10 ,20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15 ,20, 25, 30, 35, 40, 45, 50, 55]\n",
    "\n",
    "# Placeholder for the results\n",
    "results = []\n",
    "labels = []\n",
    "\n",
    "for kind in kinds:\n",
    "    for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "        epochs_to_80_percent = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                epoch = find_epoch_at_80_percent(data['accuracy'].tolist())\n",
    "                if epoch is not None:\n",
    "                    epochs_to_80_percent.append(epoch)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if epochs_to_80_percent:\n",
    "            avg_epoch = np.mean(epochs_to_80_percent)\n",
    "            results.append((kind, min_len, max_len, avg_epoch))\n",
    "            labels.append(f\"Min {min_len}, Max {max_len}\")\n",
    "\n",
    "\n",
    "# Preparing data for bar plot\n",
    "df = pd.DataFrame(results, columns=[\"Kind\", \"Min_Length\", \"Max_Length\", \"Avg_Epoch\"])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, kind in enumerate(kinds):\n",
    "    subset = df[df['Kind'] == kind]\n",
    "    x = range(len(subset))\n",
    "    plt.bar(x, subset['Avg_Epoch'], tick_label=labels[len(x)*i:len(x)*(i+1)],label=f\"{kind}\")\n",
    "\n",
    "plt.xlabel(\"Combination Index\")\n",
    "plt.ylabel(\"Average Epoch at 80% Accuracy\")\n",
    "plt.title(\"Average Epoch at 80% Accuracy for Each Configuration\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "kinds = [\"RNN\", \"RNNA\", \"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]\n",
    "min_lengths = [10 ,20, 30, 40, 50]\n",
    "max_lengths = [10 ,20, 30, 40, 50]\n",
    "num_reps = 31\n",
    "\n",
    "# Placeholder for the results\n",
    "results = []\n",
    "\n",
    "# Gathering data for the bar plot\n",
    "for kind in kinds:\n",
    "    kind_results = []\n",
    "\n",
    "    for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "        epochs_to_80_percent = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                epoch = find_epoch_at_80_percent(data['accuracy'].tolist())\n",
    "                if epoch is not None:\n",
    "                    epochs_to_80_percent.append(epoch)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if epochs_to_80_percent:\n",
    "            avg_epoch = np.mean(epochs_to_80_percent)\n",
    "            kind_results.append(avg_epoch)\n",
    "        else:\n",
    "            kind_results.append(None)\n",
    "\n",
    "    results.append(kind_results)\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(15, 10))  # Adjust the figure size as needed\n",
    "\n",
    "# Create a subplot for each model kind\n",
    "for i, kind in enumerate(kinds):\n",
    "    ax = plt.subplot(3, 2, i + 1)  # 3x2 grid for 6 kinds\n",
    "    bar_positions = np.arange(len(min_lengths))  # One bar for each min/max length combination\n",
    "\n",
    "    # Plot the bars\n",
    "    ax.bar(bar_positions, results[i], width=0.4)\n",
    "\n",
    "    # Set up the plot\n",
    "    ax.set_title(kind)\n",
    "    ax.set_xticks(bar_positions)\n",
    "    ax.set_xticklabels([f\"({min_len},{max_len})\" for min_len, max_len in zip(min_lengths, max_lengths)], rotation=45)\n",
    "    ax.set_xlabel(\"Min/Max Length\")\n",
    "    ax.set_ylabel(\"Average Epoch at 80% Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "kinds = [\"RNN\", \"RNNA\", \"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]\n",
    "min_lengths = [10 ,20, 30, 40, 50]\n",
    "max_lengths = [15 ,25, 35, 45, 55]\n",
    "num_reps = 31\n",
    "\n",
    "# Placeholder for the results\n",
    "results = []\n",
    "\n",
    "# Gathering data for the bar plot\n",
    "for kind in kinds:\n",
    "    kind_results = []\n",
    "\n",
    "    for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "        epochs_to_80_percent = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                epoch = find_epoch_at_80_percent(data['accuracy'].tolist())\n",
    "                if epoch is not None:\n",
    "                    epochs_to_80_percent.append(epoch)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if epochs_to_80_percent:\n",
    "            avg_epoch = np.mean(epochs_to_80_percent)\n",
    "            kind_results.append(avg_epoch)\n",
    "        else:\n",
    "            kind_results.append(None)\n",
    "\n",
    "    results.append(kind_results)\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(15, 10))  # Adjust the figure size as needed\n",
    "\n",
    "# Create a subplot for each model kind\n",
    "for i, kind in enumerate(kinds):\n",
    "    ax = plt.subplot(3, 2, i + 1)  # 3x2 grid for 6 kinds\n",
    "    bar_positions = np.arange(len(min_lengths))  # One bar for each min/max length combination\n",
    "\n",
    "    # Plot the bars\n",
    "    ax.bar(bar_positions, results[i], width=0.4)\n",
    "\n",
    "    # Set up the plot\n",
    "    ax.set_title(kind)\n",
    "    ax.set_xticks(bar_positions)\n",
    "    ax.set_xticklabels([f\"({min_len},{max_len})\" for min_len, max_len in zip(min_lengths, max_lengths)], rotation=45)\n",
    "    ax.set_xlabel(\"Min/Max Length\")\n",
    "    ax.set_ylabel(\"Average Epoch at 80% Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "kinds = [\"RNN\", \"RNNA\", \"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]\n",
    "min_lengths = [10, 10, 20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31\n",
    "\n",
    "# Placeholder for the results\n",
    "results = []\n",
    "\n",
    "# Gathering data for the bar plot\n",
    "for kind in kinds:\n",
    "    kind_results = []\n",
    "\n",
    "    for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "        epochs_to_80_percent = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                epoch = find_epoch_at_80_percent(data['accuracy'].tolist())\n",
    "                if epoch is not None:\n",
    "                    epochs_to_80_percent.append(epoch)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if epochs_to_80_percent:\n",
    "            avg_epoch = np.mean(epochs_to_80_percent)\n",
    "            kind_results.append(avg_epoch)\n",
    "        else:\n",
    "            kind_results.append(None)\n",
    "\n",
    "    results.append(kind_results)\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(15, 10))  # Adjust the figure size as needed\n",
    "\n",
    "# Create a subplot for each model kind\n",
    "for i, kind in enumerate(kinds):\n",
    "    ax = plt.subplot(3, 2, i + 1)  # 3x2 grid for 6 kinds\n",
    "    bar_positions = np.arange(len(min_lengths))  # One bar for each min/max length combination\n",
    "\n",
    "    # Plot the bars\n",
    "    ax.bar(bar_positions, results[i], width=0.4)\n",
    "\n",
    "    # Set up the plot\n",
    "    ax.set_title(kind)\n",
    "    ax.set_xticks(bar_positions)\n",
    "    ax.set_xticklabels([f\"({min_len},{max_len})\" for min_len, max_len in zip(min_lengths, max_lengths)], rotation=45)\n",
    "    ax.set_xlabel(\"Min/Max Length\")\n",
    "    ax.set_ylabel(\"Average Epoch at 80% Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "kinds = [\"RNN\", \"RNNA\", \"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]\n",
    "min_lengths = [10, 10, 20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31\n",
    "\n",
    "# Placeholder for the results\n",
    "results = {kind: [] for kind in kinds}\n",
    "\n",
    "# Gathering data for the bar plot\n",
    "for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "    for kind in kinds:\n",
    "        epochs_to_80_percent = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                epoch = find_epoch_at_80_percent(data['accuracy'].tolist())\n",
    "                if epoch is not None:\n",
    "                    epochs_to_80_percent.append(epoch)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if epochs_to_80_percent:\n",
    "            avg_epoch = np.mean(epochs_to_80_percent)\n",
    "            results[kind].append(avg_epoch)\n",
    "        else:\n",
    "            results[kind].append(None)\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(10, 20))  # Adjust the figure size as needed\n",
    "\n",
    "# Create a subplot for each min/max length configuration\n",
    "for i, (min_len, max_len) in enumerate(zip(min_lengths, max_lengths)):\n",
    "    ax = plt.subplot(5, 2, i + 1)  # 5x2 grid for 10 configurations\n",
    "    bar_positions = np.arange(len(kinds))  # One bar for each model kind\n",
    "\n",
    "    # Plot the bars\n",
    "    for j, kind in enumerate(kinds):\n",
    "        ax.bar(bar_positions[j], results[kind][i], width=0.4, label=kind)\n",
    "\n",
    "    # Set up the plot\n",
    "    ax.set_title(f\"Min {min_len}, Max {max_len}\")\n",
    "    ax.set_xticks(bar_positions)\n",
    "    ax.set_xticklabels(kinds, rotation=45)\n",
    "    ax.set_xlabel(\"Model Type\")\n",
    "    ax.set_ylabel(\"Average Epoch at 80% Accuracy\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "kinds = [\"RNN\", \"LSTM\", \"GRU\", ]\n",
    "min_lengths = [10, 10, 20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31\n",
    "\n",
    "# Placeholder for the results\n",
    "results = {kind: [] for kind in kinds}\n",
    "\n",
    "# Gathering data for the bar plot\n",
    "for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "    for kind in kinds:\n",
    "        epochs_to_80_percent = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                epoch = find_epoch_at_80_percent(data['accuracy'].tolist())\n",
    "                if epoch is not None:\n",
    "                    epochs_to_80_percent.append(epoch)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if epochs_to_80_percent:\n",
    "            avg_epoch = np.mean(epochs_to_80_percent)\n",
    "            results[kind].append(avg_epoch)\n",
    "        else:\n",
    "            results[kind].append(None)\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(10, 20))  # Adjust the figure size as needed\n",
    "\n",
    "# Create a subplot for each min/max length configuration\n",
    "for i, (min_len, max_len) in enumerate(zip(min_lengths, max_lengths)):\n",
    "    ax = plt.subplot(5, 2, i + 1)  # 5x2 grid for 10 configurations\n",
    "    bar_positions = np.arange(len(kinds))  # One bar for each model kind\n",
    "\n",
    "    # Plot the bars\n",
    "    for j, kind in enumerate(kinds):\n",
    "        ax.bar(bar_positions[j], results[kind][i], width=0.4, label=kind)\n",
    "\n",
    "    # Set up the plot\n",
    "    ax.set_title(f\"Min {min_len}, Max {max_len}\")\n",
    "    ax.set_xticks(bar_positions)\n",
    "    ax.set_xticklabels(kinds, rotation=45)\n",
    "    ax.set_xlabel(\"Model Type\")\n",
    "    ax.set_ylabel(\"Average Epoch at 80% Accuracy\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "kinds = [\"RNNA\", \"LSTMA\", \"GRUA\"]\n",
    "min_lengths = [10, 10, 20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31\n",
    "\n",
    "# Placeholder for the results\n",
    "results = {kind: [] for kind in kinds}\n",
    "\n",
    "# Gathering data for the bar plot\n",
    "for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "    for kind in kinds:\n",
    "        epochs_to_80_percent = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "            try:\n",
    "                data = pd.read_csv(filename)\n",
    "                data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                epoch = find_epoch_at_80_percent(data['accuracy'].tolist())\n",
    "                if epoch is not None:\n",
    "                    epochs_to_80_percent.append(epoch)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "        if epochs_to_80_percent:\n",
    "            avg_epoch = np.mean(epochs_to_80_percent)\n",
    "            results[kind].append(avg_epoch)\n",
    "        else:\n",
    "            results[kind].append(None)\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(10, 20))  # Adjust the figure size as needed\n",
    "\n",
    "# Create a subplot for each min/max length configuration\n",
    "for i, (min_len, max_len) in enumerate(zip(min_lengths, max_lengths)):\n",
    "    ax = plt.subplot(5, 2, i + 1)  # 5x2 grid for 10 configurations\n",
    "    bar_positions = np.arange(len(kinds))  # One bar for each model kind\n",
    "\n",
    "    # Plot the bars\n",
    "    for j, kind in enumerate(kinds):\n",
    "        ax.bar(bar_positions[j], results[kind][i], width=0.4, label=kind)\n",
    "\n",
    "    # Set up the plot\n",
    "    ax.set_title(f\"Min {min_len}, Max {max_len}\")\n",
    "    ax.set_xticks(bar_positions)\n",
    "    ax.set_xticklabels(kinds, rotation=45)\n",
    "    ax.set_xlabel(\"Model Type\")\n",
    "    ax.set_ylabel(\"Average Epoch at 80% Accuracy\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model groups\n",
    "model_groups = {\n",
    "    \"RNN\": [\"RNN\", \"RNNA\"],\n",
    "    \"LSTM\": [\"LSTM\", \"LSTMA\"],\n",
    "    \"GRU\": [\"GRU\", \"GRUA\"]\n",
    "}\n",
    "\n",
    "min_lengths = [10 ,20, 30, 40, 50]\n",
    "max_lengths = [10 ,20, 30, 40, 50]\n",
    "num_reps = 31\n",
    "\n",
    "# Placeholder for the results\n",
    "results = {group: [] for group in model_groups}\n",
    "\n",
    "# Gathering data for the bar plot\n",
    "for group_name, kinds in model_groups.items():\n",
    "    group_results = []\n",
    "\n",
    "    for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "        kind_avg_epochs = []\n",
    "\n",
    "        for kind in kinds:\n",
    "            epochs_to_80_percent = []\n",
    "\n",
    "            for rep in range(num_reps):\n",
    "                filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "                try:\n",
    "                    data = pd.read_csv(filename)\n",
    "                    data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                    epoch = find_epoch_at_80_percent(data['accuracy'].tolist())\n",
    "                    if epoch is not None:\n",
    "                        epochs_to_80_percent.append(epoch)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"File not found: {filename}\")\n",
    "                    continue\n",
    "\n",
    "            if epochs_to_80_percent:\n",
    "                avg_epoch = np.mean(epochs_to_80_percent)\n",
    "                kind_avg_epochs.append(avg_epoch)\n",
    "            else:\n",
    "                kind_avg_epochs.append(None)\n",
    "\n",
    "        group_results.append(kind_avg_epochs)\n",
    "    results[group_name] = group_results\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(15, 10))  # Adjust the figure size as needed\n",
    "\n",
    "# Create a subplot for each model group\n",
    "for i, (group_name, group_kinds) in enumerate(model_groups.items()):\n",
    "    ax = plt.subplot(len(model_groups), 1, i + 1)  # One row for each group\n",
    "    bar_positions = np.arange(len(min_lengths))  # One set of bars for each min/max length combination\n",
    "\n",
    "    # Plot the bars for each kind in the group\n",
    "    bar_width = 0.15\n",
    "    for j, kind in enumerate(group_kinds):\n",
    "        kind_results = [result[j] for result in results[group_name]]  # Get results for this kind\n",
    "        ax.bar(bar_positions + j * bar_width, kind_results, width=bar_width, label=kind)\n",
    "\n",
    "    # Set up the plot\n",
    "    ax.set_title(f\"{group_name} Group\")\n",
    "    ax.set_xticks(bar_positions + bar_width * (len(group_kinds) - 1) / 2)\n",
    "    ax.set_xticklabels([f\"({min_len},{max_len})\" for min_len, max_len in zip(min_lengths, max_lengths)], rotation=45)\n",
    "    ax.set_xlabel(\"Min/Max Length\")\n",
    "    ax.set_ylabel(\"Average Epoch at 80% Accuracy\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model groups\n",
    "model_groups = {\n",
    "    \"RNN\": [\"RNN\", \"RNNA\"],\n",
    "    \"LSTM\": [\"LSTM\", \"LSTMA\"],\n",
    "    \"GRU\": [\"GRU\", \"GRUA\"]\n",
    "}\n",
    "\n",
    "min_lengths = [10 ,20, 30, 40, 50]\n",
    "max_lengths = [15 ,25, 35, 45, 55]\n",
    "num_reps = 31\n",
    "\n",
    "# Placeholder for the results\n",
    "results = {group: [] for group in model_groups}\n",
    "\n",
    "# Gathering data for the bar plot\n",
    "for group_name, kinds in model_groups.items():\n",
    "    group_results = []\n",
    "\n",
    "    for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "        kind_avg_epochs = []\n",
    "\n",
    "        for kind in kinds:\n",
    "            epochs_to_80_percent = []\n",
    "\n",
    "            for rep in range(num_reps):\n",
    "                filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "                try:\n",
    "                    data = pd.read_csv(filename)\n",
    "                    data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                    epoch = find_epoch_at_80_percent(data['accuracy'].tolist())\n",
    "                    if epoch is not None:\n",
    "                        epochs_to_80_percent.append(epoch)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"File not found: {filename}\")\n",
    "                    continue\n",
    "\n",
    "            if epochs_to_80_percent:\n",
    "                avg_epoch = np.mean(epochs_to_80_percent)\n",
    "                kind_avg_epochs.append(avg_epoch)\n",
    "            else:\n",
    "                kind_avg_epochs.append(None)\n",
    "\n",
    "        group_results.append(kind_avg_epochs)\n",
    "    results[group_name] = group_results\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(15, 10))  # Adjust the figure size as needed\n",
    "\n",
    "# Create a subplot for each model group\n",
    "for i, (group_name, group_kinds) in enumerate(model_groups.items()):\n",
    "    ax = plt.subplot(len(model_groups), 1, i + 1)  # One row for each group\n",
    "    bar_positions = np.arange(len(min_lengths))  # One set of bars for each min/max length combination\n",
    "\n",
    "    # Plot the bars for each kind in the group\n",
    "    bar_width = 0.15\n",
    "    for j, kind in enumerate(group_kinds):\n",
    "        kind_results = [result[j] for result in results[group_name]]  # Get results for this kind\n",
    "        ax.bar(bar_positions + j * bar_width, kind_results, width=bar_width, label=kind)\n",
    "\n",
    "    # Set up the plot\n",
    "    ax.set_title(f\"{group_name} Group\")\n",
    "    ax.set_xticks(bar_positions + bar_width * (len(group_kinds) - 1) / 2)\n",
    "    ax.set_xticklabels([f\"({min_len},{max_len})\" for min_len, max_len in zip(min_lengths, max_lengths)], rotation=45)\n",
    "    ax.set_xlabel(\"Min/Max Length\")\n",
    "    ax.set_ylabel(\"Average Epoch at 80% Accuracy\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model groups\n",
    "model_groups = {\n",
    "    \"RNN\": [\"RNN\", \"RNNA\"],\n",
    "    \"LSTM\": [\"LSTM\", \"LSTMA\"],\n",
    "    \"GRU\": [\"GRU\", \"GRUA\"]\n",
    "}\n",
    "\n",
    "min_lengths = [10, 10, 20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31\n",
    "\n",
    "# Placeholder for the results\n",
    "results = {group: [] for group in model_groups}\n",
    "\n",
    "# Gathering data for the bar plot\n",
    "for group_name, kinds in model_groups.items():\n",
    "    group_results = []\n",
    "\n",
    "    for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "        kind_avg_epochs = []\n",
    "\n",
    "        for kind in kinds:\n",
    "            epochs_to_80_percent = []\n",
    "\n",
    "            for rep in range(num_reps):\n",
    "                filename = f'./dataScore/score_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.csv'\n",
    "                try:\n",
    "                    data = pd.read_csv(filename)\n",
    "                    data['accuracy'] = data['accuracy'].apply(tensor_str_to_float)\n",
    "                    epoch = find_epoch_at_80_percent(data['accuracy'].tolist())\n",
    "                    if epoch is not None:\n",
    "                        epochs_to_80_percent.append(epoch)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"File not found: {filename}\")\n",
    "                    continue\n",
    "\n",
    "            if epochs_to_80_percent:\n",
    "                avg_epoch = np.mean(epochs_to_80_percent)\n",
    "                kind_avg_epochs.append(avg_epoch)\n",
    "            else:\n",
    "                kind_avg_epochs.append(None)\n",
    "\n",
    "        group_results.append(kind_avg_epochs)\n",
    "    results[group_name] = group_results\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(15, 10))  # Adjust the figure size as needed\n",
    "\n",
    "# Create a subplot for each model group\n",
    "for i, (group_name, group_kinds) in enumerate(model_groups.items()):\n",
    "    ax = plt.subplot(len(model_groups), 1, i + 1)  # One row for each group\n",
    "    bar_positions = np.arange(len(min_lengths))  # One set of bars for each min/max length combination\n",
    "\n",
    "    # Plot the bars for each kind in the group\n",
    "    bar_width = 0.15\n",
    "    for j, kind in enumerate(group_kinds):\n",
    "        kind_results = [result[j] for result in results[group_name]]  # Get results for this kind\n",
    "        ax.bar(bar_positions + j * bar_width, kind_results, width=bar_width, label=kind)\n",
    "\n",
    "    # Set up the plot\n",
    "    ax.set_title(f\"{group_name} Group\")\n",
    "    ax.set_xticks(bar_positions + bar_width * (len(group_kinds) - 1) / 2)\n",
    "    ax.set_xticklabels([f\"({min_len},{max_len})\" for min_len, max_len in zip(min_lengths, max_lengths)], rotation=45)\n",
    "    ax.set_xlabel(\"Min/Max Length\")\n",
    "    ax.set_ylabel(\"Average Epoch at 80% Accuracy\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:50:29.630985800Z",
     "start_time": "2023-12-06T17:50:29.440984500Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function calculates the entropy of a dataset D using the Shannon entropy formula. Entropy is a measure of the unpredictability or randomness of the data.\n",
    "def symbolEntropy(D, base=2):\n",
    "    value, counts = numpy.unique(D, return_counts=True)  # Find unique values and their frequency in D\n",
    "    return entropy(counts, base=base)  # Calculate and return the Shannon entropy\n",
    "\n",
    "# This function seems to compute a complex metric, possibly related to information theory, involving inputs I, hidden states H, and outputs O.\n",
    "def computeTransmissionHfast(I,H,O,maskC,maskNC,iMult=2,oMult=2):\n",
    "    # Create various combinations of Inputs (I), Hidden states (H), and Outputs (O) for analysis\n",
    "    # B, IB, AB, BO, IAB, IBO, ABO, IABO are different combinations of I, H, and O\n",
    "    # Calculate entropy for each combination using symbolEntropy\n",
    "    # Return a specific information metric combining these entropies\n",
    "\n",
    "\n",
    "    #print(\"I H O\",I.shape,H.shape,O.shape)\n",
    "    B=numpy.bitwise_and(H,maskNC)\n",
    "    IB=(B*iMult)+I\n",
    "    AB=H#numpy.bitwise_and(H,maskC+maskNC)\n",
    "    BO=(B*oMult)+O\n",
    "    IAB=(AB*iMult)+I\n",
    "    IBO=(B*(iMult*oMult))+(I*oMult)+O\n",
    "    ABO=(AB*oMult)+O\n",
    "    IABO=(AB*(iMult*oMult))+(I*oMult)+O\n",
    "    hB=symbolEntropy(B, base=2)\n",
    "    hIB=symbolEntropy(IB, base=2)\n",
    "    hAB=symbolEntropy(AB, base=2)\n",
    "    hBO=symbolEntropy(BO, base=2)\n",
    "    hIAB=symbolEntropy(IAB, base=2)\n",
    "    hIBO=symbolEntropy(IBO, base=2)\n",
    "    hABO=symbolEntropy(ABO, base=2)\n",
    "    hIABO=symbolEntropy(IABO, base=2)\n",
    "    #-H(B)+H(IB)+H(AB)+H(BO)-H(IAB)-H(IBO)-H(ABO)+H(IABO)\n",
    "    #print(hB,hIB,hAB,hBO,hIAB,hIBO,hABO,hIABO)\n",
    "    return-hB+hIB+hAB+hBO-hIAB-hIBO-hABO+hIABO\n",
    "\n",
    "# This function seems to analyze how information is processed or transmitted through a network, focusing on individual components (nodes) of the hidden states H.\n",
    "def singleShrinkingDecompositionInformation(I,H,O,width,iMult=2,oMult=2):\n",
    "    nodes = list(range(width))  # Initialize a list of node indices\n",
    "    cols = []  # To store subsets of nodes\n",
    "    colh = []  # To store corresponding information values\n",
    "    # Iteratively remove a node, compute information value, and record subsets and values\n",
    "    while len(nodes)>0:\n",
    "        infos=[]\n",
    "        for node in nodes:\n",
    "            subset=copy.deepcopy(nodes)\n",
    "            subset.remove(node)\n",
    "            maskA=0\n",
    "            for s in subset:\n",
    "                maskA+=1*(2**s)\n",
    "            maskA=int(maskA)\n",
    "            maskB=numpy.bitwise_and(numpy.bitwise_not(maskA),((2**width)-1))\n",
    "            h=computeTransmissionHfast(I,H,O,maskA,maskB,iMult=iMult,oMult=oMult)\n",
    "            infos.append(h)\n",
    "        nodeToDrop=nodes[infos.index(max(infos))]\n",
    "        nodes.remove(nodeToDrop)\n",
    "        cols.append(copy.deepcopy(nodes))\n",
    "        colh.append(max(infos))\n",
    "    return cols,colh\n",
    "\n",
    "\n",
    "# This function processes the output of a model given a dataset. It appears to involve some form of clustering (using KMeans) and then re-encoding the hidden states H.\n",
    "def getOutTaH(model,dataSet):\n",
    "    O, H = model.step(torch.Tensor(dataSet))  # Get output and hidden states from the model\n",
    "    # Transform H using clustering and re-encoding\n",
    "\n",
    "    #print(H.shape,H.min(),H.max())\n",
    "    #figure()\n",
    "    #hist(H.flatten())\n",
    "\n",
    "    H = H.transpose()  # Transpose H for processing\n",
    "    O = O.transpose()  # Transpose O for processing\n",
    "    B = numpy.zeros(H.shape)  # Initialize a matrix to store cluster labels for each hidden state\n",
    "    # Apply KMeans clustering to each hidden state\n",
    "    # Recompute H as a combination of cluster labels (B)\n",
    "    clusterNr=2\n",
    "    for i in range(B.shape[0]):\n",
    "        a=H[i].reshape(-1, 1)\n",
    "        x =len(numpy.unique(a))\n",
    "        if len(numpy.unique(a))==1:\n",
    "            who=numpy.random.randint(len(a))\n",
    "            a[who]=1-a[who]\n",
    "        kmeans = KMeans(n_clusters=clusterNr,n_init=10).fit(a)\n",
    "        B[i]=kmeans.labels_\n",
    "        #B[i]=1.0*(H[i]>numpy.median(H[i]))\n",
    "\n",
    "\n",
    "    H=numpy.zeros((H.shape))\n",
    "    for i in range(12):\n",
    "        H+=B[i]*(clusterNr**i)\n",
    "    H=H.astype((int))\n",
    "    return O,H\n",
    "\n",
    "# This function seems to integrate the previous functions to analyze how information flows through the network for different input-output pairs in a dataset.\n",
    "def shrinkingDecompositionInformation(model,width,dataSet,target,numbers=[0,1,2],whichTS=5,dsLength=8):\n",
    "    output, H = getOutTaH(model, dataSet)  # Get transformed outputs and hidden states from the model\n",
    "    # Slice output and H to process only specific timesteps\n",
    "    output=output.transpose()[whichTS::dsLength].transpose()\n",
    "\n",
    "    #print(\"target.shape\",target.shape,\"output.shape\",output.shape,\"H.shape\",H.shape,\"dataset.shape\",dataSet.shape)\n",
    "    H=H.transpose()[whichTS::dsLength].transpose()\n",
    "    #target=target.transpose()[whichTS::dsLength].transpose()\n",
    "    #print(H.shape,target.shape,numpy.array(range(512))[whichTS::dsLength])\n",
    "\n",
    "    collectorSet = dict()  # To store information about subsets of nodes (S)\n",
    "    collectorH = dict()    # To store information values (H)\n",
    "    # Compute shrinking decomposition information for selected inputs/outputs\n",
    "    for number in numbers:\n",
    "        I=target[number].astype(int)\n",
    "        O=(1.0*(output[number]>0.5)).astype(int)\n",
    "        #print(\"O\",O,\"T\",target[number])\n",
    "        #print(number,\"I.shape\",I.shape,\"O.shape\",O.shape,\"H.shape\",H.shape)\n",
    "        s,h=singleShrinkingDecompositionInformation(I,H,O,width)\n",
    "        collectorSet[number]=s\n",
    "        collectorH[number]=h\n",
    "    return collectorSet,collectorH\n",
    "\n",
    "\n",
    "# These functions convert the results of the shrinking decomposition into vector and matrix forms, which are likely used for further analysis or visualization.\n",
    "def removalIntoVec(res,width,H):\n",
    "    # Convert the shrinking decomposition results into a vector form\n",
    "\n",
    "    V = numpy.zeros(width)  # Initialize a vector\n",
    "    # Calculate values for V based on the difference in information values (H) as nodes are removed\n",
    "    #for i,r in enumerate(res):\n",
    "    #    for e in r:\n",
    "    #        V[e]+=H[0]-H[i]\n",
    "    fullSet=list(range(width))\n",
    "    nRes=copy.deepcopy(res)\n",
    "    nRes.insert(0,fullSet)\n",
    "\n",
    "    nodeList=[]\n",
    "    for i in range(width):\n",
    "        removedNode=list(set(nRes[i])-set(nRes[i+1]))[0]\n",
    "        nodeList.append(removedNode)\n",
    "    \n",
    "    for i,node in enumerate(nodeList):\n",
    "        V[node]=H[0]-H[i]\n",
    "\n",
    "    #V=sqrt(V)\n",
    "    if V.sum()==0:\n",
    "        return V\n",
    "    return V#/V.max()\n",
    "\n",
    "def removalIntoMatrix(res,width,H):\n",
    "    # Convert the shrinking decomposition results into a matrix form\n",
    "\n",
    "    M=[]\n",
    "    # Convert the shrinking decomposition results (S and H) into a matrix form\n",
    "    # This matrix can be used for visualization or further analysis\n",
    "    for i in range(len(res)):\n",
    "        M.append(removalIntoVec(res[i],width,H[i]))\n",
    "    return numpy.array(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1 [[ 0.3554909   0.34230451  0.56502876  0.0361142   0.26736921  0.07466616\n",
      "   0.01104796  0.23492448  0.44361999  0.          0.53747989  0.53747989]\n",
      " [ 0.08375256  0.14115303  0.          0.44132113  0.46887001 -0.02216601\n",
      "   0.46887001  0.46887001  0.14115303  0.44132113 -0.02467967  0.38386651]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M2 [[ 0.09921413  0.02241519  0.07471761  0.04416248  0.07471761  0.07471761\n",
      "   0.01844343  0.07471761  0.         -0.00321789  0.09921413  0.07471761]\n",
      " [ 0.00108259  0.32560985  0.05835895  0.29541435  0.05835895  0.05835895\n",
      "   0.32560985  0.05835895  0.06548161  0.23166923  0.          0.05835895]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelRNN=torch.load(\"./dataModel/model_RNN_min_30_max_35_rep_16.model\")\n",
    "modelRNNWithAttention=torch.load(\"./dataModel/model_RNNA_min_30_max_35_rep_16.model\")\n",
    "s,t = generateTrainData(100, [30,30])  \n",
    "S,H=shrinkingDecompositionInformation(modelRNN,12,s,t.transpose(),numbers = [0,1],whichTS=29,dsLength=30)\n",
    "figure()\n",
    "M1 = removalIntoMatrix(S,12,H)\n",
    "print(f\"M1 {M1}\")\n",
    "#imshow(M1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(M1)\n",
    "plt.colorbar(label='Impact Scale')\n",
    "plt.title('Heatmap for Matrix RNN')\n",
    "plt.xlabel('Nodes')\n",
    "plt.ylabel('Metrics')\n",
    "plt.yticks([0, 1], ['0', '1'])  # Setting y-axis labels to 0 and 1\n",
    "plt.show()\n",
    "\n",
    "S,H=shrinkingDecompositionInformation(modelRNNWithAttention,12,s,t.transpose(),numbers = [0,1],whichTS=29,dsLength=30)\n",
    "figure()\n",
    "\n",
    "M2 = removalIntoMatrix(S,12,H)\n",
    "print(f\"M2 {M2}\")\n",
    "#imshow(M2)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(M2)\n",
    "plt.colorbar(label='Impact Scale')\n",
    "plt.title('Heatmap for Matrix RNNA')\n",
    "plt.xlabel('Nodes')\n",
    "plt.ylabel('Metrics')\n",
    "plt.yticks([0, 1], ['0', '1'])  # Setting y-axis labels to 0 and 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(matrix):\n",
    "    probabilities = matrix.flatten() \n",
    "    probabilities = probabilities[probabilities > 0]  # Remove zeros to avoid log(0)\n",
    "    probabilities = probabilities / probabilities.sum()\n",
    "    return -np.sum(probabilities * np.log2(probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.32263403351474934, 12, 0.22007693906513057, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kmeans_thresholding(matrix):\n",
    "    # Flatten the matrix\n",
    "    flat_matrix = matrix.flatten()\n",
    "\n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=2, n_init=10).fit(flat_matrix.reshape(-1, 1))\n",
    "\n",
    "    centroids = kmeans.cluster_centers_.flatten()\n",
    "    # Determine threshold as the mean of centroids or the higher centroid\n",
    "    threshold = centroids.mean() if len(centroids) > 1 else centroids[0]\n",
    "\n",
    "    # Count the number of nodes exceeding the threshold\n",
    "    count_exceeding = np.sum(flat_matrix > threshold)\n",
    "\n",
    "    return threshold, count_exceeding\n",
    "\n",
    "# Apply KMeans thresholding to M1 and M2\n",
    "threshold_M1, count_exceeding_M1 = kmeans_thresholding(M1)\n",
    "threshold_M2, count_exceeding_M2 = kmeans_thresholding(M2)\n",
    "\n",
    "(threshold_M1, count_exceeding_M1, threshold_M2, count_exceeding_M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_kmeans_thresholding_with_index(matrix, threshold):\n",
    "    # Flatten the matrix\n",
    "    flat_matrix = matrix.flatten()\n",
    "\n",
    "    # Generate indices for each point in the flattened matrix\n",
    "    indices = range(len(flat_matrix))\n",
    "\n",
    "    # Plot the flattened matrix values and the threshold line\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for index, value in zip(indices, flat_matrix):\n",
    "        plt.scatter(index, value, color='blue')\n",
    "        #plt.text(index, value, str(index) + \" \" + str(value), fontsize=9)\n",
    "        plt.text(index, value, \"  \" +str(index), fontsize=9)\n",
    "\n",
    "    plt.axhline(y=threshold, color='red', linestyle='-', label=f'Threshold: {threshold:.2f}')\n",
    "    plt.xlabel('Node Index')\n",
    "    plt.ylabel('Impact Value')\n",
    "    plt.title('Node Impact Values and K-Means Threshold with Indices')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Apply KMeans thresholding to M1 and M2\n",
    "threshold_M1, count_exceeding_M1 = kmeans_thresholding(M1)\n",
    "threshold_M2, count_exceeding_M2 = kmeans_thresholding(M2)\n",
    "\n",
    "# Plotting for M1 and M2 with indices\n",
    "plot_kmeans_thresholding_with_index(M1, threshold_M1)\n",
    "plot_kmeans_thresholding_with_index(M2, threshold_M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_binarize_matrix(matrix, threshold):\n",
    "    # Flatten the matrix\n",
    "    flat_matrix = matrix.flatten()\n",
    "\n",
    "    # Apply thresholding\n",
    "    binarized_matrix = np.where(flat_matrix >= threshold, 1, 0)\n",
    "\n",
    "    # Reshape to the original matrix shape\n",
    "    binarized_matrix = binarized_matrix.reshape(matrix.shape)\n",
    "\n",
    "    return binarized_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_counts(matrix):\n",
    "    transposed_matrix = matrix.T\n",
    "    count_11, count_00, count_mixed = 0, 0, 0\n",
    "    for row in transposed_matrix:\n",
    "        if np.array_equal(row, [1, 1]):\n",
    "            count_11 += 1\n",
    "        elif np.array_equal(row, [0, 0]):\n",
    "            count_00 += 1\n",
    "        else:\n",
    "            count_mixed += 1\n",
    "    return count_11, count_00, count_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kind</th>\n",
       "      <th>Min_Length</th>\n",
       "      <th>Max_Length</th>\n",
       "      <th>Repetition</th>\n",
       "      <th>Sum of Absolute Values</th>\n",
       "      <th>Mean Value</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Maximum Value</th>\n",
       "      <th>Minimum Value</th>\n",
       "      <th>Shannon Entropy</th>\n",
       "      <th>Kmeans Threshold</th>\n",
       "      <th>Kmeans Nodes Exceeding</th>\n",
       "      <th>Overlap with zeroes</th>\n",
       "      <th>Overlap no zeroes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>8.767316</td>\n",
       "      <td>0.358857</td>\n",
       "      <td>0.330482</td>\n",
       "      <td>0.975690</td>\n",
       "      <td>-0.025793</td>\n",
       "      <td>3.899539</td>\n",
       "      <td>0.382250</td>\n",
       "      <td>11</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RNN</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>8.574099</td>\n",
       "      <td>0.354171</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>0.996176</td>\n",
       "      <td>-0.036994</td>\n",
       "      <td>3.909338</td>\n",
       "      <td>0.484433</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RNN</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8.498660</td>\n",
       "      <td>0.341284</td>\n",
       "      <td>0.349999</td>\n",
       "      <td>0.995311</td>\n",
       "      <td>-0.078002</td>\n",
       "      <td>3.809002</td>\n",
       "      <td>0.480630</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RNN</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>9.905812</td>\n",
       "      <td>0.397720</td>\n",
       "      <td>0.379172</td>\n",
       "      <td>0.994794</td>\n",
       "      <td>-0.096422</td>\n",
       "      <td>3.887869</td>\n",
       "      <td>0.590244</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RNN</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>10.306280</td>\n",
       "      <td>0.428766</td>\n",
       "      <td>0.360074</td>\n",
       "      <td>0.996787</td>\n",
       "      <td>-0.007943</td>\n",
       "      <td>4.011255</td>\n",
       "      <td>0.572774</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>26</td>\n",
       "      <td>4.816011</td>\n",
       "      <td>0.195420</td>\n",
       "      <td>0.173078</td>\n",
       "      <td>0.429443</td>\n",
       "      <td>-0.026759</td>\n",
       "      <td>3.902166</td>\n",
       "      <td>0.195420</td>\n",
       "      <td>12</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>27</td>\n",
       "      <td>6.221845</td>\n",
       "      <td>0.253666</td>\n",
       "      <td>0.197034</td>\n",
       "      <td>0.507777</td>\n",
       "      <td>-0.037124</td>\n",
       "      <td>4.056197</td>\n",
       "      <td>0.283440</td>\n",
       "      <td>10</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>28</td>\n",
       "      <td>4.525908</td>\n",
       "      <td>0.187803</td>\n",
       "      <td>0.141851</td>\n",
       "      <td>0.386099</td>\n",
       "      <td>-0.005300</td>\n",
       "      <td>4.073716</td>\n",
       "      <td>0.198333</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>29</td>\n",
       "      <td>5.997770</td>\n",
       "      <td>0.249884</td>\n",
       "      <td>0.183649</td>\n",
       "      <td>0.513038</td>\n",
       "      <td>-0.000279</td>\n",
       "      <td>4.068657</td>\n",
       "      <td>0.236455</td>\n",
       "      <td>13</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>30</td>\n",
       "      <td>5.658958</td>\n",
       "      <td>0.235694</td>\n",
       "      <td>0.176972</td>\n",
       "      <td>0.473057</td>\n",
       "      <td>-0.001156</td>\n",
       "      <td>4.047674</td>\n",
       "      <td>0.222740</td>\n",
       "      <td>13</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1860 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Kind  Min_Length  Max_Length  Repetition  Sum of Absolute Values  \\\n",
       "0      RNN          10          10           0                8.767316   \n",
       "1      RNN          10          10           1                8.574099   \n",
       "2      RNN          10          10           2                8.498660   \n",
       "3      RNN          10          10           3                9.905812   \n",
       "4      RNN          10          10           4               10.306280   \n",
       "...    ...         ...         ...         ...                     ...   \n",
       "1855  GRUA          50          55          26                4.816011   \n",
       "1856  GRUA          50          55          27                6.221845   \n",
       "1857  GRUA          50          55          28                4.525908   \n",
       "1858  GRUA          50          55          29                5.997770   \n",
       "1859  GRUA          50          55          30                5.658958   \n",
       "\n",
       "      Mean Value  Standard Deviation  Maximum Value  Minimum Value  \\\n",
       "0       0.358857            0.330482       0.975690      -0.025793   \n",
       "1       0.354171            0.331000       0.996176      -0.036994   \n",
       "2       0.341284            0.349999       0.995311      -0.078002   \n",
       "3       0.397720            0.379172       0.994794      -0.096422   \n",
       "4       0.428766            0.360074       0.996787      -0.007943   \n",
       "...          ...                 ...            ...            ...   \n",
       "1855    0.195420            0.173078       0.429443      -0.026759   \n",
       "1856    0.253666            0.197034       0.507777      -0.037124   \n",
       "1857    0.187803            0.141851       0.386099      -0.005300   \n",
       "1858    0.249884            0.183649       0.513038      -0.000279   \n",
       "1859    0.235694            0.176972       0.473057      -0.001156   \n",
       "\n",
       "      Shannon Entropy  Kmeans Threshold  Kmeans Nodes Exceeding  \\\n",
       "0            3.899539          0.382250                      11   \n",
       "1            3.909338          0.484433                       7   \n",
       "2            3.809002          0.480630                       7   \n",
       "3            3.887869          0.590244                       6   \n",
       "4            4.011255          0.572774                       7   \n",
       "...               ...               ...                     ...   \n",
       "1855         3.902166          0.195420                      12   \n",
       "1856         4.056197          0.283440                      10   \n",
       "1857         4.073716          0.198333                      11   \n",
       "1858         4.068657          0.236455                      13   \n",
       "1859         4.047674          0.222740                      13   \n",
       "\n",
       "      Overlap with zeroes  Overlap no zeroes  \n",
       "0                0.090909           0.111111  \n",
       "1                0.000000           0.000000  \n",
       "2                0.000000           0.000000  \n",
       "3                0.000000           0.000000  \n",
       "4                0.000000           0.000000  \n",
       "...                   ...                ...  \n",
       "1855             0.200000           0.250000  \n",
       "1856             0.090909           0.125000  \n",
       "1857             0.000000           0.000000  \n",
       "1858             0.333333           0.428571  \n",
       "1859             0.333333           0.428571  \n",
       "\n",
       "[1860 rows x 14 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "# Constants\n",
    "kinds = [\"RNN\", \"RNNA\", \"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]  # Assuming you have these types\n",
    "min_lengths = [10, 10, 20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31  # Example repetitions, adjust as needed\n",
    "\n",
    "# Placeholder for the results\n",
    "results = []\n",
    "\n",
    "# Gathering data\n",
    "for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "    for kind in kinds:\n",
    "        metrics_list = []\n",
    "\n",
    "        for rep in range(num_reps):\n",
    "            filename = f\"./dataModel/model_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.model\"\n",
    "            try:\n",
    "                # Load the model\n",
    "                model = torch.load(filename)\n",
    "                s,t = generateTrainData(100, [min_len,min_len])  \n",
    "                S,H=shrinkingDecompositionInformation(modelRNN,12,s,t.transpose(),numbers = [0,1],whichTS=min_len -1,dsLength=min_len)\n",
    "                # Removal into matrix\n",
    "                M = removalIntoMatrix(S, 12, H)\n",
    "\n",
    "                # Calculate metrics\n",
    "                sum_abs = np.sum(np.abs(M))\n",
    "                mean = np.mean(M)\n",
    "                std = np.std(M)\n",
    "                entropy_s = shannon_entropy(M)\n",
    "                max_val = np.max(M)\n",
    "                min_val = np.min(M)\n",
    "                threshold, count_exceeding = kmeans_thresholding(M)\n",
    "                binarized_matrix = np.where(M.flatten() >= threshold, 1, 0).reshape(M.shape)\n",
    "                count_11, count_00, count_mixed = calculate_counts(binarized_matrix)\n",
    "                ratio_11_00_mixed = count_11 / (count_mixed + count_00) if (count_mixed + count_00) != 0 else 0\n",
    "                ratio_11_mixed = count_11 / count_mixed if count_mixed != 0 else 0\n",
    "\n",
    "                metrics = {\n",
    "                    \"Kind\": kind,\n",
    "                    \"Min_Length\": min_len,\n",
    "                    \"Max_Length\": max_len,\n",
    "                    \"Repetition\": rep,\n",
    "                    \"Sum of Absolute Values\": sum_abs,\n",
    "                    \"Mean Value\": mean,\n",
    "                    \"Standard Deviation\": std,\n",
    "                    \"Maximum Value\": max_val,\n",
    "                    \"Minimum Value\": min_val,\n",
    "                    \"Shannon Entropy\": entropy_s,\n",
    "                    \"Kmeans Threshold\": threshold,\n",
    "                    \"Kmeans Nodes Exceeding\": count_exceeding,\n",
    "                    \"Overlap with zeroes\": ratio_11_00_mixed,\n",
    "                    \"Overlap no zeroes\": ratio_11_mixed\n",
    "                }\n",
    "\n",
    "                # Append this dictionary to the results list\n",
    "                results.append(metrics)\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kind</th>\n",
       "      <th>Min_Length</th>\n",
       "      <th>Max_Length</th>\n",
       "      <th>Repetition</th>\n",
       "      <th>Sum of Absolute Values</th>\n",
       "      <th>Mean Value</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Maximum Value</th>\n",
       "      <th>Minimum Value</th>\n",
       "      <th>Shannon Entropy</th>\n",
       "      <th>Kmeans Threshold</th>\n",
       "      <th>Kmeans Nodes Exceeding</th>\n",
       "      <th>Overlap with zeroes</th>\n",
       "      <th>Overlap no zeroes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRU</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.091594</td>\n",
       "      <td>0.371292</td>\n",
       "      <td>0.342991</td>\n",
       "      <td>0.992472</td>\n",
       "      <td>-0.048738</td>\n",
       "      <td>3.906474</td>\n",
       "      <td>0.487057</td>\n",
       "      <td>7.967742</td>\n",
       "      <td>0.103687</td>\n",
       "      <td>0.151459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRU</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.035158</td>\n",
       "      <td>0.367133</td>\n",
       "      <td>0.341886</td>\n",
       "      <td>0.987537</td>\n",
       "      <td>-0.059372</td>\n",
       "      <td>3.909389</td>\n",
       "      <td>0.484614</td>\n",
       "      <td>7.903226</td>\n",
       "      <td>0.094009</td>\n",
       "      <td>0.143932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRU</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.955211</td>\n",
       "      <td>0.330968</td>\n",
       "      <td>0.255564</td>\n",
       "      <td>0.735313</td>\n",
       "      <td>-0.004345</td>\n",
       "      <td>4.047512</td>\n",
       "      <td>0.319518</td>\n",
       "      <td>12.612903</td>\n",
       "      <td>0.292543</td>\n",
       "      <td>0.381272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRU</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.130255</td>\n",
       "      <td>0.338326</td>\n",
       "      <td>0.259963</td>\n",
       "      <td>0.748367</td>\n",
       "      <td>-0.003968</td>\n",
       "      <td>4.053833</td>\n",
       "      <td>0.334176</td>\n",
       "      <td>12.225806</td>\n",
       "      <td>0.230987</td>\n",
       "      <td>0.290835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRU</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.826277</td>\n",
       "      <td>0.325425</td>\n",
       "      <td>0.233243</td>\n",
       "      <td>0.629419</td>\n",
       "      <td>-0.004886</td>\n",
       "      <td>4.086814</td>\n",
       "      <td>0.316342</td>\n",
       "      <td>12.516129</td>\n",
       "      <td>0.333026</td>\n",
       "      <td>0.495993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GRU</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.796460</td>\n",
       "      <td>0.324450</td>\n",
       "      <td>0.233778</td>\n",
       "      <td>0.636747</td>\n",
       "      <td>-0.003413</td>\n",
       "      <td>4.079067</td>\n",
       "      <td>0.312201</td>\n",
       "      <td>12.677419</td>\n",
       "      <td>0.316702</td>\n",
       "      <td>0.434677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GRU</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.009789</td>\n",
       "      <td>0.290762</td>\n",
       "      <td>0.218844</td>\n",
       "      <td>0.583531</td>\n",
       "      <td>-0.009802</td>\n",
       "      <td>4.053025</td>\n",
       "      <td>0.301528</td>\n",
       "      <td>11.451613</td>\n",
       "      <td>0.237355</td>\n",
       "      <td>0.363735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GRU</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.057833</td>\n",
       "      <td>0.292733</td>\n",
       "      <td>0.210593</td>\n",
       "      <td>0.552310</td>\n",
       "      <td>-0.009866</td>\n",
       "      <td>4.079180</td>\n",
       "      <td>0.279805</td>\n",
       "      <td>12.741935</td>\n",
       "      <td>0.406228</td>\n",
       "      <td>0.657245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GRU</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.370232</td>\n",
       "      <td>0.263399</td>\n",
       "      <td>0.200457</td>\n",
       "      <td>0.538776</td>\n",
       "      <td>-0.012842</td>\n",
       "      <td>4.044549</td>\n",
       "      <td>0.270234</td>\n",
       "      <td>11.870968</td>\n",
       "      <td>0.354476</td>\n",
       "      <td>0.550193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GRU</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.980775</td>\n",
       "      <td>0.247230</td>\n",
       "      <td>0.195134</td>\n",
       "      <td>0.525602</td>\n",
       "      <td>-0.011841</td>\n",
       "      <td>4.016980</td>\n",
       "      <td>0.255218</td>\n",
       "      <td>11.483871</td>\n",
       "      <td>0.205558</td>\n",
       "      <td>0.299078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.067347</td>\n",
       "      <td>0.371036</td>\n",
       "      <td>0.341863</td>\n",
       "      <td>0.993234</td>\n",
       "      <td>-0.044172</td>\n",
       "      <td>3.912384</td>\n",
       "      <td>0.487609</td>\n",
       "      <td>7.935484</td>\n",
       "      <td>0.110459</td>\n",
       "      <td>0.183154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.040937</td>\n",
       "      <td>0.370302</td>\n",
       "      <td>0.345377</td>\n",
       "      <td>0.992118</td>\n",
       "      <td>-0.045273</td>\n",
       "      <td>3.901566</td>\n",
       "      <td>0.512108</td>\n",
       "      <td>7.032258</td>\n",
       "      <td>0.024438</td>\n",
       "      <td>0.039068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.124755</td>\n",
       "      <td>0.337853</td>\n",
       "      <td>0.257459</td>\n",
       "      <td>0.743961</td>\n",
       "      <td>-0.006253</td>\n",
       "      <td>4.054182</td>\n",
       "      <td>0.328870</td>\n",
       "      <td>12.483871</td>\n",
       "      <td>0.266639</td>\n",
       "      <td>0.351293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.233130</td>\n",
       "      <td>0.342698</td>\n",
       "      <td>0.269494</td>\n",
       "      <td>0.784116</td>\n",
       "      <td>-0.003906</td>\n",
       "      <td>4.033742</td>\n",
       "      <td>0.332816</td>\n",
       "      <td>12.516129</td>\n",
       "      <td>0.278690</td>\n",
       "      <td>0.355991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.665922</td>\n",
       "      <td>0.318753</td>\n",
       "      <td>0.233532</td>\n",
       "      <td>0.634151</td>\n",
       "      <td>-0.005354</td>\n",
       "      <td>4.071105</td>\n",
       "      <td>0.310971</td>\n",
       "      <td>12.419355</td>\n",
       "      <td>0.284625</td>\n",
       "      <td>0.393075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.700930</td>\n",
       "      <td>0.320037</td>\n",
       "      <td>0.232212</td>\n",
       "      <td>0.619972</td>\n",
       "      <td>-0.006651</td>\n",
       "      <td>4.077499</td>\n",
       "      <td>0.306158</td>\n",
       "      <td>12.774194</td>\n",
       "      <td>0.324033</td>\n",
       "      <td>0.473285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.150012</td>\n",
       "      <td>0.296982</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.563972</td>\n",
       "      <td>-0.007254</td>\n",
       "      <td>4.100763</td>\n",
       "      <td>0.289003</td>\n",
       "      <td>12.483871</td>\n",
       "      <td>0.339492</td>\n",
       "      <td>0.558231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.794938</td>\n",
       "      <td>0.281403</td>\n",
       "      <td>0.208275</td>\n",
       "      <td>0.554393</td>\n",
       "      <td>-0.012794</td>\n",
       "      <td>4.070072</td>\n",
       "      <td>0.283836</td>\n",
       "      <td>11.903226</td>\n",
       "      <td>0.262380</td>\n",
       "      <td>0.385317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.096642</td>\n",
       "      <td>0.250922</td>\n",
       "      <td>0.199749</td>\n",
       "      <td>0.531468</td>\n",
       "      <td>-0.018740</td>\n",
       "      <td>4.007669</td>\n",
       "      <td>0.264544</td>\n",
       "      <td>11.322581</td>\n",
       "      <td>0.296998</td>\n",
       "      <td>0.433205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.207393</td>\n",
       "      <td>0.256361</td>\n",
       "      <td>0.197616</td>\n",
       "      <td>0.529488</td>\n",
       "      <td>-0.014198</td>\n",
       "      <td>4.041967</td>\n",
       "      <td>0.255804</td>\n",
       "      <td>12.129032</td>\n",
       "      <td>0.347242</td>\n",
       "      <td>0.539836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.059965</td>\n",
       "      <td>0.371179</td>\n",
       "      <td>0.336937</td>\n",
       "      <td>0.986600</td>\n",
       "      <td>-0.042580</td>\n",
       "      <td>3.928562</td>\n",
       "      <td>0.494957</td>\n",
       "      <td>7.548387</td>\n",
       "      <td>0.069892</td>\n",
       "      <td>0.112442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.165636</td>\n",
       "      <td>0.375967</td>\n",
       "      <td>0.338990</td>\n",
       "      <td>0.989640</td>\n",
       "      <td>-0.042515</td>\n",
       "      <td>3.931620</td>\n",
       "      <td>0.487543</td>\n",
       "      <td>8.032258</td>\n",
       "      <td>0.101243</td>\n",
       "      <td>0.156272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.095826</td>\n",
       "      <td>0.336622</td>\n",
       "      <td>0.262657</td>\n",
       "      <td>0.763892</td>\n",
       "      <td>-0.005494</td>\n",
       "      <td>4.040627</td>\n",
       "      <td>0.338238</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.236657</td>\n",
       "      <td>0.307092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.125952</td>\n",
       "      <td>0.337993</td>\n",
       "      <td>0.262067</td>\n",
       "      <td>0.754182</td>\n",
       "      <td>-0.004412</td>\n",
       "      <td>4.043966</td>\n",
       "      <td>0.331382</td>\n",
       "      <td>12.354839</td>\n",
       "      <td>0.249071</td>\n",
       "      <td>0.316910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.822695</td>\n",
       "      <td>0.325267</td>\n",
       "      <td>0.236085</td>\n",
       "      <td>0.631642</td>\n",
       "      <td>-0.005438</td>\n",
       "      <td>4.073989</td>\n",
       "      <td>0.315381</td>\n",
       "      <td>12.516129</td>\n",
       "      <td>0.308881</td>\n",
       "      <td>0.432591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.789221</td>\n",
       "      <td>0.323979</td>\n",
       "      <td>0.231809</td>\n",
       "      <td>0.615241</td>\n",
       "      <td>-0.005477</td>\n",
       "      <td>4.092010</td>\n",
       "      <td>0.318763</td>\n",
       "      <td>12.290323</td>\n",
       "      <td>0.283270</td>\n",
       "      <td>0.406644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.752853</td>\n",
       "      <td>0.279601</td>\n",
       "      <td>0.205247</td>\n",
       "      <td>0.544109</td>\n",
       "      <td>-0.011668</td>\n",
       "      <td>4.063520</td>\n",
       "      <td>0.275341</td>\n",
       "      <td>12.225806</td>\n",
       "      <td>0.294973</td>\n",
       "      <td>0.447824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.923735</td>\n",
       "      <td>0.286973</td>\n",
       "      <td>0.210718</td>\n",
       "      <td>0.560713</td>\n",
       "      <td>-0.010389</td>\n",
       "      <td>4.071565</td>\n",
       "      <td>0.280883</td>\n",
       "      <td>12.354839</td>\n",
       "      <td>0.302472</td>\n",
       "      <td>0.434281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.432131</td>\n",
       "      <td>0.266529</td>\n",
       "      <td>0.204166</td>\n",
       "      <td>0.540059</td>\n",
       "      <td>-0.009231</td>\n",
       "      <td>4.034577</td>\n",
       "      <td>0.282258</td>\n",
       "      <td>11.290323</td>\n",
       "      <td>0.269390</td>\n",
       "      <td>0.405671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.364443</td>\n",
       "      <td>0.263016</td>\n",
       "      <td>0.202064</td>\n",
       "      <td>0.544959</td>\n",
       "      <td>-0.012653</td>\n",
       "      <td>4.037490</td>\n",
       "      <td>0.261040</td>\n",
       "      <td>12.193548</td>\n",
       "      <td>0.362729</td>\n",
       "      <td>0.537314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.331296</td>\n",
       "      <td>0.378972</td>\n",
       "      <td>0.338029</td>\n",
       "      <td>0.986137</td>\n",
       "      <td>-0.061941</td>\n",
       "      <td>3.956373</td>\n",
       "      <td>0.466618</td>\n",
       "      <td>8.903226</td>\n",
       "      <td>0.155495</td>\n",
       "      <td>0.226575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.144373</td>\n",
       "      <td>0.373428</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.988696</td>\n",
       "      <td>-0.050473</td>\n",
       "      <td>3.922465</td>\n",
       "      <td>0.480976</td>\n",
       "      <td>8.225806</td>\n",
       "      <td>0.102807</td>\n",
       "      <td>0.150538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.063256</td>\n",
       "      <td>0.335798</td>\n",
       "      <td>0.258835</td>\n",
       "      <td>0.763905</td>\n",
       "      <td>-0.001967</td>\n",
       "      <td>4.057021</td>\n",
       "      <td>0.338612</td>\n",
       "      <td>11.903226</td>\n",
       "      <td>0.228055</td>\n",
       "      <td>0.293971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.953562</td>\n",
       "      <td>0.330751</td>\n",
       "      <td>0.256029</td>\n",
       "      <td>0.735867</td>\n",
       "      <td>-0.005565</td>\n",
       "      <td>4.043592</td>\n",
       "      <td>0.317469</td>\n",
       "      <td>12.709677</td>\n",
       "      <td>0.313071</td>\n",
       "      <td>0.406426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.865671</td>\n",
       "      <td>0.327228</td>\n",
       "      <td>0.237340</td>\n",
       "      <td>0.630761</td>\n",
       "      <td>-0.004595</td>\n",
       "      <td>4.075829</td>\n",
       "      <td>0.312965</td>\n",
       "      <td>12.741935</td>\n",
       "      <td>0.324214</td>\n",
       "      <td>0.441372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.024266</td>\n",
       "      <td>0.333312</td>\n",
       "      <td>0.242179</td>\n",
       "      <td>0.640928</td>\n",
       "      <td>-0.008851</td>\n",
       "      <td>4.075034</td>\n",
       "      <td>0.325495</td>\n",
       "      <td>12.387097</td>\n",
       "      <td>0.259084</td>\n",
       "      <td>0.345686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.216479</td>\n",
       "      <td>0.299077</td>\n",
       "      <td>0.218846</td>\n",
       "      <td>0.580323</td>\n",
       "      <td>-0.010970</td>\n",
       "      <td>4.065696</td>\n",
       "      <td>0.288367</td>\n",
       "      <td>12.645161</td>\n",
       "      <td>0.359656</td>\n",
       "      <td>0.579916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.888242</td>\n",
       "      <td>0.285197</td>\n",
       "      <td>0.211513</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>-0.012574</td>\n",
       "      <td>4.061538</td>\n",
       "      <td>0.286841</td>\n",
       "      <td>11.903226</td>\n",
       "      <td>0.303575</td>\n",
       "      <td>0.455824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.265400</td>\n",
       "      <td>0.259002</td>\n",
       "      <td>0.204106</td>\n",
       "      <td>0.556291</td>\n",
       "      <td>-0.013980</td>\n",
       "      <td>4.025412</td>\n",
       "      <td>0.291028</td>\n",
       "      <td>10.225806</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.332629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.035562</td>\n",
       "      <td>0.248981</td>\n",
       "      <td>0.197558</td>\n",
       "      <td>0.528365</td>\n",
       "      <td>-0.015413</td>\n",
       "      <td>4.018529</td>\n",
       "      <td>0.265960</td>\n",
       "      <td>11.096774</td>\n",
       "      <td>0.262156</td>\n",
       "      <td>0.391756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>RNN</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.285137</td>\n",
       "      <td>0.381886</td>\n",
       "      <td>0.340063</td>\n",
       "      <td>0.990781</td>\n",
       "      <td>-0.032622</td>\n",
       "      <td>3.939064</td>\n",
       "      <td>0.478635</td>\n",
       "      <td>8.580645</td>\n",
       "      <td>0.122329</td>\n",
       "      <td>0.185151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>RNN</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.400917</td>\n",
       "      <td>0.385869</td>\n",
       "      <td>0.344003</td>\n",
       "      <td>0.990564</td>\n",
       "      <td>-0.040165</td>\n",
       "      <td>3.942060</td>\n",
       "      <td>0.474964</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.152562</td>\n",
       "      <td>0.227138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>RNN</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.130684</td>\n",
       "      <td>0.338554</td>\n",
       "      <td>0.262090</td>\n",
       "      <td>0.765313</td>\n",
       "      <td>-0.002212</td>\n",
       "      <td>4.049387</td>\n",
       "      <td>0.336521</td>\n",
       "      <td>12.129032</td>\n",
       "      <td>0.231574</td>\n",
       "      <td>0.294060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RNN</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.134434</td>\n",
       "      <td>0.338717</td>\n",
       "      <td>0.253017</td>\n",
       "      <td>0.727516</td>\n",
       "      <td>-0.002617</td>\n",
       "      <td>4.077740</td>\n",
       "      <td>0.329677</td>\n",
       "      <td>12.483871</td>\n",
       "      <td>0.240344</td>\n",
       "      <td>0.305376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>RNN</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.927026</td>\n",
       "      <td>0.329674</td>\n",
       "      <td>0.237462</td>\n",
       "      <td>0.643403</td>\n",
       "      <td>-0.005320</td>\n",
       "      <td>4.087837</td>\n",
       "      <td>0.317743</td>\n",
       "      <td>12.677419</td>\n",
       "      <td>0.330219</td>\n",
       "      <td>0.446608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>RNN</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.801885</td>\n",
       "      <td>0.324205</td>\n",
       "      <td>0.237593</td>\n",
       "      <td>0.647277</td>\n",
       "      <td>-0.007164</td>\n",
       "      <td>4.072903</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>12.806452</td>\n",
       "      <td>0.332984</td>\n",
       "      <td>0.460548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>RNN</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.979786</td>\n",
       "      <td>0.289681</td>\n",
       "      <td>0.214676</td>\n",
       "      <td>0.576988</td>\n",
       "      <td>-0.008264</td>\n",
       "      <td>4.065833</td>\n",
       "      <td>0.285352</td>\n",
       "      <td>12.290323</td>\n",
       "      <td>0.297808</td>\n",
       "      <td>0.417716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>RNN</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.940002</td>\n",
       "      <td>0.288276</td>\n",
       "      <td>0.210895</td>\n",
       "      <td>0.568935</td>\n",
       "      <td>-0.006094</td>\n",
       "      <td>4.073929</td>\n",
       "      <td>0.291990</td>\n",
       "      <td>11.806452</td>\n",
       "      <td>0.262994</td>\n",
       "      <td>0.384204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>RNN</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.222649</td>\n",
       "      <td>0.258455</td>\n",
       "      <td>0.197139</td>\n",
       "      <td>0.533502</td>\n",
       "      <td>-0.005740</td>\n",
       "      <td>4.047630</td>\n",
       "      <td>0.263335</td>\n",
       "      <td>11.677419</td>\n",
       "      <td>0.280897</td>\n",
       "      <td>0.424386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>RNN</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.254284</td>\n",
       "      <td>0.258136</td>\n",
       "      <td>0.201722</td>\n",
       "      <td>0.544217</td>\n",
       "      <td>-0.016502</td>\n",
       "      <td>4.024632</td>\n",
       "      <td>0.259053</td>\n",
       "      <td>11.967742</td>\n",
       "      <td>0.354992</td>\n",
       "      <td>0.569214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.210317</td>\n",
       "      <td>0.376410</td>\n",
       "      <td>0.344221</td>\n",
       "      <td>0.989194</td>\n",
       "      <td>-0.045961</td>\n",
       "      <td>3.915583</td>\n",
       "      <td>0.496511</td>\n",
       "      <td>7.806452</td>\n",
       "      <td>0.085114</td>\n",
       "      <td>0.134076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.071194</td>\n",
       "      <td>0.371807</td>\n",
       "      <td>0.343402</td>\n",
       "      <td>0.992731</td>\n",
       "      <td>-0.040717</td>\n",
       "      <td>3.903966</td>\n",
       "      <td>0.481778</td>\n",
       "      <td>8.193548</td>\n",
       "      <td>0.092515</td>\n",
       "      <td>0.132258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.400694</td>\n",
       "      <td>0.349439</td>\n",
       "      <td>0.262502</td>\n",
       "      <td>0.762633</td>\n",
       "      <td>-0.004930</td>\n",
       "      <td>4.069373</td>\n",
       "      <td>0.336576</td>\n",
       "      <td>12.677419</td>\n",
       "      <td>0.320207</td>\n",
       "      <td>0.463902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.194817</td>\n",
       "      <td>0.341036</td>\n",
       "      <td>0.259005</td>\n",
       "      <td>0.743760</td>\n",
       "      <td>-0.004252</td>\n",
       "      <td>4.059532</td>\n",
       "      <td>0.328004</td>\n",
       "      <td>12.645161</td>\n",
       "      <td>0.282111</td>\n",
       "      <td>0.355671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.677009</td>\n",
       "      <td>0.319229</td>\n",
       "      <td>0.233303</td>\n",
       "      <td>0.615663</td>\n",
       "      <td>-0.004925</td>\n",
       "      <td>4.066346</td>\n",
       "      <td>0.314431</td>\n",
       "      <td>12.258065</td>\n",
       "      <td>0.267923</td>\n",
       "      <td>0.397875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.872340</td>\n",
       "      <td>0.327502</td>\n",
       "      <td>0.235562</td>\n",
       "      <td>0.640608</td>\n",
       "      <td>-0.004095</td>\n",
       "      <td>4.090960</td>\n",
       "      <td>0.315526</td>\n",
       "      <td>12.645161</td>\n",
       "      <td>0.292738</td>\n",
       "      <td>0.392563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.858989</td>\n",
       "      <td>0.284399</td>\n",
       "      <td>0.212045</td>\n",
       "      <td>0.565597</td>\n",
       "      <td>-0.008794</td>\n",
       "      <td>4.058968</td>\n",
       "      <td>0.283016</td>\n",
       "      <td>12.064516</td>\n",
       "      <td>0.302625</td>\n",
       "      <td>0.492692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.073381</td>\n",
       "      <td>0.293491</td>\n",
       "      <td>0.211533</td>\n",
       "      <td>0.563678</td>\n",
       "      <td>-0.008443</td>\n",
       "      <td>4.085710</td>\n",
       "      <td>0.289382</td>\n",
       "      <td>12.225806</td>\n",
       "      <td>0.282935</td>\n",
       "      <td>0.413671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.229108</td>\n",
       "      <td>0.258087</td>\n",
       "      <td>0.197387</td>\n",
       "      <td>0.531788</td>\n",
       "      <td>-0.009522</td>\n",
       "      <td>4.043812</td>\n",
       "      <td>0.274535</td>\n",
       "      <td>11.258065</td>\n",
       "      <td>0.299916</td>\n",
       "      <td>0.463978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.322046</td>\n",
       "      <td>0.261441</td>\n",
       "      <td>0.198312</td>\n",
       "      <td>0.530663</td>\n",
       "      <td>-0.011737</td>\n",
       "      <td>4.044165</td>\n",
       "      <td>0.267302</td>\n",
       "      <td>11.709677</td>\n",
       "      <td>0.326952</td>\n",
       "      <td>0.548861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Kind  Min_Length  Max_Length  Repetition  Sum of Absolute Values  \\\n",
       "0     GRU          10          10        15.0                9.091594   \n",
       "1     GRU          10          15        15.0                9.035158   \n",
       "2     GRU          20          20        15.0                7.955211   \n",
       "3     GRU          20          25        15.0                8.130255   \n",
       "4     GRU          30          30        15.0                7.826277   \n",
       "5     GRU          30          35        15.0                7.796460   \n",
       "6     GRU          40          40        15.0                7.009789   \n",
       "7     GRU          40          45        15.0                7.057833   \n",
       "8     GRU          50          50        15.0                6.370232   \n",
       "9     GRU          50          55        15.0                5.980775   \n",
       "10   GRUA          10          10        15.0                9.067347   \n",
       "11   GRUA          10          15        15.0                9.040937   \n",
       "12   GRUA          20          20        15.0                8.124755   \n",
       "13   GRUA          20          25        15.0                8.233130   \n",
       "14   GRUA          30          30        15.0                7.665922   \n",
       "15   GRUA          30          35        15.0                7.700930   \n",
       "16   GRUA          40          40        15.0                7.150012   \n",
       "17   GRUA          40          45        15.0                6.794938   \n",
       "18   GRUA          50          50        15.0                6.096642   \n",
       "19   GRUA          50          55        15.0                6.207393   \n",
       "20   LSTM          10          10        15.0                9.059965   \n",
       "21   LSTM          10          15        15.0                9.165636   \n",
       "22   LSTM          20          20        15.0                8.095826   \n",
       "23   LSTM          20          25        15.0                8.125952   \n",
       "24   LSTM          30          30        15.0                7.822695   \n",
       "25   LSTM          30          35        15.0                7.789221   \n",
       "26   LSTM          40          40        15.0                6.752853   \n",
       "27   LSTM          40          45        15.0                6.923735   \n",
       "28   LSTM          50          50        15.0                6.432131   \n",
       "29   LSTM          50          55        15.0                6.364443   \n",
       "30  LSTMA          10          10        15.0                9.331296   \n",
       "31  LSTMA          10          15        15.0                9.144373   \n",
       "32  LSTMA          20          20        15.0                8.063256   \n",
       "33  LSTMA          20          25        15.0                7.953562   \n",
       "34  LSTMA          30          30        15.0                7.865671   \n",
       "35  LSTMA          30          35        15.0                8.024266   \n",
       "36  LSTMA          40          40        15.0                7.216479   \n",
       "37  LSTMA          40          45        15.0                6.888242   \n",
       "38  LSTMA          50          50        15.0                6.265400   \n",
       "39  LSTMA          50          55        15.0                6.035562   \n",
       "40    RNN          10          10        15.0                9.285137   \n",
       "41    RNN          10          15        15.0                9.400917   \n",
       "42    RNN          20          20        15.0                8.130684   \n",
       "43    RNN          20          25        15.0                8.134434   \n",
       "44    RNN          30          30        15.0                7.927026   \n",
       "45    RNN          30          35        15.0                7.801885   \n",
       "46    RNN          40          40        15.0                6.979786   \n",
       "47    RNN          40          45        15.0                6.940002   \n",
       "48    RNN          50          50        15.0                6.222649   \n",
       "49    RNN          50          55        15.0                6.254284   \n",
       "50   RNNA          10          10        15.0                9.210317   \n",
       "51   RNNA          10          15        15.0                9.071194   \n",
       "52   RNNA          20          20        15.0                8.400694   \n",
       "53   RNNA          20          25        15.0                8.194817   \n",
       "54   RNNA          30          30        15.0                7.677009   \n",
       "55   RNNA          30          35        15.0                7.872340   \n",
       "56   RNNA          40          40        15.0                6.858989   \n",
       "57   RNNA          40          45        15.0                7.073381   \n",
       "58   RNNA          50          50        15.0                6.229108   \n",
       "59   RNNA          50          55        15.0                6.322046   \n",
       "\n",
       "    Mean Value  Standard Deviation  Maximum Value  Minimum Value  \\\n",
       "0     0.371292            0.342991       0.992472      -0.048738   \n",
       "1     0.367133            0.341886       0.987537      -0.059372   \n",
       "2     0.330968            0.255564       0.735313      -0.004345   \n",
       "3     0.338326            0.259963       0.748367      -0.003968   \n",
       "4     0.325425            0.233243       0.629419      -0.004886   \n",
       "5     0.324450            0.233778       0.636747      -0.003413   \n",
       "6     0.290762            0.218844       0.583531      -0.009802   \n",
       "7     0.292733            0.210593       0.552310      -0.009866   \n",
       "8     0.263399            0.200457       0.538776      -0.012842   \n",
       "9     0.247230            0.195134       0.525602      -0.011841   \n",
       "10    0.371036            0.341863       0.993234      -0.044172   \n",
       "11    0.370302            0.345377       0.992118      -0.045273   \n",
       "12    0.337853            0.257459       0.743961      -0.006253   \n",
       "13    0.342698            0.269494       0.784116      -0.003906   \n",
       "14    0.318753            0.233532       0.634151      -0.005354   \n",
       "15    0.320037            0.232212       0.619972      -0.006651   \n",
       "16    0.296982            0.210310       0.563972      -0.007254   \n",
       "17    0.281403            0.208275       0.554393      -0.012794   \n",
       "18    0.250922            0.199749       0.531468      -0.018740   \n",
       "19    0.256361            0.197616       0.529488      -0.014198   \n",
       "20    0.371179            0.336937       0.986600      -0.042580   \n",
       "21    0.375967            0.338990       0.989640      -0.042515   \n",
       "22    0.336622            0.262657       0.763892      -0.005494   \n",
       "23    0.337993            0.262067       0.754182      -0.004412   \n",
       "24    0.325267            0.236085       0.631642      -0.005438   \n",
       "25    0.323979            0.231809       0.615241      -0.005477   \n",
       "26    0.279601            0.205247       0.544109      -0.011668   \n",
       "27    0.286973            0.210718       0.560713      -0.010389   \n",
       "28    0.266529            0.204166       0.540059      -0.009231   \n",
       "29    0.263016            0.202064       0.544959      -0.012653   \n",
       "30    0.378972            0.338029       0.986137      -0.061941   \n",
       "31    0.373428            0.341000       0.988696      -0.050473   \n",
       "32    0.335798            0.258835       0.763905      -0.001967   \n",
       "33    0.330751            0.256029       0.735867      -0.005565   \n",
       "34    0.327228            0.237340       0.630761      -0.004595   \n",
       "35    0.333312            0.242179       0.640928      -0.008851   \n",
       "36    0.299077            0.218846       0.580323      -0.010970   \n",
       "37    0.285197            0.211513       0.562500      -0.012574   \n",
       "38    0.259002            0.204106       0.556291      -0.013980   \n",
       "39    0.248981            0.197558       0.528365      -0.015413   \n",
       "40    0.381886            0.340063       0.990781      -0.032622   \n",
       "41    0.385869            0.344003       0.990564      -0.040165   \n",
       "42    0.338554            0.262090       0.765313      -0.002212   \n",
       "43    0.338717            0.253017       0.727516      -0.002617   \n",
       "44    0.329674            0.237462       0.643403      -0.005320   \n",
       "45    0.324205            0.237593       0.647277      -0.007164   \n",
       "46    0.289681            0.214676       0.576988      -0.008264   \n",
       "47    0.288276            0.210895       0.568935      -0.006094   \n",
       "48    0.258455            0.197139       0.533502      -0.005740   \n",
       "49    0.258136            0.201722       0.544217      -0.016502   \n",
       "50    0.376410            0.344221       0.989194      -0.045961   \n",
       "51    0.371807            0.343402       0.992731      -0.040717   \n",
       "52    0.349439            0.262502       0.762633      -0.004930   \n",
       "53    0.341036            0.259005       0.743760      -0.004252   \n",
       "54    0.319229            0.233303       0.615663      -0.004925   \n",
       "55    0.327502            0.235562       0.640608      -0.004095   \n",
       "56    0.284399            0.212045       0.565597      -0.008794   \n",
       "57    0.293491            0.211533       0.563678      -0.008443   \n",
       "58    0.258087            0.197387       0.531788      -0.009522   \n",
       "59    0.261441            0.198312       0.530663      -0.011737   \n",
       "\n",
       "    Shannon Entropy  Kmeans Threshold  Kmeans Nodes Exceeding  \\\n",
       "0          3.906474          0.487057                7.967742   \n",
       "1          3.909389          0.484614                7.903226   \n",
       "2          4.047512          0.319518               12.612903   \n",
       "3          4.053833          0.334176               12.225806   \n",
       "4          4.086814          0.316342               12.516129   \n",
       "5          4.079067          0.312201               12.677419   \n",
       "6          4.053025          0.301528               11.451613   \n",
       "7          4.079180          0.279805               12.741935   \n",
       "8          4.044549          0.270234               11.870968   \n",
       "9          4.016980          0.255218               11.483871   \n",
       "10         3.912384          0.487609                7.935484   \n",
       "11         3.901566          0.512108                7.032258   \n",
       "12         4.054182          0.328870               12.483871   \n",
       "13         4.033742          0.332816               12.516129   \n",
       "14         4.071105          0.310971               12.419355   \n",
       "15         4.077499          0.306158               12.774194   \n",
       "16         4.100763          0.289003               12.483871   \n",
       "17         4.070072          0.283836               11.903226   \n",
       "18         4.007669          0.264544               11.322581   \n",
       "19         4.041967          0.255804               12.129032   \n",
       "20         3.928562          0.494957                7.548387   \n",
       "21         3.931620          0.487543                8.032258   \n",
       "22         4.040627          0.338238               12.000000   \n",
       "23         4.043966          0.331382               12.354839   \n",
       "24         4.073989          0.315381               12.516129   \n",
       "25         4.092010          0.318763               12.290323   \n",
       "26         4.063520          0.275341               12.225806   \n",
       "27         4.071565          0.280883               12.354839   \n",
       "28         4.034577          0.282258               11.290323   \n",
       "29         4.037490          0.261040               12.193548   \n",
       "30         3.956373          0.466618                8.903226   \n",
       "31         3.922465          0.480976                8.225806   \n",
       "32         4.057021          0.338612               11.903226   \n",
       "33         4.043592          0.317469               12.709677   \n",
       "34         4.075829          0.312965               12.741935   \n",
       "35         4.075034          0.325495               12.387097   \n",
       "36         4.065696          0.288367               12.645161   \n",
       "37         4.061538          0.286841               11.903226   \n",
       "38         4.025412          0.291028               10.225806   \n",
       "39         4.018529          0.265960               11.096774   \n",
       "40         3.939064          0.478635                8.580645   \n",
       "41         3.942060          0.474964                9.000000   \n",
       "42         4.049387          0.336521               12.129032   \n",
       "43         4.077740          0.329677               12.483871   \n",
       "44         4.087837          0.317743               12.677419   \n",
       "45         4.072903          0.309000               12.806452   \n",
       "46         4.065833          0.285352               12.290323   \n",
       "47         4.073929          0.291990               11.806452   \n",
       "48         4.047630          0.263335               11.677419   \n",
       "49         4.024632          0.259053               11.967742   \n",
       "50         3.915583          0.496511                7.806452   \n",
       "51         3.903966          0.481778                8.193548   \n",
       "52         4.069373          0.336576               12.677419   \n",
       "53         4.059532          0.328004               12.645161   \n",
       "54         4.066346          0.314431               12.258065   \n",
       "55         4.090960          0.315526               12.645161   \n",
       "56         4.058968          0.283016               12.064516   \n",
       "57         4.085710          0.289382               12.225806   \n",
       "58         4.043812          0.274535               11.258065   \n",
       "59         4.044165          0.267302               11.709677   \n",
       "\n",
       "    Overlap with zeroes  Overlap no zeroes  \n",
       "0              0.103687           0.151459  \n",
       "1              0.094009           0.143932  \n",
       "2              0.292543           0.381272  \n",
       "3              0.230987           0.290835  \n",
       "4              0.333026           0.495993  \n",
       "5              0.316702           0.434677  \n",
       "6              0.237355           0.363735  \n",
       "7              0.406228           0.657245  \n",
       "8              0.354476           0.550193  \n",
       "9              0.205558           0.299078  \n",
       "10             0.110459           0.183154  \n",
       "11             0.024438           0.039068  \n",
       "12             0.266639           0.351293  \n",
       "13             0.278690           0.355991  \n",
       "14             0.284625           0.393075  \n",
       "15             0.324033           0.473285  \n",
       "16             0.339492           0.558231  \n",
       "17             0.262380           0.385317  \n",
       "18             0.296998           0.433205  \n",
       "19             0.347242           0.539836  \n",
       "20             0.069892           0.112442  \n",
       "21             0.101243           0.156272  \n",
       "22             0.236657           0.307092  \n",
       "23             0.249071           0.316910  \n",
       "24             0.308881           0.432591  \n",
       "25             0.283270           0.406644  \n",
       "26             0.294973           0.447824  \n",
       "27             0.302472           0.434281  \n",
       "28             0.269390           0.405671  \n",
       "29             0.362729           0.537314  \n",
       "30             0.155495           0.226575  \n",
       "31             0.102807           0.150538  \n",
       "32             0.228055           0.293971  \n",
       "33             0.313071           0.406426  \n",
       "34             0.324214           0.441372  \n",
       "35             0.259084           0.345686  \n",
       "36             0.359656           0.579916  \n",
       "37             0.303575           0.455824  \n",
       "38             0.211200           0.332629  \n",
       "39             0.262156           0.391756  \n",
       "40             0.122329           0.185151  \n",
       "41             0.152562           0.227138  \n",
       "42             0.231574           0.294060  \n",
       "43             0.240344           0.305376  \n",
       "44             0.330219           0.446608  \n",
       "45             0.332984           0.460548  \n",
       "46             0.297808           0.417716  \n",
       "47             0.262994           0.384204  \n",
       "48             0.280897           0.424386  \n",
       "49             0.354992           0.569214  \n",
       "50             0.085114           0.134076  \n",
       "51             0.092515           0.132258  \n",
       "52             0.320207           0.463902  \n",
       "53             0.282111           0.355671  \n",
       "54             0.267923           0.397875  \n",
       "55             0.292738           0.392563  \n",
       "56             0.302625           0.492692  \n",
       "57             0.282935           0.413671  \n",
       "58             0.299916           0.463978  \n",
       "59             0.326952           0.548861  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, calculating the average for each kind_min_max\n",
    "average_results = df.groupby(['Kind', 'Min_Length', 'Max_Length']).mean().reset_index()\n",
    "\n",
    "# Display the averaged results\n",
    "average_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_results.to_csv(\"average_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kind</th>\n",
       "      <th>Min_Length</th>\n",
       "      <th>Max_Length</th>\n",
       "      <th>Sum of Absolute Values</th>\n",
       "      <th>Mean Value</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Maximum Value</th>\n",
       "      <th>Minimum Value</th>\n",
       "      <th>Shannon Entropy</th>\n",
       "      <th>Kmeans Threshold</th>\n",
       "      <th>Kmeans Nodes Exceeding</th>\n",
       "      <th>Overlap with zeroes</th>\n",
       "      <th>Overlap no zeroes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RNN</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>9.853125</td>\n",
       "      <td>0.410547</td>\n",
       "      <td>0.075883</td>\n",
       "      <td>0.519693</td>\n",
       "      <td>0.237190</td>\n",
       "      <td>4.559000</td>\n",
       "      <td>0.394214</td>\n",
       "      <td>15</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5.189342</td>\n",
       "      <td>0.216223</td>\n",
       "      <td>0.031756</td>\n",
       "      <td>0.292732</td>\n",
       "      <td>0.173071</td>\n",
       "      <td>4.569697</td>\n",
       "      <td>0.225471</td>\n",
       "      <td>8</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>11.277568</td>\n",
       "      <td>0.469899</td>\n",
       "      <td>0.068982</td>\n",
       "      <td>0.605611</td>\n",
       "      <td>0.348981</td>\n",
       "      <td>4.569584</td>\n",
       "      <td>0.496182</td>\n",
       "      <td>7</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4.125979</td>\n",
       "      <td>0.171916</td>\n",
       "      <td>0.043909</td>\n",
       "      <td>0.256679</td>\n",
       "      <td>0.105726</td>\n",
       "      <td>4.538215</td>\n",
       "      <td>0.182060</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRU</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>12.466574</td>\n",
       "      <td>0.519441</td>\n",
       "      <td>0.074796</td>\n",
       "      <td>0.707213</td>\n",
       "      <td>0.343362</td>\n",
       "      <td>4.569857</td>\n",
       "      <td>0.510375</td>\n",
       "      <td>14</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4.155550</td>\n",
       "      <td>0.173148</td>\n",
       "      <td>0.029043</td>\n",
       "      <td>0.256108</td>\n",
       "      <td>0.114383</td>\n",
       "      <td>4.564954</td>\n",
       "      <td>0.181051</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RNN</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>10.014051</td>\n",
       "      <td>0.417252</td>\n",
       "      <td>0.065598</td>\n",
       "      <td>0.529301</td>\n",
       "      <td>0.272434</td>\n",
       "      <td>4.566855</td>\n",
       "      <td>0.441817</td>\n",
       "      <td>7</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>5.950511</td>\n",
       "      <td>0.247938</td>\n",
       "      <td>0.043230</td>\n",
       "      <td>0.357245</td>\n",
       "      <td>0.174563</td>\n",
       "      <td>4.563027</td>\n",
       "      <td>0.239054</td>\n",
       "      <td>15</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>10.854931</td>\n",
       "      <td>0.452289</td>\n",
       "      <td>0.079023</td>\n",
       "      <td>0.603427</td>\n",
       "      <td>0.307295</td>\n",
       "      <td>4.562394</td>\n",
       "      <td>0.414895</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>5.019365</td>\n",
       "      <td>0.209140</td>\n",
       "      <td>0.027297</td>\n",
       "      <td>0.264223</td>\n",
       "      <td>0.173574</td>\n",
       "      <td>4.572828</td>\n",
       "      <td>0.215277</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GRU</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>12.923920</td>\n",
       "      <td>0.538497</td>\n",
       "      <td>0.069703</td>\n",
       "      <td>0.649130</td>\n",
       "      <td>0.360858</td>\n",
       "      <td>4.572526</td>\n",
       "      <td>0.524100</td>\n",
       "      <td>15</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>3.787641</td>\n",
       "      <td>0.157818</td>\n",
       "      <td>0.029511</td>\n",
       "      <td>0.219008</td>\n",
       "      <td>0.073732</td>\n",
       "      <td>4.558483</td>\n",
       "      <td>0.161513</td>\n",
       "      <td>10</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RNN</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10.615920</td>\n",
       "      <td>0.442330</td>\n",
       "      <td>0.096652</td>\n",
       "      <td>0.633079</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>4.550301</td>\n",
       "      <td>0.436130</td>\n",
       "      <td>13</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>2.491909</td>\n",
       "      <td>0.103830</td>\n",
       "      <td>0.049913</td>\n",
       "      <td>0.206212</td>\n",
       "      <td>0.030116</td>\n",
       "      <td>4.418902</td>\n",
       "      <td>0.111587</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10.253727</td>\n",
       "      <td>0.427239</td>\n",
       "      <td>0.071034</td>\n",
       "      <td>0.543192</td>\n",
       "      <td>0.287571</td>\n",
       "      <td>4.564618</td>\n",
       "      <td>0.422040</td>\n",
       "      <td>13</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>1.763796</td>\n",
       "      <td>0.073491</td>\n",
       "      <td>0.021479</td>\n",
       "      <td>0.125802</td>\n",
       "      <td>0.040885</td>\n",
       "      <td>4.525709</td>\n",
       "      <td>0.092856</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GRU</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>12.728090</td>\n",
       "      <td>0.530337</td>\n",
       "      <td>0.076639</td>\n",
       "      <td>0.676014</td>\n",
       "      <td>0.367730</td>\n",
       "      <td>4.569777</td>\n",
       "      <td>0.546564</td>\n",
       "      <td>9</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>2.819839</td>\n",
       "      <td>0.117493</td>\n",
       "      <td>0.031830</td>\n",
       "      <td>0.191909</td>\n",
       "      <td>0.066494</td>\n",
       "      <td>4.532836</td>\n",
       "      <td>0.124494</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RNN</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>10.806050</td>\n",
       "      <td>0.450252</td>\n",
       "      <td>0.075179</td>\n",
       "      <td>0.601652</td>\n",
       "      <td>0.325487</td>\n",
       "      <td>4.564798</td>\n",
       "      <td>0.445038</td>\n",
       "      <td>13</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>2.501258</td>\n",
       "      <td>0.104219</td>\n",
       "      <td>0.041679</td>\n",
       "      <td>0.173589</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>4.460718</td>\n",
       "      <td>0.110276</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>10.458029</td>\n",
       "      <td>0.435751</td>\n",
       "      <td>0.079466</td>\n",
       "      <td>0.582217</td>\n",
       "      <td>0.273329</td>\n",
       "      <td>4.560582</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>11</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>1.863807</td>\n",
       "      <td>0.077659</td>\n",
       "      <td>0.020178</td>\n",
       "      <td>0.130218</td>\n",
       "      <td>0.033478</td>\n",
       "      <td>4.535578</td>\n",
       "      <td>0.091694</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GRU</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>12.459252</td>\n",
       "      <td>0.519135</td>\n",
       "      <td>0.070127</td>\n",
       "      <td>0.735569</td>\n",
       "      <td>0.420810</td>\n",
       "      <td>4.572463</td>\n",
       "      <td>0.604463</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>2.079974</td>\n",
       "      <td>0.086666</td>\n",
       "      <td>0.031630</td>\n",
       "      <td>0.142924</td>\n",
       "      <td>0.042655</td>\n",
       "      <td>4.489895</td>\n",
       "      <td>0.102444</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RNN</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>11.112303</td>\n",
       "      <td>0.463013</td>\n",
       "      <td>0.084508</td>\n",
       "      <td>0.604418</td>\n",
       "      <td>0.284710</td>\n",
       "      <td>4.560205</td>\n",
       "      <td>0.469072</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1.335267</td>\n",
       "      <td>0.055636</td>\n",
       "      <td>0.015716</td>\n",
       "      <td>0.083038</td>\n",
       "      <td>0.021834</td>\n",
       "      <td>4.522195</td>\n",
       "      <td>0.052393</td>\n",
       "      <td>15</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>10.404073</td>\n",
       "      <td>0.433503</td>\n",
       "      <td>0.069959</td>\n",
       "      <td>0.573821</td>\n",
       "      <td>0.263584</td>\n",
       "      <td>4.565494</td>\n",
       "      <td>0.447082</td>\n",
       "      <td>9</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.929591</td>\n",
       "      <td>0.038733</td>\n",
       "      <td>0.014570</td>\n",
       "      <td>0.065239</td>\n",
       "      <td>0.014662</td>\n",
       "      <td>4.480950</td>\n",
       "      <td>0.040896</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>GRU</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>12.800495</td>\n",
       "      <td>0.533354</td>\n",
       "      <td>0.072296</td>\n",
       "      <td>0.676285</td>\n",
       "      <td>0.370876</td>\n",
       "      <td>4.571526</td>\n",
       "      <td>0.547943</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1.342479</td>\n",
       "      <td>0.055937</td>\n",
       "      <td>0.016347</td>\n",
       "      <td>0.094766</td>\n",
       "      <td>0.028047</td>\n",
       "      <td>4.525855</td>\n",
       "      <td>0.065898</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>RNN</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>10.923235</td>\n",
       "      <td>0.455135</td>\n",
       "      <td>0.066289</td>\n",
       "      <td>0.545509</td>\n",
       "      <td>0.323528</td>\n",
       "      <td>4.569291</td>\n",
       "      <td>0.444722</td>\n",
       "      <td>14</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>1.391212</td>\n",
       "      <td>0.057967</td>\n",
       "      <td>0.014007</td>\n",
       "      <td>0.086392</td>\n",
       "      <td>0.038342</td>\n",
       "      <td>4.543470</td>\n",
       "      <td>0.062140</td>\n",
       "      <td>8</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>10.477797</td>\n",
       "      <td>0.436575</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.571727</td>\n",
       "      <td>0.343794</td>\n",
       "      <td>4.575946</td>\n",
       "      <td>0.465816</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>0.557037</td>\n",
       "      <td>0.023210</td>\n",
       "      <td>0.008910</td>\n",
       "      <td>0.048047</td>\n",
       "      <td>0.010037</td>\n",
       "      <td>4.490074</td>\n",
       "      <td>0.031772</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>GRU</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>12.720958</td>\n",
       "      <td>0.530040</td>\n",
       "      <td>0.053726</td>\n",
       "      <td>0.631749</td>\n",
       "      <td>0.409800</td>\n",
       "      <td>4.577470</td>\n",
       "      <td>0.533672</td>\n",
       "      <td>11</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>1.754044</td>\n",
       "      <td>0.073085</td>\n",
       "      <td>0.017377</td>\n",
       "      <td>0.104329</td>\n",
       "      <td>0.038279</td>\n",
       "      <td>4.542607</td>\n",
       "      <td>0.066391</td>\n",
       "      <td>17</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>RNN</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>11.439619</td>\n",
       "      <td>0.476651</td>\n",
       "      <td>0.091127</td>\n",
       "      <td>0.711587</td>\n",
       "      <td>0.269412</td>\n",
       "      <td>4.558380</td>\n",
       "      <td>0.470878</td>\n",
       "      <td>13</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1.517237</td>\n",
       "      <td>0.063218</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.122682</td>\n",
       "      <td>0.031475</td>\n",
       "      <td>4.489622</td>\n",
       "      <td>0.063218</td>\n",
       "      <td>12</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>9.962920</td>\n",
       "      <td>0.415122</td>\n",
       "      <td>0.067353</td>\n",
       "      <td>0.517111</td>\n",
       "      <td>0.264409</td>\n",
       "      <td>4.564989</td>\n",
       "      <td>0.380597</td>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>0.620543</td>\n",
       "      <td>0.025856</td>\n",
       "      <td>0.010675</td>\n",
       "      <td>0.043736</td>\n",
       "      <td>0.007860</td>\n",
       "      <td>4.454560</td>\n",
       "      <td>0.025856</td>\n",
       "      <td>12</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>GRU</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>12.753937</td>\n",
       "      <td>0.531414</td>\n",
       "      <td>0.084362</td>\n",
       "      <td>0.741665</td>\n",
       "      <td>0.355294</td>\n",
       "      <td>4.566832</td>\n",
       "      <td>0.577565</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1.367821</td>\n",
       "      <td>0.056993</td>\n",
       "      <td>0.016966</td>\n",
       "      <td>0.101669</td>\n",
       "      <td>0.018822</td>\n",
       "      <td>4.515487</td>\n",
       "      <td>0.045847</td>\n",
       "      <td>20</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>RNN</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>11.196989</td>\n",
       "      <td>0.466541</td>\n",
       "      <td>0.062822</td>\n",
       "      <td>0.625458</td>\n",
       "      <td>0.351431</td>\n",
       "      <td>4.572055</td>\n",
       "      <td>0.495931</td>\n",
       "      <td>6</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>0.733821</td>\n",
       "      <td>0.030576</td>\n",
       "      <td>0.008144</td>\n",
       "      <td>0.043943</td>\n",
       "      <td>0.011451</td>\n",
       "      <td>4.528438</td>\n",
       "      <td>0.030055</td>\n",
       "      <td>13</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>10.240514</td>\n",
       "      <td>0.426688</td>\n",
       "      <td>0.078647</td>\n",
       "      <td>0.595012</td>\n",
       "      <td>0.285747</td>\n",
       "      <td>4.560752</td>\n",
       "      <td>0.426688</td>\n",
       "      <td>12</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>0.826878</td>\n",
       "      <td>0.034453</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>0.064945</td>\n",
       "      <td>0.013042</td>\n",
       "      <td>4.483209</td>\n",
       "      <td>0.039425</td>\n",
       "      <td>7</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>GRU</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>12.229704</td>\n",
       "      <td>0.509571</td>\n",
       "      <td>0.059699</td>\n",
       "      <td>0.610050</td>\n",
       "      <td>0.383487</td>\n",
       "      <td>4.574849</td>\n",
       "      <td>0.496391</td>\n",
       "      <td>15</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>0.752726</td>\n",
       "      <td>0.031364</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.041911</td>\n",
       "      <td>0.006011</td>\n",
       "      <td>4.529208</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>19</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>RNN</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>11.009394</td>\n",
       "      <td>0.458725</td>\n",
       "      <td>0.079485</td>\n",
       "      <td>0.595994</td>\n",
       "      <td>0.257100</td>\n",
       "      <td>4.562607</td>\n",
       "      <td>0.442214</td>\n",
       "      <td>15</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0.920187</td>\n",
       "      <td>0.038341</td>\n",
       "      <td>0.016088</td>\n",
       "      <td>0.076819</td>\n",
       "      <td>0.015220</td>\n",
       "      <td>4.466426</td>\n",
       "      <td>0.046637</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>10.612083</td>\n",
       "      <td>0.442170</td>\n",
       "      <td>0.066239</td>\n",
       "      <td>0.628599</td>\n",
       "      <td>0.307152</td>\n",
       "      <td>4.569059</td>\n",
       "      <td>0.450733</td>\n",
       "      <td>10</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0.293744</td>\n",
       "      <td>0.012239</td>\n",
       "      <td>0.005073</td>\n",
       "      <td>0.024205</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>4.461267</td>\n",
       "      <td>0.013317</td>\n",
       "      <td>9</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>GRU</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>12.934998</td>\n",
       "      <td>0.538958</td>\n",
       "      <td>0.083891</td>\n",
       "      <td>0.706078</td>\n",
       "      <td>0.347719</td>\n",
       "      <td>4.566940</td>\n",
       "      <td>0.515571</td>\n",
       "      <td>16</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0.941616</td>\n",
       "      <td>0.039234</td>\n",
       "      <td>0.013844</td>\n",
       "      <td>0.064574</td>\n",
       "      <td>0.016994</td>\n",
       "      <td>4.490219</td>\n",
       "      <td>0.037193</td>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>RNN</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>11.564016</td>\n",
       "      <td>0.481834</td>\n",
       "      <td>0.062586</td>\n",
       "      <td>0.605472</td>\n",
       "      <td>0.332852</td>\n",
       "      <td>4.572527</td>\n",
       "      <td>0.473362</td>\n",
       "      <td>14</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>RNNA</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>0.521073</td>\n",
       "      <td>0.021711</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.045016</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>4.497213</td>\n",
       "      <td>0.021711</td>\n",
       "      <td>12</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>10.102943</td>\n",
       "      <td>0.420956</td>\n",
       "      <td>0.053386</td>\n",
       "      <td>0.515347</td>\n",
       "      <td>0.324134</td>\n",
       "      <td>4.573205</td>\n",
       "      <td>0.405300</td>\n",
       "      <td>16</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>LSTMA</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>0.424692</td>\n",
       "      <td>0.017696</td>\n",
       "      <td>0.008723</td>\n",
       "      <td>0.032378</td>\n",
       "      <td>0.006378</td>\n",
       "      <td>4.399919</td>\n",
       "      <td>0.016341</td>\n",
       "      <td>14</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>GRU</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>13.008301</td>\n",
       "      <td>0.542013</td>\n",
       "      <td>0.086973</td>\n",
       "      <td>0.733257</td>\n",
       "      <td>0.356437</td>\n",
       "      <td>4.566067</td>\n",
       "      <td>0.518312</td>\n",
       "      <td>16</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>GRUA</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>0.882289</td>\n",
       "      <td>0.036762</td>\n",
       "      <td>0.011058</td>\n",
       "      <td>0.051444</td>\n",
       "      <td>0.010476</td>\n",
       "      <td>4.511713</td>\n",
       "      <td>0.035955</td>\n",
       "      <td>13</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Kind  Min_Length  Max_Length  Sum of Absolute Values  Mean Value  \\\n",
       "0     RNN          10          10                9.853125    0.410547   \n",
       "1    RNNA          10          10                5.189342    0.216223   \n",
       "2    LSTM          10          10               11.277568    0.469899   \n",
       "3   LSTMA          10          10                4.125979    0.171916   \n",
       "4     GRU          10          10               12.466574    0.519441   \n",
       "5    GRUA          10          10                4.155550    0.173148   \n",
       "6     RNN          10          15               10.014051    0.417252   \n",
       "7    RNNA          10          15                5.950511    0.247938   \n",
       "8    LSTM          10          15               10.854931    0.452289   \n",
       "9   LSTMA          10          15                5.019365    0.209140   \n",
       "10    GRU          10          15               12.923920    0.538497   \n",
       "11   GRUA          10          15                3.787641    0.157818   \n",
       "12    RNN          20          20               10.615920    0.442330   \n",
       "13   RNNA          20          20                2.491909    0.103830   \n",
       "14   LSTM          20          20               10.253727    0.427239   \n",
       "15  LSTMA          20          20                1.763796    0.073491   \n",
       "16    GRU          20          20               12.728090    0.530337   \n",
       "17   GRUA          20          20                2.819839    0.117493   \n",
       "18    RNN          20          25               10.806050    0.450252   \n",
       "19   RNNA          20          25                2.501258    0.104219   \n",
       "20   LSTM          20          25               10.458029    0.435751   \n",
       "21  LSTMA          20          25                1.863807    0.077659   \n",
       "22    GRU          20          25               12.459252    0.519135   \n",
       "23   GRUA          20          25                2.079974    0.086666   \n",
       "24    RNN          30          30               11.112303    0.463013   \n",
       "25   RNNA          30          30                1.335267    0.055636   \n",
       "26   LSTM          30          30               10.404073    0.433503   \n",
       "27  LSTMA          30          30                0.929591    0.038733   \n",
       "28    GRU          30          30               12.800495    0.533354   \n",
       "29   GRUA          30          30                1.342479    0.055937   \n",
       "30    RNN          30          35               10.923235    0.455135   \n",
       "31   RNNA          30          35                1.391212    0.057967   \n",
       "32   LSTM          30          35               10.477797    0.436575   \n",
       "33  LSTMA          30          35                0.557037    0.023210   \n",
       "34    GRU          30          35               12.720958    0.530040   \n",
       "35   GRUA          30          35                1.754044    0.073085   \n",
       "36    RNN          40          40               11.439619    0.476651   \n",
       "37   RNNA          40          40                1.517237    0.063218   \n",
       "38   LSTM          40          40                9.962920    0.415122   \n",
       "39  LSTMA          40          40                0.620543    0.025856   \n",
       "40    GRU          40          40               12.753937    0.531414   \n",
       "41   GRUA          40          40                1.367821    0.056993   \n",
       "42    RNN          40          45               11.196989    0.466541   \n",
       "43   RNNA          40          45                0.733821    0.030576   \n",
       "44   LSTM          40          45               10.240514    0.426688   \n",
       "45  LSTMA          40          45                0.826878    0.034453   \n",
       "46    GRU          40          45               12.229704    0.509571   \n",
       "47   GRUA          40          45                0.752726    0.031364   \n",
       "48    RNN          50          50               11.009394    0.458725   \n",
       "49   RNNA          50          50                0.920187    0.038341   \n",
       "50   LSTM          50          50               10.612083    0.442170   \n",
       "51  LSTMA          50          50                0.293744    0.012239   \n",
       "52    GRU          50          50               12.934998    0.538958   \n",
       "53   GRUA          50          50                0.941616    0.039234   \n",
       "54    RNN          50          55               11.564016    0.481834   \n",
       "55   RNNA          50          55                0.521073    0.021711   \n",
       "56   LSTM          50          55               10.102943    0.420956   \n",
       "57  LSTMA          50          55                0.424692    0.017696   \n",
       "58    GRU          50          55               13.008301    0.542013   \n",
       "59   GRUA          50          55                0.882289    0.036762   \n",
       "\n",
       "    Standard Deviation  Maximum Value  Minimum Value  Shannon Entropy  \\\n",
       "0             0.075883       0.519693       0.237190         4.559000   \n",
       "1             0.031756       0.292732       0.173071         4.569697   \n",
       "2             0.068982       0.605611       0.348981         4.569584   \n",
       "3             0.043909       0.256679       0.105726         4.538215   \n",
       "4             0.074796       0.707213       0.343362         4.569857   \n",
       "5             0.029043       0.256108       0.114383         4.564954   \n",
       "6             0.065598       0.529301       0.272434         4.566855   \n",
       "7             0.043230       0.357245       0.174563         4.563027   \n",
       "8             0.079023       0.603427       0.307295         4.562394   \n",
       "9             0.027297       0.264223       0.173574         4.572828   \n",
       "10            0.069703       0.649130       0.360858         4.572526   \n",
       "11            0.029511       0.219008       0.073732         4.558483   \n",
       "12            0.096652       0.633079       0.264706         4.550301   \n",
       "13            0.049913       0.206212       0.030116         4.418902   \n",
       "14            0.071034       0.543192       0.287571         4.564618   \n",
       "15            0.021479       0.125802       0.040885         4.525709   \n",
       "16            0.076639       0.676014       0.367730         4.569777   \n",
       "17            0.031830       0.191909       0.066494         4.532836   \n",
       "18            0.075179       0.601652       0.325487         4.564798   \n",
       "19            0.041679       0.173589       0.019970         4.460718   \n",
       "20            0.079466       0.582217       0.273329         4.560582   \n",
       "21            0.020178       0.130218       0.033478         4.535578   \n",
       "22            0.070127       0.735569       0.420810         4.572463   \n",
       "23            0.031630       0.142924       0.042655         4.489895   \n",
       "24            0.084508       0.604418       0.284710         4.560205   \n",
       "25            0.015716       0.083038       0.021834         4.522195   \n",
       "26            0.069959       0.573821       0.263584         4.565494   \n",
       "27            0.014570       0.065239       0.014662         4.480950   \n",
       "28            0.072296       0.676285       0.370876         4.571526   \n",
       "29            0.016347       0.094766       0.028047         4.525855   \n",
       "30            0.066289       0.545509       0.323528         4.569291   \n",
       "31            0.014007       0.086392       0.038342         4.543470   \n",
       "32            0.049341       0.571727       0.343794         4.575946   \n",
       "33            0.008910       0.048047       0.010037         4.490074   \n",
       "34            0.053726       0.631749       0.409800         4.577470   \n",
       "35            0.017377       0.104329       0.038279         4.542607   \n",
       "36            0.091127       0.711587       0.269412         4.558380   \n",
       "37            0.023305       0.122682       0.031475         4.489622   \n",
       "38            0.067353       0.517111       0.264409         4.564989   \n",
       "39            0.010675       0.043736       0.007860         4.454560   \n",
       "40            0.084362       0.741665       0.355294         4.566832   \n",
       "41            0.016966       0.101669       0.018822         4.515487   \n",
       "42            0.062822       0.625458       0.351431         4.572055   \n",
       "43            0.008144       0.043943       0.011451         4.528438   \n",
       "44            0.078647       0.595012       0.285747         4.560752   \n",
       "45            0.012954       0.064945       0.013042         4.483209   \n",
       "46            0.059699       0.610050       0.383487         4.574849   \n",
       "47            0.007890       0.041911       0.006011         4.529208   \n",
       "48            0.079485       0.595994       0.257100         4.562607   \n",
       "49            0.016088       0.076819       0.015220         4.466426   \n",
       "50            0.066239       0.628599       0.307152         4.569059   \n",
       "51            0.005073       0.024205       0.004179         4.461267   \n",
       "52            0.083891       0.706078       0.347719         4.566940   \n",
       "53            0.013844       0.064574       0.016994         4.490219   \n",
       "54            0.062586       0.605472       0.332852         4.572527   \n",
       "55            0.007670       0.045016       0.009476         4.497213   \n",
       "56            0.053386       0.515347       0.324134         4.573205   \n",
       "57            0.008723       0.032378       0.006378         4.399919   \n",
       "58            0.086973       0.733257       0.356437         4.566067   \n",
       "59            0.011058       0.051444       0.010476         4.511713   \n",
       "\n",
       "    Kmeans Threshold  Kmeans Nodes Exceeding  Overlap with zeroes  \\\n",
       "0           0.394214                      15             0.333333   \n",
       "1           0.225471                       8             0.090909   \n",
       "2           0.496182                       7             0.090909   \n",
       "3           0.182060                       9             0.000000   \n",
       "4           0.510375                      14             0.333333   \n",
       "5           0.181051                       8             0.000000   \n",
       "6           0.441817                       7             0.090909   \n",
       "7           0.239054                      15             0.333333   \n",
       "8           0.414895                      18             1.000000   \n",
       "9           0.215277                       9             0.000000   \n",
       "10          0.524100                      15             0.500000   \n",
       "11          0.161513                      10             0.200000   \n",
       "12          0.436130                      13             0.090909   \n",
       "13          0.111587                      10             0.000000   \n",
       "14          0.422040                      13             0.090909   \n",
       "15          0.092856                       3             0.000000   \n",
       "16          0.546564                       9             0.090909   \n",
       "17          0.124494                       9             0.000000   \n",
       "18          0.445038                      13             0.500000   \n",
       "19          0.110276                      10             0.000000   \n",
       "20          0.441026                      11             0.090909   \n",
       "21          0.091694                       4             0.000000   \n",
       "22          0.604463                       2             0.000000   \n",
       "23          0.102444                       6             0.000000   \n",
       "24          0.469072                      11             0.000000   \n",
       "25          0.052393                      15             0.500000   \n",
       "26          0.447082                       9             0.090909   \n",
       "27          0.040896                      10             0.000000   \n",
       "28          0.547943                       9             0.000000   \n",
       "29          0.065898                       5             0.000000   \n",
       "30          0.444722                      14             0.200000   \n",
       "31          0.062140                       8             0.090909   \n",
       "32          0.465816                       5             0.000000   \n",
       "33          0.031772                       3             0.000000   \n",
       "34          0.533672                      11             0.200000   \n",
       "35          0.066391                      17             0.714286   \n",
       "36          0.470878                      13             0.200000   \n",
       "37          0.063218                      12             0.200000   \n",
       "38          0.380597                      18             1.000000   \n",
       "39          0.025856                      12             0.090909   \n",
       "40          0.577565                       5             0.000000   \n",
       "41          0.045847                      20             2.000000   \n",
       "42          0.495931                       6             0.090909   \n",
       "43          0.030055                      13             0.333333   \n",
       "44          0.426688                      12             0.090909   \n",
       "45          0.039425                       7             0.090909   \n",
       "46          0.496391                      15             0.333333   \n",
       "47          0.026800                      19             1.400000   \n",
       "48          0.442214                      15             0.333333   \n",
       "49          0.046637                       6             0.000000   \n",
       "50          0.450733                      10             0.090909   \n",
       "51          0.013317                       9             0.090909   \n",
       "52          0.515571                      16             0.714286   \n",
       "53          0.037193                      14             0.500000   \n",
       "54          0.473362                      14             0.200000   \n",
       "55          0.021711                      12             0.200000   \n",
       "56          0.405300                      16             0.714286   \n",
       "57          0.016341                      14             0.714286   \n",
       "58          0.518312                      16             0.500000   \n",
       "59          0.035955                      13             0.333333   \n",
       "\n",
       "    Overlap no zeroes  \n",
       "0            0.333333  \n",
       "1            0.166667  \n",
       "2            0.200000  \n",
       "3            0.000000  \n",
       "4            0.375000  \n",
       "5            0.000000  \n",
       "6            0.200000  \n",
       "7            0.333333  \n",
       "8            1.000000  \n",
       "9            0.000000  \n",
       "10           0.571429  \n",
       "11           0.333333  \n",
       "12           0.090909  \n",
       "13           0.000000  \n",
       "14           0.090909  \n",
       "15           0.000000  \n",
       "16           0.142857  \n",
       "17           0.000000  \n",
       "18           0.800000  \n",
       "19           0.000000  \n",
       "20           0.111111  \n",
       "21           0.000000  \n",
       "22           0.000000  \n",
       "23           0.000000  \n",
       "24           0.000000  \n",
       "25           0.571429  \n",
       "26           0.142857  \n",
       "27           0.000000  \n",
       "28           0.000000  \n",
       "29           0.000000  \n",
       "30           0.200000  \n",
       "31           0.166667  \n",
       "32           0.000000  \n",
       "33           0.000000  \n",
       "34           0.285714  \n",
       "35           0.714286  \n",
       "36           0.222222  \n",
       "37           0.250000  \n",
       "38           1.000000  \n",
       "39           0.100000  \n",
       "40           0.000000  \n",
       "41           2.000000  \n",
       "42           0.250000  \n",
       "43           0.428571  \n",
       "44           0.100000  \n",
       "45           0.200000  \n",
       "46           0.333333  \n",
       "47           1.400000  \n",
       "48           0.333333  \n",
       "49           0.000000  \n",
       "50           0.125000  \n",
       "51           0.142857  \n",
       "52           0.833333  \n",
       "53           0.666667  \n",
       "54           0.200000  \n",
       "55           0.250000  \n",
       "56           0.833333  \n",
       "57           1.250000  \n",
       "58           0.500000  \n",
       "59           0.428571  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants\n",
    "kinds = [\"RNN\", \"RNNA\", \"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]  # Assuming you have these types\n",
    "min_lengths = [10, 10, 20, 20, 30, 30, 40, 40, 50, 50]\n",
    "max_lengths = [10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n",
    "num_reps = 31  # Number of repetitions for matrix generation\n",
    "\n",
    "# Placeholder for the results\n",
    "results = []\n",
    "\n",
    "# Gathering data\n",
    "for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "    for kind in kinds:\n",
    "        # Initialize an array to store matrices\n",
    "        matrices = []\n",
    "        for rep in range(num_reps):\n",
    "            filename = f\"./dataModel/model_{kind}_min_{min_len}_max_{max_len}_rep_{rep}.model\"\n",
    "            try:\n",
    "                # Load the model\n",
    "                model = torch.load(filename)\n",
    "\n",
    "            \n",
    "                s, t = generateTrainData(100, [min_len, min_len])  \n",
    "                S, H = shrinkingDecompositionInformation(model, 12, s, t.transpose(), numbers=[0, 1], whichTS=min_len - 1, dsLength=min_len)\n",
    "                M = removalIntoMatrix(S, 12, H)\n",
    "                matrices.append(M)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "        # Compute the average matrix\n",
    "        avg_M = np.mean(matrices, axis=0)\n",
    "\n",
    "        # Calculate metrics based on the average matrix\n",
    "        sum_abs = np.sum(np.abs(avg_M))\n",
    "        mean = np.mean(avg_M)\n",
    "        std = np.std(avg_M)\n",
    "        entropy_s = shannon_entropy(avg_M)\n",
    "        max_val = np.max(avg_M)\n",
    "        min_val = np.min(avg_M)\n",
    "        threshold, count_exceeding = kmeans_thresholding(avg_M)\n",
    "        binarized_matrix = np.where(avg_M.flatten() >= threshold, 1, 0).reshape(avg_M.shape)\n",
    "        count_11, count_00, count_mixed = calculate_counts(binarized_matrix)\n",
    "        ratio_11_00_mixed = count_11 / (count_mixed + count_00) if (count_mixed + count_00) != 0 else 0\n",
    "        ratio_11_mixed = count_11 / count_mixed if count_mixed != 0 else 0\n",
    "\n",
    "        metrics = {\n",
    "            \"Kind\": kind,\n",
    "            \"Min_Length\": min_len,\n",
    "            \"Max_Length\": max_len,\n",
    "            \"Sum of Absolute Values\": sum_abs,\n",
    "            \"Mean Value\": mean,\n",
    "            \"Standard Deviation\": std,\n",
    "            \"Maximum Value\": max_val,\n",
    "            \"Minimum Value\": min_val,\n",
    "            \"Shannon Entropy\": entropy_s,\n",
    "            \"Kmeans Threshold\": threshold,\n",
    "            \"Kmeans Nodes Exceeding\": count_exceeding,\n",
    "            \"Overlap with zeroes\": ratio_11_00_mixed,\n",
    "            \"Overlap no zeroes\": ratio_11_mixed\n",
    "        }\n",
    "\n",
    "        # Append this dictionary to the results list\n",
    "        results.append(metrics)\n",
    "\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "df1 = pd.DataFrame(results)\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"df1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
