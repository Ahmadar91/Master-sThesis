{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import numpy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib.pyplot import figure, subplots, imshow, xticks, yticks, title\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.cluster import mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "from statistics import mean\n",
    "from scipy.stats import entropy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time\n",
    "import copy\n",
    "import scipy.stats\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import ks_2samp\n",
    "import ast  \n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim**0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRNN(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Expansion layer to match CustomRNN\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        self.rnnLayer = nn.RNN(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        x = self.expand_layer(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        self.h0 = torch.Tensor(numpy.zeros((1, x.shape[0], self.hidden_dim)))\n",
    "        out, self.h0 = self.rnnLayer(x, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "        out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.Tensor(numpy.zeros((1, 1, self.hidden_dim)))\n",
    "            for i in range(x.shape[1]):\n",
    "                # Apply the expansion layer to each step\n",
    "                step_input = self.expand_layer(x[l][i].reshape((1, 1, self.inp)))\n",
    "                step_input = torch.tanh(step_input)\n",
    "\n",
    "                out, h0 = self.rnnLayer(step_input, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "            out = torch.tanh(out)\n",
    "            out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "            for i in range(x.shape[1]):\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "        return numpy.array(O), numpy.array(H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRNNWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetRNNWithAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = SelfAttention(inp)  # Assuming SelfAttention is defined elsewhere\n",
    "\n",
    "        # Expansion layer to match CustomRNN\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        self.rnnLayer = nn.RNN(self.hidden_dim, self.hidden_dim, batch_first=True, nonlinearity='tanh')\n",
    "        \n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(x)\n",
    "        print(attn_out.shape)\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        expanded_attn_out = self.expand_layer(attn_out)\n",
    "        print(expanded_attn_out.shape)\n",
    "        expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "        # RNN processing\n",
    "        h0 = torch.zeros(1, x.shape[0], self.hidden_dim)\n",
    "        rnn_out, _ = self.rnnLayer(expanded_attn_out, h0)\n",
    "        rnn_out = torch.tanh(rnn_out)\n",
    "\n",
    "        # Final output layer\n",
    "        out = torch.tanh(self.outputLayer(rnn_out[:, -1, :])).squeeze()\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                # Apply the expansion layer with tanh activation\n",
    "                expanded_attn_out = self.expand_layer(attn_out)\n",
    "                expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "                # RNN processing\n",
    "                out, h0 = self.rnnLayer(expanded_attn_out, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Expansion layer\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstmLayer = nn.LSTM(self.hidden_dim, int(self.hidden_dim/2), 1, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(int(self.hidden_dim/2), 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        x = self.expand_layer(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        # Initialize hidden and cell states\n",
    "        self.h0 = (torch.zeros(1, x.shape[0], int(self.hidden_dim/2)),\n",
    "                   torch.zeros(1, x.shape[0], int(self.hidden_dim/2)))\n",
    "\n",
    "        # LSTM processing\n",
    "        out, self.h0 = self.lstmLayer(x, self.h0)\n",
    "        out = torch.tanh(out)  # Apply tanh to the LSTM output if needed\n",
    "\n",
    "        # Concatenate hidden and cell states\n",
    "        hh = numpy.concatenate((self.h0[0].detach().numpy(), self.h0[1].detach().numpy()), 2)\n",
    "        self.hidden.append(hh)\n",
    "\n",
    "        # Final output layer with tanh activation\n",
    "        out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = (torch.zeros(1, 1, int(self.hidden_dim/2)),\n",
    "                  torch.zeros(1, 1, int(self.hidden_dim/2)))\n",
    "            for i in range(x.shape[1]):\n",
    "                # Apply the expansion layer to each step\n",
    "                step_input = self.expand_layer(x[l][i].reshape((1, 1, self.inp)))\n",
    "                step_input = torch.tanh(step_input)\n",
    "\n",
    "                out, h0 = self.lstmLayer(step_input, h0)\n",
    "                hh = numpy.concatenate((h0[0].detach().numpy().flatten(), h0[1].detach().numpy().flatten()))\n",
    "                H.append(hh.flatten())\n",
    "\n",
    "            out = torch.tanh(out)  # Apply tanh to the LSTM output if needed\n",
    "            out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "            for i in range(x.shape[1]):\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "        return numpy.array(O), numpy.array(H)\n",
    "    \n",
    "model = NetLSTM(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetLSTMWithAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = SelfAttention(inp)  # Assuming SelfAttention is defined elsewhere\n",
    "\n",
    "        # Expansion layer to match CustomRNN\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstmLayer = nn.LSTM(self.hidden_dim, int(self.hidden_dim/2), batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(int(self.hidden_dim/2), 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(x)\n",
    "\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        expanded_attn_out = self.expand_layer(attn_out)\n",
    "        expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "        # LSTM processing\n",
    "        h0 = (torch.zeros(1, x.shape[0], int(self.hidden_dim/2)),\n",
    "              torch.zeros(1, x.shape[0], int(self.hidden_dim/2)))\n",
    "        lstm_out, _ = self.lstmLayer(expanded_attn_out, h0)\n",
    "        lstm_out = torch.tanh(lstm_out)\n",
    "\n",
    "        # Final output layer\n",
    "        out = torch.tanh(self.outputLayer(lstm_out[:, -1, :])).squeeze()\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = (torch.zeros(1, 1, int(self.hidden_dim/2)),\n",
    "                  torch.zeros(1, 1, int(self.hidden_dim/2)))\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                # Apply the expansion layer with tanh activation\n",
    "                expanded_attn_out = self.expand_layer(attn_out)\n",
    "                expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "                # LSTM processing\n",
    "                out, h0 = self.lstmLayer(expanded_attn_out, h0)\n",
    "                H.append(torch.cat((h0[0].detach(), h0[1].detach()), 2).numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "    \n",
    "model = NetLSTMWithAttention(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetGRU(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetGRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Expansion layer\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # GRU layer\n",
    "        self.gruLayer = nn.GRU(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        x = self.expand_layer(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        # GRU processing\n",
    "        self.h0 = torch.zeros(1, x.shape[0], self.hidden_dim)\n",
    "        out, self.h0 = self.gruLayer(x, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "\n",
    "        # Final output layer with tanh activation\n",
    "        out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Apply the expansion layer to each step\n",
    "                step_input = self.expand_layer(x[l][i].reshape((1, 1, self.inp)))\n",
    "                step_input = torch.tanh(step_input)\n",
    "\n",
    "                # GRU processing\n",
    "                out, h0 = self.gruLayer(step_input, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)  # Apply tanh to the GRU output if needed\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "    \n",
    "model = NetGRU(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetGRUMWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetGRUMWithAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = SelfAttention(inp)  # Assuming SelfAttention is defined elsewhere\n",
    "\n",
    "        # Expansion layer\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # GRU layer\n",
    "        self.gruLayer = nn.GRU(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(x)\n",
    "\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        expanded_attn_out = self.expand_layer(attn_out)\n",
    "        expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "        # GRU processing\n",
    "        self.h0 = torch.zeros(1, x.shape[0], self.hidden_dim)\n",
    "        out, self.h0 = self.gruLayer(expanded_attn_out, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "\n",
    "        # Final output layer with tanh activation\n",
    "        out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                # Apply the expansion layer with tanh activation\n",
    "                expanded_attn_out = self.expand_layer(attn_out)\n",
    "                expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "                # GRU processing\n",
    "                out, h0 = self.gruLayer(expanded_attn_out, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)  # Apply tanh to the GRU output if needed\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "\n",
    "model = NetGRUMWithAttention(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation Top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generateTrainDataNoiseTop(num_samples, min_max, noise= 0.5 ):\n",
    "    s = []  # Sequences\n",
    "    t = []  # Labels\n",
    "    params = {\n",
    "        \"min_length\": min_max[0],\n",
    "        \"max_length\": min_max[1],\n",
    "        \"fill\": 0,\n",
    "        \"value_1\": -1,\n",
    "        \"value_2\": 1,\n",
    "        \"noise_level\": noise,\n",
    "    }\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        common_length = np.random.randint(params[\"min_length\"], params[\"max_length\"] + 1)\n",
    "\n",
    "        array_A = np.full(common_length, params[\"fill\"])\n",
    "        array_B = np.full(common_length, params[\"fill\"])\n",
    "        array_C = np.full(common_length, params[\"fill\"])\n",
    "\n",
    "        index_A = np.random.randint(0, common_length - 2)\n",
    "        index_B = np.random.randint(0, common_length - 2)\n",
    "\n",
    "        value_A = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_A[index_A] = value_A\n",
    "        value_B = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_B[index_B] = value_B\n",
    "\n",
    "        # Dynamically calculate noise level as 70% of the available positions\n",
    "        noise_level_A = int(params['noise_level'] * (common_length - index_A - 3)) # -3 accounts for the initial value and last two indices\n",
    "        noise_level_B = int(params['noise_level'] * (common_length - index_B - 3))\n",
    "\n",
    "        possible_indices_A = np.setdiff1d(np.arange(index_A + 1, common_length - 2), index_A)\n",
    "        possible_indices_B = np.setdiff1d(np.arange(index_B + 1, common_length - 2), index_B)\n",
    "\n",
    "        for array, possible_indices, noise_level in zip([array_A, array_B], [possible_indices_A, possible_indices_B], [noise_level_A, noise_level_B]):\n",
    "            if len(possible_indices) > 0 and noise_level > 0:\n",
    "                noise_indices = np.random.choice(possible_indices, size=min(len(possible_indices), noise_level), replace=False)\n",
    "                for index in noise_indices:\n",
    "                    array[index] = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "\n",
    "        value_C = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_C[-1] = value_C\n",
    "        array_C[-2] = value_C\n",
    "\n",
    "        mapped_value_A = 1 if value_A == params[\"value_2\"] else 0\n",
    "        mapped_value_B = 1 if value_B == params[\"value_2\"] else 0\n",
    "        if value_C == params[\"value_1\"]:\n",
    "            result = int(mapped_value_A != mapped_value_B)\n",
    "        else:\n",
    "            result = int(mapped_value_A == mapped_value_B)\n",
    "\n",
    "        label_value_A = params[\"value_2\"] if mapped_value_A == 1 else params[\"value_1\"]\n",
    "        label_value_B = params[\"value_2\"] if mapped_value_B == 1 else params[\"value_1\"]\n",
    "        results_XORNOR = params[\"value_2\"] if result == 1 else params[\"value_1\"]\n",
    "\n",
    "        label_arr = [label_value_A, label_value_B, results_XORNOR]\n",
    "\n",
    "        combined_array = np.vstack([array_A, array_B, array_C]).T\n",
    "        s.append(combined_array)\n",
    "        t.append(label_arr)\n",
    "\n",
    "    return s, np.array(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 1,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 1, -1,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [-1, -1,  0],\n",
       "         [-1, -1,  0],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0, -1],\n",
       "         [ 0,  0, -1]])],\n",
       " array([[ 1, -1,  1]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= generateTrainDataNoiseTop(1, [10,10])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/87/pnb9p7_n30s29gprj1gbqbbm0000gn/T/ipykernel_41135/2620812574.py:48: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  seq_tensor = torch.Tensor([seq])  # Add an extra dimension for batch\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 3])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNA, rep: 0, epoch: 1, acc: 0.45666682720184326, Loss 1.0258285421133042\n",
      "RNNA, rep: 0, epoch: 2, acc: 0.5099999904632568, Loss 1.0050234627723693\n",
      "RNNA, rep: 0, epoch: 3, acc: 0.5266665816307068, Loss 1.0015047585964203\n",
      "RNNA, rep: 0, epoch: 4, acc: 0.5499998927116394, Loss 0.9808446043729782\n",
      "RNNA, rep: 0, epoch: 5, acc: 0.5233333706855774, Loss 0.9945613420009614\n",
      "RNNA, rep: 0, epoch: 6, acc: 0.6266666650772095, Loss 0.9429814350605011\n",
      "RNNA, rep: 0, epoch: 7, acc: 0.5799999237060547, Loss 0.9614449128508568\n",
      "RNNA, rep: 0, epoch: 8, acc: 0.6333332657814026, Loss 0.9134543174505234\n",
      "RNNA, rep: 0, epoch: 9, acc: 0.6099998950958252, Loss 0.8558108365535736\n",
      "RNNA, rep: 0, epoch: 10, acc: 0.5766667723655701, Loss 0.8909306049346923\n",
      "RNNA, rep: 0, epoch: 11, acc: 0.6733334064483643, Loss 0.8202109023928642\n",
      "RNNA, rep: 0, epoch: 12, acc: 0.6333334445953369, Loss 0.8277156221866607\n",
      "RNNA, rep: 0, epoch: 13, acc: 0.6200000643730164, Loss 0.862674073278904\n",
      "RNNA, rep: 0, epoch: 14, acc: 0.6500000953674316, Loss 0.8677744075655938\n",
      "RNNA, rep: 0, epoch: 15, acc: 0.6566668152809143, Loss 0.7896239578723907\n",
      "RNNA, rep: 0, epoch: 16, acc: 0.6900001764297485, Loss 0.7687329858541488\n",
      "RNNA, rep: 0, epoch: 17, acc: 0.7066667079925537, Loss 0.7198433285951614\n",
      "RNNA, rep: 0, epoch: 18, acc: 0.6633334159851074, Loss 0.8187713357806206\n",
      "RNNA, rep: 0, epoch: 19, acc: 0.6766667366027832, Loss 0.7637684598565102\n",
      "RNNA, rep: 0, epoch: 20, acc: 0.716666579246521, Loss 0.720364762544632\n",
      "RNNA, rep: 0, epoch: 21, acc: 0.6933333873748779, Loss 0.6974153342843056\n",
      "RNNA, rep: 0, epoch: 22, acc: 0.7200002074241638, Loss 0.7365106165409088\n",
      "RNNA, rep: 0, epoch: 23, acc: 0.7066667079925537, Loss 0.7436460956931115\n",
      "RNNA, rep: 0, epoch: 24, acc: 0.7200000882148743, Loss 0.7403421106934548\n",
      "RNNA, rep: 0, epoch: 25, acc: 0.7000001668930054, Loss 0.74270758330822\n",
      "RNNA, rep: 0, epoch: 26, acc: 0.7000001668930054, Loss 0.7403669667243957\n",
      "RNNA, rep: 0, epoch: 27, acc: 0.6899999976158142, Loss 0.6908497449755668\n",
      "RNNA, rep: 0, epoch: 28, acc: 0.6700002551078796, Loss 0.7322891080379486\n",
      "RNNA, rep: 0, epoch: 29, acc: 0.7133333086967468, Loss 0.695421907901764\n",
      "RNNA, rep: 0, epoch: 30, acc: 0.7266668081283569, Loss 0.6986559712886811\n",
      "RNNA, rep: 0, epoch: 31, acc: 0.7333332896232605, Loss 0.6536201867461204\n",
      "RNNA, rep: 0, epoch: 32, acc: 0.7099999785423279, Loss 0.7002788192033768\n",
      "RNNA, rep: 0, epoch: 33, acc: 0.6666666269302368, Loss 0.8267480358481407\n",
      "RNNA, rep: 0, epoch: 34, acc: 0.7233332991600037, Loss 0.7305657505989075\n",
      "RNNA, rep: 0, epoch: 35, acc: 0.6933335065841675, Loss 0.7016174611449242\n",
      "RNNA, rep: 0, epoch: 36, acc: 0.6766666173934937, Loss 0.7690805447101593\n",
      "RNNA, rep: 0, epoch: 37, acc: 0.6966666579246521, Loss 0.7089394617080689\n",
      "RNNA, rep: 0, epoch: 38, acc: 0.6400002837181091, Loss 0.7527933418750763\n",
      "RNNA, rep: 0, epoch: 39, acc: 0.7599999308586121, Loss 0.6550162763893604\n",
      "RNNA, rep: 0, epoch: 40, acc: 0.7200000882148743, Loss 0.6758527947962284\n",
      "RNNA, rep: 0, epoch: 41, acc: 0.690000057220459, Loss 0.7349836188554764\n",
      "RNNA, rep: 0, epoch: 42, acc: 0.7100001573562622, Loss 0.6991564929485321\n",
      "RNNA, rep: 0, epoch: 43, acc: 0.7566667795181274, Loss 0.6588573408126831\n",
      "RNNA, rep: 0, epoch: 44, acc: 0.7233334183692932, Loss 0.6516290077567101\n",
      "RNNA, rep: 0, epoch: 45, acc: 0.7200002074241638, Loss 0.651149436533451\n",
      "RNNA, rep: 0, epoch: 46, acc: 0.7233334183692932, Loss 0.662055327296257\n",
      "RNNA, rep: 0, epoch: 47, acc: 0.7266669273376465, Loss 0.6236789599061012\n",
      "RNNA, rep: 0, epoch: 48, acc: 0.7000000476837158, Loss 0.6433001771569252\n",
      "RNNA, rep: 0, epoch: 49, acc: 0.763333261013031, Loss 0.6058337560296059\n",
      "RNNA, rep: 0, epoch: 50, acc: 0.7499999403953552, Loss 0.6273641481995582\n",
      "RNNA, rep: 0, epoch: 51, acc: 0.7300001382827759, Loss 0.6702581492066383\n",
      "RNNA, rep: 0, epoch: 52, acc: 0.7566666603088379, Loss 0.6694973483681679\n",
      "RNNA, rep: 0, epoch: 53, acc: 0.746666669845581, Loss 0.5927248294651508\n",
      "RNNA, rep: 0, epoch: 54, acc: 0.7399999499320984, Loss 0.6143254645168781\n",
      "RNNA, rep: 0, epoch: 55, acc: 0.7666667103767395, Loss 0.582901563346386\n",
      "RNNA, rep: 0, epoch: 56, acc: 0.7466667294502258, Loss 0.6048702232539653\n",
      "RNNA, rep: 0, epoch: 57, acc: 0.7100000977516174, Loss 0.6626384682953358\n",
      "RNNA, rep: 0, epoch: 58, acc: 0.7466667294502258, Loss 0.6209410567581654\n",
      "RNNA, rep: 0, epoch: 59, acc: 0.736666738986969, Loss 0.7019986176490783\n",
      "RNNA, rep: 0, epoch: 60, acc: 0.7033334374427795, Loss 0.727543513327837\n",
      "RNNA, rep: 0, epoch: 61, acc: 0.7433333396911621, Loss 0.6096153086423874\n",
      "RNNA, rep: 0, epoch: 62, acc: 0.7033334970474243, Loss 0.6291422599554062\n",
      "RNNA, rep: 0, epoch: 63, acc: 0.7199999094009399, Loss 0.6356616288423538\n",
      "RNNA, rep: 0, epoch: 64, acc: 0.7466667294502258, Loss 0.6006015533208847\n",
      "RNNA, rep: 0, epoch: 65, acc: 0.7266666889190674, Loss 0.6419177684187889\n",
      "RNNA, rep: 0, epoch: 66, acc: 0.7366667985916138, Loss 0.6282303822040558\n",
      "RNNA, rep: 0, epoch: 67, acc: 0.746666669845581, Loss 0.6098929882049561\n",
      "RNNA, rep: 0, epoch: 68, acc: 0.73333340883255, Loss 0.625045950114727\n",
      "RNNA, rep: 0, epoch: 69, acc: 0.7400000095367432, Loss 0.6170897924900055\n",
      "RNNA, rep: 0, epoch: 70, acc: 0.7466665506362915, Loss 0.6259150058031082\n",
      "RNNA, rep: 0, epoch: 71, acc: 0.7300000786781311, Loss 0.6385369235277176\n",
      "RNNA, rep: 0, epoch: 72, acc: 0.7266668677330017, Loss 0.6125981304049491\n",
      "RNNA, rep: 0, epoch: 73, acc: 0.7366666197776794, Loss 0.610270504951477\n",
      "RNNA, rep: 0, epoch: 74, acc: 0.7599999904632568, Loss 0.5858199045062065\n",
      "RNNA, rep: 0, epoch: 75, acc: 0.7533332109451294, Loss 0.5943429520726204\n",
      "RNNA, rep: 0, epoch: 76, acc: 0.7799999713897705, Loss 0.5583045573532581\n",
      "RNNA, rep: 0, epoch: 77, acc: 0.7766665816307068, Loss 0.5773622241616249\n",
      "RNNA, rep: 0, epoch: 78, acc: 0.7033334374427795, Loss 0.6380754402279853\n",
      "RNNA, rep: 0, epoch: 79, acc: 0.6966666579246521, Loss 0.6730531936883927\n",
      "RNNA, rep: 0, epoch: 80, acc: 0.7400000095367432, Loss 0.6582495245337486\n",
      "RNNA, rep: 0, epoch: 81, acc: 0.7566667795181274, Loss 0.6060420769453049\n",
      "RNNA, rep: 0, epoch: 82, acc: 0.7433334589004517, Loss 0.5906273019313812\n",
      "RNNA, rep: 0, epoch: 83, acc: 0.7699999809265137, Loss 0.5705441987514496\n",
      "RNNA, rep: 0, epoch: 84, acc: 0.8166666626930237, Loss 0.5238767613470554\n",
      "RNNA, rep: 0, epoch: 85, acc: 0.7733333110809326, Loss 0.5629694943130016\n",
      "RNNA, rep: 0, epoch: 86, acc: 0.7566666603088379, Loss 0.591249900907278\n",
      "RNNA, rep: 0, epoch: 87, acc: 0.7566667199134827, Loss 0.5825498765707016\n",
      "RNNA, rep: 0, epoch: 88, acc: 0.7600000500679016, Loss 0.5557818521559238\n",
      "RNNA, rep: 0, epoch: 89, acc: 0.7666667103767395, Loss 0.5461785371601582\n",
      "RNNA, rep: 0, epoch: 90, acc: 0.7433332800865173, Loss 0.6009772400557994\n",
      "RNNA, rep: 0, epoch: 91, acc: 0.763333261013031, Loss 0.5575343789160252\n",
      "RNNA, rep: 0, epoch: 92, acc: 0.736666738986969, Loss 0.6136821326613426\n",
      "RNNA, rep: 0, epoch: 93, acc: 0.7399998307228088, Loss 0.6447323207557202\n",
      "RNNA, rep: 0, epoch: 94, acc: 0.7633332014083862, Loss 0.5408861309289932\n",
      "RNNA, rep: 0, epoch: 95, acc: 0.7133334279060364, Loss 0.6914112813770771\n",
      "RNNA, rep: 0, epoch: 96, acc: 0.7566667199134827, Loss 0.5548544251918792\n",
      "RNNA, rep: 0, epoch: 97, acc: 0.7466668486595154, Loss 0.6021680605411529\n",
      "RNNA, rep: 0, epoch: 98, acc: 0.7733331918716431, Loss 0.5782882840931416\n",
      "RNNA, rep: 0, epoch: 99, acc: 0.7599999904632568, Loss 0.6471103049814702\n",
      "RNNA, rep: 0, epoch: 100, acc: 0.7333333492279053, Loss 0.5931073109805584\n",
      "RNNA, rep: 0, epoch: 101, acc: 0.7333333492279053, Loss 0.619570682644844\n",
      "RNNA, rep: 0, epoch: 102, acc: 0.763333261013031, Loss 0.5521868427097797\n",
      "RNNA, rep: 0, epoch: 103, acc: 0.75, Loss 0.5708659610152245\n",
      "RNNA, rep: 0, epoch: 104, acc: 0.7933332920074463, Loss 0.547912130355835\n",
      "RNNA, rep: 0, epoch: 105, acc: 0.7733333110809326, Loss 0.5228092530369759\n",
      "RNNA, rep: 0, epoch: 106, acc: 0.7433333396911621, Loss 0.6087733368575573\n",
      "RNNA, rep: 0, epoch: 107, acc: 0.8033332228660583, Loss 0.5756991985440254\n",
      "RNNA, rep: 0, epoch: 108, acc: 0.7633334398269653, Loss 0.5542544527351856\n",
      "RNNA, rep: 0, epoch: 109, acc: 0.746666669845581, Loss 0.6079630897939206\n",
      "RNNA, rep: 0, epoch: 110, acc: 0.7633333802223206, Loss 0.6206570695340633\n",
      "RNNA, rep: 0, epoch: 111, acc: 0.7500000596046448, Loss 0.6142699307203293\n",
      "RNNA, rep: 0, epoch: 112, acc: 0.7266665697097778, Loss 0.628052587211132\n",
      "RNNA, rep: 0, epoch: 113, acc: 0.7799999117851257, Loss 0.5463497264683247\n",
      "RNNA, rep: 0, epoch: 114, acc: 0.7400000691413879, Loss 0.5701839944720268\n",
      "RNNA, rep: 0, epoch: 115, acc: 0.7966666221618652, Loss 0.5137721234560013\n",
      "RNNA, rep: 0, epoch: 116, acc: 0.7633333802223206, Loss 0.5305466213822365\n",
      "RNNA, rep: 0, epoch: 117, acc: 0.7666666507720947, Loss 0.5722059942781925\n",
      "RNNA, rep: 0, epoch: 118, acc: 0.7166666984558105, Loss 0.6836073976755143\n",
      "RNNA, rep: 0, epoch: 119, acc: 0.7233333587646484, Loss 0.5988859784603119\n",
      "RNNA, rep: 0, epoch: 120, acc: 0.7233332991600037, Loss 0.6213325813412667\n",
      "RNNA, rep: 0, epoch: 121, acc: 0.763333261013031, Loss 0.5565756568312645\n",
      "RNNA, rep: 0, epoch: 122, acc: 0.7599999308586121, Loss 0.5527101384103298\n",
      "RNNA, rep: 0, epoch: 123, acc: 0.7599999904632568, Loss 0.5860381489992141\n",
      "RNNA, rep: 0, epoch: 124, acc: 0.756666898727417, Loss 0.5753502276539803\n",
      "RNNA, rep: 0, epoch: 125, acc: 0.7233333587646484, Loss 0.6046810621023178\n",
      "RNNA, rep: 0, epoch: 126, acc: 0.7633333802223206, Loss 0.5821864402294159\n",
      "RNNA, rep: 0, epoch: 127, acc: 0.7266668677330017, Loss 0.5743440452218056\n",
      "RNNA, rep: 0, epoch: 128, acc: 0.763333261013031, Loss 0.5538216438889504\n",
      "RNNA, rep: 0, epoch: 129, acc: 0.7366667985916138, Loss 0.6142317393422126\n",
      "RNNA, rep: 0, epoch: 130, acc: 0.7566664218902588, Loss 0.6094672071933747\n",
      "RNNA, rep: 0, epoch: 131, acc: 0.7599999904632568, Loss 0.5855925971269608\n",
      "RNNA, rep: 0, epoch: 132, acc: 0.7899999022483826, Loss 0.5222496089339256\n",
      "RNNA, rep: 0, epoch: 133, acc: 0.7900000214576721, Loss 0.5420448344945907\n",
      "RNNA, rep: 0, epoch: 134, acc: 0.7933332920074463, Loss 0.4745087976753712\n",
      "RNNA, rep: 0, epoch: 135, acc: 0.7733334302902222, Loss 0.5788000458478928\n",
      "RNNA, rep: 0, epoch: 136, acc: 0.7500001788139343, Loss 0.5938438229262829\n",
      "RNNA, rep: 0, epoch: 137, acc: 0.7900000214576721, Loss 0.5395943586528301\n",
      "RNNA, rep: 0, epoch: 138, acc: 0.7666666507720947, Loss 0.5447080411016941\n",
      "RNNA, rep: 0, epoch: 139, acc: 0.7333332896232605, Loss 0.5842934572696685\n",
      "RNNA, rep: 0, epoch: 140, acc: 0.7599999308586121, Loss 0.5543528753519058\n",
      "RNNA, rep: 0, epoch: 141, acc: 0.7533334493637085, Loss 0.5693940018117428\n",
      "RNNA, rep: 0, epoch: 142, acc: 0.7266666889190674, Loss 0.6269926771521568\n",
      "RNNA, rep: 0, epoch: 143, acc: 0.7566665410995483, Loss 0.5560423457622528\n",
      "RNNA, rep: 0, epoch: 144, acc: 0.7433332800865173, Loss 0.5303916421532631\n",
      "RNNA, rep: 0, epoch: 145, acc: 0.7766666412353516, Loss 0.5632823231816292\n",
      "RNNA, rep: 0, epoch: 146, acc: 0.7833333611488342, Loss 0.4651080544292927\n",
      "RNNA, rep: 0, epoch: 147, acc: 0.7633333802223206, Loss 0.5532774659991264\n",
      "RNNA, rep: 0, epoch: 148, acc: 0.7400001287460327, Loss 0.5416257983446121\n",
      "RNNA, rep: 0, epoch: 149, acc: 0.736666738986969, Loss 0.6003048224747181\n",
      "RNNA, rep: 0, epoch: 150, acc: 0.7600000500679016, Loss 0.5506852233409881\n",
      "RNNA, rep: 0, epoch: 151, acc: 0.7766664624214172, Loss 0.5445667465031147\n",
      "RNNA, rep: 0, epoch: 152, acc: 0.7433333396911621, Loss 0.5680577422678471\n",
      "RNNA, rep: 0, epoch: 153, acc: 0.7433334589004517, Loss 0.5438371750712395\n",
      "RNNA, rep: 0, epoch: 154, acc: 0.7400000095367432, Loss 0.585367681980133\n",
      "RNNA, rep: 0, epoch: 155, acc: 0.7433332800865173, Loss 0.5953842914104461\n",
      "RNNA, rep: 0, epoch: 156, acc: 0.7699999809265137, Loss 0.525025879740715\n",
      "RNNA, rep: 0, epoch: 157, acc: 0.8233332633972168, Loss 0.49211885660886767\n",
      "RNNA, rep: 0, epoch: 158, acc: 0.7633333802223206, Loss 0.5700466084480286\n",
      "RNNA, rep: 0, epoch: 159, acc: 0.7833333015441895, Loss 0.5303438070416451\n",
      "RNNA, rep: 0, epoch: 160, acc: 0.7766665816307068, Loss 0.5444636645913125\n",
      "RNNA, rep: 0, epoch: 161, acc: 0.7866666316986084, Loss 0.5473173987865448\n",
      "RNNA, rep: 0, epoch: 162, acc: 0.7566667199134827, Loss 0.5758164076507092\n",
      "RNNA, rep: 0, epoch: 163, acc: 0.76666659116745, Loss 0.5483268789947033\n",
      "RNNA, rep: 0, epoch: 164, acc: 0.7700001001358032, Loss 0.563803653717041\n",
      "RNNA, rep: 0, epoch: 165, acc: 0.763333261013031, Loss 0.555300185084343\n",
      "RNNA, rep: 0, epoch: 166, acc: 0.8133334517478943, Loss 0.5297590464353561\n",
      "RNNA, rep: 0, epoch: 167, acc: 0.7866663932800293, Loss 0.5032299484312535\n",
      "RNNA, rep: 0, epoch: 168, acc: 0.7699997425079346, Loss 0.5325878703594208\n",
      "RNNA, rep: 0, epoch: 169, acc: 0.7499997615814209, Loss 0.5752946323156357\n",
      "RNNA, rep: 0, epoch: 170, acc: 0.7500000596046448, Loss 0.6131347349286079\n",
      "RNNA, rep: 0, epoch: 171, acc: 0.7866666316986084, Loss 0.5257637295126915\n",
      "RNNA, rep: 0, epoch: 172, acc: 0.7433335185050964, Loss 0.5808241555094719\n",
      "RNNA, rep: 0, epoch: 173, acc: 0.7966665625572205, Loss 0.5190199384093285\n",
      "RNNA, rep: 0, epoch: 174, acc: 0.8066666126251221, Loss 0.5199991297721863\n",
      "RNNA, rep: 0, epoch: 175, acc: 0.7533332705497742, Loss 0.5673726662993431\n",
      "RNNA, rep: 0, epoch: 176, acc: 0.7799997925758362, Loss 0.5688656833767891\n",
      "RNNA, rep: 0, epoch: 177, acc: 0.7600000500679016, Loss 0.527194798886776\n",
      "RNNA, rep: 0, epoch: 178, acc: 0.8133333325386047, Loss 0.5209500673413276\n",
      "RNNA, rep: 0, epoch: 179, acc: 0.8133333325386047, Loss 0.4668667598068714\n",
      "RNNA, rep: 0, epoch: 180, acc: 0.7699999809265137, Loss 0.5135700103640556\n",
      "RNNA, rep: 0, epoch: 181, acc: 0.7966666221618652, Loss 0.48532371640205385\n",
      "RNNA, rep: 0, epoch: 182, acc: 0.7600001692771912, Loss 0.5581361779570579\n",
      "RNNA, rep: 0, epoch: 183, acc: 0.7566666603088379, Loss 0.5821527701616287\n",
      "RNNA, rep: 0, epoch: 184, acc: 0.7800000905990601, Loss 0.532849850654602\n",
      "RNNA, rep: 0, epoch: 185, acc: 0.7466667294502258, Loss 0.608820615708828\n",
      "RNNA, rep: 0, epoch: 186, acc: 0.7599999308586121, Loss 0.5621816956996918\n",
      "RNNA, rep: 0, epoch: 187, acc: 0.7633333802223206, Loss 0.5150806805491448\n",
      "RNNA, rep: 0, epoch: 188, acc: 0.7900000810623169, Loss 0.5240877316892147\n",
      "RNNA, rep: 0, epoch: 189, acc: 0.7666667103767395, Loss 0.5361423516273498\n",
      "RNNA, rep: 0, epoch: 190, acc: 0.7666668891906738, Loss 0.5347641256451606\n",
      "RNNA, rep: 0, epoch: 191, acc: 0.7766664028167725, Loss 0.5462027856707573\n",
      "RNNA, rep: 0, epoch: 192, acc: 0.8033331036567688, Loss 0.51337328851223\n",
      "RNNA, rep: 0, epoch: 193, acc: 0.7700001001358032, Loss 0.5377232193946838\n",
      "RNNA, rep: 0, epoch: 194, acc: 0.8066664934158325, Loss 0.4795450535416603\n",
      "RNNA, rep: 0, epoch: 195, acc: 0.7833331823348999, Loss 0.5085544127225876\n",
      "RNNA, rep: 0, epoch: 196, acc: 0.8033334612846375, Loss 0.5172607888281345\n",
      "RNNA, rep: 0, epoch: 197, acc: 0.7833333015441895, Loss 0.4888033197820187\n",
      "RNNA, rep: 0, epoch: 198, acc: 0.8033333420753479, Loss 0.5182332365214825\n",
      "RNNA, rep: 0, epoch: 199, acc: 0.7999998331069946, Loss 0.4928689190745354\n",
      "RNNA, rep: 0, epoch: 200, acc: 0.8166665434837341, Loss 0.4747106355428696\n",
      "RNNA, rep: 0, epoch: 201, acc: 0.7533335089683533, Loss 0.5567265681922435\n",
      "RNNA, rep: 0, epoch: 202, acc: 0.81333327293396, Loss 0.4624673482775688\n",
      "RNNA, rep: 0, epoch: 203, acc: 0.7933332324028015, Loss 0.570944817662239\n",
      "RNNA, rep: 0, epoch: 204, acc: 0.7666667699813843, Loss 0.5045599119365215\n",
      "RNNA, rep: 0, epoch: 205, acc: 0.8266664147377014, Loss 0.4160597009956837\n",
      "RNNA, rep: 0, epoch: 206, acc: 0.8166666626930237, Loss 0.5025570955872536\n",
      "RNNA, rep: 0, epoch: 207, acc: 0.8100000619888306, Loss 0.4932223975658417\n",
      "RNNA, rep: 0, epoch: 208, acc: 0.7766665816307068, Loss 0.5237284044921399\n",
      "RNNA, rep: 0, epoch: 209, acc: 0.81333327293396, Loss 0.47042860701680184\n",
      "RNNA, rep: 0, epoch: 210, acc: 0.8066667318344116, Loss 0.5038707506656647\n",
      "RNNA, rep: 0, epoch: 211, acc: 0.8199999332427979, Loss 0.5196816898882389\n",
      "RNNA, rep: 0, epoch: 212, acc: 0.7999999523162842, Loss 0.471764979660511\n",
      "RNNA, rep: 0, epoch: 213, acc: 0.7966668009757996, Loss 0.5361981584131718\n",
      "RNNA, rep: 0, epoch: 214, acc: 0.8199998736381531, Loss 0.45279097437858584\n",
      "RNNA, rep: 0, epoch: 215, acc: 0.7933332324028015, Loss 0.5609678094089031\n",
      "RNNA, rep: 0, epoch: 216, acc: 0.7966667413711548, Loss 0.5166810262203216\n",
      "RNNA, rep: 0, epoch: 217, acc: 0.7966665029525757, Loss 0.46959632441401483\n",
      "RNNA, rep: 0, epoch: 218, acc: 0.789999783039093, Loss 0.5167290610074997\n",
      "RNNA, rep: 0, epoch: 219, acc: 0.7966666221618652, Loss 0.4994981952011585\n",
      "RNNA, rep: 0, epoch: 220, acc: 0.8166665434837341, Loss 0.491975940912962\n",
      "RNNA, rep: 0, epoch: 221, acc: 0.7999998331069946, Loss 0.515934404283762\n",
      "RNNA, rep: 0, epoch: 222, acc: 0.7799999117851257, Loss 0.5139135138690472\n",
      "RNNA, rep: 0, epoch: 223, acc: 0.8066666126251221, Loss 0.4771363839507103\n",
      "RNNA, rep: 0, epoch: 224, acc: 0.8033332824707031, Loss 0.4812195684015751\n",
      "RNNA, rep: 0, epoch: 225, acc: 0.8166666626930237, Loss 0.49046075358986857\n",
      "RNNA, rep: 0, epoch: 226, acc: 0.779999852180481, Loss 0.5326220014691353\n",
      "RNNA, rep: 0, epoch: 227, acc: 0.8299999833106995, Loss 0.43628589108586313\n",
      "RNNA, rep: 0, epoch: 228, acc: 0.7733334302902222, Loss 0.5428601935505867\n",
      "RNNA, rep: 0, epoch: 229, acc: 0.8299999833106995, Loss 0.4668542532622814\n",
      "RNNA, rep: 0, epoch: 230, acc: 0.8066666126251221, Loss 0.497813106328249\n",
      "RNNA, rep: 0, epoch: 231, acc: 0.7766667008399963, Loss 0.5524152076244354\n",
      "RNNA, rep: 0, epoch: 232, acc: 0.7733333110809326, Loss 0.5339277090132236\n",
      "RNNA, rep: 0, epoch: 233, acc: 0.7966665625572205, Loss 0.4780113910138607\n",
      "RNNA, rep: 0, epoch: 234, acc: 0.8166667222976685, Loss 0.5082171593606472\n",
      "RNNA, rep: 0, epoch: 235, acc: 0.8199999332427979, Loss 0.48209269508719443\n",
      "RNNA, rep: 0, epoch: 236, acc: 0.84333336353302, Loss 0.48208693772554395\n",
      "RNNA, rep: 0, epoch: 237, acc: 0.779999852180481, Loss 0.5277180092036724\n",
      "RNNA, rep: 0, epoch: 238, acc: 0.7866666913032532, Loss 0.4712778235971928\n",
      "RNNA, rep: 0, epoch: 239, acc: 0.8133334517478943, Loss 0.5035256616771221\n",
      "RNNA, rep: 0, epoch: 240, acc: 0.7933333516120911, Loss 0.5178268288075923\n",
      "RNNA, rep: 0, epoch: 241, acc: 0.800000011920929, Loss 0.49139963075518606\n",
      "RNNA, rep: 0, epoch: 242, acc: 0.8033332824707031, Loss 0.49149847105145456\n",
      "RNNA, rep: 0, epoch: 243, acc: 0.8399999141693115, Loss 0.44378647655248643\n",
      "RNNA, rep: 0, epoch: 244, acc: 0.8166666626930237, Loss 0.5118701812624932\n",
      "RNNA, rep: 0, epoch: 245, acc: 0.8266664743423462, Loss 0.473345392793417\n",
      "RNNA, rep: 0, epoch: 246, acc: 0.8533332943916321, Loss 0.46916623279452324\n",
      "RNNA, rep: 0, epoch: 247, acc: 0.7766666412353516, Loss 0.5281307880580425\n",
      "RNNA, rep: 0, epoch: 248, acc: 0.8033333420753479, Loss 0.5465894873440266\n",
      "RNNA, rep: 0, epoch: 249, acc: 0.7999999523162842, Loss 0.5410155576467514\n",
      "RNNA, rep: 0, epoch: 250, acc: 0.7733331322669983, Loss 0.5358452136814594\n",
      "RNNA, rep: 0, epoch: 251, acc: 0.8033332228660583, Loss 0.48475406095385554\n",
      "RNNA, rep: 0, epoch: 252, acc: 0.8199999928474426, Loss 0.4561432532966137\n",
      "RNNA, rep: 0, epoch: 253, acc: 0.8166665434837341, Loss 0.4812425422668457\n",
      "RNNA, rep: 0, epoch: 254, acc: 0.8299999833106995, Loss 0.44876426324248314\n",
      "RNNA, rep: 0, epoch: 255, acc: 0.763333261013031, Loss 0.5397571520507336\n",
      "RNNA, rep: 0, epoch: 256, acc: 0.8199999332427979, Loss 0.46872130751609803\n",
      "RNNA, rep: 0, epoch: 257, acc: 0.8233332633972168, Loss 0.495694577395916\n",
      "RNNA, rep: 0, epoch: 258, acc: 0.7800001502037048, Loss 0.5178357554972172\n",
      "RNNA, rep: 0, epoch: 259, acc: 0.8766665458679199, Loss 0.41374892368912697\n",
      "RNNA, rep: 0, epoch: 260, acc: 0.8833333849906921, Loss 0.4116367293894291\n",
      "RNNA, rep: 0, epoch: 261, acc: 0.8466665148735046, Loss 0.4477913038432598\n",
      "RNNA, rep: 0, epoch: 262, acc: 0.809999942779541, Loss 0.46562793910503386\n",
      "RNNA, rep: 0, epoch: 263, acc: 0.8500000834465027, Loss 0.43334978714585304\n",
      "RNNA, rep: 0, epoch: 264, acc: 0.8466665744781494, Loss 0.4610410375893116\n",
      "RNNA, rep: 0, epoch: 265, acc: 0.8299999833106995, Loss 0.4526410408318043\n",
      "RNNA, rep: 0, epoch: 266, acc: 0.8499999046325684, Loss 0.42333284467458726\n",
      "RNNA, rep: 0, epoch: 267, acc: 0.8399998545646667, Loss 0.44178807727992536\n",
      "RNNA, rep: 0, epoch: 268, acc: 0.8166665434837341, Loss 0.4999320228397846\n",
      "RNNA, rep: 0, epoch: 269, acc: 0.8100000023841858, Loss 0.5424696872383357\n",
      "RNNA, rep: 0, epoch: 270, acc: 0.8333332538604736, Loss 0.43735248811542987\n",
      "RNNA, rep: 0, epoch: 271, acc: 0.8233332633972168, Loss 0.46714971676468847\n",
      "RNNA, rep: 0, epoch: 272, acc: 0.8266665935516357, Loss 0.44389586329460146\n",
      "RNNA, rep: 0, epoch: 273, acc: 0.8433333039283752, Loss 0.46456122525036336\n",
      "RNNA, rep: 0, epoch: 274, acc: 0.800000011920929, Loss 0.5260852802544832\n",
      "RNNA, rep: 0, epoch: 275, acc: 0.8366667032241821, Loss 0.4459836693108082\n",
      "RNNA, rep: 0, epoch: 276, acc: 0.8033331036567688, Loss 0.48553216688334944\n",
      "RNNA, rep: 0, epoch: 277, acc: 0.8499999046325684, Loss 0.4237871654331684\n",
      "RNNA, rep: 0, epoch: 278, acc: 0.8166667222976685, Loss 0.4580701740831137\n",
      "RNNA, rep: 0, epoch: 279, acc: 0.8266666531562805, Loss 0.4448480513691902\n",
      "RNNA, rep: 0, epoch: 280, acc: 0.81333327293396, Loss 0.5033286886662245\n",
      "RNNA, rep: 0, epoch: 281, acc: 0.8533333539962769, Loss 0.43661843203008177\n",
      "RNNA, rep: 0, epoch: 282, acc: 0.8399999737739563, Loss 0.4825264321267605\n",
      "RNNA, rep: 0, epoch: 283, acc: 0.8433333039283752, Loss 0.4564104361832142\n",
      "RNNA, rep: 0, epoch: 284, acc: 0.8166667222976685, Loss 0.4890038998425007\n",
      "RNNA, rep: 0, epoch: 285, acc: 0.8166664838790894, Loss 0.46568532012403013\n",
      "RNNA, rep: 0, epoch: 286, acc: 0.8700000047683716, Loss 0.41431477524340155\n",
      "RNNA, rep: 0, epoch: 287, acc: 0.8233332633972168, Loss 0.48521264977753165\n",
      "RNNA, rep: 0, epoch: 288, acc: 0.8133333325386047, Loss 0.5807345117628574\n",
      "RNNA, rep: 0, epoch: 289, acc: 0.8366667032241821, Loss 0.4715851406753063\n",
      "RNNA, rep: 0, epoch: 290, acc: 0.8533333539962769, Loss 0.4106413895636797\n",
      "RNNA, rep: 0, epoch: 291, acc: 0.8399998545646667, Loss 0.4159993918985128\n",
      "RNNA, rep: 0, epoch: 292, acc: 0.8466668128967285, Loss 0.4608926697820425\n",
      "RNNA, rep: 0, epoch: 293, acc: 0.8633333444595337, Loss 0.4458128359168768\n",
      "RNNA, rep: 0, epoch: 294, acc: 0.8566667437553406, Loss 0.4196422600001097\n",
      "RNNA, rep: 0, epoch: 295, acc: 0.8666664958000183, Loss 0.4188036975264549\n",
      "RNNA, rep: 0, epoch: 296, acc: 0.8499999046325684, Loss 0.40839547738432885\n",
      "RNNA, rep: 0, epoch: 297, acc: 0.8199999928474426, Loss 0.4734412671625614\n",
      "RNNA, rep: 0, epoch: 298, acc: 0.8433331847190857, Loss 0.4580900452658534\n",
      "RNNA, rep: 0, epoch: 299, acc: 0.8466666340827942, Loss 0.45906235568225384\n",
      "RNNA, rep: 0, epoch: 300, acc: 0.84333336353302, Loss 0.45756320357322694\n",
      "RNNA, rep: 0, epoch: 301, acc: 0.8299999833106995, Loss 0.4915370979905129\n",
      "RNNA, rep: 0, epoch: 302, acc: 0.8466666340827942, Loss 0.4587130367010832\n",
      "RNNA, rep: 0, epoch: 303, acc: 0.7966666221618652, Loss 0.5302172952890396\n",
      "RNNA, rep: 0, epoch: 304, acc: 0.8433331847190857, Loss 0.4482559494674206\n",
      "RNNA, rep: 0, epoch: 305, acc: 0.8233332633972168, Loss 0.4916325970739126\n",
      "RNNA, rep: 0, epoch: 306, acc: 0.8299997448921204, Loss 0.43470431983470914\n",
      "RNNA, rep: 0, epoch: 307, acc: 0.8666667342185974, Loss 0.4330121082440019\n",
      "RNNA, rep: 0, epoch: 308, acc: 0.8433333039283752, Loss 0.43117917351424695\n",
      "RNNA, rep: 0, epoch: 309, acc: 0.8466666340827942, Loss 0.43440659426152706\n",
      "RNNA, rep: 0, epoch: 310, acc: 0.9033331871032715, Loss 0.36851041309535504\n",
      "RNNA, rep: 0, epoch: 311, acc: 0.7999998331069946, Loss 0.4968825487792492\n",
      "RNNA, rep: 0, epoch: 312, acc: 0.7999999523162842, Loss 0.5376333598047495\n",
      "RNNA, rep: 0, epoch: 313, acc: 0.8766664862632751, Loss 0.4048245257884264\n",
      "RNNA, rep: 0, epoch: 314, acc: 0.856666624546051, Loss 0.39234942726790906\n",
      "RNNA, rep: 0, epoch: 315, acc: 0.8099998235702515, Loss 0.4973038855567575\n",
      "RNNA, rep: 0, epoch: 316, acc: 0.8399998545646667, Loss 0.4083103859424591\n",
      "RNNA, rep: 0, epoch: 317, acc: 0.81333327293396, Loss 0.4452472557127476\n",
      "RNNA, rep: 0, epoch: 318, acc: 0.8666666150093079, Loss 0.4014265611767769\n",
      "RNNA, rep: 0, epoch: 319, acc: 0.8733333349227905, Loss 0.3949152395129204\n",
      "RNNA, rep: 0, epoch: 320, acc: 0.8499999046325684, Loss 0.4305571297928691\n",
      "RNNA, rep: 0, epoch: 321, acc: 0.8399999141693115, Loss 0.4252487519010901\n",
      "RNNA, rep: 0, epoch: 322, acc: 0.8699998259544373, Loss 0.38451412558555603\n",
      "RNNA, rep: 0, epoch: 323, acc: 0.8999999165534973, Loss 0.3747398693487048\n",
      "RNNA, rep: 0, epoch: 324, acc: 0.8366667032241821, Loss 0.5032701202854514\n",
      "RNNA, rep: 0, epoch: 325, acc: 0.8500000834465027, Loss 0.41411440942436456\n",
      "RNNA, rep: 0, epoch: 326, acc: 0.8899999260902405, Loss 0.36701528061181304\n",
      "RNNA, rep: 0, epoch: 327, acc: 0.8533331155776978, Loss 0.41968653243035076\n",
      "RNNA, rep: 0, epoch: 328, acc: 0.8833334445953369, Loss 0.3729948664456606\n",
      "RNNA, rep: 0, epoch: 329, acc: 0.8866667151451111, Loss 0.3579385741427541\n",
      "RNNA, rep: 0, epoch: 330, acc: 0.8566667437553406, Loss 0.427887819558382\n",
      "RNNA, rep: 0, epoch: 331, acc: 0.8333332538604736, Loss 0.4880105483531952\n",
      "RNNA, rep: 0, epoch: 332, acc: 0.8966666460037231, Loss 0.31989636810496447\n",
      "RNNA, rep: 0, epoch: 333, acc: 0.8333332538604736, Loss 0.465683180000633\n",
      "RNNA, rep: 0, epoch: 334, acc: 0.8266665935516357, Loss 0.4684137330949307\n",
      "RNNA, rep: 0, epoch: 335, acc: 0.8699999451637268, Loss 0.3765499347448349\n",
      "RNNA, rep: 0, epoch: 336, acc: 0.8533334136009216, Loss 0.42576762087643144\n",
      "RNNA, rep: 0, epoch: 337, acc: 0.8600000143051147, Loss 0.4366679769195616\n",
      "RNNA, rep: 0, epoch: 338, acc: 0.8599998950958252, Loss 0.39956866135820746\n",
      "RNNA, rep: 0, epoch: 339, acc: 0.8633332252502441, Loss 0.4457085536792874\n",
      "RNNA, rep: 0, epoch: 340, acc: 0.8866665363311768, Loss 0.36554405208677054\n",
      "RNNA, rep: 0, epoch: 341, acc: 0.8966666460037231, Loss 0.3657410479895771\n",
      "RNNA, rep: 0, epoch: 342, acc: 0.8700000047683716, Loss 0.3881638466194272\n",
      "RNNA, rep: 0, epoch: 343, acc: 0.8433333039283752, Loss 0.41438097927719353\n",
      "RNNA, rep: 0, epoch: 344, acc: 0.8333332538604736, Loss 0.482488761190325\n",
      "RNNA, rep: 0, epoch: 345, acc: 0.8833332657814026, Loss 0.3732678573206067\n",
      "RNNA, rep: 0, epoch: 346, acc: 0.9200000762939453, Loss 0.28826500380411746\n",
      "RNNA, rep: 0, epoch: 347, acc: 0.8666665554046631, Loss 0.3781180716864765\n",
      "RNNA, rep: 0, epoch: 348, acc: 0.8833334445953369, Loss 0.36867706596851346\n",
      "RNNA, rep: 0, epoch: 349, acc: 0.8233334422111511, Loss 0.49829017696902156\n",
      "RNNA, rep: 0, epoch: 350, acc: 0.8966667056083679, Loss 0.3124409038946033\n",
      "RNNA, rep: 0, epoch: 351, acc: 0.8566668033599854, Loss 0.4355860568955541\n",
      "RNNA, rep: 0, epoch: 352, acc: 0.8599998950958252, Loss 0.4028085807431489\n",
      "RNNA, rep: 0, epoch: 353, acc: 0.8599998950958252, Loss 0.3875070118624717\n",
      "RNNA, rep: 0, epoch: 354, acc: 0.81333327293396, Loss 0.5210297938622535\n",
      "RNNA, rep: 0, epoch: 355, acc: 0.8700000047683716, Loss 0.39831598559394477\n",
      "RNNA, rep: 0, epoch: 356, acc: 0.8999999165534973, Loss 0.30377004530280827\n",
      "RNNA, rep: 0, epoch: 357, acc: 0.8866665363311768, Loss 0.36528179749846457\n",
      "RNNA, rep: 0, epoch: 358, acc: 0.8766666650772095, Loss 0.37616840057075024\n",
      "RNNA, rep: 0, epoch: 359, acc: 0.8633332252502441, Loss 0.4034862782806158\n",
      "RNNA, rep: 0, epoch: 360, acc: 0.8499998450279236, Loss 0.4266224760375917\n",
      "RNNA, rep: 0, epoch: 361, acc: 0.8766667246818542, Loss 0.364436597712338\n",
      "RNNA, rep: 0, epoch: 362, acc: 0.8400000929832458, Loss 0.47989300871267915\n",
      "RNNA, rep: 0, epoch: 363, acc: 0.8933331966400146, Loss 0.34149084366858007\n",
      "RNNA, rep: 0, epoch: 364, acc: 0.8833334445953369, Loss 0.3476021515764296\n",
      "RNNA, rep: 0, epoch: 365, acc: 0.8599998950958252, Loss 0.42842819036915897\n",
      "RNNA, rep: 0, epoch: 366, acc: 0.8500000834465027, Loss 0.41098756158724425\n",
      "RNNA, rep: 0, epoch: 367, acc: 0.81333327293396, Loss 0.5458812241349369\n",
      "RNNA, rep: 0, epoch: 368, acc: 0.8600000739097595, Loss 0.3692100251931697\n",
      "RNNA, rep: 0, epoch: 369, acc: 0.9066666960716248, Loss 0.31469738591462376\n",
      "RNNA, rep: 0, epoch: 370, acc: 0.8666667342185974, Loss 0.3643732269667089\n",
      "RNNA, rep: 0, epoch: 371, acc: 0.8733331561088562, Loss 0.36091021249070765\n",
      "RNNA, rep: 0, epoch: 372, acc: 0.8199999928474426, Loss 0.4747820406593382\n",
      "RNNA, rep: 0, epoch: 373, acc: 0.863333523273468, Loss 0.3945162244513631\n",
      "RNNA, rep: 0, epoch: 374, acc: 0.8500000238418579, Loss 0.3999462722335011\n",
      "RNNA, rep: 0, epoch: 375, acc: 0.8899999260902405, Loss 0.32883763540536165\n",
      "RNNA, rep: 0, epoch: 376, acc: 0.8666667342185974, Loss 0.39194655729457734\n",
      "RNNA, rep: 0, epoch: 377, acc: 0.8866666555404663, Loss 0.3284672512486577\n",
      "RNNA, rep: 0, epoch: 378, acc: 0.8700001239776611, Loss 0.3861648661736399\n",
      "RNNA, rep: 0, epoch: 379, acc: 0.873333215713501, Loss 0.33663804274052384\n",
      "RNNA, rep: 0, epoch: 380, acc: 0.8699999451637268, Loss 0.4077102511934936\n",
      "RNNA, rep: 0, epoch: 381, acc: 0.833333432674408, Loss 0.444051760584116\n",
      "RNNA, rep: 0, epoch: 382, acc: 0.90666663646698, Loss 0.29745009311474857\n",
      "RNNA, rep: 0, epoch: 383, acc: 0.8733333349227905, Loss 0.3616660139989108\n",
      "RNNA, rep: 0, epoch: 384, acc: 0.8599998950958252, Loss 0.40409104683436453\n",
      "RNNA, rep: 0, epoch: 385, acc: 0.8933334350585938, Loss 0.34925700136460364\n",
      "RNNA, rep: 0, epoch: 386, acc: 0.8466668128967285, Loss 0.4279668216314167\n",
      "RNNA, rep: 0, epoch: 387, acc: 0.8366665840148926, Loss 0.42769814119674265\n",
      "RNNA, rep: 0, epoch: 388, acc: 0.8633334636688232, Loss 0.409972630077973\n",
      "RNNA, rep: 0, epoch: 389, acc: 0.8633333444595337, Loss 0.3910532675124705\n",
      "RNNA, rep: 0, epoch: 390, acc: 0.8833332061767578, Loss 0.3456127954646945\n",
      "RNNA, rep: 0, epoch: 391, acc: 0.8833332657814026, Loss 0.3434619979839772\n",
      "RNNA, rep: 0, epoch: 392, acc: 0.9166667461395264, Loss 0.31578299982473257\n",
      "RNNA, rep: 0, epoch: 393, acc: 0.8766667246818542, Loss 0.3570526159275323\n",
      "RNNA, rep: 0, epoch: 394, acc: 0.8533334136009216, Loss 0.42684379473328593\n",
      "RNNA, rep: 0, epoch: 395, acc: 0.8766666650772095, Loss 0.33679344441741704\n",
      "RNNA, rep: 0, epoch: 396, acc: 0.8699999451637268, Loss 0.355145957833156\n",
      "RNNA, rep: 0, epoch: 397, acc: 0.903333306312561, Loss 0.2864841394033283\n",
      "RNNA, rep: 0, epoch: 398, acc: 0.929999828338623, Loss 0.2810253457259387\n",
      "RNNA, rep: 0, epoch: 399, acc: 0.903333306312561, Loss 0.3217531272675842\n",
      "RNNA, rep: 0, epoch: 400, acc: 0.8866665363311768, Loss 0.34855647567659614\n",
      "RNNA, rep: 0, epoch: 401, acc: 0.9033333659172058, Loss 0.31123364489525557\n",
      "RNNA, rep: 0, epoch: 402, acc: 0.8899999260902405, Loss 0.354997991444543\n",
      "RNNA, rep: 0, epoch: 403, acc: 0.8833332657814026, Loss 0.38426838083192705\n",
      "RNNA, rep: 0, epoch: 404, acc: 0.90666663646698, Loss 0.29367786237038673\n",
      "RNNA, rep: 0, epoch: 405, acc: 0.8899999856948853, Loss 0.35486842004582286\n",
      "RNNA, rep: 0, epoch: 406, acc: 0.8933332562446594, Loss 0.33737874194048345\n",
      "RNNA, rep: 0, epoch: 407, acc: 0.8233334422111511, Loss 0.44302840702235696\n",
      "RNNA, rep: 0, epoch: 408, acc: 0.8633334636688232, Loss 0.3742547114752233\n",
      "RNNA, rep: 0, epoch: 409, acc: 0.8966667056083679, Loss 0.3161507238261402\n",
      "RNNA, rep: 0, epoch: 410, acc: 0.8633334636688232, Loss 0.3506821934971958\n",
      "RNNA, rep: 0, epoch: 411, acc: 0.9099998474121094, Loss 0.2774257189128548\n",
      "RNNA, rep: 0, epoch: 412, acc: 0.8733332753181458, Loss 0.3476954569388181\n",
      "RNNA, rep: 0, epoch: 413, acc: 0.833333432674408, Loss 0.44694217763375493\n",
      "RNNA, rep: 0, epoch: 414, acc: 0.8833335041999817, Loss 0.34605553255416455\n",
      "RNNA, rep: 0, epoch: 415, acc: 0.9233332276344299, Loss 0.2460233097523451\n",
      "RNNA, rep: 0, epoch: 416, acc: 0.8633332848548889, Loss 0.39186716344207523\n",
      "RNNA, rep: 0, epoch: 417, acc: 0.8866666555404663, Loss 0.34035200795158743\n",
      "RNNA, rep: 0, epoch: 418, acc: 0.9100000858306885, Loss 0.314050890058279\n",
      "RNNA, rep: 0, epoch: 419, acc: 0.893333375453949, Loss 0.31452862805686893\n",
      "RNNA, rep: 0, epoch: 420, acc: 0.8900001645088196, Loss 0.3282500914018601\n",
      "RNNA, rep: 0, epoch: 421, acc: 0.876666784286499, Loss 0.38387372232973577\n",
      "RNNA, rep: 0, epoch: 422, acc: 0.8800000548362732, Loss 0.39159919588826597\n",
      "RNNA, rep: 0, epoch: 423, acc: 0.893333375453949, Loss 0.3278766275802627\n",
      "RNNA, rep: 0, epoch: 424, acc: 0.893333375453949, Loss 0.3290735221514478\n",
      "RNNA, rep: 0, epoch: 425, acc: 0.8833333849906921, Loss 0.3394904206506908\n",
      "RNNA, rep: 0, epoch: 426, acc: 0.9033333659172058, Loss 0.3073594976123422\n",
      "RNNA, rep: 0, epoch: 427, acc: 0.8966666460037231, Loss 0.3044805137673393\n",
      "RNNA, rep: 0, epoch: 428, acc: 0.9200000762939453, Loss 0.26635961727239194\n",
      "RNNA, rep: 0, epoch: 429, acc: 0.913333535194397, Loss 0.27204823910724374\n",
      "RNNA, rep: 0, epoch: 430, acc: 0.8600001335144043, Loss 0.4120946877356619\n",
      "RNNA, rep: 0, epoch: 431, acc: 0.8700000047683716, Loss 0.3814828933007084\n",
      "RNNA, rep: 0, epoch: 432, acc: 0.8833333849906921, Loss 0.35737223165109755\n",
      "RNNA, rep: 0, epoch: 433, acc: 0.9133333563804626, Loss 0.2779223027685657\n",
      "RNNA, rep: 0, epoch: 434, acc: 0.8766666650772095, Loss 0.3737507043918595\n",
      "RNNA, rep: 0, epoch: 435, acc: 0.8466666340827942, Loss 0.41102555898949505\n",
      "RNNA, rep: 0, epoch: 436, acc: 0.9133332967758179, Loss 0.26361202681437135\n",
      "RNNA, rep: 0, epoch: 437, acc: 0.8899999856948853, Loss 0.3195444811671041\n",
      "RNNA, rep: 0, epoch: 438, acc: 0.8633334636688232, Loss 0.41701129811350257\n",
      "RNNA, rep: 0, epoch: 439, acc: 0.8866667747497559, Loss 0.3399730566237122\n",
      "RNNA, rep: 0, epoch: 440, acc: 0.8799999356269836, Loss 0.32760410391492767\n",
      "RNNA, rep: 0, epoch: 441, acc: 0.8866665363311768, Loss 0.35747373471036553\n",
      "RNNA, rep: 0, epoch: 442, acc: 0.9033331871032715, Loss 0.2498129113856703\n",
      "RNNA, rep: 0, epoch: 443, acc: 0.8933334946632385, Loss 0.310395034160465\n",
      "RNNA, rep: 0, epoch: 444, acc: 0.8666667342185974, Loss 0.36267424742225557\n",
      "RNNA, rep: 0, epoch: 445, acc: 0.8866667151451111, Loss 0.32420501582324507\n",
      "RNNA, rep: 0, epoch: 446, acc: 0.8966665863990784, Loss 0.30803994357120246\n",
      "RNNA, rep: 0, epoch: 447, acc: 0.9299999475479126, Loss 0.24302059340756388\n",
      "RNNA, rep: 0, epoch: 448, acc: 0.919999897480011, Loss 0.26037014299537986\n",
      "RNNA, rep: 0, epoch: 449, acc: 0.893333375453949, Loss 0.3168495616130531\n",
      "RNNA, rep: 0, epoch: 450, acc: 0.883333146572113, Loss 0.32267090628389267\n",
      "RNNA, rep: 0, epoch: 451, acc: 0.8766667246818542, Loss 0.36665806370321663\n",
      "RNNA, rep: 0, epoch: 452, acc: 0.8966667652130127, Loss 0.305911066015251\n",
      "RNNA, rep: 0, epoch: 453, acc: 0.8866665363311768, Loss 0.3431005199160427\n",
      "RNNA, rep: 0, epoch: 454, acc: 0.9033333659172058, Loss 0.2804368362529203\n",
      "RNNA, rep: 0, epoch: 455, acc: 0.8999999761581421, Loss 0.305382096003741\n",
      "RNNA, rep: 0, epoch: 456, acc: 0.9433332681655884, Loss 0.2034909553639591\n",
      "RNNA, rep: 0, epoch: 457, acc: 0.9300000667572021, Loss 0.25344388870988044\n",
      "RNNA, rep: 0, epoch: 458, acc: 0.9199998378753662, Loss 0.2478630213672295\n",
      "RNNA, rep: 0, epoch: 459, acc: 0.8966665863990784, Loss 0.3507961113844067\n",
      "RNNA, rep: 0, epoch: 460, acc: 0.8833332657814026, Loss 0.32616583968512713\n",
      "RNNA, rep: 0, epoch: 461, acc: 0.8833333849906921, Loss 0.32860276791732757\n",
      "RNNA, rep: 0, epoch: 462, acc: 0.8799999952316284, Loss 0.32396899778163063\n",
      "RNNA, rep: 0, epoch: 463, acc: 0.8866667151451111, Loss 0.363197369996924\n",
      "RNNA, rep: 0, epoch: 464, acc: 0.8899999260902405, Loss 0.3430045125214383\n",
      "RNNA, rep: 0, epoch: 465, acc: 0.8666666150093079, Loss 0.35929794049821795\n",
      "RNNA, rep: 0, epoch: 466, acc: 0.8999998569488525, Loss 0.3053171711647883\n",
      "RNNA, rep: 0, epoch: 467, acc: 0.8866667151451111, Loss 0.3814419730030931\n",
      "RNNA, rep: 0, epoch: 468, acc: 0.8966667056083679, Loss 0.32469572230242194\n",
      "RNNA, rep: 0, epoch: 469, acc: 0.8799999952316284, Loss 0.34853884116979317\n",
      "RNNA, rep: 0, epoch: 470, acc: 0.903333306312561, Loss 0.2617866372317076\n",
      "RNNA, rep: 0, epoch: 471, acc: 0.873333215713501, Loss 0.4206744278757833\n",
      "RNNA, rep: 0, epoch: 472, acc: 0.8899999856948853, Loss 0.3324392997007817\n",
      "RNNA, rep: 0, epoch: 473, acc: 0.8733334541320801, Loss 0.3634824001602828\n",
      "RNNA, rep: 0, epoch: 474, acc: 0.8966666460037231, Loss 0.3201262195431627\n",
      "RNNA, rep: 0, epoch: 475, acc: 0.8866666555404663, Loss 0.34742196449777113\n",
      "RNNA, rep: 0, epoch: 476, acc: 0.8933334350585938, Loss 0.28622728943126274\n",
      "RNNA, rep: 0, epoch: 477, acc: 0.9266666173934937, Loss 0.2291638342035003\n",
      "RNNA, rep: 0, epoch: 478, acc: 0.883333146572113, Loss 0.3449030101462267\n",
      "RNNA, rep: 0, epoch: 479, acc: 0.8833334445953369, Loss 0.3572960482933559\n",
      "RNNA, rep: 0, epoch: 480, acc: 0.8833332657814026, Loss 0.3417353119305335\n",
      "RNNA, rep: 0, epoch: 481, acc: 0.8866665363311768, Loss 0.33519405643921346\n",
      "RNNA, rep: 0, epoch: 482, acc: 0.8900001049041748, Loss 0.393328397735022\n",
      "RNNA, rep: 0, epoch: 483, acc: 0.8866666555404663, Loss 0.33423345325514675\n",
      "RNNA, rep: 0, epoch: 484, acc: 0.9033334255218506, Loss 0.31957616993691773\n",
      "RNNA, rep: 0, epoch: 485, acc: 0.8966667652130127, Loss 0.2954702456691302\n",
      "RNNA, rep: 0, epoch: 486, acc: 0.8466668725013733, Loss 0.43319454417098313\n",
      "RNNA, rep: 0, epoch: 487, acc: 0.903333306312561, Loss 0.2840574669651687\n",
      "RNNA, rep: 0, epoch: 488, acc: 0.8833332657814026, Loss 0.3046012116782367\n",
      "RNNA, rep: 0, epoch: 489, acc: 0.9066666960716248, Loss 0.24718483036383987\n",
      "RNNA, rep: 0, epoch: 490, acc: 0.9166666269302368, Loss 0.26980399490101264\n",
      "RNNA, rep: 0, epoch: 491, acc: 0.8899999856948853, Loss 0.30306859837146477\n",
      "RNNA, rep: 0, epoch: 492, acc: 0.8899999856948853, Loss 0.31491202243138106\n",
      "RNNA, rep: 0, epoch: 493, acc: 0.8833332657814026, Loss 0.3521765977446921\n",
      "RNNA, rep: 0, epoch: 494, acc: 0.9066666960716248, Loss 0.32314652000786737\n",
      "RNNA, rep: 0, epoch: 495, acc: 0.8766665458679199, Loss 0.32723637378774584\n",
      "RNNA, rep: 0, epoch: 496, acc: 0.8733333349227905, Loss 0.33387545598903673\n",
      "RNNA, rep: 0, epoch: 497, acc: 0.9266666173934937, Loss 0.1981164100789465\n",
      "RNNA, rep: 0, epoch: 498, acc: 0.8533332347869873, Loss 0.3791491335281171\n",
      "RNNA, rep: 0, epoch: 499, acc: 0.8766667246818542, Loss 0.3437757419119589\n",
      "RNNA, rep: 0, epoch: 500, acc: 0.90666663646698, Loss 0.262042503869161\n",
      "RNNA, rep: 0, epoch: 501, acc: 0.9366666674613953, Loss 0.24148749361513183\n",
      "RNNA, rep: 0, epoch: 502, acc: 0.903333306312561, Loss 0.28084081946639344\n",
      "RNNA, rep: 0, epoch: 503, acc: 0.9433333873748779, Loss 0.22552285647718237\n",
      "RNNA, rep: 0, epoch: 504, acc: 0.8633332848548889, Loss 0.4008221002109349\n",
      "RNNA, rep: 0, epoch: 505, acc: 0.8933332562446594, Loss 0.327049256474711\n",
      "RNNA, rep: 0, epoch: 506, acc: 0.9200000762939453, Loss 0.27961334460414944\n",
      "RNNA, rep: 0, epoch: 507, acc: 0.8600000143051147, Loss 0.3990907453419641\n",
      "RNNA, rep: 0, epoch: 508, acc: 0.9233332872390747, Loss 0.2594969343813136\n",
      "RNNA, rep: 0, epoch: 509, acc: 0.8799999952316284, Loss 0.3649544751551002\n",
      "RNNA, rep: 0, epoch: 510, acc: 0.903333306312561, Loss 0.28140710505424066\n",
      "RNNA, rep: 0, epoch: 511, acc: 0.8733332753181458, Loss 0.33514815775910395\n",
      "RNNA, rep: 0, epoch: 512, acc: 0.8933334350585938, Loss 0.3232186580193229\n",
      "RNNA, rep: 0, epoch: 513, acc: 0.9066666960716248, Loss 0.31361436485080046\n",
      "RNNA, rep: 0, epoch: 514, acc: 0.8533333539962769, Loss 0.37798489747801794\n",
      "RNNA, rep: 0, epoch: 515, acc: 0.9433333873748779, Loss 0.17828831613529472\n",
      "RNNA, rep: 0, epoch: 516, acc: 0.8866666555404663, Loss 0.3581925863819197\n",
      "RNNA, rep: 0, epoch: 517, acc: 0.9133332967758179, Loss 0.2372600504779257\n",
      "RNNA, rep: 0, epoch: 518, acc: 0.8866665363311768, Loss 0.322274418247398\n",
      "RNNA, rep: 0, epoch: 519, acc: 0.8966665863990784, Loss 0.3266153581929393\n",
      "RNNA, rep: 0, epoch: 520, acc: 0.8466666340827942, Loss 0.4396698806225322\n",
      "RNNA, rep: 0, epoch: 521, acc: 0.893333375453949, Loss 0.32339419222669674\n",
      "RNNA, rep: 0, epoch: 522, acc: 0.8766667246818542, Loss 0.3564507096982561\n",
      "RNNA, rep: 0, epoch: 523, acc: 0.9033334255218506, Loss 0.2773240054352209\n",
      "RNNA, rep: 0, epoch: 524, acc: 0.9233332276344299, Loss 0.2753031672327779\n",
      "RNNA, rep: 0, epoch: 525, acc: 0.90666663646698, Loss 0.2744622045999858\n",
      "RNNA, rep: 0, epoch: 526, acc: 0.9200000166893005, Loss 0.2504182145697996\n",
      "RNNA, rep: 0, epoch: 527, acc: 0.8933334946632385, Loss 0.30844664335949346\n",
      "RNNA, rep: 0, epoch: 528, acc: 0.9166667461395264, Loss 0.2713429713109508\n",
      "RNNA, rep: 0, epoch: 529, acc: 0.8933334946632385, Loss 0.3113771432475187\n",
      "RNNA, rep: 0, epoch: 530, acc: 0.8766667246818542, Loss 0.32958384298486637\n",
      "RNNA, rep: 0, epoch: 531, acc: 0.8900001049041748, Loss 0.29560610907385126\n",
      "RNNA, rep: 0, epoch: 532, acc: 0.8600000143051147, Loss 0.3679468454932794\n",
      "RNNA, rep: 0, epoch: 533, acc: 0.8999999761581421, Loss 0.3193044751486741\n",
      "RNNA, rep: 0, epoch: 534, acc: 0.8799999356269836, Loss 0.35015593202086165\n",
      "RNNA, rep: 0, epoch: 535, acc: 0.8900001049041748, Loss 0.327794344285503\n",
      "RNNA, rep: 0, epoch: 536, acc: 0.9133332967758179, Loss 0.27792453750502316\n",
      "RNNA, rep: 0, epoch: 537, acc: 0.9100000262260437, Loss 0.22524287074105814\n",
      "RNNA, rep: 0, epoch: 538, acc: 0.8966667056083679, Loss 0.3117780535016209\n",
      "RNNA, rep: 0, epoch: 539, acc: 0.8800001740455627, Loss 0.33974659330910073\n",
      "RNNA, rep: 0, epoch: 540, acc: 0.90666663646698, Loss 0.2755973603227176\n",
      "RNNA, rep: 0, epoch: 541, acc: 0.8933332562446594, Loss 0.2770052710163873\n",
      "RNNA, rep: 0, epoch: 542, acc: 0.8966667056083679, Loss 0.32920401071663946\n",
      "RNNA, rep: 0, epoch: 543, acc: 0.8799999952316284, Loss 0.3412043373659253\n",
      "RNNA, rep: 0, epoch: 544, acc: 0.8899999856948853, Loss 0.3134833923005499\n",
      "RNNA, rep: 0, epoch: 545, acc: 0.9266666173934937, Loss 0.2487924636551179\n",
      "RNNA, rep: 0, epoch: 546, acc: 0.8866667151451111, Loss 0.31202309547457846\n",
      "RNNA, rep: 0, epoch: 547, acc: 0.9066666960716248, Loss 0.28833142802352085\n",
      "RNNA, rep: 0, epoch: 548, acc: 0.9099999070167542, Loss 0.2834015230811201\n",
      "RNNA, rep: 0, epoch: 549, acc: 0.9066665768623352, Loss 0.24466251383069904\n",
      "RNNA, rep: 0, epoch: 550, acc: 0.8966664671897888, Loss 0.2962488530133851\n",
      "RNNA, rep: 0, epoch: 551, acc: 0.8966664671897888, Loss 0.3006510294531472\n",
      "RNNA, rep: 0, epoch: 552, acc: 0.8566667437553406, Loss 0.3643588679772802\n",
      "RNNA, rep: 0, epoch: 553, acc: 0.9133333563804626, Loss 0.2549230977660045\n",
      "RNNA, rep: 0, epoch: 554, acc: 0.8800000548362732, Loss 0.3321097541367635\n",
      "RNNA, rep: 0, epoch: 555, acc: 0.8599998950958252, Loss 0.41655501185916366\n",
      "RNNA, rep: 0, epoch: 556, acc: 0.8733332753181458, Loss 0.35997301295632494\n",
      "RNNA, rep: 0, epoch: 557, acc: 0.9166665077209473, Loss 0.23968100632773712\n",
      "RNNA, rep: 0, epoch: 558, acc: 0.8566667437553406, Loss 0.3618605313077569\n",
      "RNNA, rep: 0, epoch: 559, acc: 0.8700000047683716, Loss 0.3583612208161503\n",
      "RNNA, rep: 0, epoch: 560, acc: 0.9133333563804626, Loss 0.29281934602651744\n",
      "RNNA, rep: 0, epoch: 561, acc: 0.9266667366027832, Loss 0.2229081146651879\n",
      "RNNA, rep: 0, epoch: 562, acc: 0.8866666555404663, Loss 0.31629553078208117\n",
      "RNNA, rep: 0, epoch: 563, acc: 0.9100000262260437, Loss 0.2708044931758195\n",
      "RNNA, rep: 0, epoch: 564, acc: 0.8833333849906921, Loss 0.3428589156153612\n",
      "RNNA, rep: 0, epoch: 565, acc: 0.9166666269302368, Loss 0.23864901231834665\n",
      "RNNA, rep: 0, epoch: 566, acc: 0.8800000548362732, Loss 0.3624262484535575\n",
      "RNNA, rep: 0, epoch: 567, acc: 0.9133332967758179, Loss 0.2806416478776373\n",
      "RNNA, rep: 0, epoch: 568, acc: 0.9300000667572021, Loss 0.19031835233327002\n",
      "RNNA, rep: 0, epoch: 569, acc: 0.8900001049041748, Loss 0.3354612621269189\n",
      "RNNA, rep: 0, epoch: 570, acc: 0.9333333373069763, Loss 0.18642780552385374\n",
      "RNNA, rep: 0, epoch: 571, acc: 0.8966665863990784, Loss 0.2984821086307056\n",
      "RNNA, rep: 0, epoch: 572, acc: 0.9133334159851074, Loss 0.2442334155435674\n",
      "RNNA, rep: 0, epoch: 573, acc: 0.8999998569488525, Loss 0.2777067868807353\n",
      "RNNA, rep: 0, epoch: 574, acc: 0.8800000548362732, Loss 0.3680986463022418\n",
      "RNNA, rep: 0, epoch: 575, acc: 0.9200001358985901, Loss 0.21278136042645202\n",
      "RNNA, rep: 0, epoch: 576, acc: 0.9099999070167542, Loss 0.2629349677357823\n",
      "RNNA, rep: 0, epoch: 577, acc: 0.8999999761581421, Loss 0.26911974887829276\n",
      "RNNA, rep: 0, epoch: 578, acc: 0.9300000071525574, Loss 0.19920101245399566\n",
      "RNNA, rep: 0, epoch: 579, acc: 0.9300000071525574, Loss 0.23849841225426643\n",
      "RNNA, rep: 0, epoch: 580, acc: 0.8800000548362732, Loss 0.3198749518499244\n",
      "RNNA, rep: 0, epoch: 581, acc: 0.9133332967758179, Loss 0.2831158304074779\n",
      "RNNA, rep: 0, epoch: 582, acc: 0.919999897480011, Loss 0.24796829547965898\n",
      "RNNA, rep: 0, epoch: 583, acc: 0.9233332276344299, Loss 0.24198489825241268\n",
      "RNNA, rep: 0, epoch: 584, acc: 0.9000000953674316, Loss 0.3149167738994583\n",
      "RNNA, rep: 0, epoch: 585, acc: 0.919999897480011, Loss 0.2742845502123237\n",
      "RNNA, rep: 0, epoch: 586, acc: 0.9100000858306885, Loss 0.2776098213880323\n",
      "RNNA, rep: 0, epoch: 587, acc: 0.8999999761581421, Loss 0.3097460105025675\n",
      "RNNA, rep: 0, epoch: 588, acc: 0.949999988079071, Loss 0.2072044521709904\n",
      "RNNA, rep: 0, epoch: 589, acc: 0.9166666269302368, Loss 0.26331416388973594\n",
      "RNNA, rep: 0, epoch: 590, acc: 0.8800001740455627, Loss 0.3590922984946519\n",
      "RNNA, rep: 0, epoch: 591, acc: 0.8999999165534973, Loss 0.31113244058797135\n",
      "RNNA, rep: 0, epoch: 592, acc: 0.9066666960716248, Loss 0.2866760381194763\n",
      "RNNA, rep: 0, epoch: 593, acc: 0.9233332872390747, Loss 0.23428054117946887\n",
      "RNNA, rep: 0, epoch: 594, acc: 0.9100000858306885, Loss 0.28900243267882614\n",
      "RNNA, rep: 0, epoch: 595, acc: 0.9033333659172058, Loss 0.2763218387670349\n",
      "RNNA, rep: 0, epoch: 596, acc: 0.916666567325592, Loss 0.24171950323041527\n",
      "RNNA, rep: 0, epoch: 597, acc: 0.8700000047683716, Loss 0.3341174793755636\n",
      "RNNA, rep: 0, epoch: 598, acc: 0.8999999761581421, Loss 0.3171911054907832\n",
      "RNNA, rep: 0, epoch: 599, acc: 0.9433332681655884, Loss 0.19503456265432761\n",
      "RNNA, rep: 0, epoch: 600, acc: 0.8133332133293152, Loss 0.46207528723985886\n",
      "RNNA, rep: 0, epoch: 601, acc: 0.916666567325592, Loss 0.2837031098129228\n",
      "RNNA, rep: 0, epoch: 602, acc: 0.8799999952316284, Loss 0.33357366480864586\n",
      "RNNA, rep: 0, epoch: 603, acc: 0.9099998474121094, Loss 0.24741404511616566\n",
      "RNNA, rep: 0, epoch: 604, acc: 0.8899999856948853, Loss 0.32552165384753606\n",
      "RNNA, rep: 0, epoch: 605, acc: 0.9300000071525574, Loss 0.2388495746057015\n",
      "RNNA, rep: 0, epoch: 606, acc: 0.8599998950958252, Loss 0.3974980611796491\n",
      "RNNA, rep: 0, epoch: 607, acc: 0.9099999070167542, Loss 0.25386409626808015\n",
      "RNNA, rep: 0, epoch: 608, acc: 0.8699999451637268, Loss 0.3087383820535615\n",
      "RNNA, rep: 0, epoch: 609, acc: 0.9233333468437195, Loss 0.24759701052797026\n",
      "RNNA, rep: 0, epoch: 610, acc: 0.916666567325592, Loss 0.2282445853354875\n",
      "RNNA, rep: 0, epoch: 611, acc: 0.8899999856948853, Loss 0.32823830465436915\n",
      "RNNA, rep: 0, epoch: 612, acc: 0.8733334541320801, Loss 0.34329024157254023\n",
      "RNNA, rep: 0, epoch: 613, acc: 0.9066666960716248, Loss 0.298857387987664\n",
      "RNNA, rep: 0, epoch: 614, acc: 0.9033333659172058, Loss 0.2642332420218736\n",
      "RNNA, rep: 0, epoch: 615, acc: 0.9266665577888489, Loss 0.23832505268161186\n",
      "RNNA, rep: 0, epoch: 616, acc: 0.8600000739097595, Loss 0.34931891186046415\n",
      "RNNA, rep: 0, epoch: 617, acc: 0.9166666269302368, Loss 0.28138950738240964\n",
      "RNNA, rep: 0, epoch: 618, acc: 0.9066665768623352, Loss 0.2653742179297842\n",
      "RNNA, rep: 0, epoch: 619, acc: 0.8633332252502441, Loss 0.4044410039938521\n",
      "RNNA, rep: 0, epoch: 620, acc: 0.893333375453949, Loss 0.28972600818495264\n",
      "RNNA, rep: 0, epoch: 621, acc: 0.9300001263618469, Loss 0.2595829771494027\n",
      "RNNA, rep: 0, epoch: 622, acc: 0.9000000953674316, Loss 0.32559412404778415\n",
      "RNNA, rep: 0, epoch: 623, acc: 0.9000000953674316, Loss 0.2675198169762734\n",
      "RNNA, rep: 0, epoch: 624, acc: 0.9333332777023315, Loss 0.21376404884969816\n",
      "RNNA, rep: 0, epoch: 625, acc: 0.903333306312561, Loss 0.24105467887944543\n",
      "RNNA, rep: 0, epoch: 626, acc: 0.9066665768623352, Loss 0.2673295035038609\n",
      "RNNA, rep: 0, epoch: 627, acc: 0.9166668057441711, Loss 0.25498263048473746\n",
      "RNNA, rep: 0, epoch: 628, acc: 0.9300000071525574, Loss 0.2381389992928598\n",
      "RNNA, rep: 0, epoch: 629, acc: 0.9033333659172058, Loss 0.3088897588977125\n",
      "RNNA, rep: 0, epoch: 630, acc: 0.90666663646698, Loss 0.30558373825508167\n",
      "RNNA, rep: 0, epoch: 631, acc: 0.9200000762939453, Loss 0.28565857241279446\n",
      "RNNA, rep: 0, epoch: 632, acc: 0.9200000762939453, Loss 0.2674510379729327\n",
      "RNNA, rep: 0, epoch: 633, acc: 0.8966668844223022, Loss 0.31725215492770076\n",
      "RNNA, rep: 0, epoch: 634, acc: 0.9066666960716248, Loss 0.252509817549726\n",
      "RNNA, rep: 0, epoch: 635, acc: 0.9100000858306885, Loss 0.2748373680701479\n",
      "RNNA, rep: 0, epoch: 636, acc: 0.9233332872390747, Loss 0.2562307309405878\n",
      "RNNA, rep: 0, epoch: 637, acc: 0.9433332085609436, Loss 0.16283901086077093\n",
      "RNNA, rep: 0, epoch: 638, acc: 0.9366666674613953, Loss 0.18501234659226612\n",
      "RNNA, rep: 0, epoch: 639, acc: 0.919999897480011, Loss 0.2373117091867607\n",
      "RNNA, rep: 0, epoch: 640, acc: 0.8999999165534973, Loss 0.30713707937160506\n",
      "RNNA, rep: 0, epoch: 641, acc: 0.8899999856948853, Loss 0.30733116512885317\n",
      "RNNA, rep: 0, epoch: 642, acc: 0.8966666460037231, Loss 0.3189305152045563\n",
      "RNNA, rep: 0, epoch: 643, acc: 0.9399999380111694, Loss 0.20845666000968777\n",
      "RNNA, rep: 0, epoch: 644, acc: 0.8966666460037231, Loss 0.2931126724323258\n",
      "RNNA, rep: 0, epoch: 645, acc: 0.9066666960716248, Loss 0.28948833372676747\n",
      "RNNA, rep: 0, epoch: 646, acc: 0.9333334565162659, Loss 0.19920730632962658\n",
      "RNNA, rep: 0, epoch: 647, acc: 0.9133331775665283, Loss 0.2761966734193265\n",
      "RNNA, rep: 0, epoch: 648, acc: 0.8800000548362732, Loss 0.36339247108320705\n",
      "RNNA, rep: 0, epoch: 649, acc: 0.9066666960716248, Loss 0.2716072306386195\n",
      "RNNA, rep: 0, epoch: 650, acc: 0.8700000643730164, Loss 0.3418025298370048\n",
      "RNNA, rep: 0, epoch: 651, acc: 0.8666667938232422, Loss 0.3523370973742567\n",
      "RNNA, rep: 0, epoch: 652, acc: 0.8866666555404663, Loss 0.29358527588890865\n",
      "RNNA, rep: 0, epoch: 653, acc: 0.9066666960716248, Loss 0.29303771375212817\n",
      "RNNA, rep: 0, epoch: 654, acc: 0.9300000071525574, Loss 0.20918280069250614\n",
      "RNNA, rep: 0, epoch: 655, acc: 0.9466665387153625, Loss 0.21351607069023884\n",
      "RNNA, rep: 0, epoch: 656, acc: 0.9133332967758179, Loss 0.29943613199633545\n",
      "RNNA, rep: 0, epoch: 657, acc: 0.8633333444595337, Loss 0.3442948386538774\n",
      "RNNA, rep: 0, epoch: 658, acc: 0.8899999260902405, Loss 0.2862031879741698\n",
      "RNNA, rep: 0, epoch: 659, acc: 0.9233333468437195, Loss 0.21330758760450408\n",
      "RNNA, rep: 0, epoch: 660, acc: 0.9200000166893005, Loss 0.23904545968514868\n",
      "RNNA, rep: 0, epoch: 661, acc: 0.8766667246818542, Loss 0.34879565503331833\n",
      "RNNA, rep: 0, epoch: 662, acc: 0.9333333373069763, Loss 0.21692576416186057\n",
      "RNNA, rep: 0, epoch: 663, acc: 0.9333333373069763, Loss 0.2038601832545828\n",
      "RNNA, rep: 0, epoch: 664, acc: 0.8933332562446594, Loss 0.31981298740487546\n",
      "RNNA, rep: 0, epoch: 665, acc: 0.9166666269302368, Loss 0.2371598343504593\n",
      "RNNA, rep: 0, epoch: 666, acc: 0.9266667366027832, Loss 0.23264252260909415\n",
      "RNNA, rep: 0, epoch: 667, acc: 0.8999999165534973, Loss 0.25351172853144815\n",
      "RNNA, rep: 0, epoch: 668, acc: 0.9300000071525574, Loss 0.2490527326508891\n",
      "RNNA, rep: 0, epoch: 669, acc: 0.8800000548362732, Loss 0.34016187030356376\n",
      "RNNA, rep: 0, epoch: 670, acc: 0.9166667461395264, Loss 0.2702887147141155\n",
      "RNNA, rep: 0, epoch: 671, acc: 0.9100000262260437, Loss 0.2829341309826123\n",
      "RNNA, rep: 0, epoch: 672, acc: 0.8733335137367249, Loss 0.3425348995742388\n",
      "RNNA, rep: 0, epoch: 673, acc: 0.9566667079925537, Loss 0.1499638763407711\n",
      "RNNA, rep: 0, epoch: 674, acc: 0.9399999380111694, Loss 0.2255247180047445\n",
      "RNNA, rep: 0, epoch: 675, acc: 0.9166667461395264, Loss 0.2495988603006117\n",
      "RNNA, rep: 0, epoch: 676, acc: 0.9233333468437195, Loss 0.23281778728123753\n",
      "RNNA, rep: 0, epoch: 677, acc: 0.9233333468437195, Loss 0.22856274492340162\n",
      "RNNA, rep: 0, epoch: 678, acc: 0.8866666555404663, Loss 0.31736487592104823\n",
      "RNNA, rep: 0, epoch: 679, acc: 0.916666567325592, Loss 0.24141336083062925\n",
      "RNNA, rep: 0, epoch: 680, acc: 0.8899999856948853, Loss 0.30808123111259195\n",
      "RNNA, rep: 0, epoch: 681, acc: 0.8999999165534973, Loss 0.28046227695420384\n",
      "RNNA, rep: 0, epoch: 682, acc: 0.8633333444595337, Loss 0.37631597196450456\n",
      "RNNA, rep: 0, epoch: 683, acc: 0.9133334159851074, Loss 0.26542571835569106\n",
      "RNNA, rep: 0, epoch: 684, acc: 0.9166668057441711, Loss 0.2317429583100602\n",
      "RNNA, rep: 0, epoch: 685, acc: 0.9300000071525574, Loss 0.20681870291475207\n",
      "RNNA, rep: 0, epoch: 686, acc: 0.893333375453949, Loss 0.2818647485342808\n",
      "RNNA, rep: 0, epoch: 687, acc: 0.9033333659172058, Loss 0.2992565110139549\n",
      "RNNA, rep: 0, epoch: 688, acc: 0.8666665554046631, Loss 0.4081150511605665\n",
      "RNNA, rep: 0, epoch: 689, acc: 0.8699998259544373, Loss 0.3835845660790801\n",
      "RNNA, rep: 0, epoch: 690, acc: 0.9066665768623352, Loss 0.25254947345587425\n",
      "RNNA, rep: 0, epoch: 691, acc: 0.9166667461395264, Loss 0.2970527586073149\n",
      "RNNA, rep: 0, epoch: 692, acc: 0.9333332777023315, Loss 0.21714328151312656\n",
      "RNNA, rep: 0, epoch: 693, acc: 0.9300000071525574, Loss 0.2319261084124446\n",
      "RNNA, rep: 0, epoch: 694, acc: 0.8866667151451111, Loss 0.32712323450716213\n",
      "RNNA, rep: 0, epoch: 695, acc: 0.8999999761581421, Loss 0.3466504552826518\n",
      "RNNA, rep: 0, epoch: 696, acc: 0.903333306312561, Loss 0.2982018282858189\n",
      "RNNA, rep: 0, epoch: 697, acc: 0.8766667246818542, Loss 0.3464867251354735\n",
      "RNNA, rep: 0, epoch: 698, acc: 0.8733334541320801, Loss 0.356287104251096\n",
      "RNNA, rep: 0, epoch: 699, acc: 0.856666624546051, Loss 0.39752172557171433\n",
      "RNNA, rep: 0, epoch: 700, acc: 0.9333333373069763, Loss 0.20254212768166327\n",
      "RNNA, rep: 0, epoch: 701, acc: 0.9266666173934937, Loss 0.2383350710454397\n",
      "RNNA, rep: 0, epoch: 702, acc: 0.9099999070167542, Loss 0.27446001275093296\n",
      "RNNA, rep: 0, epoch: 703, acc: 0.8833332061767578, Loss 0.30896857496118174\n",
      "RNNA, rep: 0, epoch: 704, acc: 0.9066666960716248, Loss 0.264221461758716\n",
      "RNNA, rep: 0, epoch: 705, acc: 0.9000000953674316, Loss 0.29790591947268696\n",
      "RNNA, rep: 0, epoch: 706, acc: 0.9133333563804626, Loss 0.26494965438731016\n",
      "RNNA, rep: 0, epoch: 707, acc: 0.9066666960716248, Loss 0.28924200660316274\n",
      "RNNA, rep: 0, epoch: 708, acc: 0.919999897480011, Loss 0.254580647732364\n",
      "RNNA, rep: 0, epoch: 709, acc: 0.9433332681655884, Loss 0.21910363836796024\n",
      "RNNA, rep: 0, epoch: 710, acc: 0.9166668057441711, Loss 0.26877359266043643\n",
      "RNNA, rep: 0, epoch: 711, acc: 0.9333333373069763, Loss 0.2197306744015077\n",
      "RNNA, rep: 0, epoch: 712, acc: 0.8899998664855957, Loss 0.30756430136272683\n",
      "RNNA, rep: 0, epoch: 713, acc: 0.9300000071525574, Loss 0.21907485333620572\n",
      "RNNA, rep: 0, epoch: 714, acc: 0.9100000262260437, Loss 0.2627689770900179\n",
      "RNNA, rep: 0, epoch: 715, acc: 0.9300000667572021, Loss 0.20972916816186626\n",
      "RNNA, rep: 0, epoch: 716, acc: 0.9366666674613953, Loss 0.20903824905806687\n",
      "RNNA, rep: 0, epoch: 717, acc: 0.8833332061767578, Loss 0.3064200245134998\n",
      "RNNA, rep: 0, epoch: 718, acc: 0.8933334350585938, Loss 0.2853747022204334\n",
      "RNNA, rep: 0, epoch: 719, acc: 0.9066668152809143, Loss 0.3010336396493949\n",
      "RNNA, rep: 0, epoch: 720, acc: 0.9333333373069763, Loss 0.21270432093646377\n",
      "RNNA, rep: 0, epoch: 721, acc: 0.9100000858306885, Loss 0.26704150505946017\n",
      "RNNA, rep: 0, epoch: 722, acc: 0.9099998474121094, Loss 0.2135276525269728\n",
      "RNNA, rep: 0, epoch: 723, acc: 0.9200000762939453, Loss 0.23208200530265458\n",
      "RNNA, rep: 0, epoch: 724, acc: 0.9133333563804626, Loss 0.23297604338848033\n",
      "RNNA, rep: 0, epoch: 725, acc: 0.9200000762939453, Loss 0.23257107467856258\n",
      "RNNA, rep: 0, epoch: 726, acc: 0.8866665363311768, Loss 0.34436223284574224\n",
      "RNNA, rep: 0, epoch: 727, acc: 0.9100000262260437, Loss 0.2612924918852514\n",
      "RNNA, rep: 0, epoch: 728, acc: 0.9166667461395264, Loss 0.23778710370068437\n",
      "RNNA, rep: 0, epoch: 729, acc: 0.9099998474121094, Loss 0.28484981798450465\n",
      "RNNA, rep: 0, epoch: 730, acc: 0.863333523273468, Loss 0.3462278525473084\n",
      "RNNA, rep: 0, epoch: 731, acc: 0.90666663646698, Loss 0.2868152676196769\n",
      "RNNA, rep: 0, epoch: 732, acc: 0.9200000762939453, Loss 0.25620658935862595\n",
      "RNNA, rep: 0, epoch: 733, acc: 0.9166667461395264, Loss 0.27552808346110397\n",
      "RNNA, rep: 0, epoch: 734, acc: 0.9166667461395264, Loss 0.23192548670223914\n",
      "RNNA, rep: 0, epoch: 735, acc: 0.9100000262260437, Loss 0.26685842405073346\n",
      "RNNA, rep: 0, epoch: 736, acc: 0.9233333468437195, Loss 0.2382037661311915\n",
      "RNNA, rep: 0, epoch: 737, acc: 0.9066665768623352, Loss 0.2606182060297579\n",
      "RNNA, rep: 0, epoch: 738, acc: 0.9233332276344299, Loss 0.23062432110949885\n",
      "RNNA, rep: 0, epoch: 739, acc: 0.9100000262260437, Loss 0.2708323937293608\n",
      "RNNA, rep: 0, epoch: 740, acc: 0.8999999165534973, Loss 0.2873845920269378\n",
      "RNNA, rep: 0, epoch: 741, acc: 0.9133332967758179, Loss 0.25804637821158394\n",
      "RNNA, rep: 0, epoch: 742, acc: 0.9133332967758179, Loss 0.29700047346064823\n",
      "RNNA, rep: 0, epoch: 743, acc: 0.8966666460037231, Loss 0.2804265031591058\n",
      "RNNA, rep: 0, epoch: 744, acc: 0.9200000166893005, Loss 0.24271326508373023\n",
      "RNNA, rep: 0, epoch: 745, acc: 0.9099999070167542, Loss 0.2718002018553671\n",
      "RNNA, rep: 0, epoch: 746, acc: 0.8900001049041748, Loss 0.299905588835245\n",
      "RNNA, rep: 0, epoch: 747, acc: 0.9066666960716248, Loss 0.2809549207024975\n",
      "RNNA, rep: 0, epoch: 748, acc: 0.8900001645088196, Loss 0.3037827047833707\n",
      "RNNA, rep: 0, epoch: 749, acc: 0.8766667246818542, Loss 0.35015890540671535\n",
      "RNNA, rep: 0, epoch: 750, acc: 0.9000000953674316, Loss 0.28255422846996225\n",
      "RNNA, rep: 0, epoch: 751, acc: 0.9166666269302368, Loss 0.2615458972309716\n",
      "RNNA, rep: 0, epoch: 752, acc: 0.9399999976158142, Loss 0.20388219854678027\n",
      "RNNA, rep: 0, epoch: 753, acc: 0.9166666269302368, Loss 0.2515737234358676\n",
      "RNNA, rep: 0, epoch: 754, acc: 0.8899999260902405, Loss 0.35142374704300894\n",
      "RNNA, rep: 0, epoch: 755, acc: 0.8999999165534973, Loss 0.3020251618698239\n",
      "RNNA, rep: 0, epoch: 756, acc: 0.9200000762939453, Loss 0.2811852266313508\n",
      "RNNA, rep: 0, epoch: 757, acc: 0.8966666460037231, Loss 0.3157140282797627\n",
      "RNNA, rep: 0, epoch: 758, acc: 0.9333334565162659, Loss 0.2271979705587728\n",
      "RNNA, rep: 0, epoch: 759, acc: 0.9033333659172058, Loss 0.2986918133043218\n",
      "RNNA, rep: 0, epoch: 760, acc: 0.9433332681655884, Loss 0.18794060569431167\n",
      "RNNA, rep: 0, epoch: 761, acc: 0.9266666173934937, Loss 0.21158796844771133\n",
      "RNNA, rep: 0, epoch: 762, acc: 0.8633332848548889, Loss 0.36967151251621544\n",
      "RNNA, rep: 0, epoch: 763, acc: 0.9100000858306885, Loss 0.2825984940095805\n",
      "RNNA, rep: 0, epoch: 764, acc: 0.8833334445953369, Loss 0.3304055267025251\n",
      "RNNA, rep: 0, epoch: 765, acc: 0.8966667056083679, Loss 0.295432702958351\n",
      "RNNA, rep: 0, epoch: 766, acc: 0.866666853427887, Loss 0.3684911060350714\n",
      "RNNA, rep: 0, epoch: 767, acc: 0.9433332681655884, Loss 0.16979117717477493\n",
      "RNNA, rep: 0, epoch: 768, acc: 0.9499999284744263, Loss 0.16107696669991128\n",
      "RNNA, rep: 0, epoch: 769, acc: 0.8966667056083679, Loss 0.3202254065813031\n",
      "RNNA, rep: 0, epoch: 770, acc: 0.9199997782707214, Loss 0.27477558409853375\n",
      "RNNA, rep: 0, epoch: 771, acc: 0.9033331871032715, Loss 0.2759680835023755\n",
      "RNNA, rep: 0, epoch: 772, acc: 0.913333535194397, Loss 0.2524866285995813\n",
      "RNNA, rep: 0, epoch: 773, acc: 0.9700000286102295, Loss 0.11331477268890013\n",
      "RNNA                 Rep: 0   Epoch: 1     Acc: 0.9700 _min_10_max_10 Time: 60.98 sec\n",
      "RNNA, rep: 1, epoch: 1, acc: 0.5133333802223206, Loss 1.0243830382823944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 3])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNA, rep: 1, epoch: 2, acc: 0.5433334112167358, Loss 0.9964196890592575\n",
      "RNNA, rep: 1, epoch: 3, acc: 0.5566665530204773, Loss 0.988303759098053\n",
      "RNNA, rep: 1, epoch: 4, acc: 0.5433332920074463, Loss 0.9958027523756027\n",
      "RNNA, rep: 1, epoch: 5, acc: 0.5666666626930237, Loss 0.9691810572147369\n",
      "RNNA, rep: 1, epoch: 6, acc: 0.6400001049041748, Loss 0.9560915952920914\n",
      "RNNA, rep: 1, epoch: 7, acc: 0.5733335614204407, Loss 0.9668534469604492\n",
      "RNNA, rep: 1, epoch: 8, acc: 0.6133333444595337, Loss 0.8850130739808083\n",
      "RNNA, rep: 1, epoch: 9, acc: 0.6200000643730164, Loss 0.8756727522611618\n",
      "RNNA, rep: 1, epoch: 10, acc: 0.6233334541320801, Loss 0.8831254327297211\n",
      "RNNA, rep: 1, epoch: 11, acc: 0.6366667747497559, Loss 0.8820998692512512\n",
      "RNNA, rep: 1, epoch: 12, acc: 0.6866667866706848, Loss 0.7881354659795761\n",
      "RNNA, rep: 1, epoch: 13, acc: 0.6500000953674316, Loss 0.8711984315514565\n",
      "RNNA, rep: 1, epoch: 14, acc: 0.6233333945274353, Loss 0.8562825354933739\n",
      "RNNA, rep: 1, epoch: 15, acc: 0.6500001549720764, Loss 0.828906069099903\n",
      "RNNA, rep: 1, epoch: 16, acc: 0.676666796207428, Loss 0.8291051015257835\n",
      "RNNA, rep: 1, epoch: 17, acc: 0.6899998188018799, Loss 0.8101412776112557\n",
      "RNNA, rep: 1, epoch: 18, acc: 0.6833334565162659, Loss 0.7499064600467682\n",
      "RNNA, rep: 1, epoch: 19, acc: 0.7200000286102295, Loss 0.7207574591040611\n",
      "RNNA, rep: 1, epoch: 20, acc: 0.7033334970474243, Loss 0.7229223763942718\n",
      "RNNA, rep: 1, epoch: 21, acc: 0.6466667652130127, Loss 0.7796593561768532\n",
      "RNNA, rep: 1, epoch: 22, acc: 0.7266666293144226, Loss 0.7107512784004212\n",
      "RNNA, rep: 1, epoch: 23, acc: 0.6933334469795227, Loss 0.7436423027515411\n",
      "RNNA, rep: 1, epoch: 24, acc: 0.7299998998641968, Loss 0.7195469105243683\n",
      "RNNA, rep: 1, epoch: 25, acc: 0.7500000596046448, Loss 0.679522763043642\n",
      "RNNA, rep: 1, epoch: 26, acc: 0.73333340883255, Loss 0.7526741471886634\n",
      "RNNA, rep: 1, epoch: 27, acc: 0.6300001740455627, Loss 0.8733437798917294\n",
      "RNNA, rep: 1, epoch: 28, acc: 0.7366669178009033, Loss 0.6827058565616607\n",
      "RNNA, rep: 1, epoch: 29, acc: 0.7066666483879089, Loss 0.7421231625974178\n",
      "RNNA, rep: 1, epoch: 30, acc: 0.7133334875106812, Loss 0.6662474119663239\n",
      "RNNA, rep: 1, epoch: 31, acc: 0.720000147819519, Loss 0.6777971178293228\n",
      "RNNA, rep: 1, epoch: 32, acc: 0.7099999189376831, Loss 0.7226476439833641\n",
      "RNNA, rep: 1, epoch: 33, acc: 0.7000000476837158, Loss 0.7035025677084923\n",
      "RNNA, rep: 1, epoch: 34, acc: 0.6733333468437195, Loss 0.7832440751791\n",
      "RNNA, rep: 1, epoch: 35, acc: 0.7266666889190674, Loss 0.6643145644664764\n",
      "RNNA, rep: 1, epoch: 36, acc: 0.7133336067199707, Loss 0.6489101672172546\n",
      "RNNA, rep: 1, epoch: 37, acc: 0.6966665387153625, Loss 0.7005118507146836\n",
      "RNNA, rep: 1, epoch: 38, acc: 0.6866666674613953, Loss 0.7333240024745464\n",
      "RNNA, rep: 1, epoch: 39, acc: 0.7066666483879089, Loss 0.6946451020240784\n",
      "RNNA, rep: 1, epoch: 40, acc: 0.7300000786781311, Loss 0.7053293350338936\n",
      "RNNA, rep: 1, epoch: 41, acc: 0.6966667175292969, Loss 0.6666726161539555\n",
      "RNNA, rep: 1, epoch: 42, acc: 0.7000002861022949, Loss 0.7272366833686829\n",
      "RNNA, rep: 1, epoch: 43, acc: 0.7066667079925537, Loss 0.7562745693325996\n",
      "RNNA, rep: 1, epoch: 44, acc: 0.690000057220459, Loss 0.6915715524554252\n",
      "RNNA, rep: 1, epoch: 45, acc: 0.7699998617172241, Loss 0.6644927029311657\n",
      "RNNA, rep: 1, epoch: 46, acc: 0.7433333396911621, Loss 0.6297748194634915\n",
      "RNNA, rep: 1, epoch: 47, acc: 0.7433335185050964, Loss 0.6541743376851081\n",
      "RNNA, rep: 1, epoch: 48, acc: 0.7533335089683533, Loss 0.6092872324585915\n",
      "RNNA, rep: 1, epoch: 49, acc: 0.6933333873748779, Loss 0.6916519170999527\n",
      "RNNA, rep: 1, epoch: 50, acc: 0.7266666889190674, Loss 0.6260008862614632\n",
      "RNNA, rep: 1, epoch: 51, acc: 0.7400000691413879, Loss 0.5999434578418732\n",
      "RNNA, rep: 1, epoch: 52, acc: 0.7533332705497742, Loss 0.57826316639781\n",
      "RNNA, rep: 1, epoch: 53, acc: 0.7166668176651001, Loss 0.6557970103621483\n",
      "RNNA, rep: 1, epoch: 54, acc: 0.73333340883255, Loss 0.7027054879069329\n",
      "RNNA, rep: 1, epoch: 55, acc: 0.770000159740448, Loss 0.5925478020310402\n",
      "RNNA, rep: 1, epoch: 56, acc: 0.7566667199134827, Loss 0.6172646987438202\n",
      "RNNA, rep: 1, epoch: 57, acc: 0.7366666197776794, Loss 0.6638554614782334\n",
      "RNNA, rep: 1, epoch: 58, acc: 0.746666669845581, Loss 0.6555765041708946\n",
      "RNNA, rep: 1, epoch: 59, acc: 0.7433333396911621, Loss 0.6522574532032013\n",
      "RNNA, rep: 1, epoch: 60, acc: 0.7466667294502258, Loss 0.6310362796485424\n",
      "RNNA, rep: 1, epoch: 61, acc: 0.7599999308586121, Loss 0.564562410414219\n",
      "RNNA, rep: 1, epoch: 62, acc: 0.7233332991600037, Loss 0.6225404864549637\n",
      "RNNA, rep: 1, epoch: 63, acc: 0.6933333873748779, Loss 0.6551834553480148\n",
      "RNNA, rep: 1, epoch: 64, acc: 0.6999999284744263, Loss 0.6582795846462249\n",
      "RNNA, rep: 1, epoch: 65, acc: 0.8100000619888306, Loss 0.5522302848100662\n",
      "RNNA, rep: 1, epoch: 66, acc: 0.6933333873748779, Loss 0.6665400323271752\n",
      "RNNA, rep: 1, epoch: 67, acc: 0.7400001287460327, Loss 0.6319568756222725\n",
      "RNNA, rep: 1, epoch: 68, acc: 0.7233335971832275, Loss 0.6206548655033112\n",
      "RNNA, rep: 1, epoch: 69, acc: 0.7800000905990601, Loss 0.5790014460682869\n",
      "RNNA, rep: 1, epoch: 70, acc: 0.7733331918716431, Loss 0.5720363287627697\n",
      "RNNA, rep: 1, epoch: 71, acc: 0.7266665697097778, Loss 0.6628271675109864\n",
      "RNNA, rep: 1, epoch: 72, acc: 0.7433333396911621, Loss 0.5841217011213302\n",
      "RNNA, rep: 1, epoch: 73, acc: 0.736666738986969, Loss 0.633157258182764\n",
      "RNNA, rep: 1, epoch: 74, acc: 0.763333261013031, Loss 0.5394845491647721\n",
      "RNNA, rep: 1, epoch: 75, acc: 0.7499998211860657, Loss 0.5836688929796219\n",
      "RNNA, rep: 1, epoch: 76, acc: 0.7399999499320984, Loss 0.6228290092945099\n",
      "RNNA, rep: 1, epoch: 77, acc: 0.7266666889190674, Loss 0.668871246278286\n",
      "RNNA, rep: 1, epoch: 78, acc: 0.8000000715255737, Loss 0.6172356152534485\n",
      "RNNA, rep: 1, epoch: 79, acc: 0.7500000596046448, Loss 0.6295527264475822\n",
      "RNNA, rep: 1, epoch: 80, acc: 0.720000147819519, Loss 0.671592292189598\n",
      "RNNA, rep: 1, epoch: 81, acc: 0.7100001573562622, Loss 0.6655426824092865\n",
      "RNNA, rep: 1, epoch: 82, acc: 0.7133334279060364, Loss 0.6457176995277405\n",
      "RNNA, rep: 1, epoch: 83, acc: 0.7633334398269653, Loss 0.5529556399583817\n",
      "RNNA, rep: 1, epoch: 84, acc: 0.7533335089683533, Loss 0.5710508608818055\n",
      "RNNA, rep: 1, epoch: 85, acc: 0.7266668081283569, Loss 0.6631751668453216\n",
      "RNNA, rep: 1, epoch: 86, acc: 0.7500001788139343, Loss 0.6240129420161247\n",
      "RNNA, rep: 1, epoch: 87, acc: 0.75, Loss 0.6231414586305618\n",
      "RNNA, rep: 1, epoch: 88, acc: 0.7266665697097778, Loss 0.6669818198680878\n",
      "RNNA, rep: 1, epoch: 89, acc: 0.7166666984558105, Loss 0.680009805560112\n",
      "RNNA, rep: 1, epoch: 90, acc: 0.7300000786781311, Loss 0.6214159889519215\n",
      "RNNA, rep: 1, epoch: 91, acc: 0.7299998998641968, Loss 0.5794277381896973\n",
      "RNNA, rep: 1, epoch: 92, acc: 0.7266665697097778, Loss 0.6337069645524025\n",
      "RNNA, rep: 1, epoch: 93, acc: 0.770000159740448, Loss 0.5950576475262642\n",
      "RNNA, rep: 1, epoch: 94, acc: 0.7433334589004517, Loss 0.6475355045497417\n",
      "RNNA, rep: 1, epoch: 95, acc: 0.7500001788139343, Loss 0.5709919345378875\n",
      "RNNA, rep: 1, epoch: 96, acc: 0.7466667294502258, Loss 0.6047103017568588\n",
      "RNNA, rep: 1, epoch: 97, acc: 0.8199999332427979, Loss 0.5156572702527046\n",
      "RNNA, rep: 1, epoch: 98, acc: 0.7999999523162842, Loss 0.5267962285876274\n",
      "RNNA, rep: 1, epoch: 99, acc: 0.7699999809265137, Loss 0.5716178023815155\n",
      "RNNA, rep: 1, epoch: 100, acc: 0.7999998331069946, Loss 0.5300470544397831\n",
      "RNNA, rep: 1, epoch: 101, acc: 0.7699998617172241, Loss 0.5673810194432736\n",
      "RNNA, rep: 1, epoch: 102, acc: 0.75, Loss 0.587648650854826\n",
      "RNNA, rep: 1, epoch: 103, acc: 0.753333330154419, Loss 0.5681008931994438\n",
      "RNNA, rep: 1, epoch: 104, acc: 0.7600000500679016, Loss 0.591169998049736\n",
      "RNNA, rep: 1, epoch: 105, acc: 0.7266666293144226, Loss 0.5785179716348648\n",
      "RNNA, rep: 1, epoch: 106, acc: 0.7633334994316101, Loss 0.5814381730556488\n",
      "RNNA, rep: 1, epoch: 107, acc: 0.7500000596046448, Loss 0.6134113878011703\n",
      "RNNA, rep: 1, epoch: 108, acc: 0.7899999022483826, Loss 0.5724698749184608\n",
      "RNNA, rep: 1, epoch: 109, acc: 0.7400000095367432, Loss 0.6118837997317315\n",
      "RNNA, rep: 1, epoch: 110, acc: 0.736666738986969, Loss 0.6239134827256203\n",
      "RNNA, rep: 1, epoch: 111, acc: 0.7566666603088379, Loss 0.579958611279726\n",
      "RNNA, rep: 1, epoch: 112, acc: 0.73333340883255, Loss 0.6527194979786873\n",
      "RNNA, rep: 1, epoch: 113, acc: 0.7433332800865173, Loss 0.6120314508676529\n",
      "RNNA, rep: 1, epoch: 114, acc: 0.76666659116745, Loss 0.5799936875700951\n",
      "RNNA, rep: 1, epoch: 115, acc: 0.7566666603088379, Loss 0.5918894453346729\n",
      "RNNA, rep: 1, epoch: 116, acc: 0.7366666197776794, Loss 0.6313089749217033\n",
      "RNNA, rep: 1, epoch: 117, acc: 0.7266665697097778, Loss 0.5926472030580043\n",
      "RNNA, rep: 1, epoch: 118, acc: 0.7500000596046448, Loss 0.5850705474615097\n",
      "RNNA, rep: 1, epoch: 119, acc: 0.746666669845581, Loss 0.5860470902919769\n",
      "RNNA, rep: 1, epoch: 120, acc: 0.7366668581962585, Loss 0.6069559633731842\n",
      "RNNA, rep: 1, epoch: 121, acc: 0.7433333396911621, Loss 0.6463619047403335\n",
      "RNNA, rep: 1, epoch: 122, acc: 0.7366668581962585, Loss 0.5735331358015537\n",
      "RNNA, rep: 1, epoch: 123, acc: 0.7433333396911621, Loss 0.5921128839254379\n",
      "RNNA, rep: 1, epoch: 124, acc: 0.6899999976158142, Loss 0.6683815893530846\n",
      "RNNA, rep: 1, epoch: 125, acc: 0.7733331918716431, Loss 0.5837790943682194\n",
      "RNNA, rep: 1, epoch: 126, acc: 0.7466668486595154, Loss 0.5909727726876736\n",
      "RNNA, rep: 1, epoch: 127, acc: 0.7666667103767395, Loss 0.5497826373577118\n",
      "RNNA, rep: 1, epoch: 128, acc: 0.7333333492279053, Loss 0.5720567287504673\n",
      "RNNA, rep: 1, epoch: 129, acc: 0.7666667103767395, Loss 0.590720365345478\n",
      "RNNA, rep: 1, epoch: 130, acc: 0.7866665124893188, Loss 0.563555088788271\n",
      "RNNA, rep: 1, epoch: 131, acc: 0.7733333706855774, Loss 0.5499202893674373\n",
      "RNNA, rep: 1, epoch: 132, acc: 0.746666669845581, Loss 0.6280948257446289\n",
      "RNNA, rep: 1, epoch: 133, acc: 0.7599998712539673, Loss 0.5681215646862984\n",
      "RNNA, rep: 1, epoch: 134, acc: 0.763333261013031, Loss 0.5669089487195015\n",
      "RNNA, rep: 1, epoch: 135, acc: 0.7600001692771912, Loss 0.5538270607590675\n",
      "RNNA, rep: 1, epoch: 136, acc: 0.75, Loss 0.5926002976298332\n",
      "RNNA, rep: 1, epoch: 137, acc: 0.753333330154419, Loss 0.5996159918606281\n",
      "RNNA, rep: 1, epoch: 138, acc: 0.7200000286102295, Loss 0.6272205153107643\n",
      "RNNA, rep: 1, epoch: 139, acc: 0.7400000691413879, Loss 0.6132428601384163\n",
      "RNNA, rep: 1, epoch: 140, acc: 0.7766665816307068, Loss 0.530177469998598\n",
      "RNNA, rep: 1, epoch: 141, acc: 0.73333340883255, Loss 0.6071275308728218\n",
      "RNNA, rep: 1, epoch: 142, acc: 0.76666659116745, Loss 0.5946761992573738\n",
      "RNNA, rep: 1, epoch: 143, acc: 0.7633334994316101, Loss 0.5406445418298245\n",
      "RNNA, rep: 1, epoch: 144, acc: 0.7833333611488342, Loss 0.5434135825932026\n",
      "RNNA, rep: 1, epoch: 145, acc: 0.8199998736381531, Loss 0.48192050129175185\n",
      "RNNA, rep: 1, epoch: 146, acc: 0.7666666507720947, Loss 0.5458321735262871\n",
      "RNNA, rep: 1, epoch: 147, acc: 0.7433334589004517, Loss 0.6303580613434314\n",
      "RNNA, rep: 1, epoch: 148, acc: 0.7766666412353516, Loss 0.5483780114352703\n",
      "RNNA, rep: 1, epoch: 149, acc: 0.7533332705497742, Loss 0.5838531264662743\n",
      "RNNA, rep: 1, epoch: 150, acc: 0.7899998426437378, Loss 0.562437246888876\n",
      "RNNA, rep: 1, epoch: 151, acc: 0.736666738986969, Loss 0.5692176604270935\n",
      "RNNA, rep: 1, epoch: 152, acc: 0.7400000691413879, Loss 0.5867484459280967\n",
      "RNNA, rep: 1, epoch: 153, acc: 0.7866666316986084, Loss 0.5113241510093212\n",
      "RNNA, rep: 1, epoch: 154, acc: 0.7266666293144226, Loss 0.6111612595617771\n",
      "RNNA, rep: 1, epoch: 155, acc: 0.699999988079071, Loss 0.596326835155487\n",
      "RNNA, rep: 1, epoch: 156, acc: 0.7599999904632568, Loss 0.6003280830383301\n",
      "RNNA, rep: 1, epoch: 157, acc: 0.7766666412353516, Loss 0.5203672230243683\n",
      "RNNA, rep: 1, epoch: 158, acc: 0.7333333492279053, Loss 0.6224933847784996\n",
      "RNNA, rep: 1, epoch: 159, acc: 0.7233332991600037, Loss 0.5832925683259964\n",
      "RNNA, rep: 1, epoch: 160, acc: 0.7433333396911621, Loss 0.6118292540311814\n",
      "RNNA, rep: 1, epoch: 161, acc: 0.7233331799507141, Loss 0.5985082602500915\n",
      "RNNA, rep: 1, epoch: 162, acc: 0.7433334589004517, Loss 0.5932684132456779\n",
      "RNNA, rep: 1, epoch: 163, acc: 0.7533331513404846, Loss 0.6276986619830132\n",
      "RNNA, rep: 1, epoch: 164, acc: 0.7633334398269653, Loss 0.6105102917551994\n",
      "RNNA, rep: 1, epoch: 165, acc: 0.7733333706855774, Loss 0.5724491584300995\n",
      "RNNA, rep: 1, epoch: 166, acc: 0.763333261013031, Loss 0.5545606428384781\n",
      "RNNA, rep: 1, epoch: 167, acc: 0.7366667985916138, Loss 0.5552075392007828\n",
      "RNNA, rep: 1, epoch: 168, acc: 0.7600000500679016, Loss 0.5725136408209801\n",
      "RNNA, rep: 1, epoch: 169, acc: 0.7133333683013916, Loss 0.5837367480993271\n",
      "RNNA, rep: 1, epoch: 170, acc: 0.7599999904632568, Loss 0.5459681132435799\n",
      "RNNA, rep: 1, epoch: 171, acc: 0.7366665601730347, Loss 0.5390407553315163\n",
      "RNNA, rep: 1, epoch: 172, acc: 0.7666667699813843, Loss 0.5573029401898384\n",
      "RNNA, rep: 1, epoch: 173, acc: 0.7166666984558105, Loss 0.6427186486124993\n",
      "RNNA, rep: 1, epoch: 174, acc: 0.7566666603088379, Loss 0.5882952775061131\n",
      "RNNA, rep: 1, epoch: 175, acc: 0.7266668081283569, Loss 0.6329420486092567\n",
      "RNNA, rep: 1, epoch: 176, acc: 0.7566664814949036, Loss 0.5398122555017472\n",
      "RNNA, rep: 1, epoch: 177, acc: 0.7233333587646484, Loss 0.602326093018055\n",
      "RNNA, rep: 1, epoch: 178, acc: 0.779999852180481, Loss 0.5420097669959069\n",
      "RNNA, rep: 1, epoch: 179, acc: 0.7633333802223206, Loss 0.5373972123861313\n",
      "RNNA, rep: 1, epoch: 180, acc: 0.7599999904632568, Loss 0.5682392627000808\n",
      "RNNA, rep: 1, epoch: 181, acc: 0.7600001692771912, Loss 0.5598567873239517\n",
      "RNNA, rep: 1, epoch: 182, acc: 0.7599999904632568, Loss 0.5717524641752243\n",
      "RNNA, rep: 1, epoch: 183, acc: 0.7166666388511658, Loss 0.6610089135169983\n",
      "RNNA, rep: 1, epoch: 184, acc: 0.779999852180481, Loss 0.5101273965835571\n",
      "RNNA, rep: 1, epoch: 185, acc: 0.7833333611488342, Loss 0.5631363704800606\n",
      "RNNA, rep: 1, epoch: 186, acc: 0.7633334398269653, Loss 0.5702562388777733\n",
      "RNNA, rep: 1, epoch: 187, acc: 0.7866666316986084, Loss 0.4958704563975334\n",
      "RNNA, rep: 1, epoch: 188, acc: 0.7566667199134827, Loss 0.5668639788031578\n",
      "RNNA, rep: 1, epoch: 189, acc: 0.7733333110809326, Loss 0.5457468220591545\n",
      "RNNA, rep: 1, epoch: 190, acc: 0.7066667079925537, Loss 0.6684045788645744\n",
      "RNNA, rep: 1, epoch: 191, acc: 0.7433334589004517, Loss 0.5861989215016365\n",
      "RNNA, rep: 1, epoch: 192, acc: 0.7766665816307068, Loss 0.6104674956202507\n",
      "RNNA, rep: 1, epoch: 193, acc: 0.7900000214576721, Loss 0.5080112054944038\n",
      "RNNA, rep: 1, epoch: 194, acc: 0.7233335375785828, Loss 0.6119904997944832\n",
      "RNNA, rep: 1, epoch: 195, acc: 0.7699999213218689, Loss 0.605517435669899\n",
      "RNNA, rep: 1, epoch: 196, acc: 0.7799999713897705, Loss 0.5429597944021225\n",
      "RNNA, rep: 1, epoch: 197, acc: 0.8033332824707031, Loss 0.5019121788442135\n",
      "RNNA, rep: 1, epoch: 198, acc: 0.7533332705497742, Loss 0.5793289455771446\n",
      "RNNA, rep: 1, epoch: 199, acc: 0.7099999785423279, Loss 0.5780890849232674\n",
      "RNNA, rep: 1, epoch: 200, acc: 0.7566667199134827, Loss 0.5187031799554824\n",
      "RNNA, rep: 1, epoch: 201, acc: 0.7699999809265137, Loss 0.5902711799740792\n",
      "RNNA, rep: 1, epoch: 202, acc: 0.7266666293144226, Loss 0.6174613294005394\n",
      "RNNA, rep: 1, epoch: 203, acc: 0.7766666412353516, Loss 0.547692182958126\n",
      "RNNA, rep: 1, epoch: 204, acc: 0.7400000691413879, Loss 0.5839055833220482\n",
      "RNNA, rep: 1, epoch: 205, acc: 0.7666666507720947, Loss 0.5271707502007484\n",
      "RNNA, rep: 1, epoch: 206, acc: 0.7733333110809326, Loss 0.5990092355012894\n",
      "RNNA, rep: 1, epoch: 207, acc: 0.7533332705497742, Loss 0.5647491720318795\n",
      "RNNA, rep: 1, epoch: 208, acc: 0.7733331918716431, Loss 0.5008192232251167\n",
      "RNNA, rep: 1, epoch: 209, acc: 0.7766665816307068, Loss 0.5647890318930149\n",
      "RNNA, rep: 1, epoch: 210, acc: 0.7733334302902222, Loss 0.5189305819571018\n",
      "RNNA, rep: 1, epoch: 211, acc: 0.7433332204818726, Loss 0.5518058252334594\n",
      "RNNA, rep: 1, epoch: 212, acc: 0.7333333492279053, Loss 0.580306216776371\n",
      "RNNA, rep: 1, epoch: 213, acc: 0.7333333492279053, Loss 0.6192793476581574\n",
      "RNNA, rep: 1, epoch: 214, acc: 0.783333420753479, Loss 0.5736855199933052\n",
      "RNNA, rep: 1, epoch: 215, acc: 0.7633333802223206, Loss 0.5628063163161278\n",
      "RNNA, rep: 1, epoch: 216, acc: 0.7433333396911621, Loss 0.6035903126001358\n",
      "RNNA, rep: 1, epoch: 217, acc: 0.7833333611488342, Loss 0.5312414368987084\n",
      "RNNA, rep: 1, epoch: 218, acc: 0.7733334302902222, Loss 0.5280698326230049\n",
      "RNNA, rep: 1, epoch: 219, acc: 0.7566666603088379, Loss 0.5569916725158691\n",
      "RNNA, rep: 1, epoch: 220, acc: 0.7733333706855774, Loss 0.5565421169996262\n",
      "RNNA, rep: 1, epoch: 221, acc: 0.7766667008399963, Loss 0.5402019566297531\n",
      "RNNA, rep: 1, epoch: 222, acc: 0.7233333587646484, Loss 0.6301071950793267\n",
      "RNNA, rep: 1, epoch: 223, acc: 0.753333330154419, Loss 0.5305354963243007\n",
      "RNNA, rep: 1, epoch: 224, acc: 0.7733334302902222, Loss 0.5616774839162827\n",
      "RNNA, rep: 1, epoch: 225, acc: 0.7799999713897705, Loss 0.5607948327064514\n",
      "RNNA, rep: 1, epoch: 226, acc: 0.7499999403953552, Loss 0.6140550580620766\n",
      "RNNA, rep: 1, epoch: 227, acc: 0.753333330154419, Loss 0.5784241279959679\n",
      "RNNA, rep: 1, epoch: 228, acc: 0.76666659116745, Loss 0.5729998698830605\n",
      "RNNA, rep: 1, epoch: 229, acc: 0.7766666412353516, Loss 0.5099981078505516\n",
      "RNNA, rep: 1, epoch: 230, acc: 0.7599999904632568, Loss 0.5838243588805199\n",
      "RNNA, rep: 1, epoch: 231, acc: 0.7733333110809326, Loss 0.5569220843911171\n",
      "RNNA, rep: 1, epoch: 232, acc: 0.7733333706855774, Loss 0.536657123863697\n",
      "RNNA, rep: 1, epoch: 233, acc: 0.7533331513404846, Loss 0.5829383710026741\n",
      "RNNA, rep: 1, epoch: 234, acc: 0.7899999022483826, Loss 0.5019502648711205\n",
      "RNNA, rep: 1, epoch: 235, acc: 0.7699999809265137, Loss 0.5439891822636127\n",
      "RNNA, rep: 1, epoch: 236, acc: 0.7966667413711548, Loss 0.5467104081809521\n",
      "RNNA, rep: 1, epoch: 237, acc: 0.753333330154419, Loss 0.5773681603372097\n",
      "RNNA, rep: 1, epoch: 238, acc: 0.7566667199134827, Loss 0.5382368338108062\n",
      "RNNA, rep: 1, epoch: 239, acc: 0.7799999713897705, Loss 0.47467722117900846\n",
      "RNNA, rep: 1, epoch: 240, acc: 0.7600000500679016, Loss 0.5178354522585868\n",
      "RNNA, rep: 1, epoch: 241, acc: 0.7733331918716431, Loss 0.538492029607296\n",
      "RNNA, rep: 1, epoch: 242, acc: 0.7799999713897705, Loss 0.5293609845638275\n",
      "RNNA, rep: 1, epoch: 243, acc: 0.7533332705497742, Loss 0.5684674492478371\n",
      "RNNA, rep: 1, epoch: 244, acc: 0.7766668200492859, Loss 0.4936676698923111\n",
      "RNNA, rep: 1, epoch: 245, acc: 0.7866666316986084, Loss 0.5491302666068077\n",
      "RNNA, rep: 1, epoch: 246, acc: 0.763333261013031, Loss 0.5126955541968345\n",
      "RNNA, rep: 1, epoch: 247, acc: 0.7733333706855774, Loss 0.5032153901457787\n",
      "RNNA, rep: 1, epoch: 248, acc: 0.7933335304260254, Loss 0.503082634806633\n",
      "RNNA, rep: 1, epoch: 249, acc: 0.7899998426437378, Loss 0.5080222555994988\n",
      "RNNA, rep: 1, epoch: 250, acc: 0.746666669845581, Loss 0.5802991193532944\n",
      "RNNA, rep: 1, epoch: 251, acc: 0.770000159740448, Loss 0.5155537074804306\n",
      "RNNA, rep: 1, epoch: 252, acc: 0.7699999809265137, Loss 0.5259684687852859\n",
      "RNNA, rep: 1, epoch: 253, acc: 0.7933333516120911, Loss 0.4826657095551491\n",
      "RNNA, rep: 1, epoch: 254, acc: 0.7600001692771912, Loss 0.5185982605814934\n",
      "RNNA, rep: 1, epoch: 255, acc: 0.7766666412353516, Loss 0.479043171107769\n",
      "RNNA, rep: 1, epoch: 256, acc: 0.7666666507720947, Loss 0.5880479431152343\n",
      "RNNA, rep: 1, epoch: 257, acc: 0.7766666412353516, Loss 0.5694226293265819\n",
      "RNNA, rep: 1, epoch: 258, acc: 0.7666668891906738, Loss 0.540853720754385\n",
      "RNNA, rep: 1, epoch: 259, acc: 0.7033333778381348, Loss 0.5996926185488701\n",
      "RNNA, rep: 1, epoch: 260, acc: 0.7966665625572205, Loss 0.45277165770530703\n",
      "RNNA, rep: 1, epoch: 261, acc: 0.7333333492279053, Loss 0.6001219834387302\n",
      "RNNA, rep: 1, epoch: 262, acc: 0.7700001001358032, Loss 0.5394671618938446\n",
      "RNNA, rep: 1, epoch: 263, acc: 0.783333420753479, Loss 0.5078717586398125\n",
      "RNNA, rep: 1, epoch: 264, acc: 0.7666664719581604, Loss 0.5281365439295769\n",
      "RNNA, rep: 1, epoch: 265, acc: 0.823333203792572, Loss 0.5142882513999939\n",
      "RNNA, rep: 1, epoch: 266, acc: 0.7799999713897705, Loss 0.5050143392384052\n",
      "RNNA, rep: 1, epoch: 267, acc: 0.7699999809265137, Loss 0.5637950724363328\n",
      "RNNA, rep: 1, epoch: 268, acc: 0.7533332109451294, Loss 0.5469388140738011\n",
      "RNNA, rep: 1, epoch: 269, acc: 0.7666667699813843, Loss 0.5483089201152325\n",
      "RNNA, rep: 1, epoch: 270, acc: 0.7599999904632568, Loss 0.5604985809326172\n",
      "RNNA, rep: 1, epoch: 271, acc: 0.7799999713897705, Loss 0.5498075719177723\n",
      "RNNA, rep: 1, epoch: 272, acc: 0.7699999213218689, Loss 0.5080542013049125\n",
      "RNNA, rep: 1, epoch: 273, acc: 0.75, Loss 0.5320046845078469\n",
      "RNNA, rep: 1, epoch: 274, acc: 0.7966665625572205, Loss 0.5116698887944221\n",
      "RNNA, rep: 1, epoch: 275, acc: 0.7766665816307068, Loss 0.5603897388279439\n",
      "RNNA, rep: 1, epoch: 276, acc: 0.7833331823348999, Loss 0.47049168065190317\n",
      "RNNA, rep: 1, epoch: 277, acc: 0.7933332920074463, Loss 0.5030827546119689\n",
      "RNNA, rep: 1, epoch: 278, acc: 0.7699998617172241, Loss 0.549775450527668\n",
      "RNNA, rep: 1, epoch: 279, acc: 0.7799999117851257, Loss 0.5252166095376015\n",
      "RNNA, rep: 1, epoch: 280, acc: 0.7666666507720947, Loss 0.5325708875060081\n",
      "RNNA, rep: 1, epoch: 281, acc: 0.7800001502037048, Loss 0.5156054931879044\n",
      "RNNA, rep: 1, epoch: 282, acc: 0.8066666126251221, Loss 0.4899157240986824\n",
      "RNNA, rep: 1, epoch: 283, acc: 0.7566667199134827, Loss 0.5664848734438419\n",
      "RNNA, rep: 1, epoch: 284, acc: 0.7766665816307068, Loss 0.5488242128491402\n",
      "RNNA, rep: 1, epoch: 285, acc: 0.8100000023841858, Loss 0.5015020179748535\n",
      "RNNA, rep: 1, epoch: 286, acc: 0.746666669845581, Loss 0.5734927000105381\n",
      "RNNA, rep: 1, epoch: 287, acc: 0.7666666507720947, Loss 0.4932128004729748\n",
      "RNNA, rep: 1, epoch: 288, acc: 0.8099997043609619, Loss 0.4861075784265995\n",
      "RNNA, rep: 1, epoch: 289, acc: 0.7866666913032532, Loss 0.4721705637872219\n",
      "RNNA, rep: 1, epoch: 290, acc: 0.7933333516120911, Loss 0.45406363651156423\n",
      "RNNA, rep: 1, epoch: 291, acc: 0.7500001788139343, Loss 0.5972290875017643\n",
      "RNNA, rep: 1, epoch: 292, acc: 0.7899999022483826, Loss 0.49689454391598703\n",
      "RNNA, rep: 1, epoch: 293, acc: 0.7633332014083862, Loss 0.5371510595083236\n",
      "RNNA, rep: 1, epoch: 294, acc: 0.7633334398269653, Loss 0.5171416747570038\n",
      "RNNA, rep: 1, epoch: 295, acc: 0.8066666126251221, Loss 0.48469768941402436\n",
      "RNNA, rep: 1, epoch: 296, acc: 0.7466667294502258, Loss 0.5398767079412937\n",
      "RNNA, rep: 1, epoch: 297, acc: 0.753333330154419, Loss 0.55077084004879\n",
      "RNNA, rep: 1, epoch: 298, acc: 0.7499999403953552, Loss 0.6076533840596676\n",
      "RNNA, rep: 1, epoch: 299, acc: 0.7566666603088379, Loss 0.6095317927002907\n",
      "RNNA, rep: 1, epoch: 300, acc: 0.8033334612846375, Loss 0.46792820930480955\n",
      "RNNA, rep: 1, epoch: 301, acc: 0.763333261013031, Loss 0.5289695343375206\n",
      "RNNA, rep: 1, epoch: 302, acc: 0.7566667199134827, Loss 0.537554683983326\n",
      "RNNA, rep: 1, epoch: 303, acc: 0.76666659116745, Loss 0.5111135502159595\n",
      "RNNA, rep: 1, epoch: 304, acc: 0.7866665720939636, Loss 0.46503540500998497\n",
      "RNNA, rep: 1, epoch: 305, acc: 0.7633334398269653, Loss 0.5279957857728005\n",
      "RNNA, rep: 1, epoch: 306, acc: 0.7233334183692932, Loss 0.6123007103800774\n",
      "RNNA, rep: 1, epoch: 307, acc: 0.7666666507720947, Loss 0.5604496744275093\n",
      "RNNA, rep: 1, epoch: 308, acc: 0.8033334612846375, Loss 0.47488201439380645\n",
      "RNNA, rep: 1, epoch: 309, acc: 0.7566666603088379, Loss 0.5538086515665054\n",
      "RNNA, rep: 1, epoch: 310, acc: 0.7866666913032532, Loss 0.48366288557648657\n",
      "RNNA, rep: 1, epoch: 311, acc: 0.7866666913032532, Loss 0.5253070920705796\n",
      "RNNA, rep: 1, epoch: 312, acc: 0.7699999809265137, Loss 0.5170143029093742\n",
      "RNNA, rep: 1, epoch: 313, acc: 0.7833333611488342, Loss 0.48177688717842104\n",
      "RNNA, rep: 1, epoch: 314, acc: 0.8066665530204773, Loss 0.4532306233048439\n",
      "RNNA, rep: 1, epoch: 315, acc: 0.7866666913032532, Loss 0.4752993580698967\n",
      "RNNA, rep: 1, epoch: 316, acc: 0.8100000023841858, Loss 0.4539314377307892\n",
      "RNNA, rep: 1, epoch: 317, acc: 0.7599999904632568, Loss 0.492610824406147\n",
      "RNNA, rep: 1, epoch: 318, acc: 0.8166666626930237, Loss 0.5039215379953385\n",
      "RNNA, rep: 1, epoch: 319, acc: 0.759999692440033, Loss 0.5495390854775906\n",
      "RNNA, rep: 1, epoch: 320, acc: 0.7633332014083862, Loss 0.5829708288609982\n",
      "RNNA, rep: 1, epoch: 321, acc: 0.7866666913032532, Loss 0.5396622511744499\n",
      "RNNA, rep: 1, epoch: 322, acc: 0.7866666913032532, Loss 0.4620687249302864\n",
      "RNNA, rep: 1, epoch: 323, acc: 0.8266665935516357, Loss 0.4752884437143803\n",
      "RNNA, rep: 1, epoch: 324, acc: 0.7866665720939636, Loss 0.5453339514136314\n",
      "RNNA, rep: 1, epoch: 325, acc: 0.7633334398269653, Loss 0.5059244360029698\n",
      "RNNA, rep: 1, epoch: 326, acc: 0.7499999403953552, Loss 0.5919080278277398\n",
      "RNNA, rep: 1, epoch: 327, acc: 0.8166665434837341, Loss 0.47125194877386095\n",
      "RNNA, rep: 1, epoch: 328, acc: 0.7966665029525757, Loss 0.5019060538709164\n",
      "RNNA, rep: 1, epoch: 329, acc: 0.7666664123535156, Loss 0.5474940919876099\n",
      "RNNA, rep: 1, epoch: 330, acc: 0.7933332324028015, Loss 0.5200887063145637\n",
      "RNNA, rep: 1, epoch: 331, acc: 0.7233333587646484, Loss 0.6008728042244911\n",
      "RNNA, rep: 1, epoch: 332, acc: 0.8066665530204773, Loss 0.5045495584607125\n",
      "RNNA, rep: 1, epoch: 333, acc: 0.8033331036567688, Loss 0.4774935042858124\n",
      "RNNA, rep: 1, epoch: 334, acc: 0.7799999713897705, Loss 0.5472546572983265\n",
      "RNNA, rep: 1, epoch: 335, acc: 0.73333340883255, Loss 0.6009868280589581\n",
      "RNNA, rep: 1, epoch: 336, acc: 0.7433332800865173, Loss 0.503207471370697\n",
      "RNNA, rep: 1, epoch: 337, acc: 0.7533332705497742, Loss 0.5512837249040604\n",
      "RNNA, rep: 1, epoch: 338, acc: 0.8033331036567688, Loss 0.48181766211986543\n",
      "RNNA, rep: 1, epoch: 339, acc: 0.7766667008399963, Loss 0.5780317936837673\n",
      "RNNA, rep: 1, epoch: 340, acc: 0.7866666316986084, Loss 0.5299838922917843\n",
      "RNNA, rep: 1, epoch: 341, acc: 0.7733334302902222, Loss 0.539740141928196\n",
      "RNNA, rep: 1, epoch: 342, acc: 0.7733333706855774, Loss 0.5262195786833763\n",
      "RNNA, rep: 1, epoch: 343, acc: 0.7833333015441895, Loss 0.5233681847155094\n",
      "RNNA, rep: 1, epoch: 344, acc: 0.7366665601730347, Loss 0.5714540940523147\n",
      "RNNA, rep: 1, epoch: 345, acc: 0.7533332705497742, Loss 0.5461989852786064\n",
      "RNNA, rep: 1, epoch: 346, acc: 0.7700001001358032, Loss 0.49563786447048186\n",
      "RNNA, rep: 1, epoch: 347, acc: 0.76666659116745, Loss 0.53077807366848\n",
      "RNNA, rep: 1, epoch: 348, acc: 0.75, Loss 0.5078390207886696\n",
      "RNNA, rep: 1, epoch: 349, acc: 0.7833333015441895, Loss 0.5100083759427071\n",
      "RNNA, rep: 1, epoch: 350, acc: 0.7766665816307068, Loss 0.4765699350833893\n",
      "RNNA, rep: 1, epoch: 351, acc: 0.7966665625572205, Loss 0.4431055274605751\n",
      "RNNA, rep: 1, epoch: 352, acc: 0.75, Loss 0.5264588326215744\n",
      "RNNA, rep: 1, epoch: 353, acc: 0.7833333611488342, Loss 0.4879792460799217\n",
      "RNNA, rep: 1, epoch: 354, acc: 0.7566667199134827, Loss 0.5308955550193787\n",
      "RNNA, rep: 1, epoch: 355, acc: 0.7966667413711548, Loss 0.4710879895091057\n",
      "RNNA, rep: 1, epoch: 356, acc: 0.7533334493637085, Loss 0.5335642811655998\n",
      "RNNA, rep: 1, epoch: 357, acc: 0.7966666221618652, Loss 0.4298801448941231\n",
      "RNNA, rep: 1, epoch: 358, acc: 0.7633333802223206, Loss 0.5264368847012519\n",
      "RNNA, rep: 1, epoch: 359, acc: 0.836666464805603, Loss 0.4526475834846497\n",
      "RNNA, rep: 1, epoch: 360, acc: 0.7866666913032532, Loss 0.48611721485853193\n",
      "RNNA, rep: 1, epoch: 361, acc: 0.81333327293396, Loss 0.4673888611793518\n",
      "RNNA, rep: 1, epoch: 362, acc: 0.8266665935516357, Loss 0.482784828543663\n",
      "RNNA, rep: 1, epoch: 363, acc: 0.8066665530204773, Loss 0.5076091651618481\n",
      "RNNA, rep: 1, epoch: 364, acc: 0.76666659116745, Loss 0.49817824855446813\n",
      "RNNA, rep: 1, epoch: 365, acc: 0.7699999809265137, Loss 0.5143689784407616\n",
      "RNNA, rep: 1, epoch: 366, acc: 0.7900000214576721, Loss 0.503955153375864\n",
      "RNNA, rep: 1, epoch: 367, acc: 0.7766667008399963, Loss 0.5160773083567619\n",
      "RNNA, rep: 1, epoch: 368, acc: 0.7566667199134827, Loss 0.5519905641674996\n",
      "RNNA, rep: 1, epoch: 369, acc: 0.7733333706855774, Loss 0.5168897114694119\n",
      "RNNA, rep: 1, epoch: 370, acc: 0.8100000023841858, Loss 0.4802881021797657\n",
      "RNNA, rep: 1, epoch: 371, acc: 0.7866665720939636, Loss 0.44974845334887503\n",
      "RNNA, rep: 1, epoch: 372, acc: 0.8100000023841858, Loss 0.46190364614129065\n",
      "RNNA, rep: 1, epoch: 373, acc: 0.7866666316986084, Loss 0.472717120051384\n",
      "RNNA, rep: 1, epoch: 374, acc: 0.7699998617172241, Loss 0.5498599427938461\n",
      "RNNA, rep: 1, epoch: 375, acc: 0.7900000214576721, Loss 0.4934576216340065\n",
      "RNNA, rep: 1, epoch: 376, acc: 0.7700001001358032, Loss 0.4964546148478985\n",
      "RNNA, rep: 1, epoch: 377, acc: 0.73333340883255, Loss 0.544424899071455\n",
      "RNNA, rep: 1, epoch: 378, acc: 0.800000011920929, Loss 0.4958416196703911\n",
      "RNNA, rep: 1, epoch: 379, acc: 0.7566666603088379, Loss 0.5279657071828843\n",
      "RNNA, rep: 1, epoch: 380, acc: 0.7866666913032532, Loss 0.505673850029707\n",
      "RNNA, rep: 1, epoch: 381, acc: 0.7933332324028015, Loss 0.4720934109389782\n",
      "RNNA, rep: 1, epoch: 382, acc: 0.7966667413711548, Loss 0.5167417661845684\n",
      "RNNA, rep: 1, epoch: 383, acc: 0.8066667318344116, Loss 0.44148721888661385\n",
      "RNNA, rep: 1, epoch: 384, acc: 0.7899999022483826, Loss 0.47052782505750657\n",
      "RNNA, rep: 1, epoch: 385, acc: 0.7833331823348999, Loss 0.5049026083946228\n",
      "RNNA, rep: 1, epoch: 386, acc: 0.8133333325386047, Loss 0.4590251955389977\n",
      "RNNA, rep: 1, epoch: 387, acc: 0.7766666412353516, Loss 0.5226045890152454\n",
      "RNNA, rep: 1, epoch: 388, acc: 0.7633334398269653, Loss 0.5558842125535012\n",
      "RNNA, rep: 1, epoch: 389, acc: 0.7533332705497742, Loss 0.5205184154212474\n",
      "RNNA, rep: 1, epoch: 390, acc: 0.7566667199134827, Loss 0.5395328049361706\n",
      "RNNA, rep: 1, epoch: 391, acc: 0.75, Loss 0.5556706826388836\n",
      "RNNA, rep: 1, epoch: 392, acc: 0.7566667199134827, Loss 0.527513575553894\n",
      "RNNA, rep: 1, epoch: 393, acc: 0.7966666221618652, Loss 0.5393439400196075\n",
      "RNNA, rep: 1, epoch: 394, acc: 0.7866665720939636, Loss 0.47794062316417696\n",
      "RNNA, rep: 1, epoch: 395, acc: 0.7899999022483826, Loss 0.5390792223811149\n",
      "RNNA, rep: 1, epoch: 396, acc: 0.7333332896232605, Loss 0.5363452526926994\n",
      "RNNA, rep: 1, epoch: 397, acc: 0.7433333396911621, Loss 0.4833356657624245\n",
      "RNNA, rep: 1, epoch: 398, acc: 0.7933332324028015, Loss 0.4929031139612198\n",
      "RNNA, rep: 1, epoch: 399, acc: 0.7799999117851257, Loss 0.5554542496800423\n",
      "RNNA, rep: 1, epoch: 400, acc: 0.7666664719581604, Loss 0.5239784535765648\n",
      "RNNA, rep: 1, epoch: 401, acc: 0.8099998235702515, Loss 0.47495599418878554\n",
      "RNNA, rep: 1, epoch: 402, acc: 0.7566667199134827, Loss 0.5147075872123241\n",
      "RNNA, rep: 1, epoch: 403, acc: 0.7933332920074463, Loss 0.5179982602596283\n",
      "RNNA, rep: 1, epoch: 404, acc: 0.8000000715255737, Loss 0.5066209691762924\n",
      "RNNA, rep: 1, epoch: 405, acc: 0.7966666221618652, Loss 0.460764746516943\n",
      "RNNA, rep: 1, epoch: 406, acc: 0.7999998331069946, Loss 0.496591694355011\n",
      "RNNA, rep: 1, epoch: 407, acc: 0.7799999713897705, Loss 0.5209173160791397\n",
      "RNNA, rep: 1, epoch: 408, acc: 0.7699997425079346, Loss 0.47262837037444116\n",
      "RNNA, rep: 1, epoch: 409, acc: 0.7466665506362915, Loss 0.5404374799132348\n",
      "RNNA, rep: 1, epoch: 410, acc: 0.7733333706855774, Loss 0.5413888356089592\n",
      "RNNA, rep: 1, epoch: 411, acc: 0.7766666412353516, Loss 0.5068000027537346\n",
      "RNNA, rep: 1, epoch: 412, acc: 0.7533332705497742, Loss 0.4987748131155968\n",
      "RNNA, rep: 1, epoch: 413, acc: 0.7900000214576721, Loss 0.462901551425457\n",
      "RNNA, rep: 1, epoch: 414, acc: 0.7966668009757996, Loss 0.47666055560112\n",
      "RNNA, rep: 1, epoch: 415, acc: 0.7799999117851257, Loss 0.47123376190662386\n",
      "RNNA, rep: 1, epoch: 416, acc: 0.7933332920074463, Loss 0.4934809395670891\n",
      "RNNA, rep: 1, epoch: 417, acc: 0.7766666412353516, Loss 0.5186006890237331\n",
      "RNNA, rep: 1, epoch: 418, acc: 0.7833331227302551, Loss 0.5235925659537315\n",
      "RNNA, rep: 1, epoch: 419, acc: 0.7799999117851257, Loss 0.4730273874104023\n",
      "RNNA, rep: 1, epoch: 420, acc: 0.7499999403953552, Loss 0.5389625544846058\n",
      "RNNA, rep: 1, epoch: 421, acc: 0.7599999308586121, Loss 0.502086142450571\n",
      "RNNA, rep: 1, epoch: 422, acc: 0.7733333110809326, Loss 0.5157669386267663\n",
      "RNNA, rep: 1, epoch: 423, acc: 0.7566667199134827, Loss 0.5453136485815048\n",
      "RNNA, rep: 1, epoch: 424, acc: 0.793333113193512, Loss 0.5405909222364426\n",
      "RNNA, rep: 1, epoch: 425, acc: 0.7566667199134827, Loss 0.570765976011753\n",
      "RNNA, rep: 1, epoch: 426, acc: 0.7933332324028015, Loss 0.484120697081089\n",
      "RNNA, rep: 1, epoch: 427, acc: 0.7866666316986084, Loss 0.48460195571184156\n",
      "RNNA, rep: 1, epoch: 428, acc: 0.7566666603088379, Loss 0.5618522085249424\n",
      "RNNA, rep: 1, epoch: 429, acc: 0.7866666316986084, Loss 0.46627997130155563\n",
      "RNNA, rep: 1, epoch: 430, acc: 0.7733333110809326, Loss 0.49634619534015656\n",
      "RNNA, rep: 1, epoch: 431, acc: 0.7766667008399963, Loss 0.5142670542001724\n",
      "RNNA, rep: 1, epoch: 432, acc: 0.7433332800865173, Loss 0.48563599348068237\n",
      "RNNA, rep: 1, epoch: 433, acc: 0.7766665816307068, Loss 0.5116624745726586\n",
      "RNNA, rep: 1, epoch: 434, acc: 0.7866665124893188, Loss 0.4635637384653091\n",
      "RNNA, rep: 1, epoch: 435, acc: 0.7866665720939636, Loss 0.47967339754104615\n",
      "RNNA, rep: 1, epoch: 436, acc: 0.7933332920074463, Loss 0.49096445232629776\n",
      "RNNA, rep: 1, epoch: 437, acc: 0.7633334398269653, Loss 0.535738993883133\n",
      "RNNA, rep: 1, epoch: 438, acc: 0.8066664934158325, Loss 0.504374957382679\n",
      "RNNA, rep: 1, epoch: 439, acc: 0.7866666316986084, Loss 0.4463093109428883\n",
      "RNNA, rep: 1, epoch: 440, acc: 0.7866665720939636, Loss 0.4493547543883324\n",
      "RNNA, rep: 1, epoch: 441, acc: 0.7733333110809326, Loss 0.4916345828771591\n",
      "RNNA, rep: 1, epoch: 442, acc: 0.8033334612846375, Loss 0.4991418308019638\n",
      "RNNA, rep: 1, epoch: 443, acc: 0.8066666126251221, Loss 0.4674396815896034\n",
      "RNNA, rep: 1, epoch: 444, acc: 0.7733333110809326, Loss 0.5004673558473587\n",
      "RNNA, rep: 1, epoch: 445, acc: 0.7999999523162842, Loss 0.5333689302206039\n",
      "RNNA, rep: 1, epoch: 446, acc: 0.7533334493637085, Loss 0.5452410459518433\n",
      "RNNA, rep: 1, epoch: 447, acc: 0.76666659116745, Loss 0.5444929760694504\n",
      "RNNA, rep: 1, epoch: 448, acc: 0.8033331036567688, Loss 0.5004812350869179\n",
      "RNNA, rep: 1, epoch: 449, acc: 0.7699997425079346, Loss 0.5064167886972427\n",
      "RNNA, rep: 1, epoch: 450, acc: 0.7966666221618652, Loss 0.48164223954081536\n",
      "RNNA, rep: 1, epoch: 451, acc: 0.7699998617172241, Loss 0.5237698975205421\n",
      "RNNA, rep: 1, epoch: 452, acc: 0.7666667103767395, Loss 0.4864497396349907\n",
      "RNNA, rep: 1, epoch: 453, acc: 0.7433333396911621, Loss 0.5063085177540779\n",
      "RNNA, rep: 1, epoch: 454, acc: 0.7900000810623169, Loss 0.4632572394609451\n",
      "RNNA, rep: 1, epoch: 455, acc: 0.7933332920074463, Loss 0.4765545579791069\n",
      "RNNA, rep: 1, epoch: 456, acc: 0.7999998331069946, Loss 0.49167796075344083\n",
      "RNNA, rep: 1, epoch: 457, acc: 0.7933333516120911, Loss 0.5151890498399735\n",
      "RNNA, rep: 1, epoch: 458, acc: 0.7933332920074463, Loss 0.45771924167871475\n",
      "RNNA, rep: 1, epoch: 459, acc: 0.7533334493637085, Loss 0.5202872088551521\n",
      "RNNA, rep: 1, epoch: 460, acc: 0.7866666316986084, Loss 0.4597094044089317\n",
      "RNNA, rep: 1, epoch: 461, acc: 0.7999998331069946, Loss 0.430170484483242\n",
      "RNNA, rep: 1, epoch: 462, acc: 0.7933332324028015, Loss 0.47830141067504883\n",
      "RNNA, rep: 1, epoch: 463, acc: 0.7766666412353516, Loss 0.502694680094719\n",
      "RNNA, rep: 1, epoch: 464, acc: 0.7799999713897705, Loss 0.430579072535038\n",
      "RNNA, rep: 1, epoch: 465, acc: 0.7766665816307068, Loss 0.5379928103089333\n",
      "RNNA, rep: 1, epoch: 466, acc: 0.7966667413711548, Loss 0.48591761872172357\n",
      "RNNA, rep: 1, epoch: 467, acc: 0.7699999809265137, Loss 0.47977741569280624\n",
      "RNNA, rep: 1, epoch: 468, acc: 0.7700001001358032, Loss 0.4797188159823418\n",
      "RNNA, rep: 1, epoch: 469, acc: 0.8166665434837341, Loss 0.45464640825986863\n",
      "RNNA, rep: 1, epoch: 470, acc: 0.7866666913032532, Loss 0.4834379333257675\n",
      "RNNA, rep: 1, epoch: 471, acc: 0.763333261013031, Loss 0.4990051954984665\n",
      "RNNA, rep: 1, epoch: 472, acc: 0.7666667103767395, Loss 0.4702620378136635\n",
      "RNNA, rep: 1, epoch: 473, acc: 0.7766666412353516, Loss 0.530700690895319\n",
      "RNNA, rep: 1, epoch: 474, acc: 0.8166667222976685, Loss 0.4385703235864639\n",
      "RNNA, rep: 1, epoch: 475, acc: 0.789999783039093, Loss 0.5610650458931923\n",
      "RNNA, rep: 1, epoch: 476, acc: 0.7999998331069946, Loss 0.4707740122079849\n",
      "RNNA, rep: 1, epoch: 477, acc: 0.7500002384185791, Loss 0.5069063812494278\n",
      "RNNA, rep: 1, epoch: 478, acc: 0.8000000715255737, Loss 0.4826519912481308\n",
      "RNNA, rep: 1, epoch: 479, acc: 0.7766666412353516, Loss 0.5078544104099274\n",
      "RNNA, rep: 1, epoch: 480, acc: 0.7933330535888672, Loss 0.4990780887007713\n",
      "RNNA, rep: 1, epoch: 481, acc: 0.809999942779541, Loss 0.48382750660181045\n",
      "RNNA, rep: 1, epoch: 482, acc: 0.8066667914390564, Loss 0.4422080664336681\n",
      "RNNA, rep: 1, epoch: 483, acc: 0.7866668105125427, Loss 0.4830633674561977\n",
      "RNNA, rep: 1, epoch: 484, acc: 0.7400000095367432, Loss 0.5699740055203438\n",
      "RNNA, rep: 1, epoch: 485, acc: 0.7833331823348999, Loss 0.4840188881754875\n",
      "RNNA, rep: 1, epoch: 486, acc: 0.7566667199134827, Loss 0.4667549350857735\n",
      "RNNA, rep: 1, epoch: 487, acc: 0.756666898727417, Loss 0.5187570941448212\n",
      "RNNA, rep: 1, epoch: 488, acc: 0.736666738986969, Loss 0.5065394991636276\n",
      "RNNA, rep: 1, epoch: 489, acc: 0.7866666913032532, Loss 0.49368243098258974\n",
      "RNNA, rep: 1, epoch: 490, acc: 0.7633332014083862, Loss 0.5249673476815224\n",
      "RNNA, rep: 1, epoch: 491, acc: 0.7933334112167358, Loss 0.4912144023180008\n",
      "RNNA, rep: 1, epoch: 492, acc: 0.7533332705497742, Loss 0.5526733329892158\n",
      "RNNA, rep: 1, epoch: 493, acc: 0.7933333516120911, Loss 0.48336028516292573\n",
      "RNNA, rep: 1, epoch: 494, acc: 0.770000159740448, Loss 0.4932997588813305\n",
      "RNNA, rep: 1, epoch: 495, acc: 0.8133331537246704, Loss 0.46764335185289385\n",
      "RNNA, rep: 1, epoch: 496, acc: 0.7766666412353516, Loss 0.5147803778946399\n",
      "RNNA, rep: 1, epoch: 497, acc: 0.7733334302902222, Loss 0.4949596932530403\n",
      "RNNA, rep: 1, epoch: 498, acc: 0.7866666316986084, Loss 0.45160654217004775\n",
      "RNNA, rep: 1, epoch: 499, acc: 0.7766666412353516, Loss 0.4711648917198181\n",
      "RNNA, rep: 1, epoch: 500, acc: 0.7666667103767395, Loss 0.5096648789942264\n",
      "RNNA, rep: 1, epoch: 501, acc: 0.7733333110809326, Loss 0.49779258504509927\n",
      "RNNA, rep: 1, epoch: 502, acc: 0.746666669845581, Loss 0.4905005967617035\n",
      "RNNA, rep: 1, epoch: 503, acc: 0.7499999403953552, Loss 0.5542701464891434\n",
      "RNNA, rep: 1, epoch: 504, acc: 0.7566667199134827, Loss 0.5205793797969818\n",
      "RNNA, rep: 1, epoch: 505, acc: 0.7766665816307068, Loss 0.5238454553484917\n",
      "RNNA, rep: 1, epoch: 506, acc: 0.7766665816307068, Loss 0.49509611338376996\n",
      "RNNA, rep: 1, epoch: 507, acc: 0.8033331036567688, Loss 0.5156507965922356\n",
      "RNNA, rep: 1, epoch: 508, acc: 0.7966667413711548, Loss 0.47876993864774703\n",
      "RNNA, rep: 1, epoch: 509, acc: 0.7833333611488342, Loss 0.4437028872966766\n",
      "RNNA, rep: 1, epoch: 510, acc: 0.7733333110809326, Loss 0.4924048973619938\n",
      "RNNA, rep: 1, epoch: 511, acc: 0.8033332824707031, Loss 0.4625173932313919\n",
      "RNNA, rep: 1, epoch: 512, acc: 0.7533332109451294, Loss 0.509154531955719\n",
      "RNNA, rep: 1, epoch: 513, acc: 0.7866666316986084, Loss 0.5423833718895912\n",
      "RNNA, rep: 1, epoch: 514, acc: 0.7799999713897705, Loss 0.4961348582804203\n",
      "RNNA, rep: 1, epoch: 515, acc: 0.7466667890548706, Loss 0.5133698961138725\n",
      "RNNA, rep: 1, epoch: 516, acc: 0.76666659116745, Loss 0.5301615098118782\n",
      "RNNA, rep: 1, epoch: 517, acc: 0.753333330154419, Loss 0.5338140079379081\n",
      "RNNA, rep: 1, epoch: 518, acc: 0.7866666913032532, Loss 0.48754444152116777\n",
      "RNNA, rep: 1, epoch: 519, acc: 0.7833333611488342, Loss 0.4653721669316292\n",
      "RNNA, rep: 1, epoch: 520, acc: 0.7699998617172241, Loss 0.5015267160534859\n",
      "RNNA, rep: 1, epoch: 521, acc: 0.7766667008399963, Loss 0.5019683036208152\n",
      "RNNA, rep: 1, epoch: 522, acc: 0.8033332824707031, Loss 0.41222360223531723\n",
      "RNNA, rep: 1, epoch: 523, acc: 0.8133330345153809, Loss 0.4254161429405212\n",
      "RNNA, rep: 1, epoch: 524, acc: 0.7833333611488342, Loss 0.4277537214756012\n",
      "RNNA, rep: 1, epoch: 525, acc: 0.8199999332427979, Loss 0.4473530748486519\n",
      "RNNA, rep: 1, epoch: 526, acc: 0.7833333015441895, Loss 0.44974863693118095\n",
      "RNNA, rep: 1, epoch: 527, acc: 0.7400000095367432, Loss 0.5625983145833016\n",
      "RNNA, rep: 1, epoch: 528, acc: 0.76666659116745, Loss 0.49018401026725766\n",
      "RNNA, rep: 1, epoch: 529, acc: 0.7866665720939636, Loss 0.484217864125967\n",
      "RNNA, rep: 1, epoch: 530, acc: 0.7700001001358032, Loss 0.5171878875792026\n",
      "RNNA, rep: 1, epoch: 531, acc: 0.7700001001358032, Loss 0.48993931472301483\n",
      "RNNA, rep: 1, epoch: 532, acc: 0.7699999809265137, Loss 0.5025330962240696\n",
      "RNNA, rep: 1, epoch: 533, acc: 0.7600000500679016, Loss 0.4941408766806126\n",
      "RNNA, rep: 1, epoch: 534, acc: 0.7800000905990601, Loss 0.511536223590374\n",
      "RNNA, rep: 1, epoch: 535, acc: 0.7933333516120911, Loss 0.45749535903334615\n",
      "RNNA, rep: 1, epoch: 536, acc: 0.7599998712539673, Loss 0.5229884269833565\n",
      "RNNA, rep: 1, epoch: 537, acc: 0.8166665434837341, Loss 0.4801057924330234\n",
      "RNNA, rep: 1, epoch: 538, acc: 0.7666666507720947, Loss 0.4767133691906929\n",
      "RNNA, rep: 1, epoch: 539, acc: 0.7966666221618652, Loss 0.45212104603648184\n",
      "RNNA, rep: 1, epoch: 540, acc: 0.7966665625572205, Loss 0.5149201768636703\n",
      "RNNA, rep: 1, epoch: 541, acc: 0.8100000023841858, Loss 0.42799865424633027\n",
      "RNNA, rep: 1, epoch: 542, acc: 0.8133331537246704, Loss 0.4480660057067871\n",
      "RNNA, rep: 1, epoch: 543, acc: 0.7999999523162842, Loss 0.49223260417580605\n",
      "RNNA, rep: 1, epoch: 544, acc: 0.7666666507720947, Loss 0.4982659111917019\n",
      "RNNA, rep: 1, epoch: 545, acc: 0.8133333325386047, Loss 0.4505752755701542\n",
      "RNNA, rep: 1, epoch: 546, acc: 0.7933333516120911, Loss 0.4932295316457748\n",
      "RNNA, rep: 1, epoch: 547, acc: 0.7699999213218689, Loss 0.4752149699628353\n",
      "RNNA, rep: 1, epoch: 548, acc: 0.7900000214576721, Loss 0.49625740498304366\n",
      "RNNA, rep: 1, epoch: 549, acc: 0.7900000810623169, Loss 0.48211658716201783\n",
      "RNNA, rep: 1, epoch: 550, acc: 0.7633333802223206, Loss 0.48241914838552474\n",
      "RNNA, rep: 1, epoch: 551, acc: 0.7866665124893188, Loss 0.5395168572664261\n",
      "RNNA, rep: 1, epoch: 552, acc: 0.783333420753479, Loss 0.48370663106441497\n",
      "RNNA, rep: 1, epoch: 553, acc: 0.7433334589004517, Loss 0.5152565881609916\n",
      "RNNA, rep: 1, epoch: 554, acc: 0.7833333611488342, Loss 0.5086809504032135\n",
      "RNNA, rep: 1, epoch: 555, acc: 0.7933332920074463, Loss 0.498277418166399\n",
      "RNNA, rep: 1, epoch: 556, acc: 0.8033331036567688, Loss 0.43575797766447066\n",
      "RNNA, rep: 1, epoch: 557, acc: 0.7866666316986084, Loss 0.4533318531513214\n",
      "RNNA, rep: 1, epoch: 558, acc: 0.7799999117851257, Loss 0.5106627395749093\n",
      "RNNA, rep: 1, epoch: 559, acc: 0.7899998426437378, Loss 0.45132476136088373\n",
      "RNNA, rep: 1, epoch: 560, acc: 0.7966665625572205, Loss 0.4734814627468586\n",
      "RNNA, rep: 1, epoch: 561, acc: 0.8166667222976685, Loss 0.442519573867321\n",
      "RNNA, rep: 1, epoch: 562, acc: 0.7933333516120911, Loss 0.44822510808706284\n",
      "RNNA, rep: 1, epoch: 563, acc: 0.8033332824707031, Loss 0.42528406769037247\n",
      "RNNA, rep: 1, epoch: 564, acc: 0.8133331537246704, Loss 0.4634756690263748\n",
      "RNNA, rep: 1, epoch: 565, acc: 0.8066666126251221, Loss 0.45135976016521456\n",
      "RNNA, rep: 1, epoch: 566, acc: 0.7999998331069946, Loss 0.5026128999888897\n",
      "RNNA, rep: 1, epoch: 567, acc: 0.7599998712539673, Loss 0.5198214520514012\n",
      "RNNA, rep: 1, epoch: 568, acc: 0.8033332228660583, Loss 0.46928577780723574\n",
      "RNNA, rep: 1, epoch: 569, acc: 0.81333327293396, Loss 0.41956597805023194\n",
      "RNNA, rep: 1, epoch: 570, acc: 0.7966668009757996, Loss 0.4609816293418407\n",
      "RNNA, rep: 1, epoch: 571, acc: 0.7766665816307068, Loss 0.5068049730360508\n",
      "RNNA, rep: 1, epoch: 572, acc: 0.7966665625572205, Loss 0.46975770831108093\n",
      "RNNA, rep: 1, epoch: 573, acc: 0.7699999809265137, Loss 0.5236618845164775\n",
      "RNNA, rep: 1, epoch: 574, acc: 0.7333333492279053, Loss 0.569956576526165\n",
      "RNNA, rep: 1, epoch: 575, acc: 0.8066666126251221, Loss 0.4477104011178017\n",
      "RNNA, rep: 1, epoch: 576, acc: 0.7666666507720947, Loss 0.5395243805646897\n",
      "RNNA, rep: 1, epoch: 577, acc: 0.8033332228660583, Loss 0.46616644352674486\n",
      "RNNA, rep: 1, epoch: 578, acc: 0.7833333611488342, Loss 0.49402349174022675\n",
      "RNNA, rep: 1, epoch: 579, acc: 0.7966667413711548, Loss 0.4767144463956356\n",
      "RNNA, rep: 1, epoch: 580, acc: 0.8066665530204773, Loss 0.4968467317521572\n",
      "RNNA, rep: 1, epoch: 581, acc: 0.7566667795181274, Loss 0.5272770023345947\n",
      "RNNA, rep: 1, epoch: 582, acc: 0.8266667127609253, Loss 0.4334942999482155\n",
      "RNNA, rep: 1, epoch: 583, acc: 0.8166667222976685, Loss 0.4384848453104496\n",
      "RNNA, rep: 1, epoch: 584, acc: 0.7799999117851257, Loss 0.47057485952973366\n",
      "RNNA, rep: 1, epoch: 585, acc: 0.7733333110809326, Loss 0.5266378168761731\n",
      "RNNA, rep: 1, epoch: 586, acc: 0.75, Loss 0.49712344467639924\n",
      "RNNA, rep: 1, epoch: 587, acc: 0.7566666603088379, Loss 0.48505757361650464\n",
      "RNNA, rep: 1, epoch: 588, acc: 0.7933333516120911, Loss 0.44472979873418805\n",
      "RNNA, rep: 1, epoch: 589, acc: 0.753333330154419, Loss 0.5123719942569732\n",
      "RNNA, rep: 1, epoch: 590, acc: 0.8100000023841858, Loss 0.4261241778731346\n",
      "RNNA, rep: 1, epoch: 591, acc: 0.8099998235702515, Loss 0.47265975788235665\n",
      "RNNA, rep: 1, epoch: 592, acc: 0.8166665434837341, Loss 0.4536773851513863\n",
      "RNNA, rep: 1, epoch: 593, acc: 0.8066665530204773, Loss 0.4727740898728371\n",
      "RNNA, rep: 1, epoch: 594, acc: 0.7933333516120911, Loss 0.47526866286993025\n",
      "RNNA, rep: 1, epoch: 595, acc: 0.7666666507720947, Loss 0.49027340710163114\n",
      "RNNA, rep: 1, epoch: 596, acc: 0.7633333802223206, Loss 0.5224133592844009\n",
      "RNNA, rep: 1, epoch: 597, acc: 0.7933333516120911, Loss 0.46883155971765517\n",
      "RNNA, rep: 1, epoch: 598, acc: 0.7733331918716431, Loss 0.4986595065891743\n",
      "RNNA, rep: 1, epoch: 599, acc: 0.7733331322669983, Loss 0.46007844150066374\n",
      "RNNA, rep: 1, epoch: 600, acc: 0.8166666626930237, Loss 0.4553612281382084\n",
      "RNNA, rep: 1, epoch: 601, acc: 0.7799999117851257, Loss 0.47107501566410065\n",
      "RNNA, rep: 1, epoch: 602, acc: 0.7533334493637085, Loss 0.5203673705458641\n",
      "RNNA, rep: 1, epoch: 603, acc: 0.7733333110809326, Loss 0.44943077564239503\n",
      "RNNA, rep: 1, epoch: 604, acc: 0.7766665816307068, Loss 0.5069547992944717\n",
      "RNNA, rep: 1, epoch: 605, acc: 0.8333331942558289, Loss 0.46828513652086257\n",
      "RNNA, rep: 1, epoch: 606, acc: 0.779999852180481, Loss 0.46268307626247407\n",
      "RNNA, rep: 1, epoch: 607, acc: 0.76666659116745, Loss 0.5198845681548119\n",
      "RNNA, rep: 1, epoch: 608, acc: 0.7633332014083862, Loss 0.48271303325891496\n",
      "RNNA, rep: 1, epoch: 609, acc: 0.7766666412353516, Loss 0.4847772005200386\n",
      "RNNA, rep: 1, epoch: 610, acc: 0.7600000500679016, Loss 0.5187490436434746\n",
      "RNNA, rep: 1, epoch: 611, acc: 0.8033334612846375, Loss 0.41745632380247116\n",
      "RNNA, rep: 1, epoch: 612, acc: 0.8199999332427979, Loss 0.401518192589283\n",
      "RNNA, rep: 1, epoch: 613, acc: 0.7900001406669617, Loss 0.49375901579856873\n",
      "RNNA, rep: 1, epoch: 614, acc: 0.8166665434837341, Loss 0.4572706373035908\n",
      "RNNA, rep: 1, epoch: 615, acc: 0.8133332133293152, Loss 0.4411653722822666\n",
      "RNNA, rep: 1, epoch: 616, acc: 0.8066667914390564, Loss 0.44106695786118505\n",
      "RNNA, rep: 1, epoch: 617, acc: 0.7733333706855774, Loss 0.4374934434890747\n",
      "RNNA, rep: 1, epoch: 618, acc: 0.7666664719581604, Loss 0.4461684727668762\n",
      "RNNA, rep: 1, epoch: 619, acc: 0.8066665530204773, Loss 0.42647991329431534\n",
      "RNNA, rep: 1, epoch: 620, acc: 0.7499999403953552, Loss 0.47311257749795915\n",
      "RNNA, rep: 1, epoch: 621, acc: 0.8066664934158325, Loss 0.44527055382728575\n",
      "RNNA, rep: 1, epoch: 622, acc: 0.7866665124893188, Loss 0.46473292157053947\n",
      "RNNA, rep: 1, epoch: 623, acc: 0.7733334302902222, Loss 0.4722480256855488\n",
      "RNNA, rep: 1, epoch: 624, acc: 0.7699999809265137, Loss 0.4545800437033176\n",
      "RNNA, rep: 1, epoch: 625, acc: 0.7700001001358032, Loss 0.4782517513632774\n",
      "RNNA, rep: 1, epoch: 626, acc: 0.7933332920074463, Loss 0.4648815609514713\n",
      "RNNA, rep: 1, epoch: 627, acc: 0.7866668701171875, Loss 0.448157853782177\n",
      "RNNA, rep: 1, epoch: 628, acc: 0.81333327293396, Loss 0.4441947290301323\n",
      "RNNA, rep: 1, epoch: 629, acc: 0.7733333110809326, Loss 0.4459900644421577\n",
      "RNNA, rep: 1, epoch: 630, acc: 0.7833333611488342, Loss 0.5114513675868512\n",
      "RNNA, rep: 1, epoch: 631, acc: 0.7900000214576721, Loss 0.4335485988855362\n",
      "RNNA, rep: 1, epoch: 632, acc: 0.8133334517478943, Loss 0.45173751190304756\n",
      "RNNA, rep: 1, epoch: 633, acc: 0.823333203792572, Loss 0.4815374983847141\n",
      "RNNA, rep: 1, epoch: 634, acc: 0.7766666412353516, Loss 0.4708997729420662\n",
      "RNNA, rep: 1, epoch: 635, acc: 0.81333327293396, Loss 0.4486292853951454\n",
      "RNNA, rep: 1, epoch: 636, acc: 0.7999999523162842, Loss 0.4346542550623417\n",
      "RNNA, rep: 1, epoch: 637, acc: 0.8033331036567688, Loss 0.4602473451197147\n",
      "RNNA, rep: 1, epoch: 638, acc: 0.7966666221618652, Loss 0.45420234560966494\n",
      "RNNA, rep: 1, epoch: 639, acc: 0.8100000023841858, Loss 0.4697938260436058\n",
      "RNNA, rep: 1, epoch: 640, acc: 0.7633333802223206, Loss 0.5019996836781502\n",
      "RNNA, rep: 1, epoch: 641, acc: 0.7999999523162842, Loss 0.43095830231904986\n",
      "RNNA, rep: 1, epoch: 642, acc: 0.7799999713897705, Loss 0.45508522033691406\n",
      "RNNA, rep: 1, epoch: 643, acc: 0.7900000214576721, Loss 0.45071631371974946\n",
      "RNNA, rep: 1, epoch: 644, acc: 0.8266665935516357, Loss 0.4390810173749924\n",
      "RNNA, rep: 1, epoch: 645, acc: 0.7866665720939636, Loss 0.42076403722167016\n",
      "RNNA, rep: 1, epoch: 646, acc: 0.7733333706855774, Loss 0.4908886459469795\n",
      "RNNA, rep: 1, epoch: 647, acc: 0.7866668105125427, Loss 0.5246794319152832\n",
      "RNNA, rep: 1, epoch: 648, acc: 0.8299999833106995, Loss 0.4069056950509548\n",
      "RNNA, rep: 1, epoch: 649, acc: 0.793333113193512, Loss 0.4614011526107788\n",
      "RNNA, rep: 1, epoch: 650, acc: 0.8133332133293152, Loss 0.42463245406746863\n",
      "RNNA, rep: 1, epoch: 651, acc: 0.7933332920074463, Loss 0.515393925011158\n",
      "RNNA, rep: 1, epoch: 652, acc: 0.763333261013031, Loss 0.5056140729784966\n",
      "RNNA, rep: 1, epoch: 653, acc: 0.7733333110809326, Loss 0.4234189185500145\n",
      "RNNA, rep: 1, epoch: 654, acc: 0.7300000786781311, Loss 0.5197486928105355\n",
      "RNNA, rep: 1, epoch: 655, acc: 0.75, Loss 0.5056443493068218\n",
      "RNNA, rep: 1, epoch: 656, acc: 0.7966666221618652, Loss 0.4767460519075394\n",
      "RNNA, rep: 1, epoch: 657, acc: 0.8033331036567688, Loss 0.5085395321249961\n",
      "RNNA, rep: 1, epoch: 658, acc: 0.7966665029525757, Loss 0.473245500177145\n",
      "RNNA, rep: 1, epoch: 659, acc: 0.7866666316986084, Loss 0.4743463082611561\n",
      "RNNA, rep: 1, epoch: 660, acc: 0.7933332920074463, Loss 0.429325742572546\n",
      "RNNA, rep: 1, epoch: 661, acc: 0.7999999523162842, Loss 0.43734906136989593\n",
      "RNNA, rep: 1, epoch: 662, acc: 0.7766664624214172, Loss 0.4965201872587204\n",
      "RNNA, rep: 1, epoch: 663, acc: 0.8366666436195374, Loss 0.40739318132400515\n",
      "RNNA, rep: 1, epoch: 664, acc: 0.809999942779541, Loss 0.4161429314315319\n",
      "RNNA, rep: 1, epoch: 665, acc: 0.7833331823348999, Loss 0.46098258838057515\n",
      "RNNA, rep: 1, epoch: 666, acc: 0.7966665625572205, Loss 0.43696251928806307\n",
      "RNNA, rep: 1, epoch: 667, acc: 0.8166666626930237, Loss 0.4347642371058464\n",
      "RNNA, rep: 1, epoch: 668, acc: 0.7666667103767395, Loss 0.46372207641601565\n",
      "RNNA, rep: 1, epoch: 669, acc: 0.7733334302902222, Loss 0.5002798068523407\n",
      "RNNA, rep: 1, epoch: 670, acc: 0.7866665124893188, Loss 0.464272994697094\n",
      "RNNA, rep: 1, epoch: 671, acc: 0.8066666126251221, Loss 0.4278832261264324\n",
      "RNNA, rep: 1, epoch: 672, acc: 0.7599999308586121, Loss 0.4708679035305977\n",
      "RNNA, rep: 1, epoch: 673, acc: 0.81333327293396, Loss 0.4693915568292141\n",
      "RNNA, rep: 1, epoch: 674, acc: 0.8033332228660583, Loss 0.5003808444738388\n",
      "RNNA, rep: 1, epoch: 675, acc: 0.7866665720939636, Loss 0.44898096889257433\n",
      "RNNA, rep: 1, epoch: 676, acc: 0.7833331227302551, Loss 0.5072840401530265\n",
      "RNNA, rep: 1, epoch: 677, acc: 0.7933332920074463, Loss 0.4403845949470997\n",
      "RNNA, rep: 1, epoch: 678, acc: 0.7866668105125427, Loss 0.4747063961625099\n",
      "RNNA, rep: 1, epoch: 679, acc: 0.7999998331069946, Loss 0.4143043714761734\n",
      "RNNA, rep: 1, epoch: 680, acc: 0.7799999117851257, Loss 0.4537725731730461\n",
      "RNNA, rep: 1, epoch: 681, acc: 0.8066664934158325, Loss 0.4579595017433167\n",
      "RNNA, rep: 1, epoch: 682, acc: 0.7899998426437378, Loss 0.4641420415043831\n",
      "RNNA, rep: 1, epoch: 683, acc: 0.7933332920074463, Loss 0.44922418415546417\n",
      "RNNA, rep: 1, epoch: 684, acc: 0.8166667222976685, Loss 0.44777768149971964\n",
      "RNNA, rep: 1, epoch: 685, acc: 0.7733333110809326, Loss 0.5150694721937179\n",
      "RNNA, rep: 1, epoch: 686, acc: 0.7766666412353516, Loss 0.48052336886525154\n",
      "RNNA, rep: 1, epoch: 687, acc: 0.7899999022483826, Loss 0.49664094001054765\n",
      "RNNA, rep: 1, epoch: 688, acc: 0.7899999022483826, Loss 0.46440973967313764\n",
      "RNNA, rep: 1, epoch: 689, acc: 0.76666659116745, Loss 0.45435249149799345\n",
      "RNNA, rep: 1, epoch: 690, acc: 0.7700001001358032, Loss 0.4678842285275459\n",
      "RNNA, rep: 1, epoch: 691, acc: 0.7699999213218689, Loss 0.46124494284391404\n",
      "RNNA, rep: 1, epoch: 692, acc: 0.7666666507720947, Loss 0.4767767509818077\n",
      "RNNA, rep: 1, epoch: 693, acc: 0.789999783039093, Loss 0.4459182047843933\n",
      "RNNA, rep: 1, epoch: 694, acc: 0.7666666507720947, Loss 0.47642562568187713\n",
      "RNNA, rep: 1, epoch: 695, acc: 0.8033332228660583, Loss 0.39097620606422423\n",
      "RNNA, rep: 1, epoch: 696, acc: 0.8066668510437012, Loss 0.4444741249084473\n",
      "RNNA, rep: 1, epoch: 697, acc: 0.8100000023841858, Loss 0.4152298209071159\n",
      "RNNA, rep: 1, epoch: 698, acc: 0.8099998235702515, Loss 0.4267818769812584\n",
      "RNNA, rep: 1, epoch: 699, acc: 0.7699998617172241, Loss 0.4588501325249672\n",
      "RNNA, rep: 1, epoch: 700, acc: 0.7633334398269653, Loss 0.49497985750436785\n",
      "RNNA, rep: 1, epoch: 701, acc: 0.7899999022483826, Loss 0.461758251786232\n",
      "RNNA, rep: 1, epoch: 702, acc: 0.7666667103767395, Loss 0.4734104335308075\n",
      "RNNA, rep: 1, epoch: 703, acc: 0.7833333611488342, Loss 0.46564701408147813\n",
      "RNNA, rep: 1, epoch: 704, acc: 0.8533332347869873, Loss 0.3937962940335274\n",
      "RNNA, rep: 1, epoch: 705, acc: 0.8066664338111877, Loss 0.4526301148533821\n",
      "RNNA, rep: 1, epoch: 706, acc: 0.7766664028167725, Loss 0.43831722348928454\n",
      "RNNA, rep: 1, epoch: 707, acc: 0.7999999523162842, Loss 0.45002572864294055\n",
      "RNNA, rep: 1, epoch: 708, acc: 0.8199998736381531, Loss 0.45022703900933264\n",
      "RNNA, rep: 1, epoch: 709, acc: 0.7766665816307068, Loss 0.48927237540483476\n",
      "RNNA, rep: 1, epoch: 710, acc: 0.7799999117851257, Loss 0.4736455714702606\n",
      "RNNA, rep: 1, epoch: 711, acc: 0.7933332920074463, Loss 0.46686948001384737\n",
      "RNNA, rep: 1, epoch: 712, acc: 0.7966665625572205, Loss 0.40438407510519025\n",
      "RNNA, rep: 1, epoch: 713, acc: 0.7833331823348999, Loss 0.414424569606781\n",
      "RNNA, rep: 1, epoch: 714, acc: 0.7833333015441895, Loss 0.4439812213182449\n",
      "RNNA, rep: 1, epoch: 715, acc: 0.7866665720939636, Loss 0.43390133410692217\n",
      "RNNA, rep: 1, epoch: 716, acc: 0.7733333110809326, Loss 0.45106600046157835\n",
      "RNNA, rep: 1, epoch: 717, acc: 0.7833333611488342, Loss 0.47389527469873427\n",
      "RNNA, rep: 1, epoch: 718, acc: 0.7999998331069946, Loss 0.4330811586976051\n",
      "RNNA, rep: 1, epoch: 719, acc: 0.8000000715255737, Loss 0.49419297695159914\n",
      "RNNA, rep: 1, epoch: 720, acc: 0.8033332824707031, Loss 0.4574980156123638\n",
      "RNNA, rep: 1, epoch: 721, acc: 0.7900000214576721, Loss 0.5133599933981895\n",
      "RNNA, rep: 1, epoch: 722, acc: 0.8199999928474426, Loss 0.4377106502652168\n",
      "RNNA, rep: 1, epoch: 723, acc: 0.7933333516120911, Loss 0.4007288716733456\n",
      "RNNA, rep: 1, epoch: 724, acc: 0.779999852180481, Loss 0.44305639892816545\n",
      "RNNA, rep: 1, epoch: 725, acc: 0.8133332133293152, Loss 0.4046815037727356\n",
      "RNNA, rep: 1, epoch: 726, acc: 0.7733333706855774, Loss 0.5079668173193932\n",
      "RNNA, rep: 1, epoch: 727, acc: 0.7900000214576721, Loss 0.46021189212799074\n",
      "RNNA, rep: 1, epoch: 728, acc: 0.8199999332427979, Loss 0.4474857248365879\n",
      "RNNA, rep: 1, epoch: 729, acc: 0.7899999022483826, Loss 0.48355029955506323\n",
      "RNNA, rep: 1, epoch: 730, acc: 0.76666659116745, Loss 0.4831608363986015\n",
      "RNNA, rep: 1, epoch: 731, acc: 0.7733334302902222, Loss 0.4762443596124649\n",
      "RNNA, rep: 1, epoch: 732, acc: 0.7633334398269653, Loss 0.47695033371448514\n",
      "RNNA, rep: 1, epoch: 733, acc: 0.8133331537246704, Loss 0.41636863976717\n",
      "RNNA, rep: 1, epoch: 734, acc: 0.7966666221618652, Loss 0.4350676271319389\n",
      "RNNA, rep: 1, epoch: 735, acc: 0.8399998545646667, Loss 0.4494661396741867\n",
      "RNNA, rep: 1, epoch: 736, acc: 0.7999999523162842, Loss 0.4381971652805805\n",
      "RNNA, rep: 1, epoch: 737, acc: 0.8366666436195374, Loss 0.4331557336449623\n",
      "RNNA, rep: 1, epoch: 738, acc: 0.8033331036567688, Loss 0.40451192393898966\n",
      "RNNA, rep: 1, epoch: 739, acc: 0.8000000715255737, Loss 0.4238605925440788\n",
      "RNNA, rep: 1, epoch: 740, acc: 0.7833333611488342, Loss 0.5063448865711689\n",
      "RNNA, rep: 1, epoch: 741, acc: 0.7833333015441895, Loss 0.4306385278701782\n",
      "RNNA, rep: 1, epoch: 742, acc: 0.7866666913032532, Loss 0.46680914491415026\n",
      "RNNA, rep: 1, epoch: 743, acc: 0.8199999928474426, Loss 0.4432975164055824\n",
      "RNNA, rep: 1, epoch: 744, acc: 0.7999997138977051, Loss 0.46074480161070824\n",
      "RNNA, rep: 1, epoch: 745, acc: 0.7966666221618652, Loss 0.4732087928056717\n",
      "RNNA, rep: 1, epoch: 746, acc: 0.8133331537246704, Loss 0.4620146478712559\n",
      "RNNA, rep: 1, epoch: 747, acc: 0.8133333325386047, Loss 0.4618448123335838\n",
      "RNNA, rep: 1, epoch: 748, acc: 0.76666659116745, Loss 0.45476947590708733\n",
      "RNNA, rep: 1, epoch: 749, acc: 0.783333420753479, Loss 0.44158200949430465\n",
      "RNNA, rep: 1, epoch: 750, acc: 0.8100000619888306, Loss 0.4264480468630791\n",
      "RNNA, rep: 1, epoch: 751, acc: 0.8033332228660583, Loss 0.45407612845301626\n",
      "RNNA, rep: 1, epoch: 752, acc: 0.8166666626930237, Loss 0.4148874309659004\n",
      "RNNA, rep: 1, epoch: 753, acc: 0.7733333706855774, Loss 0.4469745594263077\n",
      "RNNA, rep: 1, epoch: 754, acc: 0.7866666316986084, Loss 0.4792344695329666\n",
      "RNNA, rep: 1, epoch: 755, acc: 0.8000000715255737, Loss 0.4245849317312241\n",
      "RNNA, rep: 1, epoch: 756, acc: 0.7900000810623169, Loss 0.45877074807882307\n",
      "RNNA, rep: 1, epoch: 757, acc: 0.8100000023841858, Loss 0.41070625811815265\n",
      "RNNA, rep: 1, epoch: 758, acc: 0.8100000619888306, Loss 0.4663424377143383\n",
      "RNNA, rep: 1, epoch: 759, acc: 0.8166664242744446, Loss 0.42926070123910903\n",
      "RNNA, rep: 1, epoch: 760, acc: 0.8199999332427979, Loss 0.43091006368398665\n",
      "RNNA, rep: 1, epoch: 761, acc: 0.7933333516120911, Loss 0.4405676357448101\n",
      "RNNA, rep: 1, epoch: 762, acc: 0.8033332824707031, Loss 0.43571652978658676\n",
      "RNNA, rep: 1, epoch: 763, acc: 0.820000171661377, Loss 0.44985863104462626\n",
      "RNNA, rep: 1, epoch: 764, acc: 0.7866665720939636, Loss 0.4238441531360149\n",
      "RNNA, rep: 1, epoch: 765, acc: 0.7799999713897705, Loss 0.48496483132243157\n",
      "RNNA, rep: 1, epoch: 766, acc: 0.8066665530204773, Loss 0.4538626569509506\n",
      "RNNA, rep: 1, epoch: 767, acc: 0.7933332920074463, Loss 0.44224831983447077\n",
      "RNNA, rep: 1, epoch: 768, acc: 0.7800001502037048, Loss 0.4586599750816822\n",
      "RNNA, rep: 1, epoch: 769, acc: 0.7899998426437378, Loss 0.4387693466246128\n",
      "RNNA, rep: 1, epoch: 770, acc: 0.7933333516120911, Loss 0.4197225773334503\n",
      "RNNA, rep: 1, epoch: 771, acc: 0.8266666531562805, Loss 0.3872869274020195\n",
      "RNNA, rep: 1, epoch: 772, acc: 0.7899998426437378, Loss 0.4521557883918285\n",
      "RNNA, rep: 1, epoch: 773, acc: 0.8133333325386047, Loss 0.4099012294411659\n",
      "RNNA, rep: 1, epoch: 774, acc: 0.7999998331069946, Loss 0.45604273200035095\n",
      "RNNA, rep: 1, epoch: 775, acc: 0.7833331823348999, Loss 0.44187252670526506\n",
      "RNNA, rep: 1, epoch: 776, acc: 0.7733333706855774, Loss 0.45617130607366563\n",
      "RNNA, rep: 1, epoch: 777, acc: 0.783333420753479, Loss 0.4818724051117897\n",
      "RNNA, rep: 1, epoch: 778, acc: 0.809999942779541, Loss 0.40391104981303216\n",
      "RNNA, rep: 1, epoch: 779, acc: 0.7500000596046448, Loss 0.5040095989406109\n",
      "RNNA, rep: 1, epoch: 780, acc: 0.7933333516120911, Loss 0.481448230445385\n",
      "RNNA, rep: 1, epoch: 781, acc: 0.7799999117851257, Loss 0.4493043702840805\n",
      "RNNA, rep: 1, epoch: 782, acc: 0.8166664242744446, Loss 0.43363579660654067\n",
      "RNNA, rep: 1, epoch: 783, acc: 0.7866666913032532, Loss 0.42927121311426164\n",
      "RNNA, rep: 1, epoch: 784, acc: 0.8033333420753479, Loss 0.4010087525844574\n",
      "RNNA, rep: 1, epoch: 785, acc: 0.8299997448921204, Loss 0.4184693670272827\n",
      "RNNA, rep: 1, epoch: 786, acc: 0.7999999523162842, Loss 0.4775064904987812\n",
      "RNNA, rep: 1, epoch: 787, acc: 0.7999999523162842, Loss 0.4329770915210247\n",
      "RNNA, rep: 1, epoch: 788, acc: 0.8066667318344116, Loss 0.4224074204266071\n",
      "RNNA, rep: 1, epoch: 789, acc: 0.8333332538604736, Loss 0.3979421949386597\n",
      "RNNA, rep: 1, epoch: 790, acc: 0.7933332324028015, Loss 0.4521986809372902\n",
      "RNNA, rep: 1, epoch: 791, acc: 0.8133332133293152, Loss 0.4397483293712139\n",
      "RNNA, rep: 1, epoch: 792, acc: 0.7999997735023499, Loss 0.41569105386734007\n",
      "RNNA, rep: 1, epoch: 793, acc: 0.8199998736381531, Loss 0.43646237209439276\n",
      "RNNA, rep: 1, epoch: 794, acc: 0.7933330535888672, Loss 0.4577505569159985\n",
      "RNNA, rep: 1, epoch: 795, acc: 0.81333327293396, Loss 0.4354757772386074\n",
      "RNNA, rep: 1, epoch: 796, acc: 0.7766668796539307, Loss 0.49141504764556887\n",
      "RNNA, rep: 1, epoch: 797, acc: 0.8033333420753479, Loss 0.4382899922132492\n",
      "RNNA, rep: 1, epoch: 798, acc: 0.8166665434837341, Loss 0.4277329133450985\n",
      "RNNA, rep: 1, epoch: 799, acc: 0.7866666913032532, Loss 0.4684950539469719\n",
      "RNNA, rep: 1, epoch: 800, acc: 0.7833333611488342, Loss 0.4761702021956444\n",
      "RNNA, rep: 1, epoch: 801, acc: 0.7733334302902222, Loss 0.47658814817667006\n",
      "RNNA, rep: 1, epoch: 802, acc: 0.7600001692771912, Loss 0.4727548807859421\n",
      "RNNA, rep: 1, epoch: 803, acc: 0.7733331322669983, Loss 0.4823124298453331\n",
      "RNNA, rep: 1, epoch: 804, acc: 0.8033332228660583, Loss 0.44673168420791626\n",
      "RNNA, rep: 1, epoch: 805, acc: 0.823333203792572, Loss 0.4435822546482086\n",
      "RNNA, rep: 1, epoch: 806, acc: 0.7899998426437378, Loss 0.4558209857344627\n",
      "RNNA, rep: 1, epoch: 807, acc: 0.8033332228660583, Loss 0.44949864596128464\n",
      "RNNA, rep: 1, epoch: 808, acc: 0.8000000715255737, Loss 0.48071050494909284\n",
      "RNNA, rep: 1, epoch: 809, acc: 0.8266664743423462, Loss 0.43811879873275755\n",
      "RNNA, rep: 1, epoch: 810, acc: 0.7933334112167358, Loss 0.44619772911071776\n",
      "RNNA, rep: 1, epoch: 811, acc: 0.8066664934158325, Loss 0.44165324747562407\n",
      "RNNA, rep: 1, epoch: 812, acc: 0.8166666626930237, Loss 0.43356585010886195\n",
      "RNNA, rep: 1, epoch: 813, acc: 0.8333332538604736, Loss 0.41001139461994174\n",
      "RNNA, rep: 1, epoch: 814, acc: 0.8033332228660583, Loss 0.44781352058053014\n",
      "RNNA, rep: 1, epoch: 815, acc: 0.7900000214576721, Loss 0.4426371282339096\n",
      "RNNA, rep: 1, epoch: 816, acc: 0.7700001001358032, Loss 0.45714789360761643\n",
      "RNNA, rep: 1, epoch: 817, acc: 0.7700001001358032, Loss 0.5047997942566872\n",
      "RNNA, rep: 1, epoch: 818, acc: 0.8333332538604736, Loss 0.4130043579638004\n",
      "RNNA, rep: 1, epoch: 819, acc: 0.7966666221618652, Loss 0.4281542211771011\n",
      "RNNA, rep: 1, epoch: 820, acc: 0.7900000214576721, Loss 0.4607247184216976\n",
      "RNNA, rep: 1, epoch: 821, acc: 0.7666667103767395, Loss 0.4252613727748394\n",
      "RNNA, rep: 1, epoch: 822, acc: 0.7866665720939636, Loss 0.47349680960178375\n",
      "RNNA, rep: 1, epoch: 823, acc: 0.8033332228660583, Loss 0.43688820987939836\n",
      "RNNA, rep: 1, epoch: 824, acc: 0.7999999523162842, Loss 0.44010702580213545\n",
      "RNNA, rep: 1, epoch: 825, acc: 0.8099997639656067, Loss 0.44542529225349425\n",
      "RNNA, rep: 1, epoch: 826, acc: 0.7833333015441895, Loss 0.4696029119193554\n",
      "RNNA, rep: 1, epoch: 827, acc: 0.7933332920074463, Loss 0.4104536923766136\n",
      "RNNA, rep: 1, epoch: 828, acc: 0.7833333015441895, Loss 0.4002441677451134\n",
      "RNNA, rep: 1, epoch: 829, acc: 0.7999999523162842, Loss 0.4134070301055908\n",
      "RNNA, rep: 1, epoch: 830, acc: 0.7566666603088379, Loss 0.4994071024656296\n",
      "RNNA, rep: 1, epoch: 831, acc: 0.8366665840148926, Loss 0.42507780373096465\n",
      "RNNA, rep: 1, epoch: 832, acc: 0.7966665625572205, Loss 0.4406834238767624\n",
      "RNNA, rep: 1, epoch: 833, acc: 0.7700001001358032, Loss 0.44620187222957614\n",
      "RNNA, rep: 1, epoch: 834, acc: 0.7999999523162842, Loss 0.4377992644906044\n",
      "RNNA, rep: 1, epoch: 835, acc: 0.7999999523162842, Loss 0.48378638714551925\n",
      "RNNA, rep: 1, epoch: 836, acc: 0.7766666412353516, Loss 0.4733367371559143\n",
      "RNNA, rep: 1, epoch: 837, acc: 0.7933332324028015, Loss 0.44944420337677005\n",
      "RNNA, rep: 1, epoch: 838, acc: 0.8199999928474426, Loss 0.4102678383886814\n",
      "RNNA, rep: 1, epoch: 839, acc: 0.7699999809265137, Loss 0.4720028826594353\n",
      "RNNA, rep: 1, epoch: 840, acc: 0.7966665029525757, Loss 0.47256133317947385\n",
      "RNNA, rep: 1, epoch: 841, acc: 0.7799999713897705, Loss 0.48060708433389665\n",
      "RNNA, rep: 1, epoch: 842, acc: 0.7866665124893188, Loss 0.47533499866724016\n",
      "RNNA, rep: 1, epoch: 843, acc: 0.8399999737739563, Loss 0.3864670616388321\n",
      "RNNA, rep: 1, epoch: 844, acc: 0.8099997639656067, Loss 0.4288148260116577\n",
      "RNNA, rep: 1, epoch: 845, acc: 0.7999998331069946, Loss 0.4314210772514343\n",
      "RNNA, rep: 1, epoch: 846, acc: 0.7666667103767395, Loss 0.4731880921125412\n",
      "RNNA, rep: 1, epoch: 847, acc: 0.7966668009757996, Loss 0.4619691300392151\n",
      "RNNA, rep: 1, epoch: 848, acc: 0.8166666626930237, Loss 0.3857340195775032\n",
      "RNNA, rep: 1, epoch: 849, acc: 0.8266665935516357, Loss 0.4116718351840973\n",
      "RNNA, rep: 1, epoch: 850, acc: 0.8033333420753479, Loss 0.427323509156704\n",
      "RNNA, rep: 1, epoch: 851, acc: 0.8166664838790894, Loss 0.4355480018258095\n",
      "RNNA, rep: 1, epoch: 852, acc: 0.7933332324028015, Loss 0.4642167371511459\n",
      "RNNA, rep: 1, epoch: 853, acc: 0.7899999022483826, Loss 0.4330129078030586\n",
      "RNNA, rep: 1, epoch: 854, acc: 0.8033332824707031, Loss 0.4079112458229065\n",
      "RNNA, rep: 1, epoch: 855, acc: 0.7933334112167358, Loss 0.42588870108127597\n",
      "RNNA, rep: 1, epoch: 856, acc: 0.800000011920929, Loss 0.4389946123957634\n",
      "RNNA, rep: 1, epoch: 857, acc: 0.7766665816307068, Loss 0.45894773870706557\n",
      "RNNA, rep: 1, epoch: 858, acc: 0.7966665029525757, Loss 0.42278218060731887\n",
      "RNNA, rep: 1, epoch: 859, acc: 0.7966667413711548, Loss 0.47548640221357347\n",
      "RNNA, rep: 1, epoch: 860, acc: 0.7466667294502258, Loss 0.45038032174110415\n",
      "RNNA, rep: 1, epoch: 861, acc: 0.7866665720939636, Loss 0.42480039775371553\n",
      "RNNA, rep: 1, epoch: 862, acc: 0.7600001692771912, Loss 0.4499263453483582\n",
      "RNNA, rep: 1, epoch: 863, acc: 0.7999999523162842, Loss 0.42461520820856097\n",
      "RNNA, rep: 1, epoch: 864, acc: 0.7899999022483826, Loss 0.41604365557432177\n",
      "RNNA, rep: 1, epoch: 865, acc: 0.800000011920929, Loss 0.40520568937063217\n",
      "RNNA, rep: 1, epoch: 866, acc: 0.7833333015441895, Loss 0.43922837913036344\n",
      "RNNA, rep: 1, epoch: 867, acc: 0.7799999713897705, Loss 0.4484610119462013\n",
      "RNNA, rep: 1, epoch: 868, acc: 0.8133332133293152, Loss 0.45476835906505586\n",
      "RNNA, rep: 1, epoch: 869, acc: 0.7766666412353516, Loss 0.4141498747467995\n",
      "RNNA, rep: 1, epoch: 870, acc: 0.7600000500679016, Loss 0.427321115732193\n",
      "RNNA, rep: 1, epoch: 871, acc: 0.7799999117851257, Loss 0.4338210541009903\n",
      "RNNA, rep: 1, epoch: 872, acc: 0.7966667413711548, Loss 0.4494835615158081\n",
      "RNNA, rep: 1, epoch: 873, acc: 0.8199998736381531, Loss 0.3969836112856865\n",
      "RNNA, rep: 1, epoch: 874, acc: 0.7966667413711548, Loss 0.42604062497615813\n",
      "RNNA, rep: 1, epoch: 875, acc: 0.8099998235702515, Loss 0.4041360396146774\n",
      "RNNA, rep: 1, epoch: 876, acc: 0.8100000619888306, Loss 0.4322322180867195\n",
      "RNNA, rep: 1, epoch: 877, acc: 0.7933334112167358, Loss 0.4153911143541336\n",
      "RNNA, rep: 1, epoch: 878, acc: 0.7899998426437378, Loss 0.4527168568968773\n",
      "RNNA, rep: 1, epoch: 879, acc: 0.800000011920929, Loss 0.45165623620152473\n",
      "RNNA, rep: 1, epoch: 880, acc: 0.793333113193512, Loss 0.41157278284430504\n",
      "RNNA, rep: 1, epoch: 881, acc: 0.8166665434837341, Loss 0.4152382290363312\n",
      "RNNA, rep: 1, epoch: 882, acc: 0.7733334302902222, Loss 0.5034652444720268\n",
      "RNNA, rep: 1, epoch: 883, acc: 0.8199997544288635, Loss 0.4456440883874893\n",
      "RNNA, rep: 1, epoch: 884, acc: 0.7933332920074463, Loss 0.449160046428442\n",
      "RNNA, rep: 1, epoch: 885, acc: 0.8033332228660583, Loss 0.4691998979449272\n",
      "RNNA, rep: 1, epoch: 886, acc: 0.7566667795181274, Loss 0.4673890492320061\n",
      "RNNA, rep: 1, epoch: 887, acc: 0.81333327293396, Loss 0.42767793774604795\n",
      "RNNA, rep: 1, epoch: 888, acc: 0.8099998235702515, Loss 0.4590754881501198\n",
      "RNNA, rep: 1, epoch: 889, acc: 0.793333113193512, Loss 0.44954202994704245\n",
      "RNNA, rep: 1, epoch: 890, acc: 0.7966666221618652, Loss 0.5294991473853589\n",
      "RNNA, rep: 1, epoch: 891, acc: 0.800000011920929, Loss 0.45950624719262123\n",
      "RNNA, rep: 1, epoch: 892, acc: 0.7666667103767395, Loss 0.4692094773054123\n",
      "RNNA, rep: 1, epoch: 893, acc: 0.7966666221618652, Loss 0.45066955626010896\n",
      "RNNA, rep: 1, epoch: 894, acc: 0.8199999332427979, Loss 0.4227577188611031\n",
      "RNNA, rep: 1, epoch: 895, acc: 0.8133331537246704, Loss 0.4343659880757332\n",
      "RNNA, rep: 1, epoch: 896, acc: 0.8100000619888306, Loss 0.4627201905846596\n",
      "RNNA, rep: 1, epoch: 897, acc: 0.7799999713897705, Loss 0.4758736938238144\n",
      "RNNA, rep: 1, epoch: 898, acc: 0.8433333039283752, Loss 0.4287447734177113\n",
      "RNNA, rep: 1, epoch: 899, acc: 0.8066666126251221, Loss 0.42659463569521905\n",
      "RNNA, rep: 1, epoch: 900, acc: 0.7933329939842224, Loss 0.44358076751232145\n",
      "RNNA, rep: 1, epoch: 901, acc: 0.8233330249786377, Loss 0.40536504209041596\n",
      "RNNA, rep: 1, epoch: 902, acc: 0.7833335995674133, Loss 0.43419274300336835\n",
      "RNNA, rep: 1, epoch: 903, acc: 0.7799999713897705, Loss 0.4911847308278084\n",
      "RNNA, rep: 1, epoch: 904, acc: 0.8133333325386047, Loss 0.44953523129224776\n",
      "RNNA, rep: 1, epoch: 905, acc: 0.809999942779541, Loss 0.41144931703805926\n",
      "RNNA, rep: 1, epoch: 906, acc: 0.8066666126251221, Loss 0.4371090494096279\n",
      "RNNA, rep: 1, epoch: 907, acc: 0.7899998426437378, Loss 0.42242592602968215\n",
      "RNNA, rep: 1, epoch: 908, acc: 0.8100000023841858, Loss 0.4223182046413422\n",
      "RNNA, rep: 1, epoch: 909, acc: 0.81333327293396, Loss 0.410252560377121\n",
      "RNNA, rep: 1, epoch: 910, acc: 0.7966665029525757, Loss 0.4211422684788704\n",
      "RNNA, rep: 1, epoch: 911, acc: 0.8033332824707031, Loss 0.42923401564359664\n",
      "RNNA, rep: 1, epoch: 912, acc: 0.8033334612846375, Loss 0.4463721588253975\n",
      "RNNA, rep: 1, epoch: 913, acc: 0.7999999523162842, Loss 0.4279496344923973\n",
      "RNNA, rep: 1, epoch: 914, acc: 0.8100000619888306, Loss 0.48268414705991747\n",
      "RNNA, rep: 1, epoch: 915, acc: 0.800000011920929, Loss 0.46185772225260735\n",
      "RNNA, rep: 1, epoch: 916, acc: 0.7866668701171875, Loss 0.46602589964866636\n",
      "RNNA, rep: 1, epoch: 917, acc: 0.7899999022483826, Loss 0.4100922238826752\n",
      "RNNA, rep: 1, epoch: 918, acc: 0.809999942779541, Loss 0.4295231169462204\n",
      "RNNA, rep: 1, epoch: 919, acc: 0.7966665625572205, Loss 0.46712673515081404\n",
      "RNNA, rep: 1, epoch: 920, acc: 0.7766667008399963, Loss 0.452532474398613\n",
      "RNNA, rep: 1, epoch: 921, acc: 0.800000011920929, Loss 0.41263029128313067\n",
      "RNNA, rep: 1, epoch: 922, acc: 0.8033331036567688, Loss 0.3991213101148605\n",
      "RNNA, rep: 1, epoch: 923, acc: 0.7833333015441895, Loss 0.4775458124279976\n",
      "RNNA, rep: 1, epoch: 924, acc: 0.809999942779541, Loss 0.4032531672716141\n",
      "RNNA, rep: 1, epoch: 925, acc: 0.8266664743423462, Loss 0.45164141431450844\n",
      "RNNA, rep: 1, epoch: 926, acc: 0.7800000905990601, Loss 0.43925790399312975\n",
      "RNNA, rep: 1, epoch: 927, acc: 0.7699999213218689, Loss 0.4498159584403038\n",
      "RNNA, rep: 1, epoch: 928, acc: 0.7933334112167358, Loss 0.4296544271707535\n",
      "RNNA, rep: 1, epoch: 929, acc: 0.7999999523162842, Loss 0.4601160778105259\n",
      "RNNA, rep: 1, epoch: 930, acc: 0.8166664838790894, Loss 0.46336585462093355\n",
      "RNNA, rep: 1, epoch: 931, acc: 0.8066664338111877, Loss 0.3986658427119255\n",
      "RNNA, rep: 1, epoch: 932, acc: 0.7999997735023499, Loss 0.3897942776978016\n",
      "RNNA, rep: 1, epoch: 933, acc: 0.7799999117851257, Loss 0.48549836277961733\n",
      "RNNA, rep: 1, epoch: 934, acc: 0.809999942779541, Loss 0.44220089331269263\n",
      "RNNA, rep: 1, epoch: 935, acc: 0.7933333516120911, Loss 0.4122399294376373\n",
      "RNNA, rep: 1, epoch: 936, acc: 0.8033333420753479, Loss 0.4297492915391922\n",
      "RNNA, rep: 1, epoch: 937, acc: 0.8199999332427979, Loss 0.4500070506334305\n",
      "RNNA, rep: 1, epoch: 938, acc: 0.8366665840148926, Loss 0.4292940732836723\n",
      "RNNA, rep: 1, epoch: 939, acc: 0.8399999141693115, Loss 0.38523429825901984\n",
      "RNNA, rep: 1, epoch: 940, acc: 0.8199999928474426, Loss 0.4349882410466671\n",
      "RNNA, rep: 1, epoch: 941, acc: 0.8233330249786377, Loss 0.41333383083343506\n",
      "RNNA, rep: 1, epoch: 942, acc: 0.7866665124893188, Loss 0.45976426467299464\n",
      "RNNA, rep: 1, epoch: 943, acc: 0.8033331036567688, Loss 0.3837124453485012\n",
      "RNNA, rep: 1, epoch: 944, acc: 0.8066665530204773, Loss 0.4526784147322178\n",
      "RNNA, rep: 1, epoch: 945, acc: 0.7666667103767395, Loss 0.45823581963777543\n",
      "RNNA, rep: 1, epoch: 946, acc: 0.8033332228660583, Loss 0.4336113074421883\n",
      "RNNA, rep: 1, epoch: 947, acc: 0.8033334612846375, Loss 0.4718268409371376\n",
      "RNNA, rep: 1, epoch: 948, acc: 0.7933334112167358, Loss 0.43668788284063337\n",
      "RNNA, rep: 1, epoch: 949, acc: 0.8166667222976685, Loss 0.4499260814487934\n",
      "RNNA, rep: 1, epoch: 950, acc: 0.7899998426437378, Loss 0.4503462965786457\n",
      "RNNA, rep: 1, epoch: 951, acc: 0.7866665720939636, Loss 0.41730317190289495\n",
      "RNNA, rep: 1, epoch: 952, acc: 0.8033332824707031, Loss 0.458157958984375\n",
      "RNNA, rep: 1, epoch: 953, acc: 0.8199999332427979, Loss 0.4468717175722122\n",
      "RNNA, rep: 1, epoch: 954, acc: 0.800000011920929, Loss 0.4214237105846405\n",
      "RNNA, rep: 1, epoch: 955, acc: 0.7933332920074463, Loss 0.447186798453331\n",
      "RNNA, rep: 1, epoch: 956, acc: 0.7733333706855774, Loss 0.4363412918150425\n",
      "RNNA, rep: 1, epoch: 957, acc: 0.7899999022483826, Loss 0.45036219924688337\n",
      "RNNA, rep: 1, epoch: 958, acc: 0.8366666436195374, Loss 0.40835764572024347\n",
      "RNNA, rep: 1, epoch: 959, acc: 0.75, Loss 0.5209795957803727\n",
      "RNNA, rep: 1, epoch: 960, acc: 0.7933332920074463, Loss 0.4318293434381485\n",
      "RNNA, rep: 1, epoch: 961, acc: 0.8166664838790894, Loss 0.4295481079816818\n",
      "RNNA, rep: 1, epoch: 962, acc: 0.7999998331069946, Loss 0.44964536756277085\n",
      "RNNA, rep: 1, epoch: 963, acc: 0.8100000023841858, Loss 0.408470992743969\n",
      "RNNA, rep: 1, epoch: 964, acc: 0.8099998235702515, Loss 0.41519676059484484\n",
      "RNNA, rep: 1, epoch: 965, acc: 0.8066665530204773, Loss 0.47524158641695974\n",
      "RNNA, rep: 1, epoch: 966, acc: 0.8100000023841858, Loss 0.4747699019312859\n",
      "RNNA, rep: 1, epoch: 967, acc: 0.7799999117851257, Loss 0.462441979944706\n",
      "RNNA, rep: 1, epoch: 968, acc: 0.7833333015441895, Loss 0.4468713875114918\n",
      "RNNA, rep: 1, epoch: 969, acc: 0.800000011920929, Loss 0.43096060067415237\n",
      "RNNA, rep: 1, epoch: 970, acc: 0.8099997639656067, Loss 0.4418770265579224\n",
      "RNNA, rep: 1, epoch: 971, acc: 0.7933332324028015, Loss 0.42850318670272824\n",
      "RNNA, rep: 1, epoch: 972, acc: 0.7966667413711548, Loss 0.49383205771446226\n",
      "RNNA, rep: 1, epoch: 973, acc: 0.7799999713897705, Loss 0.5211031351983547\n",
      "RNNA, rep: 1, epoch: 974, acc: 0.8266665935516357, Loss 0.3928090934455395\n",
      "RNNA, rep: 1, epoch: 975, acc: 0.8166666626930237, Loss 0.40779726192355154\n",
      "RNNA, rep: 1, epoch: 976, acc: 0.7966666221618652, Loss 0.4389334526658058\n",
      "RNNA, rep: 1, epoch: 977, acc: 0.8033331036567688, Loss 0.40547198385000227\n",
      "RNNA, rep: 1, epoch: 978, acc: 0.7400000691413879, Loss 0.5123842230439186\n",
      "RNNA, rep: 1, epoch: 979, acc: 0.8033333420753479, Loss 0.4504419696331024\n",
      "RNNA, rep: 1, epoch: 980, acc: 0.7466665506362915, Loss 0.5383280605077744\n",
      "RNNA, rep: 1, epoch: 981, acc: 0.8033332824707031, Loss 0.43852909505367277\n",
      "RNNA, rep: 1, epoch: 982, acc: 0.7933332920074463, Loss 0.491695813536644\n",
      "RNNA, rep: 1, epoch: 983, acc: 0.8199999332427979, Loss 0.4155993866920471\n",
      "RNNA, rep: 1, epoch: 984, acc: 0.7933332920074463, Loss 0.4735105746984482\n",
      "RNNA, rep: 1, epoch: 985, acc: 0.8199998736381531, Loss 0.4570114479959011\n",
      "RNNA, rep: 1, epoch: 986, acc: 0.7899998426437378, Loss 0.42870028331875804\n",
      "RNNA, rep: 1, epoch: 987, acc: 0.8299998641014099, Loss 0.398401241004467\n",
      "RNNA, rep: 1, epoch: 988, acc: 0.8033332824707031, Loss 0.4502793125808239\n",
      "RNNA, rep: 1, epoch: 989, acc: 0.8233334422111511, Loss 0.44033529609441757\n",
      "RNNA, rep: 1, epoch: 990, acc: 0.809999942779541, Loss 0.4327548064291477\n",
      "RNNA, rep: 1, epoch: 991, acc: 0.7833333015441895, Loss 0.49461865305900576\n",
      "RNNA, rep: 1, epoch: 992, acc: 0.7766666412353516, Loss 0.40005779325962065\n",
      "RNNA, rep: 1, epoch: 993, acc: 0.8166666626930237, Loss 0.4304641462862492\n",
      "RNNA, rep: 1, epoch: 994, acc: 0.8166667222976685, Loss 0.4435403461754322\n",
      "RNNA, rep: 1, epoch: 995, acc: 0.7799999117851257, Loss 0.4658715006709099\n",
      "RNNA, rep: 1, epoch: 996, acc: 0.8166665434837341, Loss 0.4270377373695374\n",
      "RNNA, rep: 1, epoch: 997, acc: 0.8399999141693115, Loss 0.42837330892682074\n",
      "RNNA, rep: 1, epoch: 998, acc: 0.7933335304260254, Loss 0.46046442687511446\n",
      "RNNA, rep: 1, epoch: 999, acc: 0.8199999928474426, Loss 0.4036112527549267\n",
      "RNNA, rep: 1, epoch: 1000, acc: 0.7666667103767395, Loss 0.47732245564460757\n",
      "RNNA, rep: 1, epoch: 1001, acc: 0.7799999117851257, Loss 0.43179450184106827\n",
      "RNNA, rep: 1, epoch: 1002, acc: 0.7633333802223206, Loss 0.43376939475536347\n",
      "RNNA, rep: 1, epoch: 1003, acc: 0.7799999713897705, Loss 0.4181867411732674\n",
      "RNNA, rep: 1, epoch: 1004, acc: 0.7933332920074463, Loss 0.4630330806970596\n",
      "RNNA, rep: 1, epoch: 1005, acc: 0.7933333516120911, Loss 0.4682803246378899\n",
      "RNNA, rep: 1, epoch: 1006, acc: 0.7766666412353516, Loss 0.46881976574659345\n",
      "RNNA, rep: 1, epoch: 1007, acc: 0.8233334422111511, Loss 0.3899199217557907\n",
      "RNNA, rep: 1, epoch: 1008, acc: 0.836666464805603, Loss 0.39317514389753344\n",
      "RNNA, rep: 1, epoch: 1009, acc: 0.8266664743423462, Loss 0.36029208183288575\n",
      "RNNA, rep: 1, epoch: 1010, acc: 0.8066664934158325, Loss 0.44233711779117585\n",
      "RNNA, rep: 1, epoch: 1011, acc: 0.8033332228660583, Loss 0.43613387197256087\n",
      "RNNA, rep: 1, epoch: 1012, acc: 0.7733334302902222, Loss 0.4448000617325306\n",
      "RNNA, rep: 1, epoch: 1013, acc: 0.7766665816307068, Loss 0.46286840811371804\n",
      "RNNA, rep: 1, epoch: 1014, acc: 0.81333327293396, Loss 0.3938899177312851\n",
      "RNNA, rep: 1, epoch: 1015, acc: 0.8099997639656067, Loss 0.39096379473805426\n",
      "RNNA, rep: 1, epoch: 1016, acc: 0.7766665816307068, Loss 0.44590747356414795\n",
      "RNNA, rep: 1, epoch: 1017, acc: 0.8199998736381531, Loss 0.45761200070381164\n",
      "RNNA, rep: 1, epoch: 1018, acc: 0.8299997448921204, Loss 0.4140321478247643\n",
      "RNNA, rep: 1, epoch: 1019, acc: 0.8099998235702515, Loss 0.4049104180932045\n",
      "RNNA, rep: 1, epoch: 1020, acc: 0.7766666412353516, Loss 0.42187965035438535\n",
      "RNNA, rep: 1, epoch: 1021, acc: 0.7666666507720947, Loss 0.4701700288057327\n",
      "RNNA, rep: 1, epoch: 1022, acc: 0.8033332228660583, Loss 0.4513393777608872\n",
      "RNNA, rep: 1, epoch: 1023, acc: 0.8033332824707031, Loss 0.4374672815203667\n",
      "RNNA, rep: 1, epoch: 1024, acc: 0.76666659116745, Loss 0.44333464533090594\n",
      "RNNA, rep: 1, epoch: 1025, acc: 0.8199998736381531, Loss 0.4184019309282303\n",
      "RNNA, rep: 1, epoch: 1026, acc: 0.809999942779541, Loss 0.3916426691412926\n",
      "RNNA, rep: 1, epoch: 1027, acc: 0.7933333516120911, Loss 0.4705170452594757\n",
      "RNNA, rep: 1, epoch: 1028, acc: 0.7933332324028015, Loss 0.4834275822341442\n",
      "RNNA, rep: 1, epoch: 1029, acc: 0.7900000214576721, Loss 0.407166250795126\n",
      "RNNA, rep: 1, epoch: 1030, acc: 0.7999999523162842, Loss 0.44387826800346375\n",
      "RNNA, rep: 1, epoch: 1031, acc: 0.8033332824707031, Loss 0.4233751156926155\n",
      "RNNA, rep: 1, epoch: 1032, acc: 0.7933332324028015, Loss 0.4128404152393341\n",
      "RNNA, rep: 1, epoch: 1033, acc: 0.7766664624214172, Loss 0.4742315486073494\n",
      "RNNA, rep: 1, epoch: 1034, acc: 0.7833333611488342, Loss 0.4500992584228516\n",
      "RNNA, rep: 1, epoch: 1035, acc: 0.8233330249786377, Loss 0.4358544626832008\n",
      "RNNA, rep: 1, epoch: 1036, acc: 0.8066665530204773, Loss 0.402208656668663\n",
      "RNNA, rep: 1, epoch: 1037, acc: 0.8166663646697998, Loss 0.4106671816110611\n",
      "RNNA, rep: 1, epoch: 1038, acc: 0.7900000214576721, Loss 0.4377206763625145\n",
      "RNNA, rep: 1, epoch: 1039, acc: 0.7866666316986084, Loss 0.4517587687075138\n",
      "RNNA, rep: 1, epoch: 1040, acc: 0.8266664743423462, Loss 0.41453186243772505\n",
      "RNNA, rep: 1, epoch: 1041, acc: 0.7899999022483826, Loss 0.43259990274906157\n",
      "RNNA, rep: 1, epoch: 1042, acc: 0.81333327293396, Loss 0.42941743493080137\n",
      "RNNA, rep: 1, epoch: 1043, acc: 0.8266664743423462, Loss 0.3941465309262276\n",
      "RNNA, rep: 1, epoch: 1044, acc: 0.8299998641014099, Loss 0.41199482247233393\n",
      "RNNA, rep: 1, epoch: 1045, acc: 0.7766666412353516, Loss 0.4626911863684654\n",
      "RNNA, rep: 1, epoch: 1046, acc: 0.8033332824707031, Loss 0.43302218213677407\n",
      "RNNA, rep: 1, epoch: 1047, acc: 0.7766667008399963, Loss 0.42604010701179507\n",
      "RNNA, rep: 1, epoch: 1048, acc: 0.7633334994316101, Loss 0.4199111223220825\n",
      "RNNA, rep: 1, epoch: 1049, acc: 0.7733331918716431, Loss 0.465948768556118\n",
      "RNNA, rep: 1, epoch: 1050, acc: 0.8099997639656067, Loss 0.4202485775947571\n",
      "RNNA, rep: 1, epoch: 1051, acc: 0.836666464805603, Loss 0.37485399752855303\n",
      "RNNA, rep: 1, epoch: 1052, acc: 0.7866666913032532, Loss 0.5059981921315193\n",
      "RNNA, rep: 1, epoch: 1053, acc: 0.7566665410995483, Loss 0.48238694489002226\n",
      "RNNA, rep: 1, epoch: 1054, acc: 0.7833333015441895, Loss 0.4784596407413483\n",
      "RNNA, rep: 1, epoch: 1055, acc: 0.8166666626930237, Loss 0.4077251625061035\n",
      "RNNA, rep: 1, epoch: 1056, acc: 0.8066665530204773, Loss 0.4290199352800846\n",
      "RNNA, rep: 1, epoch: 1057, acc: 0.8066666126251221, Loss 0.40397593557834627\n",
      "RNNA, rep: 1, epoch: 1058, acc: 0.84333336353302, Loss 0.39301961928606033\n",
      "RNNA, rep: 1, epoch: 1059, acc: 0.8366664052009583, Loss 0.37747184589505195\n",
      "RNNA, rep: 1, epoch: 1060, acc: 0.8166664838790894, Loss 0.4152232447266579\n",
      "RNNA, rep: 1, epoch: 1061, acc: 0.8066666126251221, Loss 0.4399488291144371\n",
      "RNNA, rep: 1, epoch: 1062, acc: 0.7699999213218689, Loss 0.4273267692327499\n",
      "RNNA, rep: 1, epoch: 1063, acc: 0.7900000214576721, Loss 0.4591015473008156\n",
      "RNNA, rep: 1, epoch: 1064, acc: 0.7866663932800293, Loss 0.4659991881251335\n",
      "RNNA, rep: 1, epoch: 1065, acc: 0.7666667103767395, Loss 0.4883162096142769\n",
      "RNNA, rep: 1, epoch: 1066, acc: 0.8066666126251221, Loss 0.3879567113518715\n",
      "RNNA, rep: 1, epoch: 1067, acc: 0.7999999523162842, Loss 0.42560808748006823\n",
      "RNNA, rep: 1, epoch: 1068, acc: 0.8066664934158325, Loss 0.4480421617627144\n",
      "RNNA, rep: 1, epoch: 1069, acc: 0.7833333015441895, Loss 0.4903378376364708\n",
      "RNNA, rep: 1, epoch: 1070, acc: 0.81333327293396, Loss 0.45960714280605314\n",
      "RNNA, rep: 1, epoch: 1071, acc: 0.8033333420753479, Loss 0.4316329491138458\n",
      "RNNA, rep: 1, epoch: 1072, acc: 0.7999999523162842, Loss 0.4592388132214546\n",
      "RNNA, rep: 1, epoch: 1073, acc: 0.8033332824707031, Loss 0.42270048320293424\n",
      "RNNA, rep: 1, epoch: 1074, acc: 0.8033332824707031, Loss 0.48486434876918794\n",
      "RNNA, rep: 1, epoch: 1075, acc: 0.7999998331069946, Loss 0.442130704075098\n",
      "RNNA, rep: 1, epoch: 1076, acc: 0.7966665029525757, Loss 0.46215585738420484\n",
      "RNNA, rep: 1, epoch: 1077, acc: 0.8399999141693115, Loss 0.4256269620358944\n",
      "RNNA, rep: 1, epoch: 1078, acc: 0.7933329939842224, Loss 0.45274020329117776\n",
      "RNNA, rep: 1, epoch: 1079, acc: 0.7900000214576721, Loss 0.5048597726225853\n",
      "RNNA, rep: 1, epoch: 1080, acc: 0.7899999022483826, Loss 0.4397594499588013\n",
      "RNNA, rep: 1, epoch: 1081, acc: 0.8000000715255737, Loss 0.4096851068735123\n",
      "RNNA, rep: 1, epoch: 1082, acc: 0.7900000810623169, Loss 0.4405250057578087\n",
      "RNNA, rep: 1, epoch: 1083, acc: 0.8066664338111877, Loss 0.4082230058312416\n",
      "RNNA, rep: 1, epoch: 1084, acc: 0.7900001406669617, Loss 0.4531119829416275\n",
      "RNNA, rep: 1, epoch: 1085, acc: 0.7699999809265137, Loss 0.42119658932089804\n",
      "RNNA, rep: 1, epoch: 1086, acc: 0.7999998331069946, Loss 0.47688011050224305\n",
      "RNNA, rep: 1, epoch: 1087, acc: 0.7899998426437378, Loss 0.4229660847783089\n",
      "RNNA, rep: 1, epoch: 1088, acc: 0.7366667985916138, Loss 0.46605023980140686\n",
      "RNNA, rep: 1, epoch: 1089, acc: 0.7966665029525757, Loss 0.45837485313415527\n",
      "RNNA, rep: 1, epoch: 1090, acc: 0.8033332228660583, Loss 0.41944529116153717\n",
      "RNNA, rep: 1, epoch: 1091, acc: 0.8366667032241821, Loss 0.38948947057127953\n",
      "RNNA, rep: 1, epoch: 1092, acc: 0.8033332824707031, Loss 0.4484482342004776\n",
      "RNNA, rep: 1, epoch: 1093, acc: 0.7900001406669617, Loss 0.45762423872947694\n",
      "RNNA, rep: 1, epoch: 1094, acc: 0.8033332824707031, Loss 0.4762746959924698\n",
      "RNNA, rep: 1, epoch: 1095, acc: 0.8100000023841858, Loss 0.39863435983657836\n",
      "RNNA, rep: 1, epoch: 1096, acc: 0.7966666221618652, Loss 0.4269710025191307\n",
      "RNNA, rep: 1, epoch: 1097, acc: 0.8100000619888306, Loss 0.4690523582696915\n",
      "RNNA, rep: 1, epoch: 1098, acc: 0.7999999523162842, Loss 0.44998835563659667\n",
      "RNNA, rep: 1, epoch: 1099, acc: 0.8166665434837341, Loss 0.4172843533754349\n",
      "RNNA, rep: 1, epoch: 1100, acc: 0.7933332920074463, Loss 0.499069376885891\n",
      "RNNA, rep: 1, epoch: 1101, acc: 0.7699998617172241, Loss 0.4859297926723957\n",
      "RNNA, rep: 1, epoch: 1102, acc: 0.8000000715255737, Loss 0.41554784059524535\n",
      "RNNA, rep: 1, epoch: 1103, acc: 0.800000011920929, Loss 0.45151967376470564\n",
      "RNNA, rep: 1, epoch: 1104, acc: 0.7566665410995483, Loss 0.4430145224928856\n",
      "RNNA, rep: 1, epoch: 1105, acc: 0.8299998641014099, Loss 0.387654981315136\n",
      "RNNA, rep: 1, epoch: 1106, acc: 0.7833331823348999, Loss 0.4816631069779396\n",
      "RNNA, rep: 1, epoch: 1107, acc: 0.7933333516120911, Loss 0.43775463074445725\n",
      "RNNA, rep: 1, epoch: 1108, acc: 0.7999999523162842, Loss 0.4016968032717705\n",
      "RNNA, rep: 1, epoch: 1109, acc: 0.7699999809265137, Loss 0.45258441716432574\n",
      "RNNA, rep: 1, epoch: 1110, acc: 0.8233332633972168, Loss 0.39887867838144303\n",
      "RNNA, rep: 1, epoch: 1111, acc: 0.8299999833106995, Loss 0.4252974264323711\n",
      "RNNA, rep: 1, epoch: 1112, acc: 0.81333327293396, Loss 0.4416665667295456\n",
      "RNNA, rep: 1, epoch: 1113, acc: 0.7900000214576721, Loss 0.4733910936117172\n",
      "RNNA, rep: 1, epoch: 1114, acc: 0.8366666436195374, Loss 0.38362460166215895\n",
      "RNNA, rep: 1, epoch: 1115, acc: 0.7766665816307068, Loss 0.4369920408725739\n",
      "RNNA, rep: 1, epoch: 1116, acc: 0.8166667819023132, Loss 0.41416142880916595\n",
      "RNNA, rep: 1, epoch: 1117, acc: 0.809999942779541, Loss 0.45241025239229204\n",
      "RNNA, rep: 1, epoch: 1118, acc: 0.8199999332427979, Loss 0.4112337963283062\n",
      "RNNA, rep: 1, epoch: 1119, acc: 0.7999997735023499, Loss 0.43816311612725256\n",
      "RNNA, rep: 1, epoch: 1120, acc: 0.7766667008399963, Loss 0.4925842535495758\n",
      "RNNA, rep: 1, epoch: 1121, acc: 0.7800000905990601, Loss 0.4209257946908474\n",
      "RNNA, rep: 1, epoch: 1122, acc: 0.8033333420753479, Loss 0.45798281848430633\n",
      "RNNA, rep: 1, epoch: 1123, acc: 0.7899998426437378, Loss 0.43804310232400895\n",
      "RNNA, rep: 1, epoch: 1124, acc: 0.7966667413711548, Loss 0.42625924736261367\n",
      "RNNA, rep: 1, epoch: 1125, acc: 0.7700001001358032, Loss 0.4304802995920181\n",
      "RNNA, rep: 1, epoch: 1126, acc: 0.8499998450279236, Loss 0.39354568272829055\n",
      "RNNA, rep: 1, epoch: 1127, acc: 0.7900000214576721, Loss 0.4407756868004799\n",
      "RNNA, rep: 1, epoch: 1128, acc: 0.8533333539962769, Loss 0.3996025130152702\n",
      "RNNA, rep: 1, epoch: 1129, acc: 0.8100000023841858, Loss 0.418353833258152\n",
      "RNNA, rep: 1, epoch: 1130, acc: 0.8266665935516357, Loss 0.404559009373188\n",
      "RNNA, rep: 1, epoch: 1131, acc: 0.7566665410995483, Loss 0.45589912712574004\n",
      "RNNA, rep: 1, epoch: 1132, acc: 0.7933332920074463, Loss 0.43789402961730955\n",
      "RNNA, rep: 1, epoch: 1133, acc: 0.823333203792572, Loss 0.40720084190368655\n",
      "RNNA, rep: 1, epoch: 1134, acc: 0.8033334612846375, Loss 0.4433085849881172\n",
      "RNNA, rep: 1, epoch: 1135, acc: 0.8299998641014099, Loss 0.42044910818338393\n",
      "RNNA, rep: 1, epoch: 1136, acc: 0.789999783039093, Loss 0.413953472673893\n",
      "RNNA, rep: 1, epoch: 1137, acc: 0.7833333015441895, Loss 0.4163720768690109\n",
      "RNNA, rep: 1, epoch: 1138, acc: 0.8166667222976685, Loss 0.41638435661792755\n",
      "RNNA, rep: 1, epoch: 1139, acc: 0.809999942779541, Loss 0.45146337017416954\n",
      "RNNA, rep: 1, epoch: 1140, acc: 0.8333332538604736, Loss 0.38311113297939303\n",
      "RNNA, rep: 1, epoch: 1141, acc: 0.8333333730697632, Loss 0.36618602871894834\n",
      "RNNA, rep: 1, epoch: 1142, acc: 0.7833333015441895, Loss 0.4534436205029488\n",
      "RNNA, rep: 1, epoch: 1143, acc: 0.8100000023841858, Loss 0.43819231286644933\n",
      "RNNA, rep: 1, epoch: 1144, acc: 0.7833333015441895, Loss 0.46923851266503336\n",
      "RNNA, rep: 1, epoch: 1145, acc: 0.8133333325386047, Loss 0.42636611983180045\n",
      "RNNA, rep: 1, epoch: 1146, acc: 0.8166665434837341, Loss 0.43319027677178384\n",
      "RNNA, rep: 1, epoch: 1147, acc: 0.7933332324028015, Loss 0.45226685181260107\n",
      "RNNA, rep: 1, epoch: 1148, acc: 0.7900001406669617, Loss 0.4919558808207512\n",
      "RNNA, rep: 1, epoch: 1149, acc: 0.8266666531562805, Loss 0.4034570877254009\n",
      "RNNA, rep: 1, epoch: 1150, acc: 0.8066664934158325, Loss 0.437225880920887\n",
      "RNNA, rep: 1, epoch: 1151, acc: 0.7999999523162842, Loss 0.4283070701360703\n",
      "RNNA, rep: 1, epoch: 1152, acc: 0.8033332824707031, Loss 0.4161528518795967\n",
      "RNNA, rep: 1, epoch: 1153, acc: 0.7866666913032532, Loss 0.4628567560017109\n",
      "RNNA, rep: 1, epoch: 1154, acc: 0.7799999713897705, Loss 0.4441476191580296\n",
      "RNNA, rep: 1, epoch: 1155, acc: 0.7966665625572205, Loss 0.47635682180523875\n",
      "RNNA, rep: 1, epoch: 1156, acc: 0.81333327293396, Loss 0.43138271003961565\n",
      "RNNA, rep: 1, epoch: 1157, acc: 0.8166665434837341, Loss 0.38781906351447104\n",
      "RNNA, rep: 1, epoch: 1158, acc: 0.8299998641014099, Loss 0.4252616040408611\n",
      "RNNA, rep: 1, epoch: 1159, acc: 0.8099998235702515, Loss 0.4696259608864784\n",
      "RNNA, rep: 1, epoch: 1160, acc: 0.7999999523162842, Loss 0.4455378863215447\n",
      "RNNA, rep: 1, epoch: 1161, acc: 0.8033332228660583, Loss 0.43214037626981733\n",
      "RNNA, rep: 1, epoch: 1162, acc: 0.8199999332427979, Loss 0.4293688094615936\n",
      "RNNA, rep: 1, epoch: 1163, acc: 0.7999998331069946, Loss 0.44110267266631126\n",
      "RNNA, rep: 1, epoch: 1164, acc: 0.7933333516120911, Loss 0.4177722482383251\n",
      "RNNA, rep: 1, epoch: 1165, acc: 0.8133333325386047, Loss 0.44698680624365805\n",
      "RNNA, rep: 1, epoch: 1166, acc: 0.8233333826065063, Loss 0.4109963834285736\n",
      "RNNA, rep: 1, epoch: 1167, acc: 0.783333420753479, Loss 0.42643323123455046\n",
      "RNNA, rep: 1, epoch: 1168, acc: 0.7799999713897705, Loss 0.4480858227610588\n",
      "RNNA, rep: 1, epoch: 1169, acc: 0.8166667819023132, Loss 0.4441826930642128\n",
      "RNNA, rep: 1, epoch: 1170, acc: 0.8233333826065063, Loss 0.43093246072530744\n",
      "RNNA, rep: 1, epoch: 1171, acc: 0.7799999713897705, Loss 0.42327854067087173\n",
      "RNNA, rep: 1, epoch: 1172, acc: 0.7900000214576721, Loss 0.44474749207496644\n",
      "RNNA, rep: 1, epoch: 1173, acc: 0.7766665816307068, Loss 0.4511022680997849\n",
      "RNNA, rep: 1, epoch: 1174, acc: 0.8099998235702515, Loss 0.435683640986681\n",
      "RNNA, rep: 1, epoch: 1175, acc: 0.7799999713897705, Loss 0.452778716981411\n",
      "RNNA, rep: 1, epoch: 1176, acc: 0.8066666126251221, Loss 0.4465066650509834\n",
      "RNNA, rep: 1, epoch: 1177, acc: 0.7700001001358032, Loss 0.46638807639479635\n",
      "RNNA, rep: 1, epoch: 1178, acc: 0.7899998426437378, Loss 0.47164573922753333\n",
      "RNNA, rep: 1, epoch: 1179, acc: 0.7966667413711548, Loss 0.44653927549719813\n",
      "RNNA, rep: 1, epoch: 1180, acc: 0.8366667032241821, Loss 0.41725152656435965\n",
      "RNNA, rep: 1, epoch: 1181, acc: 0.823333203792572, Loss 0.3975631146132946\n",
      "RNNA, rep: 1, epoch: 1182, acc: 0.7866668701171875, Loss 0.43840665116906163\n",
      "RNNA, rep: 1, epoch: 1183, acc: 0.8033334612846375, Loss 0.42856893002986907\n",
      "RNNA, rep: 1, epoch: 1184, acc: 0.7633333802223206, Loss 0.49851642340421676\n",
      "RNNA, rep: 1, epoch: 1185, acc: 0.809999942779541, Loss 0.43878460258245466\n",
      "RNNA, rep: 1, epoch: 1186, acc: 0.7900000810623169, Loss 0.4704210567474365\n",
      "RNNA, rep: 1, epoch: 1187, acc: 0.7566666603088379, Loss 0.4517516578733921\n",
      "RNNA, rep: 1, epoch: 1188, acc: 0.8200000524520874, Loss 0.4391445696353912\n",
      "RNNA, rep: 1, epoch: 1189, acc: 0.7933332920074463, Loss 0.4540053391456604\n",
      "RNNA, rep: 1, epoch: 1190, acc: 0.779999852180481, Loss 0.42296505510807036\n",
      "RNNA, rep: 1, epoch: 1191, acc: 0.8000000715255737, Loss 0.42331499308347703\n",
      "RNNA, rep: 1, epoch: 1192, acc: 0.7933332920074463, Loss 0.3961407974362373\n",
      "RNNA, rep: 1, epoch: 1193, acc: 0.8266663551330566, Loss 0.4081299677491188\n",
      "RNNA, rep: 1, epoch: 1194, acc: 0.8166666626930237, Loss 0.4207260367274284\n",
      "RNNA, rep: 1, epoch: 1195, acc: 0.7966667413711548, Loss 0.4063521510362625\n",
      "RNNA, rep: 1, epoch: 1196, acc: 0.8099998235702515, Loss 0.4355848070979118\n",
      "RNNA, rep: 1, epoch: 1197, acc: 0.7933333516120911, Loss 0.4198792082071304\n",
      "RNNA, rep: 1, epoch: 1198, acc: 0.8166665434837341, Loss 0.4535405126214027\n",
      "RNNA, rep: 1, epoch: 1199, acc: 0.8066664934158325, Loss 0.37129732698202134\n",
      "RNNA, rep: 1, epoch: 1200, acc: 0.7599999904632568, Loss 0.49108954042196273\n",
      "RNNA, rep: 1, epoch: 1201, acc: 0.7966666221618652, Loss 0.4223041704297066\n",
      "RNNA, rep: 1, epoch: 1202, acc: 0.7966665625572205, Loss 0.42489549428224566\n",
      "RNNA, rep: 1, epoch: 1203, acc: 0.7533332705497742, Loss 0.439059117436409\n",
      "RNNA, rep: 1, epoch: 1204, acc: 0.823333203792572, Loss 0.3870129093527794\n",
      "RNNA, rep: 1, epoch: 1205, acc: 0.7900000214576721, Loss 0.44676494508981707\n",
      "RNNA, rep: 1, epoch: 1206, acc: 0.809999942779541, Loss 0.433106609582901\n",
      "RNNA, rep: 1, epoch: 1207, acc: 0.7899998426437378, Loss 0.46956503242254255\n",
      "RNNA, rep: 1, epoch: 1208, acc: 0.8066666126251221, Loss 0.458727126121521\n",
      "RNNA, rep: 1, epoch: 1209, acc: 0.8166666626930237, Loss 0.37724947929382324\n",
      "RNNA, rep: 1, epoch: 1210, acc: 0.7900000810623169, Loss 0.43240795701742174\n",
      "RNNA, rep: 1, epoch: 1211, acc: 0.8000000715255737, Loss 0.45916986495256423\n",
      "RNNA, rep: 1, epoch: 1212, acc: 0.7999999523162842, Loss 0.4304580545425415\n",
      "RNNA, rep: 1, epoch: 1213, acc: 0.7999998331069946, Loss 0.44299313068389895\n",
      "RNNA, rep: 1, epoch: 1214, acc: 0.8299998641014099, Loss 0.40782795250415804\n",
      "RNNA, rep: 1, epoch: 1215, acc: 0.7999998331069946, Loss 0.41935549467802047\n",
      "RNNA, rep: 1, epoch: 1216, acc: 0.7833333015441895, Loss 0.4809314918518066\n",
      "RNNA, rep: 1, epoch: 1217, acc: 0.8066665530204773, Loss 0.45551728904247285\n",
      "RNNA, rep: 1, epoch: 1218, acc: 0.8066667318344116, Loss 0.4561623004078865\n",
      "RNNA, rep: 1, epoch: 1219, acc: 0.8100000023841858, Loss 0.44419029235839846\n",
      "RNNA, rep: 1, epoch: 1220, acc: 0.7966665625572205, Loss 0.43917567282915115\n",
      "RNNA, rep: 1, epoch: 1221, acc: 0.7833331823348999, Loss 0.42938334912061693\n",
      "RNNA, rep: 1, epoch: 1222, acc: 0.8000000715255737, Loss 0.4331850472092629\n",
      "RNNA, rep: 1, epoch: 1223, acc: 0.8033332824707031, Loss 0.4482837787270546\n",
      "RNNA, rep: 1, epoch: 1224, acc: 0.7566666603088379, Loss 0.5011573562026024\n",
      "RNNA, rep: 1, epoch: 1225, acc: 0.8433331251144409, Loss 0.40651161804795266\n",
      "RNNA, rep: 1, epoch: 1226, acc: 0.7833333015441895, Loss 0.4709774459898472\n",
      "RNNA, rep: 1, epoch: 1227, acc: 0.7833333015441895, Loss 0.5222680987417698\n",
      "RNNA, rep: 1, epoch: 1228, acc: 0.7766666412353516, Loss 0.4307422235608101\n",
      "RNNA, rep: 1, epoch: 1229, acc: 0.8066667318344116, Loss 0.42812959134578704\n",
      "RNNA, rep: 1, epoch: 1230, acc: 0.8299998641014099, Loss 0.40203511357307437\n",
      "RNNA, rep: 1, epoch: 1231, acc: 0.7933332920074463, Loss 0.44003448843955995\n",
      "RNNA, rep: 1, epoch: 1232, acc: 0.8299997448921204, Loss 0.3974998313188553\n",
      "RNNA, rep: 1, epoch: 1233, acc: 0.7999997735023499, Loss 0.4253525733947754\n",
      "RNNA, rep: 1, epoch: 1234, acc: 0.8100000023841858, Loss 0.41984797552227976\n",
      "RNNA, rep: 1, epoch: 1235, acc: 0.789999783039093, Loss 0.4683868332207203\n",
      "RNNA, rep: 1, epoch: 1236, acc: 0.81333327293396, Loss 0.38839345559477806\n",
      "RNNA, rep: 1, epoch: 1237, acc: 0.8233331441879272, Loss 0.42743091359734536\n",
      "RNNA, rep: 1, epoch: 1238, acc: 0.7966665625572205, Loss 0.4319718319177628\n",
      "RNNA, rep: 1, epoch: 1239, acc: 0.809999942779541, Loss 0.47323855131864545\n",
      "RNNA, rep: 1, epoch: 1240, acc: 0.809999942779541, Loss 0.4293848474323749\n",
      "RNNA, rep: 1, epoch: 1241, acc: 0.7999999523162842, Loss 0.4550719702243805\n",
      "RNNA, rep: 1, epoch: 1242, acc: 0.7799999713897705, Loss 0.4341972628235817\n",
      "RNNA, rep: 1, epoch: 1243, acc: 0.7966666221618652, Loss 0.4618107648193836\n",
      "RNNA, rep: 1, epoch: 1244, acc: 0.7900000214576721, Loss 0.42747983664274214\n",
      "RNNA, rep: 1, epoch: 1245, acc: 0.8366665840148926, Loss 0.37932888209819793\n",
      "RNNA, rep: 1, epoch: 1246, acc: 0.7966667413711548, Loss 0.4529337042570114\n",
      "RNNA, rep: 1, epoch: 1247, acc: 0.8199999928474426, Loss 0.47643677368760107\n",
      "RNNA, rep: 1, epoch: 1248, acc: 0.7699999809265137, Loss 0.43671451359987257\n",
      "RNNA, rep: 1, epoch: 1249, acc: 0.7700001001358032, Loss 0.42025732427835466\n",
      "RNNA, rep: 1, epoch: 1250, acc: 0.7699998617172241, Loss 0.4069791477918625\n",
      "RNNA, rep: 1, epoch: 1251, acc: 0.800000011920929, Loss 0.4007312533259392\n",
      "RNNA, rep: 1, epoch: 1252, acc: 0.7966665625572205, Loss 0.40911212205886843\n",
      "RNNA, rep: 1, epoch: 1253, acc: 0.8066665530204773, Loss 0.4025163596868515\n",
      "RNNA, rep: 1, epoch: 1254, acc: 0.793333113193512, Loss 0.48011914134025574\n",
      "RNNA, rep: 1, epoch: 1255, acc: 0.7866665720939636, Loss 0.4386090549826622\n",
      "RNNA, rep: 1, epoch: 1256, acc: 0.8133331537246704, Loss 0.40354915767908095\n",
      "RNNA, rep: 1, epoch: 1257, acc: 0.7866666913032532, Loss 0.45036501288414\n",
      "RNNA, rep: 1, epoch: 1258, acc: 0.8233333826065063, Loss 0.42834830909967425\n",
      "RNNA, rep: 1, epoch: 1259, acc: 0.7899998426437378, Loss 0.43653868794441225\n",
      "RNNA, rep: 1, epoch: 1260, acc: 0.8333331942558289, Loss 0.40045386642217634\n",
      "RNNA, rep: 1, epoch: 1261, acc: 0.7933333516120911, Loss 0.47697504773736\n",
      "RNNA, rep: 1, epoch: 1262, acc: 0.7800000905990601, Loss 0.4630093732476234\n",
      "RNNA, rep: 1, epoch: 1263, acc: 0.7800000905990601, Loss 0.4475412914156914\n",
      "RNNA, rep: 1, epoch: 1264, acc: 0.7966666221618652, Loss 0.4754094484448433\n",
      "RNNA, rep: 1, epoch: 1265, acc: 0.8099997639656067, Loss 0.43505835682153704\n",
      "RNNA, rep: 1, epoch: 1266, acc: 0.7666667699813843, Loss 0.4842826110124588\n",
      "RNNA, rep: 1, epoch: 1267, acc: 0.7966667413711548, Loss 0.4343468081951141\n",
      "RNNA, rep: 1, epoch: 1268, acc: 0.779999852180481, Loss 0.4521669465303421\n",
      "RNNA, rep: 1, epoch: 1269, acc: 0.8333332538604736, Loss 0.40862701550126074\n",
      "RNNA, rep: 1, epoch: 1270, acc: 0.8166667222976685, Loss 0.41355780556797983\n",
      "RNNA, rep: 1, epoch: 1271, acc: 0.7966665029525757, Loss 0.4367689025402069\n",
      "RNNA, rep: 1, epoch: 1272, acc: 0.823333203792572, Loss 0.3992647884786129\n",
      "RNNA, rep: 1, epoch: 1273, acc: 0.7566666603088379, Loss 0.4755645591020584\n",
      "RNNA, rep: 1, epoch: 1274, acc: 0.81333327293396, Loss 0.4295194697380066\n",
      "RNNA, rep: 1, epoch: 1275, acc: 0.7733333110809326, Loss 0.47390666395425796\n",
      "RNNA, rep: 1, epoch: 1276, acc: 0.7766667008399963, Loss 0.45059040039777754\n",
      "RNNA, rep: 1, epoch: 1277, acc: 0.7966667413711548, Loss 0.4427685099840164\n",
      "RNNA, rep: 1, epoch: 1278, acc: 0.7766668796539307, Loss 0.4819120928645134\n",
      "RNNA, rep: 1, epoch: 1279, acc: 0.7899999022483826, Loss 0.4498414692282677\n",
      "RNNA, rep: 1, epoch: 1280, acc: 0.7900000214576721, Loss 0.4184920760989189\n",
      "RNNA, rep: 1, epoch: 1281, acc: 0.7799999713897705, Loss 0.4287101200222969\n",
      "RNNA, rep: 1, epoch: 1282, acc: 0.7966665029525757, Loss 0.46386268600821495\n",
      "RNNA, rep: 1, epoch: 1283, acc: 0.7800001502037048, Loss 0.4418992191553116\n",
      "RNNA, rep: 1, epoch: 1284, acc: 0.7966665625572205, Loss 0.418421710729599\n",
      "RNNA, rep: 1, epoch: 1285, acc: 0.8033331036567688, Loss 0.4772392836213112\n",
      "RNNA, rep: 1, epoch: 1286, acc: 0.7999997138977051, Loss 0.41590863317251203\n",
      "RNNA, rep: 1, epoch: 1287, acc: 0.8266667127609253, Loss 0.39028307735919954\n",
      "RNNA, rep: 1, epoch: 1288, acc: 0.7899999022483826, Loss 0.468140007853508\n",
      "RNNA, rep: 1, epoch: 1289, acc: 0.7799999117851257, Loss 0.4693447607755661\n",
      "RNNA, rep: 1, epoch: 1290, acc: 0.7900000214576721, Loss 0.4656401264667511\n",
      "RNNA, rep: 1, epoch: 1291, acc: 0.7966665625572205, Loss 0.42077443391084673\n",
      "RNNA, rep: 1, epoch: 1292, acc: 0.8033332824707031, Loss 0.4696723771095276\n",
      "RNNA, rep: 1, epoch: 1293, acc: 0.7933334112167358, Loss 0.44866242319345473\n",
      "RNNA, rep: 1, epoch: 1294, acc: 0.8266666531562805, Loss 0.43215820655226705\n",
      "RNNA, rep: 1, epoch: 1295, acc: 0.8533332943916321, Loss 0.3746815566718578\n",
      "RNNA, rep: 1, epoch: 1296, acc: 0.7700001001358032, Loss 0.4714165760576725\n",
      "RNNA, rep: 1, epoch: 1297, acc: 0.7533334493637085, Loss 0.4742625872790813\n",
      "RNNA, rep: 1, epoch: 1298, acc: 0.7966665625572205, Loss 0.4548385637998581\n",
      "RNNA, rep: 1, epoch: 1299, acc: 0.8499998450279236, Loss 0.3602937914431095\n",
      "RNNA, rep: 1, epoch: 1300, acc: 0.8233330249786377, Loss 0.3942882113158703\n",
      "RNNA, rep: 1, epoch: 1301, acc: 0.7999997735023499, Loss 0.437786411345005\n",
      "RNNA, rep: 1, epoch: 1302, acc: 0.8466666340827942, Loss 0.4135022522509098\n",
      "RNNA, rep: 1, epoch: 1303, acc: 0.7933334112167358, Loss 0.3930636169016361\n",
      "RNNA, rep: 1, epoch: 1304, acc: 0.8166665434837341, Loss 0.4311924557387829\n",
      "RNNA, rep: 1, epoch: 1305, acc: 0.8166665434837341, Loss 0.4291556964814663\n",
      "RNNA, rep: 1, epoch: 1306, acc: 0.7866666913032532, Loss 0.43411658018827437\n",
      "RNNA, rep: 1, epoch: 1307, acc: 0.7900000214576721, Loss 0.4384324003756046\n",
      "RNNA, rep: 1, epoch: 1308, acc: 0.8033335208892822, Loss 0.4312140014767647\n",
      "RNNA, rep: 1, epoch: 1309, acc: 0.7999998331069946, Loss 0.40188950195908546\n",
      "RNNA, rep: 1, epoch: 1310, acc: 0.8199998736381531, Loss 0.4243977706134319\n",
      "RNNA, rep: 1, epoch: 1311, acc: 0.7833331823348999, Loss 0.47479970633983615\n",
      "RNNA, rep: 1, epoch: 1312, acc: 0.8199998736381531, Loss 0.4051525190472603\n",
      "RNNA, rep: 1, epoch: 1313, acc: 0.833333432674408, Loss 0.42689623177051544\n",
      "RNNA, rep: 1, epoch: 1314, acc: 0.7800000905990601, Loss 0.47022641375660895\n",
      "RNNA, rep: 1, epoch: 1315, acc: 0.7866665720939636, Loss 0.49188281923532484\n",
      "RNNA, rep: 1, epoch: 1316, acc: 0.7933332324028015, Loss 0.433494666069746\n",
      "RNNA, rep: 1, epoch: 1317, acc: 0.7799999713897705, Loss 0.43038378477096556\n",
      "RNNA, rep: 1, epoch: 1318, acc: 0.7899998426437378, Loss 0.4443807029724121\n",
      "RNNA, rep: 1, epoch: 1319, acc: 0.8033334612846375, Loss 0.41603418827056887\n",
      "RNNA, rep: 1, epoch: 1320, acc: 0.7933333516120911, Loss 0.44096992641687394\n",
      "RNNA, rep: 1, epoch: 1321, acc: 0.7933332920074463, Loss 0.427270470559597\n",
      "RNNA, rep: 1, epoch: 1322, acc: 0.7733333706855774, Loss 0.463929095864296\n",
      "RNNA, rep: 1, epoch: 1323, acc: 0.8299999237060547, Loss 0.4374534545838833\n",
      "RNNA, rep: 1, epoch: 1324, acc: 0.8066664934158325, Loss 0.47661625549197195\n",
      "RNNA, rep: 1, epoch: 1325, acc: 0.8266664743423462, Loss 0.4028763884305954\n",
      "RNNA, rep: 1, epoch: 1326, acc: 0.7866665720939636, Loss 0.4928634925186634\n",
      "RNNA, rep: 1, epoch: 1327, acc: 0.7566666603088379, Loss 0.4816305460035801\n",
      "RNNA, rep: 1, epoch: 1328, acc: 0.8033332824707031, Loss 0.42865960121154784\n",
      "RNNA, rep: 1, epoch: 1329, acc: 0.7933332920074463, Loss 0.45556356757879257\n",
      "RNNA, rep: 1, epoch: 1330, acc: 0.8033335208892822, Loss 0.43387397527694704\n",
      "RNNA, rep: 1, epoch: 1331, acc: 0.7766666412353516, Loss 0.4149596121907234\n",
      "RNNA, rep: 1, epoch: 1332, acc: 0.7833331823348999, Loss 0.4326183247566223\n",
      "RNNA, rep: 1, epoch: 1333, acc: 0.81333327293396, Loss 0.414953034222126\n",
      "RNNA, rep: 1, epoch: 1334, acc: 0.8299999237060547, Loss 0.37665742963552473\n",
      "RNNA, rep: 1, epoch: 1335, acc: 0.8199998736381531, Loss 0.41019037991762164\n",
      "RNNA, rep: 1, epoch: 1336, acc: 0.7766666412353516, Loss 0.4428076270222664\n",
      "RNNA, rep: 1, epoch: 1337, acc: 0.8266664743423462, Loss 0.40381704837083815\n",
      "RNNA, rep: 1, epoch: 1338, acc: 0.7900000214576721, Loss 0.43500097423791884\n",
      "RNNA, rep: 1, epoch: 1339, acc: 0.8466665148735046, Loss 0.4073163068294525\n",
      "RNNA, rep: 1, epoch: 1340, acc: 0.8233332633972168, Loss 0.409565948843956\n",
      "RNNA, rep: 1, epoch: 1341, acc: 0.8033332228660583, Loss 0.4467887637019157\n",
      "RNNA, rep: 1, epoch: 1342, acc: 0.7733331322669983, Loss 0.5058122181892395\n",
      "RNNA, rep: 1, epoch: 1343, acc: 0.8299997448921204, Loss 0.41994138911366463\n",
      "RNNA, rep: 1, epoch: 1344, acc: 0.8066665530204773, Loss 0.4028139746189117\n",
      "RNNA, rep: 1, epoch: 1345, acc: 0.7766664624214172, Loss 0.4239651104807854\n",
      "RNNA, rep: 1, epoch: 1346, acc: 0.7833333015441895, Loss 0.413510357439518\n",
      "RNNA, rep: 1, epoch: 1347, acc: 0.7899998426437378, Loss 0.4362741878628731\n",
      "RNNA, rep: 1, epoch: 1348, acc: 0.8333333730697632, Loss 0.4048706912994385\n",
      "RNNA, rep: 1, epoch: 1349, acc: 0.7899999022483826, Loss 0.4570763784646988\n",
      "RNNA, rep: 1, epoch: 1350, acc: 0.8133333325386047, Loss 0.44476117968559264\n",
      "RNNA, rep: 1, epoch: 1351, acc: 0.7533332705497742, Loss 0.5116628700494766\n",
      "RNNA, rep: 1, epoch: 1352, acc: 0.8066666126251221, Loss 0.4194926393032074\n",
      "RNNA, rep: 1, epoch: 1353, acc: 0.8166666626930237, Loss 0.4325289288163185\n",
      "RNNA, rep: 1, epoch: 1354, acc: 0.7766667008399963, Loss 0.44279230892658233\n",
      "RNNA, rep: 1, epoch: 1355, acc: 0.81333327293396, Loss 0.408546501994133\n",
      "RNNA, rep: 1, epoch: 1356, acc: 0.7833333611488342, Loss 0.4152097600698471\n",
      "RNNA, rep: 1, epoch: 1357, acc: 0.7900000214576721, Loss 0.4767280748486519\n",
      "RNNA, rep: 1, epoch: 1358, acc: 0.7866665720939636, Loss 0.43649196028709414\n",
      "RNNA, rep: 1, epoch: 1359, acc: 0.7799999117851257, Loss 0.4777021914720535\n",
      "RNNA, rep: 1, epoch: 1360, acc: 0.7933333516120911, Loss 0.4116021963953972\n",
      "RNNA, rep: 1, epoch: 1361, acc: 0.7999997735023499, Loss 0.4211510872840881\n",
      "RNNA, rep: 1, epoch: 1362, acc: 0.8066666126251221, Loss 0.44050144255161283\n",
      "RNNA, rep: 1, epoch: 1363, acc: 0.7866665720939636, Loss 0.417512358725071\n",
      "RNNA, rep: 1, epoch: 1364, acc: 0.7600001692771912, Loss 0.45164636731147767\n",
      "RNNA, rep: 1, epoch: 1365, acc: 0.8299999237060547, Loss 0.37128193348646166\n",
      "RNNA, rep: 1, epoch: 1366, acc: 0.7933334112167358, Loss 0.4620134341716766\n",
      "RNNA, rep: 1, epoch: 1367, acc: 0.8066664934158325, Loss 0.44345001488924024\n",
      "RNNA, rep: 1, epoch: 1368, acc: 0.7933333516120911, Loss 0.42279736667871476\n",
      "RNNA, rep: 1, epoch: 1369, acc: 0.8233332633972168, Loss 0.38944836139678957\n",
      "RNNA, rep: 1, epoch: 1370, acc: 0.7966667413711548, Loss 0.42795205548405646\n",
      "RNNA, rep: 1, epoch: 1371, acc: 0.7966666221618652, Loss 0.41092353031039236\n",
      "RNNA, rep: 1, epoch: 1372, acc: 0.8333332538604736, Loss 0.4350038242340088\n",
      "RNNA, rep: 1, epoch: 1373, acc: 0.8033332228660583, Loss 0.4247786729037762\n",
      "RNNA, rep: 1, epoch: 1374, acc: 0.779999852180481, Loss 0.4205978283286095\n",
      "RNNA, rep: 1, epoch: 1375, acc: 0.7966665029525757, Loss 0.45331681236624716\n",
      "RNNA, rep: 1, epoch: 1376, acc: 0.7566666603088379, Loss 0.5094660659134388\n",
      "RNNA, rep: 1, epoch: 1377, acc: 0.7699999213218689, Loss 0.4569979244470596\n",
      "RNNA, rep: 1, epoch: 1378, acc: 0.8166666626930237, Loss 0.4081710559129715\n",
      "RNNA, rep: 1, epoch: 1379, acc: 0.8133335113525391, Loss 0.4537004777789116\n",
      "RNNA, rep: 1, epoch: 1380, acc: 0.7999999523162842, Loss 0.42166236221790315\n",
      "RNNA, rep: 1, epoch: 1381, acc: 0.763333261013031, Loss 0.4737171870470047\n",
      "RNNA, rep: 1, epoch: 1382, acc: 0.8366665840148926, Loss 0.4011989167332649\n",
      "RNNA, rep: 1, epoch: 1383, acc: 0.7966665625572205, Loss 0.4507633927464485\n",
      "RNNA, rep: 1, epoch: 1384, acc: 0.8199999332427979, Loss 0.4682553100585938\n",
      "RNNA, rep: 1, epoch: 1385, acc: 0.7633332014083862, Loss 0.4801965484023094\n",
      "RNNA, rep: 1, epoch: 1386, acc: 0.8033332824707031, Loss 0.4353064689040184\n",
      "RNNA, rep: 1, epoch: 1387, acc: 0.7966667413711548, Loss 0.4116841465234756\n",
      "RNNA, rep: 1, epoch: 1388, acc: 0.7833331823348999, Loss 0.44639899060130117\n",
      "RNNA, rep: 1, epoch: 1389, acc: 0.7833333015441895, Loss 0.44358166217803957\n",
      "RNNA, rep: 1, epoch: 1390, acc: 0.8233332633972168, Loss 0.4472863262891769\n",
      "RNNA, rep: 1, epoch: 1391, acc: 0.7966666221618652, Loss 0.48071838676929474\n",
      "RNNA, rep: 1, epoch: 1392, acc: 0.7533334493637085, Loss 0.4729023331403732\n",
      "RNNA, rep: 1, epoch: 1393, acc: 0.7899999022483826, Loss 0.42648223131895063\n",
      "RNNA, rep: 1, epoch: 1394, acc: 0.7866665124893188, Loss 0.48668302744627\n",
      "RNNA, rep: 1, epoch: 1395, acc: 0.7999998331069946, Loss 0.45975394755601884\n",
      "RNNA, rep: 1, epoch: 1396, acc: 0.7966666221618652, Loss 0.4652727124094963\n",
      "RNNA, rep: 1, epoch: 1397, acc: 0.7866666316986084, Loss 0.44509733706712723\n",
      "RNNA, rep: 1, epoch: 1398, acc: 0.8166667222976685, Loss 0.42372077256441115\n",
      "RNNA, rep: 1, epoch: 1399, acc: 0.8033332824707031, Loss 0.40627070754766464\n",
      "RNNA, rep: 1, epoch: 1400, acc: 0.8166665434837341, Loss 0.449565150141716\n",
      "RNNA, rep: 1, epoch: 1401, acc: 0.8266665935516357, Loss 0.42901943624019623\n",
      "RNNA, rep: 1, epoch: 1402, acc: 0.7599999904632568, Loss 0.4486534520983696\n",
      "RNNA, rep: 1, epoch: 1403, acc: 0.8066664934158325, Loss 0.43236626803874967\n",
      "RNNA, rep: 1, epoch: 1404, acc: 0.8366665840148926, Loss 0.4367999079823494\n",
      "RNNA, rep: 1, epoch: 1405, acc: 0.8233331441879272, Loss 0.42857778921723366\n",
      "RNNA, rep: 1, epoch: 1406, acc: 0.8000000715255737, Loss 0.4414047406613827\n",
      "RNNA, rep: 1, epoch: 1407, acc: 0.7766665816307068, Loss 0.45770724549889563\n",
      "RNNA, rep: 1, epoch: 1408, acc: 0.8099997639656067, Loss 0.4117832687497139\n",
      "RNNA, rep: 1, epoch: 1409, acc: 0.7866666913032532, Loss 0.43640499383211134\n",
      "RNNA, rep: 1, epoch: 1410, acc: 0.8299999237060547, Loss 0.40663487076759336\n",
      "RNNA, rep: 1, epoch: 1411, acc: 0.8199999332427979, Loss 0.4358682107925415\n",
      "RNNA, rep: 1, epoch: 1412, acc: 0.7599998712539673, Loss 0.48318480372428896\n",
      "RNNA, rep: 1, epoch: 1413, acc: 0.8033333420753479, Loss 0.4158198630809784\n",
      "RNNA, rep: 1, epoch: 1414, acc: 0.800000011920929, Loss 0.40537108600139615\n",
      "RNNA, rep: 1, epoch: 1415, acc: 0.8099997639656067, Loss 0.40017506301403044\n",
      "RNNA, rep: 1, epoch: 1416, acc: 0.8066664934158325, Loss 0.3918376502394676\n",
      "RNNA, rep: 1, epoch: 1417, acc: 0.7999999523162842, Loss 0.4402015656232834\n",
      "RNNA, rep: 1, epoch: 1418, acc: 0.7699997425079346, Loss 0.4745165249705315\n",
      "RNNA, rep: 1, epoch: 1419, acc: 0.7733334302902222, Loss 0.4763997119665146\n",
      "RNNA, rep: 1, epoch: 1420, acc: 0.800000011920929, Loss 0.4239626607298851\n",
      "RNNA, rep: 1, epoch: 1421, acc: 0.7866665124893188, Loss 0.4569578132033348\n",
      "RNNA, rep: 1, epoch: 1422, acc: 0.7966667413711548, Loss 0.39932466357946395\n",
      "RNNA, rep: 1, epoch: 1423, acc: 0.7966665625572205, Loss 0.41450690567493437\n",
      "RNNA, rep: 1, epoch: 1424, acc: 0.8099998235702515, Loss 0.48092573434114455\n",
      "RNNA, rep: 1, epoch: 1425, acc: 0.7933332324028015, Loss 0.40848413050174714\n",
      "RNNA, rep: 1, epoch: 1426, acc: 0.7800001502037048, Loss 0.4506238153576851\n",
      "RNNA, rep: 1, epoch: 1427, acc: 0.7499998211860657, Loss 0.45678866893053055\n",
      "RNNA, rep: 1, epoch: 1428, acc: 0.8399998545646667, Loss 0.3857357966899872\n",
      "RNNA, rep: 1, epoch: 1429, acc: 0.7799999713897705, Loss 0.4598193296790123\n",
      "RNNA, rep: 1, epoch: 1430, acc: 0.7933332324028015, Loss 0.4531432804465294\n",
      "RNNA, rep: 1, epoch: 1431, acc: 0.8066667318344116, Loss 0.4296702820062637\n",
      "RNNA, rep: 1, epoch: 1432, acc: 0.8066665530204773, Loss 0.42151752069592474\n",
      "RNNA, rep: 1, epoch: 1433, acc: 0.8166664838790894, Loss 0.39965827882289884\n",
      "RNNA, rep: 1, epoch: 1434, acc: 0.7833333015441895, Loss 0.4335008698701859\n",
      "RNNA, rep: 1, epoch: 1435, acc: 0.7966665625572205, Loss 0.4747559231519699\n",
      "RNNA, rep: 1, epoch: 1436, acc: 0.7966666221618652, Loss 0.4843800213932991\n",
      "RNNA, rep: 1, epoch: 1437, acc: 0.7999999523162842, Loss 0.43223953396081927\n",
      "RNNA, rep: 1, epoch: 1438, acc: 0.7800000905990601, Loss 0.44635106295347216\n",
      "RNNA, rep: 1, epoch: 1439, acc: 0.8233334422111511, Loss 0.4407523939013481\n",
      "RNNA, rep: 1, epoch: 1440, acc: 0.7899998426437378, Loss 0.40973822996020315\n",
      "RNNA, rep: 1, epoch: 1441, acc: 0.8133330345153809, Loss 0.41049493044614793\n",
      "RNNA, rep: 1, epoch: 1442, acc: 0.8066667914390564, Loss 0.40765522986650465\n",
      "RNNA, rep: 1, epoch: 1443, acc: 0.7999998331069946, Loss 0.4293926122784615\n",
      "RNNA, rep: 1, epoch: 1444, acc: 0.7933332324028015, Loss 0.48807498201727867\n",
      "RNNA, rep: 1, epoch: 1445, acc: 0.823333203792572, Loss 0.3930897694826126\n",
      "RNNA, rep: 1, epoch: 1446, acc: 0.7900000810623169, Loss 0.414995754212141\n",
      "RNNA, rep: 1, epoch: 1447, acc: 0.7766665816307068, Loss 0.4758269762992859\n",
      "RNNA, rep: 1, epoch: 1448, acc: 0.7999998331069946, Loss 0.4015406835079193\n",
      "RNNA, rep: 1, epoch: 1449, acc: 0.8033332824707031, Loss 0.439609155356884\n",
      "RNNA, rep: 1, epoch: 1450, acc: 0.7900000214576721, Loss 0.42733803421258926\n",
      "RNNA, rep: 1, epoch: 1451, acc: 0.8233332633972168, Loss 0.4100859281420708\n",
      "RNNA, rep: 1, epoch: 1452, acc: 0.8133334517478943, Loss 0.41498125582933426\n",
      "RNNA, rep: 1, epoch: 1453, acc: 0.7900000214576721, Loss 0.403128590285778\n",
      "RNNA, rep: 1, epoch: 1454, acc: 0.8166665434837341, Loss 0.4082868719100952\n",
      "RNNA, rep: 1, epoch: 1455, acc: 0.7699999809265137, Loss 0.4997553601861\n",
      "RNNA, rep: 1, epoch: 1456, acc: 0.8099998235702515, Loss 0.457252262532711\n",
      "RNNA, rep: 1, epoch: 1457, acc: 0.7699999213218689, Loss 0.4306143870949745\n",
      "RNNA, rep: 1, epoch: 1458, acc: 0.800000011920929, Loss 0.47014399379491806\n",
      "RNNA, rep: 1, epoch: 1459, acc: 0.8366665840148926, Loss 0.3848667362332344\n",
      "RNNA, rep: 1, epoch: 1460, acc: 0.7933333516120911, Loss 0.40853083282709124\n",
      "RNNA, rep: 1, epoch: 1461, acc: 0.8199998736381531, Loss 0.3897852063179016\n",
      "RNNA, rep: 1, epoch: 1462, acc: 0.7699999213218689, Loss 0.4391279594600201\n",
      "RNNA, rep: 1, epoch: 1463, acc: 0.8233334422111511, Loss 0.3984099358320236\n",
      "RNNA, rep: 1, epoch: 1464, acc: 0.8166664838790894, Loss 0.4195552775263786\n",
      "RNNA, rep: 1, epoch: 1465, acc: 0.8299998641014099, Loss 0.3813776117563248\n",
      "RNNA, rep: 1, epoch: 1466, acc: 0.8033332824707031, Loss 0.41339545980095865\n",
      "RNNA, rep: 1, epoch: 1467, acc: 0.8166664838790894, Loss 0.38596023604273794\n",
      "RNNA, rep: 1, epoch: 1468, acc: 0.8033332824707031, Loss 0.43170384168624876\n",
      "RNNA, rep: 1, epoch: 1469, acc: 0.7733333706855774, Loss 0.43049934655427935\n",
      "RNNA, rep: 1, epoch: 1470, acc: 0.8033332824707031, Loss 0.4667652839422226\n",
      "RNNA, rep: 1, epoch: 1471, acc: 0.7833333611488342, Loss 0.48923041075468066\n",
      "RNNA, rep: 1, epoch: 1472, acc: 0.8199998736381531, Loss 0.42265984311699867\n",
      "RNNA, rep: 1, epoch: 1473, acc: 0.8099998235702515, Loss 0.4147977554798126\n",
      "RNNA, rep: 1, epoch: 1474, acc: 0.8066665530204773, Loss 0.4310933999717236\n",
      "RNNA, rep: 1, epoch: 1475, acc: 0.7933335304260254, Loss 0.4899757394194603\n",
      "RNNA, rep: 1, epoch: 1476, acc: 0.8133331537246704, Loss 0.42112904384732247\n",
      "RNNA, rep: 1, epoch: 1477, acc: 0.809999942779541, Loss 0.4028740984201431\n",
      "RNNA, rep: 1, epoch: 1478, acc: 0.7966667413711548, Loss 0.4503352409601212\n",
      "RNNA, rep: 1, epoch: 1479, acc: 0.7933332324028015, Loss 0.40603855878114703\n",
      "RNNA, rep: 1, epoch: 1480, acc: 0.8399998545646667, Loss 0.423464774787426\n",
      "RNNA, rep: 1, epoch: 1481, acc: 0.7699998617172241, Loss 0.4756897681951523\n",
      "RNNA, rep: 1, epoch: 1482, acc: 0.7933332920074463, Loss 0.42171856373548505\n",
      "RNNA, rep: 1, epoch: 1483, acc: 0.8100000023841858, Loss 0.4316390520334244\n",
      "RNNA, rep: 1, epoch: 1484, acc: 0.81333327293396, Loss 0.40347515016794205\n",
      "RNNA, rep: 1, epoch: 1485, acc: 0.8133333325386047, Loss 0.4345990291237831\n",
      "RNNA, rep: 1, epoch: 1486, acc: 0.7966668009757996, Loss 0.45429575204849243\n",
      "RNNA, rep: 1, epoch: 1487, acc: 0.8299999833106995, Loss 0.3818316002190113\n",
      "RNNA, rep: 1, epoch: 1488, acc: 0.7900001406669617, Loss 0.4408194637298584\n",
      "RNNA, rep: 1, epoch: 1489, acc: 0.8166667222976685, Loss 0.38404996365308763\n",
      "RNNA, rep: 1, epoch: 1490, acc: 0.7966665029525757, Loss 0.4463803645968437\n",
      "RNNA, rep: 1, epoch: 1491, acc: 0.7966665029525757, Loss 0.41831614524126054\n",
      "RNNA, rep: 1, epoch: 1492, acc: 0.809999942779541, Loss 0.4438644781708717\n",
      "RNNA, rep: 1, epoch: 1493, acc: 0.7933333516120911, Loss 0.4299946501851082\n",
      "RNNA, rep: 1, epoch: 1494, acc: 0.7866666316986084, Loss 0.45979446947574615\n",
      "RNNA, rep: 1, epoch: 1495, acc: 0.8466665148735046, Loss 0.38327629029750826\n",
      "RNNA, rep: 1, epoch: 1496, acc: 0.7666667103767395, Loss 0.4488100512325764\n",
      "RNNA, rep: 1, epoch: 1497, acc: 0.8066664934158325, Loss 0.3896112930774689\n",
      "RNNA, rep: 1, epoch: 1498, acc: 0.7899999022483826, Loss 0.43919182479381563\n",
      "RNNA, rep: 1, epoch: 1499, acc: 0.8199998736381531, Loss 0.401362883746624\n",
      "RNNA, rep: 1, epoch: 1500, acc: 0.7999999523162842, Loss 0.45216038942337033\n",
      "RNNA, rep: 1, epoch: 1501, acc: 0.7900000214576721, Loss 0.42530030563473703\n",
      "RNNA, rep: 1, epoch: 1502, acc: 0.7799999117851257, Loss 0.4597341886162758\n",
      "RNNA, rep: 1, epoch: 1503, acc: 0.8233331441879272, Loss 0.41145199358463286\n",
      "RNNA, rep: 1, epoch: 1504, acc: 0.7933332324028015, Loss 0.4728783030807972\n",
      "RNNA, rep: 1, epoch: 1505, acc: 0.8033332824707031, Loss 0.41105343386530874\n",
      "RNNA, rep: 1, epoch: 1506, acc: 0.7966668009757996, Loss 0.42182246446609495\n",
      "RNNA, rep: 1, epoch: 1507, acc: 0.8033331036567688, Loss 0.4353631755709648\n",
      "RNNA, rep: 1, epoch: 1508, acc: 0.7900000214576721, Loss 0.43326172038912775\n",
      "RNNA, rep: 1, epoch: 1509, acc: 0.7933334112167358, Loss 0.43759434029459954\n",
      "RNNA, rep: 1, epoch: 1510, acc: 0.7699998617172241, Loss 0.45199271112680434\n",
      "RNNA, rep: 1, epoch: 1511, acc: 0.8099998235702515, Loss 0.49118784308433533\n",
      "RNNA, rep: 1, epoch: 1512, acc: 0.8333331346511841, Loss 0.4408647735416889\n",
      "RNNA, rep: 1, epoch: 1513, acc: 0.7900000214576721, Loss 0.4605011762678623\n",
      "RNNA, rep: 1, epoch: 1514, acc: 0.8033333420753479, Loss 0.42148137241601946\n",
      "RNNA, rep: 1, epoch: 1515, acc: 0.7499999403953552, Loss 0.4809765625\n",
      "RNNA, rep: 1, epoch: 1516, acc: 0.7699998617172241, Loss 0.4953534385561943\n",
      "RNNA, rep: 1, epoch: 1517, acc: 0.8100000619888306, Loss 0.3785276758670807\n",
      "RNNA, rep: 1, epoch: 1518, acc: 0.7833331227302551, Loss 0.4308485233783722\n",
      "RNNA, rep: 1, epoch: 1519, acc: 0.8266666531562805, Loss 0.40916587293148043\n",
      "RNNA, rep: 1, epoch: 1520, acc: 0.7633334398269653, Loss 0.43819345682859423\n",
      "RNNA, rep: 1, epoch: 1521, acc: 0.8033331036567688, Loss 0.4386724269390106\n",
      "RNNA, rep: 1, epoch: 1522, acc: 0.8266664147377014, Loss 0.40639477550983427\n",
      "RNNA, rep: 1, epoch: 1523, acc: 0.8066665530204773, Loss 0.4721733844280243\n",
      "RNNA, rep: 1, epoch: 1524, acc: 0.8266667723655701, Loss 0.4075175365805626\n",
      "RNNA, rep: 1, epoch: 1525, acc: 0.8100000023841858, Loss 0.4621319243311882\n",
      "RNNA, rep: 1, epoch: 1526, acc: 0.7900000810623169, Loss 0.4700332947075367\n",
      "RNNA, rep: 1, epoch: 1527, acc: 0.7899998426437378, Loss 0.44804886788129805\n",
      "RNNA, rep: 1, epoch: 1528, acc: 0.8066666126251221, Loss 0.4385252296924591\n",
      "RNNA, rep: 1, epoch: 1529, acc: 0.7900000214576721, Loss 0.4544088181853294\n",
      "RNNA, rep: 1, epoch: 1530, acc: 0.81333327293396, Loss 0.44066182494163514\n",
      "RNNA, rep: 1, epoch: 1531, acc: 0.8233332633972168, Loss 0.4274794444441795\n",
      "RNNA, rep: 1, epoch: 1532, acc: 0.7933334112167358, Loss 0.4113005824387074\n",
      "RNNA, rep: 1, epoch: 1533, acc: 0.7766667008399963, Loss 0.4451866349577904\n",
      "RNNA, rep: 1, epoch: 1534, acc: 0.8199999332427979, Loss 0.451231906414032\n",
      "RNNA, rep: 1, epoch: 1535, acc: 0.8366666436195374, Loss 0.3726458628475666\n",
      "RNNA, rep: 1, epoch: 1536, acc: 0.7866665720939636, Loss 0.4320498783886433\n",
      "RNNA, rep: 1, epoch: 1537, acc: 0.7866666913032532, Loss 0.49954756975173953\n",
      "RNNA, rep: 1, epoch: 1538, acc: 0.8033332228660583, Loss 0.4540454405546188\n",
      "RNNA, rep: 1, epoch: 1539, acc: 0.7900000214576721, Loss 0.45985238581895826\n",
      "RNNA, rep: 1, epoch: 1540, acc: 0.8233334422111511, Loss 0.431361765563488\n",
      "RNNA, rep: 1, epoch: 1541, acc: 0.7999999523162842, Loss 0.4914300361275673\n",
      "RNNA, rep: 1, epoch: 1542, acc: 0.8033333420753479, Loss 0.4079436358809471\n",
      "RNNA, rep: 1, epoch: 1543, acc: 0.7566667199134827, Loss 0.5510928454995155\n",
      "RNNA, rep: 1, epoch: 1544, acc: 0.800000011920929, Loss 0.4415570130944252\n",
      "RNNA, rep: 1, epoch: 1545, acc: 0.8166665434837341, Loss 0.40714270502328875\n",
      "RNNA, rep: 1, epoch: 1546, acc: 0.7633332014083862, Loss 0.5137056997418403\n",
      "RNNA, rep: 1, epoch: 1547, acc: 0.8266667127609253, Loss 0.42323175340890884\n",
      "RNNA, rep: 1, epoch: 1548, acc: 0.7866666316986084, Loss 0.46483143776655195\n",
      "RNNA, rep: 1, epoch: 1549, acc: 0.8499999046325684, Loss 0.3992896454036236\n",
      "RNNA, rep: 1, epoch: 1550, acc: 0.8199999332427979, Loss 0.41225863590836526\n",
      "RNNA, rep: 1, epoch: 1551, acc: 0.8100000619888306, Loss 0.41313773989677427\n",
      "RNNA, rep: 1, epoch: 1552, acc: 0.8200000524520874, Loss 0.40367710679769514\n",
      "RNNA, rep: 1, epoch: 1553, acc: 0.8100000023841858, Loss 0.4437698996067047\n",
      "RNNA, rep: 1, epoch: 1554, acc: 0.8233333826065063, Loss 0.46521732479333877\n",
      "RNNA, rep: 1, epoch: 1555, acc: 0.7833331823348999, Loss 0.4629937660694122\n",
      "RNNA, rep: 1, epoch: 1556, acc: 0.8266665935516357, Loss 0.4137912051379681\n",
      "RNNA, rep: 1, epoch: 1557, acc: 0.8000000715255737, Loss 0.41412282451987265\n",
      "RNNA, rep: 1, epoch: 1558, acc: 0.7866665720939636, Loss 0.4530409947037697\n",
      "RNNA, rep: 1, epoch: 1559, acc: 0.793333113193512, Loss 0.4082168173789978\n",
      "RNNA, rep: 1, epoch: 1560, acc: 0.7733334302902222, Loss 0.4200484104454517\n",
      "RNNA, rep: 1, epoch: 1561, acc: 0.8199999332427979, Loss 0.39871110409498217\n",
      "RNNA, rep: 1, epoch: 1562, acc: 0.8199999928474426, Loss 0.4465251332521439\n",
      "RNNA, rep: 1, epoch: 1563, acc: 0.7699999213218689, Loss 0.49911177307367327\n",
      "RNNA, rep: 1, epoch: 1564, acc: 0.8233332633972168, Loss 0.48033441573381425\n",
      "RNNA, rep: 1, epoch: 1565, acc: 0.8233331441879272, Loss 0.4025561636686325\n",
      "RNNA, rep: 1, epoch: 1566, acc: 0.8533332943916321, Loss 0.4013145908713341\n",
      "RNNA, rep: 1, epoch: 1567, acc: 0.7899999022483826, Loss 0.4286751468479633\n",
      "RNNA, rep: 1, epoch: 1568, acc: 0.7733331322669983, Loss 0.4364430204033852\n",
      "RNNA, rep: 1, epoch: 1569, acc: 0.7833333015441895, Loss 0.40039781749248504\n",
      "RNNA, rep: 1, epoch: 1570, acc: 0.7966667413711548, Loss 0.4124151599407196\n",
      "RNNA, rep: 1, epoch: 1571, acc: 0.7966665625572205, Loss 0.4305443835258484\n",
      "RNNA, rep: 1, epoch: 1572, acc: 0.7799999713897705, Loss 0.40480347588658333\n",
      "RNNA, rep: 1, epoch: 1573, acc: 0.7899999022483826, Loss 0.43039266228675843\n",
      "RNNA, rep: 1, epoch: 1574, acc: 0.8100000619888306, Loss 0.4253607866168022\n",
      "RNNA, rep: 1, epoch: 1575, acc: 0.7799999117851257, Loss 0.4181587606668472\n",
      "RNNA, rep: 1, epoch: 1576, acc: 0.7933333516120911, Loss 0.41582004010677337\n",
      "RNNA, rep: 1, epoch: 1577, acc: 0.7766667008399963, Loss 0.4445318552851677\n",
      "RNNA, rep: 1, epoch: 1578, acc: 0.7966667413711548, Loss 0.46373742312192917\n",
      "RNNA, rep: 1, epoch: 1579, acc: 0.7866666316986084, Loss 0.4215936595201492\n",
      "RNNA, rep: 1, epoch: 1580, acc: 0.8099997043609619, Loss 0.39618040680885314\n",
      "RNNA, rep: 1, epoch: 1581, acc: 0.783333420753479, Loss 0.4274407935142517\n",
      "RNNA, rep: 1, epoch: 1582, acc: 0.8199997544288635, Loss 0.39622246354818347\n",
      "RNNA, rep: 1, epoch: 1583, acc: 0.8233331441879272, Loss 0.3904325476288795\n",
      "RNNA, rep: 1, epoch: 1584, acc: 0.7766666412353516, Loss 0.45714194267988206\n",
      "RNNA, rep: 1, epoch: 1585, acc: 0.783333420753479, Loss 0.43427534312009813\n",
      "RNNA, rep: 1, epoch: 1586, acc: 0.8000000715255737, Loss 0.44192141890525816\n",
      "RNNA, rep: 1, epoch: 1587, acc: 0.8066664934158325, Loss 0.4004306608438492\n",
      "RNNA, rep: 1, epoch: 1588, acc: 0.7800000905990601, Loss 0.49174170464277267\n",
      "RNNA, rep: 1, epoch: 1589, acc: 0.7899999022483826, Loss 0.45215666204690935\n",
      "RNNA, rep: 1, epoch: 1590, acc: 0.7733334302902222, Loss 0.4108973044157028\n",
      "RNNA, rep: 1, epoch: 1591, acc: 0.7999999523162842, Loss 0.4228619669377804\n",
      "RNNA, rep: 1, epoch: 1592, acc: 0.8166665434837341, Loss 0.39641525655984877\n",
      "RNNA, rep: 1, epoch: 1593, acc: 0.7966666221618652, Loss 0.4452172127366066\n",
      "RNNA, rep: 1, epoch: 1594, acc: 0.8100000023841858, Loss 0.4050765940546989\n",
      "RNNA, rep: 1, epoch: 1595, acc: 0.7966665625572205, Loss 0.45753659278154374\n",
      "RNNA, rep: 1, epoch: 1596, acc: 0.8033333420753479, Loss 0.41587788105010987\n",
      "RNNA, rep: 1, epoch: 1597, acc: 0.7799999713897705, Loss 0.44243722170591354\n",
      "RNNA, rep: 1, epoch: 1598, acc: 0.8366666436195374, Loss 0.4039841869473457\n",
      "RNNA, rep: 1, epoch: 1599, acc: 0.8000001311302185, Loss 0.4299527125060558\n",
      "RNNA, rep: 1, epoch: 1600, acc: 0.8166665434837341, Loss 0.3983178797364235\n",
      "RNNA, rep: 1, epoch: 1601, acc: 0.7966665625572205, Loss 0.43663531854748727\n",
      "RNNA, rep: 1, epoch: 1602, acc: 0.7699999809265137, Loss 0.41544913113117216\n",
      "RNNA, rep: 1, epoch: 1603, acc: 0.8066664934158325, Loss 0.4067742055654526\n",
      "RNNA, rep: 1, epoch: 1604, acc: 0.7933332920074463, Loss 0.43488898277282717\n",
      "RNNA, rep: 1, epoch: 1605, acc: 0.800000011920929, Loss 0.4253763711452484\n",
      "RNNA, rep: 1, epoch: 1606, acc: 0.8099998235702515, Loss 0.3922341108322144\n",
      "RNNA, rep: 1, epoch: 1607, acc: 0.8100000619888306, Loss 0.462781363427639\n",
      "RNNA, rep: 1, epoch: 1608, acc: 0.7700002193450928, Loss 0.5088695502281189\n",
      "RNNA, rep: 1, epoch: 1609, acc: 0.809999942779541, Loss 0.4312540692090988\n",
      "RNNA, rep: 1, epoch: 1610, acc: 0.7633332014083862, Loss 0.4553755196928978\n",
      "RNNA, rep: 1, epoch: 1611, acc: 0.8033332228660583, Loss 0.42706024289131167\n",
      "RNNA, rep: 1, epoch: 1612, acc: 0.7999999523162842, Loss 0.4316376113891602\n",
      "RNNA, rep: 1, epoch: 1613, acc: 0.7733334898948669, Loss 0.42006720811128617\n",
      "RNNA, rep: 1, epoch: 1614, acc: 0.7933334112167358, Loss 0.423754078745842\n",
      "RNNA, rep: 1, epoch: 1615, acc: 0.800000011920929, Loss 0.43053869366645814\n",
      "RNNA, rep: 1, epoch: 1616, acc: 0.8399998545646667, Loss 0.3849241180717945\n",
      "RNNA, rep: 1, epoch: 1617, acc: 0.7966665029525757, Loss 0.4271646136045456\n",
      "RNNA, rep: 1, epoch: 1618, acc: 0.81333327293396, Loss 0.401539484411478\n",
      "RNNA, rep: 1, epoch: 1619, acc: 0.7833333015441895, Loss 0.45806662827730177\n",
      "RNNA, rep: 1, epoch: 1620, acc: 0.779999852180481, Loss 0.4642769107222557\n",
      "RNNA, rep: 1, epoch: 1621, acc: 0.7966667413711548, Loss 0.4342499503493309\n",
      "RNNA, rep: 1, epoch: 1622, acc: 0.8333333730697632, Loss 0.4279521554708481\n",
      "RNNA, rep: 1, epoch: 1623, acc: 0.8199999928474426, Loss 0.402553882598877\n",
      "RNNA, rep: 1, epoch: 1624, acc: 0.7966665625572205, Loss 0.39747511491179466\n",
      "RNNA, rep: 1, epoch: 1625, acc: 0.8099998235702515, Loss 0.45398628786206247\n",
      "RNNA, rep: 1, epoch: 1626, acc: 0.7999999523162842, Loss 0.4222267313301563\n",
      "RNNA, rep: 1, epoch: 1627, acc: 0.7966665625572205, Loss 0.4488683405518532\n",
      "RNNA, rep: 1, epoch: 1628, acc: 0.8066667914390564, Loss 0.4380472004413605\n",
      "RNNA, rep: 1, epoch: 1629, acc: 0.8099997639656067, Loss 0.43194236516952517\n",
      "RNNA, rep: 1, epoch: 1630, acc: 0.7900000810623169, Loss 0.44545824110507964\n",
      "RNNA, rep: 1, epoch: 1631, acc: 0.7966666221618652, Loss 0.4250790689885616\n",
      "RNNA, rep: 1, epoch: 1632, acc: 0.8100000023841858, Loss 0.44080827027559283\n",
      "RNNA, rep: 1, epoch: 1633, acc: 0.8066664934158325, Loss 0.3796044304966927\n",
      "RNNA, rep: 1, epoch: 1634, acc: 0.8299999833106995, Loss 0.3766964477300644\n",
      "RNNA, rep: 1, epoch: 1635, acc: 0.8066665530204773, Loss 0.3955134233832359\n",
      "RNNA, rep: 1, epoch: 1636, acc: 0.8166665434837341, Loss 0.4367403528094292\n",
      "RNNA, rep: 1, epoch: 1637, acc: 0.7766666412353516, Loss 0.44936042457818987\n",
      "RNNA, rep: 1, epoch: 1638, acc: 0.7933332324028015, Loss 0.41708371609449385\n",
      "RNNA, rep: 1, epoch: 1639, acc: 0.793333113193512, Loss 0.46437136709690097\n",
      "RNNA, rep: 1, epoch: 1640, acc: 0.7933332324028015, Loss 0.4453646695613861\n",
      "RNNA, rep: 1, epoch: 1641, acc: 0.8199999928474426, Loss 0.4050151163339615\n",
      "RNNA, rep: 1, epoch: 1642, acc: 0.8299999833106995, Loss 0.36854751825332643\n",
      "RNNA, rep: 1, epoch: 1643, acc: 0.8166665434837341, Loss 0.4250012083351612\n",
      "RNNA, rep: 1, epoch: 1644, acc: 0.7400000691413879, Loss 0.45752230793237686\n",
      "RNNA, rep: 1, epoch: 1645, acc: 0.7766666412353516, Loss 0.44063389986753465\n",
      "RNNA, rep: 1, epoch: 1646, acc: 0.7766665816307068, Loss 0.43750998824834825\n",
      "RNNA, rep: 1, epoch: 1647, acc: 0.8266665935516357, Loss 0.38156042218208314\n",
      "RNNA, rep: 1, epoch: 1648, acc: 0.7900000214576721, Loss 0.45119955569505693\n",
      "RNNA, rep: 1, epoch: 1649, acc: 0.8133332133293152, Loss 0.4240181738138199\n",
      "RNNA, rep: 1, epoch: 1650, acc: 0.7900000214576721, Loss 0.43490165650844576\n",
      "RNNA, rep: 1, epoch: 1651, acc: 0.7933332920074463, Loss 0.4325749143958092\n",
      "RNNA, rep: 1, epoch: 1652, acc: 0.7766667008399963, Loss 0.4725125175714493\n",
      "RNNA, rep: 1, epoch: 1653, acc: 0.809999942779541, Loss 0.43097503870725634\n",
      "RNNA, rep: 1, epoch: 1654, acc: 0.7966666221618652, Loss 0.4898023760318756\n",
      "RNNA, rep: 1, epoch: 1655, acc: 0.7733331322669983, Loss 0.45036866575479506\n",
      "RNNA, rep: 1, epoch: 1656, acc: 0.8066666126251221, Loss 0.4109244212508202\n",
      "RNNA, rep: 1, epoch: 1657, acc: 0.7733333110809326, Loss 0.45951241463422776\n",
      "RNNA, rep: 1, epoch: 1658, acc: 0.8133332133293152, Loss 0.42717361405491827\n",
      "RNNA, rep: 1, epoch: 1659, acc: 0.7799999117851257, Loss 0.4288807284832001\n",
      "RNNA, rep: 1, epoch: 1660, acc: 0.7933333516120911, Loss 0.4421046221256256\n",
      "RNNA, rep: 1, epoch: 1661, acc: 0.8033332824707031, Loss 0.41670298963785174\n",
      "RNNA, rep: 1, epoch: 1662, acc: 0.7966666221618652, Loss 0.4314396533370018\n",
      "RNNA, rep: 1, epoch: 1663, acc: 0.779999852180481, Loss 0.4156401652097702\n",
      "RNNA, rep: 1, epoch: 1664, acc: 0.7766666412353516, Loss 0.4566418120265007\n",
      "RNNA, rep: 1, epoch: 1665, acc: 0.809999942779541, Loss 0.4118223586678505\n",
      "RNNA, rep: 1, epoch: 1666, acc: 0.7766666412353516, Loss 0.40580121368169786\n",
      "RNNA, rep: 1, epoch: 1667, acc: 0.8100000023841858, Loss 0.4550080120563507\n",
      "RNNA, rep: 1, epoch: 1668, acc: 0.8100000023841858, Loss 0.4167028301954269\n",
      "RNNA, rep: 1, epoch: 1669, acc: 0.8166666626930237, Loss 0.4639654353260994\n",
      "RNNA, rep: 1, epoch: 1670, acc: 0.8199997544288635, Loss 0.372582028657198\n",
      "RNNA, rep: 1, epoch: 1671, acc: 0.7666667699813843, Loss 0.4977305965125561\n",
      "RNNA, rep: 1, epoch: 1672, acc: 0.8033334612846375, Loss 0.42094687312841417\n",
      "RNNA, rep: 1, epoch: 1673, acc: 0.7766666412353516, Loss 0.4480819010734558\n",
      "RNNA, rep: 1, epoch: 1674, acc: 0.800000011920929, Loss 0.38104488521814345\n",
      "RNNA, rep: 1, epoch: 1675, acc: 0.7799999713897705, Loss 0.43994565039873124\n",
      "RNNA, rep: 1, epoch: 1676, acc: 0.8099998235702515, Loss 0.42832846254110335\n",
      "RNNA, rep: 1, epoch: 1677, acc: 0.8033332228660583, Loss 0.4290441830456257\n",
      "RNNA, rep: 1, epoch: 1678, acc: 0.7933332324028015, Loss 0.4311178168654442\n",
      "RNNA, rep: 1, epoch: 1679, acc: 0.8333331346511841, Loss 0.4359467875957489\n",
      "RNNA, rep: 1, epoch: 1680, acc: 0.7766666412353516, Loss 0.47651226550340653\n",
      "RNNA, rep: 1, epoch: 1681, acc: 0.823333203792572, Loss 0.4053275343775749\n",
      "RNNA, rep: 1, epoch: 1682, acc: 0.8033332824707031, Loss 0.4376441591978073\n",
      "RNNA, rep: 1, epoch: 1683, acc: 0.8033333420753479, Loss 0.4373473536968231\n",
      "RNNA, rep: 1, epoch: 1684, acc: 0.7966665625572205, Loss 0.44650517582893373\n",
      "RNNA, rep: 1, epoch: 1685, acc: 0.7900000810623169, Loss 0.4965987166762352\n",
      "RNNA, rep: 1, epoch: 1686, acc: 0.7933332920074463, Loss 0.4706005537509918\n",
      "RNNA, rep: 1, epoch: 1687, acc: 0.7966666221618652, Loss 0.41351120948791503\n",
      "RNNA, rep: 1, epoch: 1688, acc: 0.8266664743423462, Loss 0.4268922133743763\n",
      "RNNA, rep: 1, epoch: 1689, acc: 0.8033333420753479, Loss 0.3974987740814686\n",
      "RNNA, rep: 1, epoch: 1690, acc: 0.7966665625572205, Loss 0.39000239729881286\n",
      "RNNA, rep: 1, epoch: 1691, acc: 0.8100001811981201, Loss 0.4523410075902939\n",
      "RNNA, rep: 1, epoch: 1692, acc: 0.7866665720939636, Loss 0.4196229723095894\n",
      "RNNA, rep: 1, epoch: 1693, acc: 0.7899998426437378, Loss 0.43769980400800707\n",
      "RNNA, rep: 1, epoch: 1694, acc: 0.8299999237060547, Loss 0.37787223011255267\n",
      "RNNA, rep: 1, epoch: 1695, acc: 0.8233334422111511, Loss 0.4330832740664482\n",
      "RNNA, rep: 1, epoch: 1696, acc: 0.8399999737739563, Loss 0.4142743989825249\n",
      "RNNA, rep: 1, epoch: 1697, acc: 0.8033332824707031, Loss 0.4423302885890007\n",
      "RNNA, rep: 1, epoch: 1698, acc: 0.7933333516120911, Loss 0.4366277439892292\n",
      "RNNA, rep: 1, epoch: 1699, acc: 0.8066665530204773, Loss 0.43654769644141195\n",
      "RNNA, rep: 1, epoch: 1700, acc: 0.7866666316986084, Loss 0.4433500999212265\n",
      "RNNA, rep: 1, epoch: 1701, acc: 0.7766666412353516, Loss 0.41763675421476365\n",
      "RNNA, rep: 1, epoch: 1702, acc: 0.8066667318344116, Loss 0.38842388838529585\n",
      "RNNA, rep: 1, epoch: 1703, acc: 0.8033332228660583, Loss 0.4268597635626793\n",
      "RNNA, rep: 1, epoch: 1704, acc: 0.830000102519989, Loss 0.41796876639127734\n",
      "RNNA, rep: 1, epoch: 1705, acc: 0.81333327293396, Loss 0.42004200994968416\n",
      "RNNA, rep: 1, epoch: 1706, acc: 0.8066665530204773, Loss 0.39150689110159875\n",
      "RNNA, rep: 1, epoch: 1707, acc: 0.7866668105125427, Loss 0.4208789297938347\n",
      "RNNA, rep: 1, epoch: 1708, acc: 0.7733331918716431, Loss 0.42293941020965575\n",
      "RNNA, rep: 1, epoch: 1709, acc: 0.7800000905990601, Loss 0.4231908082962036\n",
      "RNNA, rep: 1, epoch: 1710, acc: 0.8033333420753479, Loss 0.4253661260008812\n",
      "RNNA, rep: 1, epoch: 1711, acc: 0.7900000214576721, Loss 0.4393254244327545\n",
      "RNNA, rep: 1, epoch: 1712, acc: 0.7766666412353516, Loss 0.4690807646512985\n",
      "RNNA, rep: 1, epoch: 1713, acc: 0.7766666412353516, Loss 0.49235792696475983\n",
      "RNNA, rep: 1, epoch: 1714, acc: 0.7900000214576721, Loss 0.4290800738334656\n",
      "RNNA, rep: 1, epoch: 1715, acc: 0.7599999904632568, Loss 0.4793632060289383\n",
      "RNNA, rep: 1, epoch: 1716, acc: 0.7699999213218689, Loss 0.45431360602378845\n",
      "RNNA, rep: 1, epoch: 1717, acc: 0.8100000619888306, Loss 0.3873423758149147\n",
      "RNNA, rep: 1, epoch: 1718, acc: 0.7833333015441895, Loss 0.472131749689579\n",
      "RNNA, rep: 1, epoch: 1719, acc: 0.8066664934158325, Loss 0.44237385123968126\n",
      "RNNA, rep: 1, epoch: 1720, acc: 0.7800001502037048, Loss 0.4261617833375931\n",
      "RNNA, rep: 1, epoch: 1721, acc: 0.81333327293396, Loss 0.41649929314851764\n",
      "RNNA, rep: 1, epoch: 1722, acc: 0.7966663837432861, Loss 0.46127959817647934\n",
      "RNNA, rep: 1, epoch: 1723, acc: 0.8066666126251221, Loss 0.4039926406741142\n",
      "RNNA, rep: 1, epoch: 1724, acc: 0.7999999523162842, Loss 0.4469886639714241\n",
      "RNNA, rep: 1, epoch: 1725, acc: 0.8199998736381531, Loss 0.423333951830864\n",
      "RNNA, rep: 1, epoch: 1726, acc: 0.7933332324028015, Loss 0.47031479120254516\n",
      "RNNA, rep: 1, epoch: 1727, acc: 0.779999852180481, Loss 0.46076203674077987\n",
      "RNNA, rep: 1, epoch: 1728, acc: 0.8266666531562805, Loss 0.42521476447582246\n",
      "RNNA, rep: 1, epoch: 1729, acc: 0.8266665935516357, Loss 0.4262716782093048\n",
      "RNNA, rep: 1, epoch: 1730, acc: 0.7900000214576721, Loss 0.4432730320096016\n",
      "RNNA, rep: 1, epoch: 1731, acc: 0.7733333110809326, Loss 0.4432288634777069\n",
      "RNNA, rep: 1, epoch: 1732, acc: 0.8033332228660583, Loss 0.4275305700302124\n",
      "RNNA, rep: 1, epoch: 1733, acc: 0.8299999237060547, Loss 0.41755715042352676\n",
      "RNNA, rep: 1, epoch: 1734, acc: 0.823333203792572, Loss 0.4150182086229324\n",
      "RNNA, rep: 1, epoch: 1735, acc: 0.800000011920929, Loss 0.4246974001824856\n",
      "RNNA, rep: 1, epoch: 1736, acc: 0.7966667413711548, Loss 0.42624653145670893\n",
      "RNNA, rep: 1, epoch: 1737, acc: 0.8099998235702515, Loss 0.45588820084929466\n",
      "RNNA, rep: 1, epoch: 1738, acc: 0.800000011920929, Loss 0.4319053491950035\n",
      "RNNA, rep: 1, epoch: 1739, acc: 0.7999999523162842, Loss 0.43837632596492765\n",
      "RNNA, rep: 1, epoch: 1740, acc: 0.809999942779541, Loss 0.4519004617631435\n",
      "RNNA, rep: 1, epoch: 1741, acc: 0.8233334422111511, Loss 0.4322131706774235\n",
      "RNNA, rep: 1, epoch: 1742, acc: 0.783333420753479, Loss 0.43564490050077437\n",
      "RNNA, rep: 1, epoch: 1743, acc: 0.8033331036567688, Loss 0.43551745772361755\n",
      "RNNA, rep: 1, epoch: 1744, acc: 0.8266665935516357, Loss 0.42992043182253836\n",
      "RNNA, rep: 1, epoch: 1745, acc: 0.7899999022483826, Loss 0.4532324180006981\n",
      "RNNA, rep: 1, epoch: 1746, acc: 0.7900000214576721, Loss 0.43533709555864336\n",
      "RNNA, rep: 1, epoch: 1747, acc: 0.7699999809265137, Loss 0.4598366332054138\n",
      "RNNA, rep: 1, epoch: 1748, acc: 0.7699999213218689, Loss 0.4692342656850815\n",
      "RNNA, rep: 1, epoch: 1749, acc: 0.793333113193512, Loss 0.42188992977142337\n",
      "RNNA, rep: 1, epoch: 1750, acc: 0.7866665720939636, Loss 0.41727607667446137\n",
      "RNNA, rep: 1, epoch: 1751, acc: 0.7966665029525757, Loss 0.4347959473729134\n",
      "RNNA, rep: 1, epoch: 1752, acc: 0.7966666221618652, Loss 0.4426412206888199\n",
      "RNNA, rep: 1, epoch: 1753, acc: 0.7800000905990601, Loss 0.4008892494440079\n",
      "RNNA, rep: 1, epoch: 1754, acc: 0.8333331346511841, Loss 0.4035231927037239\n",
      "RNNA, rep: 1, epoch: 1755, acc: 0.7833333015441895, Loss 0.44962468355894086\n",
      "RNNA, rep: 1, epoch: 1756, acc: 0.8199998736381531, Loss 0.397425644993782\n",
      "RNNA, rep: 1, epoch: 1757, acc: 0.7566665410995483, Loss 0.5037787505984306\n",
      "RNNA, rep: 1, epoch: 1758, acc: 0.8166665434837341, Loss 0.4465399584174156\n",
      "RNNA, rep: 1, epoch: 1759, acc: 0.7866666913032532, Loss 0.42706420838832854\n",
      "RNNA, rep: 1, epoch: 1760, acc: 0.8100001811981201, Loss 0.4200286588072777\n",
      "RNNA, rep: 1, epoch: 1761, acc: 0.7966665625572205, Loss 0.3924287396669388\n",
      "RNNA, rep: 1, epoch: 1762, acc: 0.7866666316986084, Loss 0.43732082307338715\n",
      "RNNA, rep: 1, epoch: 1763, acc: 0.8199999928474426, Loss 0.4070135444402695\n",
      "RNNA, rep: 1, epoch: 1764, acc: 0.779999852180481, Loss 0.43193079203367235\n",
      "RNNA, rep: 1, epoch: 1765, acc: 0.8066665530204773, Loss 0.4266577059030533\n",
      "RNNA, rep: 1, epoch: 1766, acc: 0.8433331847190857, Loss 0.3858649943768978\n",
      "RNNA, rep: 1, epoch: 1767, acc: 0.7933332324028015, Loss 0.42435394108295443\n",
      "RNNA, rep: 1, epoch: 1768, acc: 0.7933332324028015, Loss 0.44759943172335626\n",
      "RNNA, rep: 1, epoch: 1769, acc: 0.7999998331069946, Loss 0.44421930849552155\n",
      "RNNA, rep: 1, epoch: 1770, acc: 0.8133333325386047, Loss 0.4017468623816967\n",
      "RNNA, rep: 1, epoch: 1771, acc: 0.823333203792572, Loss 0.44904409572482107\n",
      "RNNA, rep: 1, epoch: 1772, acc: 0.81333327293396, Loss 0.4310305991768837\n",
      "RNNA, rep: 1, epoch: 1773, acc: 0.7866666316986084, Loss 0.4736525236070156\n",
      "RNNA, rep: 1, epoch: 1774, acc: 0.7633333802223206, Loss 0.4704193836450577\n",
      "RNNA, rep: 1, epoch: 1775, acc: 0.8099998235702515, Loss 0.4600470247864723\n",
      "RNNA, rep: 1, epoch: 1776, acc: 0.8033332824707031, Loss 0.4632801669836044\n",
      "RNNA, rep: 1, epoch: 1777, acc: 0.7699999213218689, Loss 0.4399888226389885\n",
      "RNNA, rep: 1, epoch: 1778, acc: 0.8099998235702515, Loss 0.415489453971386\n",
      "RNNA, rep: 1, epoch: 1779, acc: 0.8399999141693115, Loss 0.39458533108234406\n",
      "RNNA, rep: 1, epoch: 1780, acc: 0.7966667413711548, Loss 0.4120209467411041\n",
      "RNNA, rep: 1, epoch: 1781, acc: 0.8266665935516357, Loss 0.4107193173468113\n",
      "RNNA, rep: 1, epoch: 1782, acc: 0.8033332228660583, Loss 0.45345491901040075\n",
      "RNNA, rep: 1, epoch: 1783, acc: 0.7833331823348999, Loss 0.483599970638752\n",
      "RNNA, rep: 1, epoch: 1784, acc: 0.8299999833106995, Loss 0.4045914103090763\n",
      "RNNA, rep: 1, epoch: 1785, acc: 0.8333331346511841, Loss 0.4060865223407745\n",
      "RNNA, rep: 1, epoch: 1786, acc: 0.800000011920929, Loss 0.4465952529013157\n",
      "RNNA, rep: 1, epoch: 1787, acc: 0.7833333611488342, Loss 0.45056479424238205\n",
      "RNNA, rep: 1, epoch: 1788, acc: 0.836666464805603, Loss 0.3921560630202293\n",
      "RNNA, rep: 1, epoch: 1789, acc: 0.8299996852874756, Loss 0.3964019827544689\n",
      "RNNA, rep: 1, epoch: 1790, acc: 0.8099997043609619, Loss 0.39380758672952654\n",
      "RNNA, rep: 1, epoch: 1791, acc: 0.7966666221618652, Loss 0.4471674245595932\n",
      "RNNA, rep: 1, epoch: 1792, acc: 0.7966666221618652, Loss 0.40925021320581434\n",
      "RNNA, rep: 1, epoch: 1793, acc: 0.7733333110809326, Loss 0.4298013059794903\n",
      "RNNA, rep: 1, epoch: 1794, acc: 0.8533332943916321, Loss 0.40251206457614896\n",
      "RNNA, rep: 1, epoch: 1795, acc: 0.8133331537246704, Loss 0.38679778054356573\n",
      "RNNA, rep: 1, epoch: 1796, acc: 0.763333261013031, Loss 0.4382476308941841\n",
      "RNNA, rep: 1, epoch: 1797, acc: 0.7500002384185791, Loss 0.43868624478578566\n",
      "RNNA, rep: 1, epoch: 1798, acc: 0.7933332324028015, Loss 0.4650389924645424\n",
      "RNNA, rep: 1, epoch: 1799, acc: 0.7966667413711548, Loss 0.4434232312440872\n",
      "RNNA, rep: 1, epoch: 1800, acc: 0.809999942779541, Loss 0.41792851746082305\n",
      "RNNA, rep: 1, epoch: 1801, acc: 0.8033332824707031, Loss 0.4077978095412254\n",
      "RNNA, rep: 1, epoch: 1802, acc: 0.7666667103767395, Loss 0.49152190282940866\n",
      "RNNA, rep: 1, epoch: 1803, acc: 0.8333329558372498, Loss 0.416407403498888\n",
      "RNNA, rep: 1, epoch: 1804, acc: 0.8233330249786377, Loss 0.3623503676056862\n",
      "RNNA, rep: 1, epoch: 1805, acc: 0.823333203792572, Loss 0.4387195861339569\n",
      "RNNA, rep: 1, epoch: 1806, acc: 0.7899999022483826, Loss 0.44775866508483886\n",
      "RNNA, rep: 1, epoch: 1807, acc: 0.7799999713897705, Loss 0.4784901764988899\n",
      "RNNA, rep: 1, epoch: 1808, acc: 0.75, Loss 0.4773893290758133\n",
      "RNNA, rep: 1, epoch: 1809, acc: 0.8199999332427979, Loss 0.40778394043445587\n",
      "RNNA, rep: 1, epoch: 1810, acc: 0.7900000214576721, Loss 0.4011108610033989\n",
      "RNNA, rep: 1, epoch: 1811, acc: 0.7966666221618652, Loss 0.3995939001441002\n",
      "RNNA, rep: 1, epoch: 1812, acc: 0.7866666913032532, Loss 0.4379243379831314\n",
      "RNNA, rep: 1, epoch: 1813, acc: 0.7999999523162842, Loss 0.4459408751130104\n",
      "RNNA, rep: 1, epoch: 1814, acc: 0.81333327293396, Loss 0.4352475568652153\n",
      "RNNA, rep: 1, epoch: 1815, acc: 0.7866666913032532, Loss 0.4601042205095291\n",
      "RNNA, rep: 1, epoch: 1816, acc: 0.830000102519989, Loss 0.43504695892333983\n",
      "RNNA, rep: 1, epoch: 1817, acc: 0.7899998426437378, Loss 0.41111525624990464\n",
      "RNNA, rep: 1, epoch: 1818, acc: 0.8033332824707031, Loss 0.4385048307478428\n",
      "RNNA, rep: 1, epoch: 1819, acc: 0.8133330345153809, Loss 0.4350302243232727\n",
      "RNNA, rep: 1, epoch: 1820, acc: 0.7799999117851257, Loss 0.4448948608338833\n",
      "RNNA, rep: 1, epoch: 1821, acc: 0.7866666316986084, Loss 0.4742502978444099\n",
      "RNNA, rep: 1, epoch: 1822, acc: 0.8033332228660583, Loss 0.43103150874376295\n",
      "RNNA, rep: 1, epoch: 1823, acc: 0.8033333420753479, Loss 0.4435072922706604\n",
      "RNNA, rep: 1, epoch: 1824, acc: 0.7866666316986084, Loss 0.412929379940033\n",
      "RNNA, rep: 1, epoch: 1825, acc: 0.7766666412353516, Loss 0.462031367123127\n",
      "RNNA, rep: 1, epoch: 1826, acc: 0.7866665720939636, Loss 0.43177911698818205\n",
      "RNNA, rep: 1, epoch: 1827, acc: 0.7599999904632568, Loss 0.46690174371004106\n",
      "RNNA, rep: 1, epoch: 1828, acc: 0.8033332228660583, Loss 0.387211075425148\n",
      "RNNA, rep: 1, epoch: 1829, acc: 0.7966667413711548, Loss 0.4242245441675186\n",
      "RNNA, rep: 1, epoch: 1830, acc: 0.84333336353302, Loss 0.39470116555690765\n",
      "RNNA, rep: 1, epoch: 1831, acc: 0.8133331537246704, Loss 0.4147786146402359\n",
      "RNNA, rep: 1, epoch: 1832, acc: 0.8033332228660583, Loss 0.41435051664710043\n",
      "RNNA, rep: 1, epoch: 1833, acc: 0.8199999928474426, Loss 0.4228598150610924\n",
      "RNNA, rep: 1, epoch: 1834, acc: 0.8066664934158325, Loss 0.4415165624022484\n",
      "RNNA, rep: 1, epoch: 1835, acc: 0.8066666126251221, Loss 0.45504523858428003\n",
      "RNNA, rep: 1, epoch: 1836, acc: 0.75, Loss 0.5150801396369934\n",
      "RNNA, rep: 1, epoch: 1837, acc: 0.770000159740448, Loss 0.4888509456813335\n",
      "RNNA, rep: 1, epoch: 1838, acc: 0.7933332324028015, Loss 0.4408932700753212\n",
      "RNNA, rep: 1, epoch: 1839, acc: 0.8066667318344116, Loss 0.4248200060427189\n",
      "RNNA, rep: 1, epoch: 1840, acc: 0.8066667318344116, Loss 0.4156932757794857\n",
      "RNNA, rep: 1, epoch: 1841, acc: 0.8299998641014099, Loss 0.3754807683825493\n",
      "RNNA, rep: 1, epoch: 1842, acc: 0.8266664743423462, Loss 0.4424664776027203\n",
      "RNNA, rep: 1, epoch: 1843, acc: 0.8066667318344116, Loss 0.42215400487184523\n",
      "RNNA, rep: 1, epoch: 1844, acc: 0.8133333325386047, Loss 0.4227097320556641\n",
      "RNNA, rep: 1, epoch: 1845, acc: 0.8166665434837341, Loss 0.37404773756861687\n",
      "RNNA, rep: 1, epoch: 1846, acc: 0.779999852180481, Loss 0.45198861628770826\n",
      "RNNA, rep: 1, epoch: 1847, acc: 0.8066665530204773, Loss 0.43244123488664626\n",
      "RNNA, rep: 1, epoch: 1848, acc: 0.779999852180481, Loss 0.4556505060195923\n",
      "RNNA, rep: 1, epoch: 1849, acc: 0.7666666507720947, Loss 0.44993070781230926\n",
      "RNNA, rep: 1, epoch: 1850, acc: 0.783333420753479, Loss 0.47384430587291715\n",
      "RNNA, rep: 1, epoch: 1851, acc: 0.8233332633972168, Loss 0.4171824839711189\n",
      "RNNA, rep: 1, epoch: 1852, acc: 0.7700001001358032, Loss 0.4542322221398354\n",
      "RNNA, rep: 1, epoch: 1853, acc: 0.8133333325386047, Loss 0.42214383095502855\n",
      "RNNA, rep: 1, epoch: 1854, acc: 0.7866666316986084, Loss 0.4551078042387962\n",
      "RNNA, rep: 1, epoch: 1855, acc: 0.8500000834465027, Loss 0.3572824765741825\n",
      "RNNA, rep: 1, epoch: 1856, acc: 0.7900001406669617, Loss 0.44848480239510535\n",
      "RNNA, rep: 1, epoch: 1857, acc: 0.8099998235702515, Loss 0.40370153784751894\n",
      "RNNA, rep: 1, epoch: 1858, acc: 0.8166667222976685, Loss 0.3904105551540852\n",
      "RNNA, rep: 1, epoch: 1859, acc: 0.7733334302902222, Loss 0.4401650331914425\n",
      "RNNA, rep: 1, epoch: 1860, acc: 0.8200000524520874, Loss 0.4442434549331665\n",
      "RNNA, rep: 1, epoch: 1861, acc: 0.7899998426437378, Loss 0.4904177489876747\n",
      "RNNA, rep: 1, epoch: 1862, acc: 0.8033332824707031, Loss 0.4472098182141781\n",
      "RNNA, rep: 1, epoch: 1863, acc: 0.7700001001358032, Loss 0.44422115534543993\n",
      "RNNA, rep: 1, epoch: 1864, acc: 0.8000000715255737, Loss 0.43105463951826095\n",
      "RNNA, rep: 1, epoch: 1865, acc: 0.8066666126251221, Loss 0.4612000846862793\n",
      "RNNA, rep: 1, epoch: 1866, acc: 0.8266666531562805, Loss 0.4128517496585846\n",
      "RNNA, rep: 1, epoch: 1867, acc: 0.8333332538604736, Loss 0.4183727225661278\n",
      "RNNA, rep: 1, epoch: 1868, acc: 0.7966667413711548, Loss 0.42959507182240486\n",
      "RNNA, rep: 1, epoch: 1869, acc: 0.7900000214576721, Loss 0.42276555761694906\n",
      "RNNA, rep: 1, epoch: 1870, acc: 0.7866663932800293, Loss 0.44784122735261916\n",
      "RNNA, rep: 1, epoch: 1871, acc: 0.7933332920074463, Loss 0.4213588359951973\n",
      "RNNA, rep: 1, epoch: 1872, acc: 0.8133333325386047, Loss 0.4257092624902725\n",
      "RNNA, rep: 1, epoch: 1873, acc: 0.8133332133293152, Loss 0.39941094249486925\n",
      "RNNA, rep: 1, epoch: 1874, acc: 0.8266665935516357, Loss 0.430973644554615\n",
      "RNNA, rep: 1, epoch: 1875, acc: 0.7866666316986084, Loss 0.42201660737395286\n",
      "RNNA, rep: 1, epoch: 1876, acc: 0.8133332133293152, Loss 0.4164042654633522\n",
      "RNNA, rep: 1, epoch: 1877, acc: 0.7766665816307068, Loss 0.45486400187015535\n",
      "RNNA, rep: 1, epoch: 1878, acc: 0.7999999523162842, Loss 0.40900214642286303\n",
      "RNNA, rep: 1, epoch: 1879, acc: 0.7766667008399963, Loss 0.43223953545093535\n",
      "RNNA, rep: 1, epoch: 1880, acc: 0.8266665935516357, Loss 0.43189853489398955\n",
      "RNNA, rep: 1, epoch: 1881, acc: 0.7933332920074463, Loss 0.45237052857875826\n",
      "RNNA, rep: 1, epoch: 1882, acc: 0.809999942779541, Loss 0.41164586901664735\n",
      "RNNA, rep: 1, epoch: 1883, acc: 0.8066664934158325, Loss 0.4024409325420856\n",
      "RNNA, rep: 1, epoch: 1884, acc: 0.7800000905990601, Loss 0.49988200768828395\n",
      "RNNA, rep: 1, epoch: 1885, acc: 0.7933332324028015, Loss 0.48025313183665275\n",
      "RNNA, rep: 1, epoch: 1886, acc: 0.7799999713897705, Loss 0.46910141333937644\n",
      "RNNA, rep: 1, epoch: 1887, acc: 0.81333327293396, Loss 0.41210868790745736\n",
      "RNNA, rep: 1, epoch: 1888, acc: 0.8133332133293152, Loss 0.41280847117304803\n",
      "RNNA, rep: 1, epoch: 1889, acc: 0.783333420753479, Loss 0.462027208507061\n",
      "RNNA, rep: 1, epoch: 1890, acc: 0.8066665530204773, Loss 0.40880622416734697\n",
      "RNNA, rep: 1, epoch: 1891, acc: 0.779999852180481, Loss 0.44307834923267364\n",
      "RNNA, rep: 1, epoch: 1892, acc: 0.783333420753479, Loss 0.4527346950769424\n",
      "RNNA, rep: 1, epoch: 1893, acc: 0.8299999833106995, Loss 0.4048424178361893\n",
      "RNNA, rep: 1, epoch: 1894, acc: 0.8333332538604736, Loss 0.4363593515753746\n",
      "RNNA, rep: 1, epoch: 1895, acc: 0.75, Loss 0.44559857398271563\n",
      "RNNA, rep: 1, epoch: 1896, acc: 0.81333327293396, Loss 0.43901838064193727\n",
      "RNNA, rep: 1, epoch: 1897, acc: 0.81333327293396, Loss 0.37868434458971023\n",
      "RNNA, rep: 1, epoch: 1898, acc: 0.8066667318344116, Loss 0.416117000579834\n",
      "RNNA, rep: 1, epoch: 1899, acc: 0.8466665148735046, Loss 0.40688121542334554\n",
      "RNNA, rep: 1, epoch: 1900, acc: 0.8299996256828308, Loss 0.37462385565042494\n",
      "RNNA, rep: 1, epoch: 1901, acc: 0.833333432674408, Loss 0.3719442641735077\n",
      "RNNA, rep: 1, epoch: 1902, acc: 0.8299999833106995, Loss 0.4217821781337261\n",
      "RNNA, rep: 1, epoch: 1903, acc: 0.7866666316986084, Loss 0.45591782316565516\n",
      "RNNA, rep: 1, epoch: 1904, acc: 0.7933332920074463, Loss 0.431371733546257\n",
      "RNNA, rep: 1, epoch: 1905, acc: 0.800000011920929, Loss 0.4737261238694191\n",
      "RNNA, rep: 1, epoch: 1906, acc: 0.8066667318344116, Loss 0.4462765339016914\n",
      "RNNA, rep: 1, epoch: 1907, acc: 0.8466666340827942, Loss 0.3781598629057407\n",
      "RNNA, rep: 1, epoch: 1908, acc: 0.76666659116745, Loss 0.48018497437238694\n",
      "RNNA, rep: 1, epoch: 1909, acc: 0.839999794960022, Loss 0.41982693180441855\n",
      "RNNA, rep: 1, epoch: 1910, acc: 0.8133333325386047, Loss 0.4147744606435299\n",
      "RNNA, rep: 1, epoch: 1911, acc: 0.793333113193512, Loss 0.44049777776002885\n",
      "RNNA, rep: 1, epoch: 1912, acc: 0.8066664934158325, Loss 0.40661989882588384\n",
      "RNNA, rep: 1, epoch: 1913, acc: 0.7933334112167358, Loss 0.41053534269332886\n",
      "RNNA, rep: 1, epoch: 1914, acc: 0.7933333516120911, Loss 0.45750347346067427\n",
      "RNNA, rep: 1, epoch: 1915, acc: 0.8133333325386047, Loss 0.40243406385183333\n",
      "RNNA, rep: 1, epoch: 1916, acc: 0.7700001001358032, Loss 0.5053842815756798\n",
      "RNNA, rep: 1, epoch: 1917, acc: 0.8266666531562805, Loss 0.4092377418279648\n",
      "RNNA, rep: 1, epoch: 1918, acc: 0.76666659116745, Loss 0.43646910429000857\n",
      "RNNA, rep: 1, epoch: 1919, acc: 0.8200000524520874, Loss 0.38948107898235323\n",
      "RNNA, rep: 1, epoch: 1920, acc: 0.7733331918716431, Loss 0.46459713220596316\n",
      "RNNA, rep: 1, epoch: 1921, acc: 0.7733333706855774, Loss 0.4793521574139595\n",
      "RNNA, rep: 1, epoch: 1922, acc: 0.7966663837432861, Loss 0.458771299123764\n",
      "RNNA, rep: 1, epoch: 1923, acc: 0.7766664624214172, Loss 0.42990033596754074\n",
      "RNNA, rep: 1, epoch: 1924, acc: 0.7900000214576721, Loss 0.4340967461466789\n",
      "RNNA, rep: 1, epoch: 1925, acc: 0.7899999022483826, Loss 0.44152344912290575\n",
      "RNNA, rep: 1, epoch: 1926, acc: 0.8199999928474426, Loss 0.447766892015934\n",
      "RNNA, rep: 1, epoch: 1927, acc: 0.7866666913032532, Loss 0.46723345905542374\n",
      "RNNA, rep: 1, epoch: 1928, acc: 0.81333327293396, Loss 0.4385435763001442\n",
      "RNNA, rep: 1, epoch: 1929, acc: 0.8399999141693115, Loss 0.37669803231954574\n",
      "RNNA, rep: 1, epoch: 1930, acc: 0.789999783039093, Loss 0.4388955083489418\n",
      "RNNA, rep: 1, epoch: 1931, acc: 0.800000011920929, Loss 0.42926592111587525\n",
      "RNNA, rep: 1, epoch: 1932, acc: 0.800000011920929, Loss 0.47683907389640806\n",
      "RNNA, rep: 1, epoch: 1933, acc: 0.8133333325386047, Loss 0.40095505699515344\n",
      "RNNA, rep: 1, epoch: 1934, acc: 0.7766666412353516, Loss 0.4070105251669884\n",
      "RNNA, rep: 1, epoch: 1935, acc: 0.809999942779541, Loss 0.4684806662797928\n",
      "RNNA, rep: 1, epoch: 1936, acc: 0.7900000214576721, Loss 0.4398304569721222\n",
      "RNNA, rep: 1, epoch: 1937, acc: 0.8166667222976685, Loss 0.4395421779155731\n",
      "RNNA, rep: 1, epoch: 1938, acc: 0.7999998331069946, Loss 0.4367653761804104\n",
      "RNNA, rep: 1, epoch: 1939, acc: 0.7866665124893188, Loss 0.4627295610308647\n",
      "RNNA, rep: 1, epoch: 1940, acc: 0.8133334517478943, Loss 0.41149150133132933\n",
      "RNNA, rep: 1, epoch: 1941, acc: 0.7666666507720947, Loss 0.4429778419435024\n",
      "RNNA, rep: 1, epoch: 1942, acc: 0.7999998331069946, Loss 0.4457262606918812\n",
      "RNNA, rep: 1, epoch: 1943, acc: 0.8133332133293152, Loss 0.3931076934933662\n",
      "RNNA, rep: 1, epoch: 1944, acc: 0.7999998331069946, Loss 0.4298110122978687\n",
      "RNNA, rep: 1, epoch: 1945, acc: 0.8266665935516357, Loss 0.44429987490177153\n",
      "RNNA, rep: 1, epoch: 1946, acc: 0.8066664934158325, Loss 0.3933437266945839\n",
      "RNNA, rep: 1, epoch: 1947, acc: 0.8166666626930237, Loss 0.4108515892922878\n",
      "RNNA, rep: 1, epoch: 1948, acc: 0.846666693687439, Loss 0.3704932495951653\n",
      "RNNA, rep: 1, epoch: 1949, acc: 0.7666667103767395, Loss 0.4821752151846886\n",
      "RNNA, rep: 1, epoch: 1950, acc: 0.7900000214576721, Loss 0.4637445226311684\n",
      "RNNA, rep: 1, epoch: 1951, acc: 0.8266666531562805, Loss 0.42803286731243134\n",
      "RNNA, rep: 1, epoch: 1952, acc: 0.809999942779541, Loss 0.4685143022239208\n",
      "RNNA, rep: 1, epoch: 1953, acc: 0.8033333420753479, Loss 0.4001201492547989\n",
      "RNNA, rep: 1, epoch: 1954, acc: 0.7666666507720947, Loss 0.4575338762998581\n",
      "RNNA, rep: 1, epoch: 1955, acc: 0.783333420753479, Loss 0.4343629267811775\n",
      "RNNA, rep: 1, epoch: 1956, acc: 0.8266665935516357, Loss 0.40149018868803976\n",
      "RNNA, rep: 1, epoch: 1957, acc: 0.8199999928474426, Loss 0.36828291311860084\n",
      "RNNA, rep: 1, epoch: 1958, acc: 0.7966666221618652, Loss 0.43034988313913347\n",
      "RNNA, rep: 1, epoch: 1959, acc: 0.8166667222976685, Loss 0.4080605460703373\n",
      "RNNA, rep: 1, epoch: 1960, acc: 0.800000011920929, Loss 0.41370267361402513\n",
      "RNNA, rep: 1, epoch: 1961, acc: 0.7833331227302551, Loss 0.41403883188962937\n",
      "RNNA, rep: 1, epoch: 1962, acc: 0.7599999308586121, Loss 0.4358984613418579\n",
      "RNNA, rep: 1, epoch: 1963, acc: 0.8099998235702515, Loss 0.43229214996099474\n",
      "RNNA, rep: 1, epoch: 1964, acc: 0.7833333611488342, Loss 0.4598200878500938\n",
      "RNNA, rep: 1, epoch: 1965, acc: 0.8133332133293152, Loss 0.3811810687184334\n",
      "RNNA, rep: 1, epoch: 1966, acc: 0.7999997735023499, Loss 0.4022500240802765\n",
      "RNNA, rep: 1, epoch: 1967, acc: 0.8399998545646667, Loss 0.39347317308187485\n",
      "RNNA, rep: 1, epoch: 1968, acc: 0.7666664719581604, Loss 0.4632770937681198\n",
      "RNNA, rep: 1, epoch: 1969, acc: 0.7700001001358032, Loss 0.44454134196043016\n",
      "RNNA, rep: 1, epoch: 1970, acc: 0.8133332133293152, Loss 0.43664138436317446\n",
      "RNNA, rep: 1, epoch: 1971, acc: 0.8100000023841858, Loss 0.4356433752179146\n",
      "RNNA, rep: 1, epoch: 1972, acc: 0.7666666507720947, Loss 0.5021429514884949\n",
      "RNNA, rep: 1, epoch: 1973, acc: 0.7899998426437378, Loss 0.409187054336071\n",
      "RNNA, rep: 1, epoch: 1974, acc: 0.8066665530204773, Loss 0.43519583135843276\n",
      "RNNA, rep: 1, epoch: 1975, acc: 0.8099998235702515, Loss 0.4022739166021347\n",
      "RNNA, rep: 1, epoch: 1976, acc: 0.8066667318344116, Loss 0.40255706071853636\n",
      "RNNA, rep: 1, epoch: 1977, acc: 0.8166664838790894, Loss 0.36472699493169786\n",
      "RNNA, rep: 1, epoch: 1978, acc: 0.8299999237060547, Loss 0.39859255492687223\n",
      "RNNA, rep: 1, epoch: 1979, acc: 0.789999783039093, Loss 0.4028092619776726\n",
      "RNNA, rep: 1, epoch: 1980, acc: 0.8166666626930237, Loss 0.37699801683425904\n",
      "RNNA, rep: 1, epoch: 1981, acc: 0.8033333420753479, Loss 0.39287682473659513\n",
      "RNNA, rep: 1, epoch: 1982, acc: 0.7833331823348999, Loss 0.417392615377903\n",
      "RNNA, rep: 1, epoch: 1983, acc: 0.8033333420753479, Loss 0.4280969575047493\n",
      "RNNA, rep: 1, epoch: 1984, acc: 0.8033331036567688, Loss 0.3876474177837372\n",
      "RNNA, rep: 1, epoch: 1985, acc: 0.8266665935516357, Loss 0.40748835444450376\n",
      "RNNA, rep: 1, epoch: 1986, acc: 0.8266664147377014, Loss 0.3745137567818165\n",
      "RNNA, rep: 1, epoch: 1987, acc: 0.8100000023841858, Loss 0.37860529392957687\n",
      "RNNA, rep: 1, epoch: 1988, acc: 0.8299996852874756, Loss 0.39296852141618727\n",
      "RNNA, rep: 1, epoch: 1989, acc: 0.7999998331069946, Loss 0.4221713595092297\n",
      "RNNA, rep: 1, epoch: 1990, acc: 0.8366664052009583, Loss 0.38078135922551154\n",
      "RNNA, rep: 1, epoch: 1991, acc: 0.7966665625572205, Loss 0.47274232655763626\n",
      "RNNA, rep: 1, epoch: 1992, acc: 0.7866666316986084, Loss 0.41773476630449297\n",
      "RNNA, rep: 1, epoch: 1993, acc: 0.8033332228660583, Loss 0.4552599334716797\n",
      "RNNA, rep: 1, epoch: 1994, acc: 0.7933333516120911, Loss 0.4939169746637344\n",
      "RNNA, rep: 1, epoch: 1995, acc: 0.7799999713897705, Loss 0.44436328947544096\n",
      "RNNA, rep: 1, epoch: 1996, acc: 0.8033332824707031, Loss 0.46503739774227143\n",
      "RNNA, rep: 1, epoch: 1997, acc: 0.7933332920074463, Loss 0.48292633444070815\n",
      "RNNA, rep: 1, epoch: 1998, acc: 0.7866666316986084, Loss 0.4598790271580219\n",
      "RNNA, rep: 1, epoch: 1999, acc: 0.8199999332427979, Loss 0.3930262833833694\n",
      "RNNA, rep: 1, epoch: 2000, acc: 0.7800000905990601, Loss 0.4523413933813572\n",
      "RNNA, rep: 1, epoch: 2001, acc: 0.8200000524520874, Loss 0.4533229918777943\n",
      "RNNA                 Rep: 1   Epoch: 1     Acc: 0.8200 _min_10_max_10 Time: 159.60 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 3])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNA, rep: 2, epoch: 1, acc: 0.473333477973938, Loss 1.0564478009939193\n",
      "RNNA, rep: 2, epoch: 2, acc: 0.5033332705497742, Loss 0.9906326335668564\n",
      "RNNA, rep: 2, epoch: 3, acc: 0.5766668915748596, Loss 0.9559332102537155\n",
      "RNNA, rep: 2, epoch: 4, acc: 0.6033334136009216, Loss 0.9372281035780907\n",
      "RNNA, rep: 2, epoch: 5, acc: 0.6133334636688232, Loss 0.9038552930951118\n",
      "RNNA, rep: 2, epoch: 6, acc: 0.6100002527236938, Loss 0.8945771354436874\n",
      "RNNA, rep: 2, epoch: 7, acc: 0.580000102519989, Loss 0.9200924900174141\n",
      "RNNA, rep: 2, epoch: 8, acc: 0.6166667342185974, Loss 0.8830657458305359\n",
      "RNNA, rep: 2, epoch: 9, acc: 0.6433334946632385, Loss 0.8122418195009231\n",
      "RNNA, rep: 2, epoch: 10, acc: 0.6200000643730164, Loss 0.8352696451544762\n",
      "RNNA, rep: 2, epoch: 11, acc: 0.5966667532920837, Loss 0.8995660918951035\n",
      "RNNA, rep: 2, epoch: 12, acc: 0.6200000643730164, Loss 0.8576941838860512\n",
      "RNNA, rep: 2, epoch: 13, acc: 0.6000000238418579, Loss 0.9081146442890167\n",
      "RNNA, rep: 2, epoch: 14, acc: 0.6299999952316284, Loss 0.8524239128828048\n",
      "RNNA, rep: 2, epoch: 15, acc: 0.6166667342185974, Loss 0.8781025877594948\n",
      "RNNA, rep: 2, epoch: 16, acc: 0.5766668915748596, Loss 0.8578403055667877\n",
      "RNNA, rep: 2, epoch: 17, acc: 0.6033334732055664, Loss 0.8838800513744354\n",
      "RNNA, rep: 2, epoch: 18, acc: 0.6400002241134644, Loss 0.8609715640544892\n",
      "RNNA, rep: 2, epoch: 19, acc: 0.6733335256576538, Loss 0.868474437892437\n",
      "RNNA, rep: 2, epoch: 20, acc: 0.6533333659172058, Loss 0.8558500510454178\n",
      "RNNA, rep: 2, epoch: 21, acc: 0.6800000667572021, Loss 0.8160994693636894\n",
      "RNNA, rep: 2, epoch: 22, acc: 0.6366668939590454, Loss 0.865381212234497\n",
      "RNNA, rep: 2, epoch: 23, acc: 0.6333335638046265, Loss 0.8207545113563538\n",
      "RNNA, rep: 2, epoch: 24, acc: 0.6500002145767212, Loss 0.7936182940006256\n",
      "RNNA, rep: 2, epoch: 25, acc: 0.6700000762939453, Loss 0.8208139923214912\n",
      "RNNA, rep: 2, epoch: 26, acc: 0.7166666984558105, Loss 0.7696180510520935\n",
      "RNNA, rep: 2, epoch: 27, acc: 0.7266665697097778, Loss 0.7522118192911148\n",
      "RNNA, rep: 2, epoch: 28, acc: 0.6933333873748779, Loss 0.7659418389201165\n",
      "RNNA, rep: 2, epoch: 29, acc: 0.676666796207428, Loss 0.7893400612473488\n",
      "RNNA, rep: 2, epoch: 30, acc: 0.7066667675971985, Loss 0.7519042837619782\n",
      "RNNA, rep: 2, epoch: 31, acc: 0.6900001764297485, Loss 0.7404105487465859\n",
      "RNNA, rep: 2, epoch: 32, acc: 0.6766668558120728, Loss 0.7875719967484475\n",
      "RNNA, rep: 2, epoch: 33, acc: 0.7233332991600037, Loss 0.7108581563830376\n",
      "RNNA, rep: 2, epoch: 34, acc: 0.6933332681655884, Loss 0.7648144194483757\n",
      "RNNA, rep: 2, epoch: 35, acc: 0.7033334970474243, Loss 0.7137763005495071\n",
      "RNNA, rep: 2, epoch: 36, acc: 0.6866667866706848, Loss 0.7194613169133663\n",
      "RNNA, rep: 2, epoch: 37, acc: 0.7400002479553223, Loss 0.6693758913874626\n",
      "RNNA, rep: 2, epoch: 38, acc: 0.7366664409637451, Loss 0.665912683904171\n",
      "RNNA, rep: 2, epoch: 39, acc: 0.6933335065841675, Loss 0.7572784578800201\n",
      "RNNA, rep: 2, epoch: 40, acc: 0.6733333468437195, Loss 0.7754480925202369\n",
      "RNNA, rep: 2, epoch: 41, acc: 0.7200000286102295, Loss 0.73415678024292\n",
      "RNNA, rep: 2, epoch: 42, acc: 0.7300001382827759, Loss 0.6927535480260849\n",
      "RNNA, rep: 2, epoch: 43, acc: 0.6933335065841675, Loss 0.774544352889061\n",
      "RNNA, rep: 2, epoch: 44, acc: 0.7199999094009399, Loss 0.7135308241844177\n",
      "RNNA, rep: 2, epoch: 45, acc: 0.6933332681655884, Loss 0.7244300693273544\n",
      "RNNA, rep: 2, epoch: 46, acc: 0.7233334183692932, Loss 0.7083721753954887\n",
      "RNNA, rep: 2, epoch: 47, acc: 0.6933333873748779, Loss 0.7569533386826515\n",
      "RNNA, rep: 2, epoch: 48, acc: 0.7033332586288452, Loss 0.687294987142086\n",
      "RNNA, rep: 2, epoch: 49, acc: 0.6933334469795227, Loss 0.7693573987483978\n",
      "RNNA, rep: 2, epoch: 50, acc: 0.6533334255218506, Loss 0.7577105221152306\n",
      "RNNA, rep: 2, epoch: 51, acc: 0.7133334279060364, Loss 0.7326165470480919\n",
      "RNNA, rep: 2, epoch: 52, acc: 0.7166666388511658, Loss 0.7022136631608009\n",
      "RNNA, rep: 2, epoch: 53, acc: 0.7099999785423279, Loss 0.692781400680542\n",
      "RNNA, rep: 2, epoch: 54, acc: 0.736666738986969, Loss 0.6736705547571182\n",
      "RNNA, rep: 2, epoch: 55, acc: 0.7300000786781311, Loss 0.6972919052839279\n",
      "RNNA, rep: 2, epoch: 56, acc: 0.68666672706604, Loss 0.7242700812220574\n",
      "RNNA, rep: 2, epoch: 57, acc: 0.6800000667572021, Loss 0.75165966629982\n",
      "RNNA, rep: 2, epoch: 58, acc: 0.7033332586288452, Loss 0.674183429479599\n",
      "RNNA, rep: 2, epoch: 59, acc: 0.7466668486595154, Loss 0.6447361814975738\n",
      "RNNA, rep: 2, epoch: 60, acc: 0.7366666197776794, Loss 0.5901804190874099\n",
      "RNNA, rep: 2, epoch: 61, acc: 0.7400000691413879, Loss 0.6600428903102875\n",
      "RNNA, rep: 2, epoch: 62, acc: 0.6900001764297485, Loss 0.7370780670642852\n",
      "RNNA, rep: 2, epoch: 63, acc: 0.676666796207428, Loss 0.7432482925057411\n",
      "RNNA, rep: 2, epoch: 64, acc: 0.7366666197776794, Loss 0.6703592324256897\n",
      "RNNA, rep: 2, epoch: 65, acc: 0.6966667771339417, Loss 0.7095892336964608\n",
      "RNNA, rep: 2, epoch: 66, acc: 0.7266666293144226, Loss 0.6681969973444939\n",
      "RNNA, rep: 2, epoch: 67, acc: 0.736666738986969, Loss 0.6923228828608989\n",
      "RNNA, rep: 2, epoch: 68, acc: 0.7066667079925537, Loss 0.6952804690599441\n",
      "RNNA, rep: 2, epoch: 69, acc: 0.6733333468437195, Loss 0.7897417554259301\n",
      "RNNA, rep: 2, epoch: 70, acc: 0.7266666889190674, Loss 0.6286113759875298\n",
      "RNNA, rep: 2, epoch: 71, acc: 0.6800000667572021, Loss 0.7561067500710488\n",
      "RNNA, rep: 2, epoch: 72, acc: 0.7000001668930054, Loss 0.6834696406126022\n",
      "RNNA, rep: 2, epoch: 73, acc: 0.7466668486595154, Loss 0.6342617431282997\n",
      "RNNA, rep: 2, epoch: 74, acc: 0.7099999189376831, Loss 0.6816382762789727\n",
      "RNNA, rep: 2, epoch: 75, acc: 0.7499998211860657, Loss 0.6250816756486892\n",
      "RNNA, rep: 2, epoch: 76, acc: 0.6800000667572021, Loss 0.7253656536340714\n",
      "RNNA, rep: 2, epoch: 77, acc: 0.7366666197776794, Loss 0.672697092294693\n",
      "RNNA, rep: 2, epoch: 78, acc: 0.68666672706604, Loss 0.709623540341854\n",
      "RNNA, rep: 2, epoch: 79, acc: 0.7099999785423279, Loss 0.7167727375030517\n",
      "RNNA, rep: 2, epoch: 80, acc: 0.7100000977516174, Loss 0.6848452779650688\n",
      "RNNA, rep: 2, epoch: 81, acc: 0.7600000500679016, Loss 0.6198214961588383\n",
      "RNNA, rep: 2, epoch: 82, acc: 0.7600000500679016, Loss 0.6593691481649876\n",
      "RNNA, rep: 2, epoch: 83, acc: 0.7466667890548706, Loss 0.6250774626433849\n",
      "RNNA, rep: 2, epoch: 84, acc: 0.6966667771339417, Loss 0.656805413365364\n",
      "RNNA, rep: 2, epoch: 85, acc: 0.7499999403953552, Loss 0.6148143514990807\n",
      "RNNA, rep: 2, epoch: 86, acc: 0.7266666293144226, Loss 0.6472855770587921\n",
      "RNNA, rep: 2, epoch: 87, acc: 0.7066667079925537, Loss 0.6530862736701966\n",
      "RNNA, rep: 2, epoch: 88, acc: 0.7099999785423279, Loss 0.6769143149256707\n",
      "RNNA, rep: 2, epoch: 89, acc: 0.7266666889190674, Loss 0.6118527513742447\n",
      "RNNA, rep: 2, epoch: 90, acc: 0.7333335280418396, Loss 0.6410700112581253\n",
      "RNNA, rep: 2, epoch: 91, acc: 0.7200000882148743, Loss 0.6332553976774216\n",
      "RNNA, rep: 2, epoch: 92, acc: 0.7533332109451294, Loss 0.6236676359176636\n",
      "RNNA, rep: 2, epoch: 93, acc: 0.7566666603088379, Loss 0.5745639610290527\n",
      "RNNA, rep: 2, epoch: 94, acc: 0.7300000786781311, Loss 0.6507982468605041\n",
      "RNNA, rep: 2, epoch: 95, acc: 0.7599998712539673, Loss 0.6311662046611309\n",
      "RNNA, rep: 2, epoch: 96, acc: 0.753333330154419, Loss 0.6183795304596424\n",
      "RNNA, rep: 2, epoch: 97, acc: 0.7300001382827759, Loss 0.6080122822523117\n",
      "RNNA, rep: 2, epoch: 98, acc: 0.7499999403953552, Loss 0.5789387933909893\n",
      "RNNA, rep: 2, epoch: 99, acc: 0.7333332896232605, Loss 0.6803754428029061\n",
      "RNNA, rep: 2, epoch: 100, acc: 0.7533332705497742, Loss 0.5968576008081437\n",
      "RNNA, rep: 2, epoch: 101, acc: 0.7966665625572205, Loss 0.5514794379472733\n",
      "RNNA, rep: 2, epoch: 102, acc: 0.7533332705497742, Loss 0.5899042703211308\n",
      "RNNA, rep: 2, epoch: 103, acc: 0.7466664910316467, Loss 0.5899105294048786\n",
      "RNNA, rep: 2, epoch: 104, acc: 0.7866666913032532, Loss 0.5701758000254631\n",
      "RNNA, rep: 2, epoch: 105, acc: 0.763333261013031, Loss 0.5850470983982086\n",
      "RNNA, rep: 2, epoch: 106, acc: 0.7933333516120911, Loss 0.5702481788396835\n",
      "RNNA, rep: 2, epoch: 107, acc: 0.7600000500679016, Loss 0.530063405930996\n",
      "RNNA, rep: 2, epoch: 108, acc: 0.7700001001358032, Loss 0.5474287894368172\n",
      "RNNA, rep: 2, epoch: 109, acc: 0.753333330154419, Loss 0.638109145462513\n",
      "RNNA, rep: 2, epoch: 110, acc: 0.8033332824707031, Loss 0.5765071651339531\n",
      "RNNA, rep: 2, epoch: 111, acc: 0.746666669845581, Loss 0.6390260416269302\n",
      "RNNA, rep: 2, epoch: 112, acc: 0.7633333802223206, Loss 0.5664416663348675\n",
      "RNNA, rep: 2, epoch: 113, acc: 0.7366666197776794, Loss 0.6127475747466087\n",
      "RNNA, rep: 2, epoch: 114, acc: 0.729999840259552, Loss 0.6379477480053901\n",
      "RNNA, rep: 2, epoch: 115, acc: 0.7200000286102295, Loss 0.6197989714145661\n",
      "RNNA, rep: 2, epoch: 116, acc: 0.756666898727417, Loss 0.550244913995266\n",
      "RNNA, rep: 2, epoch: 117, acc: 0.7466664910316467, Loss 0.6473379361629487\n",
      "RNNA, rep: 2, epoch: 118, acc: 0.7400001287460327, Loss 0.5841673386096954\n",
      "RNNA, rep: 2, epoch: 119, acc: 0.7433332800865173, Loss 0.5670900925993919\n",
      "RNNA, rep: 2, epoch: 120, acc: 0.7500000596046448, Loss 0.5584097467362881\n",
      "RNNA, rep: 2, epoch: 121, acc: 0.7366666197776794, Loss 0.5984271901845932\n",
      "RNNA, rep: 2, epoch: 122, acc: 0.7599999308586121, Loss 0.5715836137533188\n",
      "RNNA, rep: 2, epoch: 123, acc: 0.7033334374427795, Loss 0.6468378388881684\n",
      "RNNA, rep: 2, epoch: 124, acc: 0.7233335375785828, Loss 0.5755910515785218\n",
      "RNNA, rep: 2, epoch: 125, acc: 0.7466667890548706, Loss 0.574099602997303\n",
      "RNNA, rep: 2, epoch: 126, acc: 0.6833334565162659, Loss 0.6746330228447914\n",
      "RNNA, rep: 2, epoch: 127, acc: 0.7633334398269653, Loss 0.6123673021793365\n",
      "RNNA, rep: 2, epoch: 128, acc: 0.763333261013031, Loss 0.5201834505796432\n",
      "RNNA, rep: 2, epoch: 129, acc: 0.7900000810623169, Loss 0.5254535165429115\n",
      "RNNA, rep: 2, epoch: 130, acc: 0.7766665816307068, Loss 0.5833610692620277\n",
      "RNNA, rep: 2, epoch: 131, acc: 0.7400000691413879, Loss 0.6397215989232063\n",
      "RNNA, rep: 2, epoch: 132, acc: 0.7266666293144226, Loss 0.6519147053360939\n",
      "RNNA, rep: 2, epoch: 133, acc: 0.7266668677330017, Loss 0.5659565636515618\n",
      "RNNA, rep: 2, epoch: 134, acc: 0.7533331513404846, Loss 0.5841388121247292\n",
      "RNNA, rep: 2, epoch: 135, acc: 0.7866666913032532, Loss 0.5817811790108681\n",
      "RNNA, rep: 2, epoch: 136, acc: 0.7266666889190674, Loss 0.6120021232962608\n",
      "RNNA, rep: 2, epoch: 137, acc: 0.7500000596046448, Loss 0.6155879563093185\n",
      "RNNA, rep: 2, epoch: 138, acc: 0.7200000286102295, Loss 0.6294533535838127\n",
      "RNNA, rep: 2, epoch: 139, acc: 0.8033333420753479, Loss 0.5198121075332165\n",
      "RNNA, rep: 2, epoch: 140, acc: 0.7266666293144226, Loss 0.6388611190021038\n",
      "RNNA, rep: 2, epoch: 141, acc: 0.7466667294502258, Loss 0.5819281738996506\n",
      "RNNA, rep: 2, epoch: 142, acc: 0.7466667294502258, Loss 0.6337611828744412\n",
      "RNNA, rep: 2, epoch: 143, acc: 0.7666666507720947, Loss 0.5741996812820435\n",
      "RNNA, rep: 2, epoch: 144, acc: 0.7366667985916138, Loss 0.56147685110569\n",
      "RNNA, rep: 2, epoch: 145, acc: 0.7566666603088379, Loss 0.5681995552778244\n",
      "RNNA, rep: 2, epoch: 146, acc: 0.7933332920074463, Loss 0.5441847191751004\n",
      "RNNA, rep: 2, epoch: 147, acc: 0.7333333492279053, Loss 0.6415149238705635\n",
      "RNNA, rep: 2, epoch: 148, acc: 0.746666669845581, Loss 0.6201563966274262\n",
      "RNNA, rep: 2, epoch: 149, acc: 0.7866666913032532, Loss 0.5965299074351788\n",
      "RNNA, rep: 2, epoch: 150, acc: 0.7300000190734863, Loss 0.6066367048025131\n",
      "RNNA, rep: 2, epoch: 151, acc: 0.7633334398269653, Loss 0.5863814133405686\n",
      "RNNA, rep: 2, epoch: 152, acc: 0.7400002479553223, Loss 0.5806681457161903\n",
      "RNNA, rep: 2, epoch: 153, acc: 0.7433332204818726, Loss 0.5713042530417443\n",
      "RNNA, rep: 2, epoch: 154, acc: 0.7200000286102295, Loss 0.5738143965601921\n",
      "RNNA, rep: 2, epoch: 155, acc: 0.7966663837432861, Loss 0.5318879598379135\n",
      "RNNA, rep: 2, epoch: 156, acc: 0.7300000786781311, Loss 0.5772142596542835\n",
      "RNNA, rep: 2, epoch: 157, acc: 0.8133332133293152, Loss 0.5163043566048146\n",
      "RNNA, rep: 2, epoch: 158, acc: 0.7466667890548706, Loss 0.5472243073582649\n",
      "RNNA, rep: 2, epoch: 159, acc: 0.7200000286102295, Loss 0.6515571300685405\n",
      "RNNA, rep: 2, epoch: 160, acc: 0.7266666889190674, Loss 0.5862976655364036\n",
      "RNNA, rep: 2, epoch: 161, acc: 0.7633334398269653, Loss 0.5505632255971432\n",
      "RNNA, rep: 2, epoch: 162, acc: 0.7933332324028015, Loss 0.5143718403577805\n",
      "RNNA, rep: 2, epoch: 163, acc: 0.7733333110809326, Loss 0.5944054955244065\n",
      "RNNA, rep: 2, epoch: 164, acc: 0.7833333611488342, Loss 0.552476831972599\n",
      "RNNA, rep: 2, epoch: 165, acc: 0.7633333802223206, Loss 0.558755468428135\n",
      "RNNA, rep: 2, epoch: 166, acc: 0.7766666412353516, Loss 0.5716036126017571\n",
      "RNNA, rep: 2, epoch: 167, acc: 0.7666666507720947, Loss 0.5591045662760734\n",
      "RNNA, rep: 2, epoch: 168, acc: 0.763333261013031, Loss 0.5952031859755516\n",
      "RNNA, rep: 2, epoch: 169, acc: 0.7899999022483826, Loss 0.5642975369095802\n",
      "RNNA, rep: 2, epoch: 170, acc: 0.7799999713897705, Loss 0.5186879189312458\n",
      "RNNA, rep: 2, epoch: 171, acc: 0.7433334589004517, Loss 0.5739925555884838\n",
      "RNNA, rep: 2, epoch: 172, acc: 0.8066667318344116, Loss 0.521070330440998\n",
      "RNNA, rep: 2, epoch: 173, acc: 0.7466667890548706, Loss 0.5804851070046425\n",
      "RNNA, rep: 2, epoch: 174, acc: 0.7833333015441895, Loss 0.5648076179623603\n",
      "RNNA, rep: 2, epoch: 175, acc: 0.7599999308586121, Loss 0.5643319863080979\n",
      "RNNA, rep: 2, epoch: 176, acc: 0.7433335185050964, Loss 0.5448781976103783\n",
      "RNNA, rep: 2, epoch: 177, acc: 0.7533334493637085, Loss 0.5213364098966121\n",
      "RNNA, rep: 2, epoch: 178, acc: 0.8133331537246704, Loss 0.5165496377646923\n",
      "RNNA, rep: 2, epoch: 179, acc: 0.7966665029525757, Loss 0.5435798764228821\n",
      "RNNA, rep: 2, epoch: 180, acc: 0.7433332800865173, Loss 0.6417714378237724\n",
      "RNNA, rep: 2, epoch: 181, acc: 0.7699999213218689, Loss 0.554609472900629\n",
      "RNNA, rep: 2, epoch: 182, acc: 0.7433334589004517, Loss 0.5388992559909821\n",
      "RNNA, rep: 2, epoch: 183, acc: 0.746666669845581, Loss 0.6939274105429649\n",
      "RNNA, rep: 2, epoch: 184, acc: 0.7266666293144226, Loss 0.6542472317814827\n",
      "RNNA, rep: 2, epoch: 185, acc: 0.7566667795181274, Loss 0.5847980512678623\n",
      "RNNA, rep: 2, epoch: 186, acc: 0.8133334517478943, Loss 0.5414296619594097\n",
      "RNNA, rep: 2, epoch: 187, acc: 0.7900001406669617, Loss 0.5578655043244362\n",
      "RNNA, rep: 2, epoch: 188, acc: 0.7766665816307068, Loss 0.5555839385092258\n",
      "RNNA, rep: 2, epoch: 189, acc: 0.763333261013031, Loss 0.5797357559204102\n",
      "RNNA, rep: 2, epoch: 190, acc: 0.7666666507720947, Loss 0.5229407419264317\n",
      "RNNA, rep: 2, epoch: 191, acc: 0.7666667103767395, Loss 0.5807820814847946\n",
      "RNNA, rep: 2, epoch: 192, acc: 0.7833335399627686, Loss 0.584775445908308\n",
      "RNNA, rep: 2, epoch: 193, acc: 0.7733333706855774, Loss 0.5701950733363629\n",
      "RNNA, rep: 2, epoch: 194, acc: 0.7666667699813843, Loss 0.559076007604599\n",
      "RNNA, rep: 2, epoch: 195, acc: 0.75, Loss 0.5968989902734756\n",
      "RNNA, rep: 2, epoch: 196, acc: 0.7833333015441895, Loss 0.550173417031765\n",
      "RNNA, rep: 2, epoch: 197, acc: 0.8166667819023132, Loss 0.49968926146626474\n",
      "RNNA, rep: 2, epoch: 198, acc: 0.8066664934158325, Loss 0.4698040147125721\n",
      "RNNA, rep: 2, epoch: 199, acc: 0.8166667222976685, Loss 0.4998506939411163\n",
      "RNNA, rep: 2, epoch: 200, acc: 0.7599999904632568, Loss 0.615537900775671\n",
      "RNNA, rep: 2, epoch: 201, acc: 0.7566666603088379, Loss 0.627511874139309\n",
      "RNNA, rep: 2, epoch: 202, acc: 0.7666668891906738, Loss 0.5903795455396176\n",
      "RNNA, rep: 2, epoch: 203, acc: 0.7566666603088379, Loss 0.5612220832705498\n",
      "RNNA, rep: 2, epoch: 204, acc: 0.830000102519989, Loss 0.48584630221128466\n",
      "RNNA, rep: 2, epoch: 205, acc: 0.7966665029525757, Loss 0.5487925289571285\n",
      "RNNA, rep: 2, epoch: 206, acc: 0.7633334994316101, Loss 0.5887994934618473\n",
      "RNNA, rep: 2, epoch: 207, acc: 0.7799999117851257, Loss 0.5325881525874138\n",
      "RNNA, rep: 2, epoch: 208, acc: 0.7966666221618652, Loss 0.5577457365393639\n",
      "RNNA, rep: 2, epoch: 209, acc: 0.7899999022483826, Loss 0.5581306929886342\n",
      "RNNA, rep: 2, epoch: 210, acc: 0.8333332538604736, Loss 0.4928031848371029\n",
      "RNNA, rep: 2, epoch: 211, acc: 0.7833331823348999, Loss 0.5406873919069767\n",
      "RNNA, rep: 2, epoch: 212, acc: 0.779999852180481, Loss 0.5415428856015205\n",
      "RNNA, rep: 2, epoch: 213, acc: 0.7999999523162842, Loss 0.47714383229613305\n",
      "RNNA, rep: 2, epoch: 214, acc: 0.763333261013031, Loss 0.5730335132777691\n",
      "RNNA, rep: 2, epoch: 215, acc: 0.7766666412353516, Loss 0.5532737758755684\n",
      "RNNA, rep: 2, epoch: 216, acc: 0.7966667413711548, Loss 0.5236960232257843\n",
      "RNNA, rep: 2, epoch: 217, acc: 0.8066666126251221, Loss 0.514261272251606\n",
      "RNNA, rep: 2, epoch: 218, acc: 0.779999852180481, Loss 0.49597382202744483\n",
      "RNNA, rep: 2, epoch: 219, acc: 0.7699998617172241, Loss 0.5867386071383953\n",
      "RNNA, rep: 2, epoch: 220, acc: 0.770000159740448, Loss 0.5614128862321377\n",
      "RNNA, rep: 2, epoch: 221, acc: 0.7666667699813843, Loss 0.5101076835393905\n",
      "RNNA, rep: 2, epoch: 222, acc: 0.793333113193512, Loss 0.5250391939282417\n",
      "RNNA, rep: 2, epoch: 223, acc: 0.8400000929832458, Loss 0.5140081812441349\n",
      "RNNA, rep: 2, epoch: 224, acc: 0.8033332824707031, Loss 0.5292127424478531\n",
      "RNNA, rep: 2, epoch: 225, acc: 0.8233332633972168, Loss 0.48786669090390206\n",
      "RNNA, rep: 2, epoch: 226, acc: 0.8399999141693115, Loss 0.4500997711718082\n",
      "RNNA, rep: 2, epoch: 227, acc: 0.7300001382827759, Loss 0.6145514859259129\n",
      "RNNA, rep: 2, epoch: 228, acc: 0.8100000619888306, Loss 0.4953030325472355\n",
      "RNNA, rep: 2, epoch: 229, acc: 0.8366665840148926, Loss 0.44728977248072627\n",
      "RNNA, rep: 2, epoch: 230, acc: 0.8299999237060547, Loss 0.48337908789515494\n",
      "RNNA, rep: 2, epoch: 231, acc: 0.8066667318344116, Loss 0.48759504064917564\n",
      "RNNA, rep: 2, epoch: 232, acc: 0.8366666436195374, Loss 0.4646747240424156\n",
      "RNNA, rep: 2, epoch: 233, acc: 0.8199999928474426, Loss 0.49834418535232544\n",
      "RNNA, rep: 2, epoch: 234, acc: 0.8166666626930237, Loss 0.5035396575927734\n",
      "RNNA, rep: 2, epoch: 235, acc: 0.8133334517478943, Loss 0.5189430475234985\n",
      "RNNA, rep: 2, epoch: 236, acc: 0.7999998331069946, Loss 0.5414420445263386\n",
      "RNNA, rep: 2, epoch: 237, acc: 0.8633332848548889, Loss 0.4262278624624014\n",
      "RNNA, rep: 2, epoch: 238, acc: 0.7899999022483826, Loss 0.5341488569229841\n",
      "RNNA, rep: 2, epoch: 239, acc: 0.8466665744781494, Loss 0.4481563813239336\n",
      "RNNA, rep: 2, epoch: 240, acc: 0.8366668820381165, Loss 0.5061881207674742\n",
      "RNNA, rep: 2, epoch: 241, acc: 0.809999942779541, Loss 0.49381492093205454\n",
      "RNNA, rep: 2, epoch: 242, acc: 0.7966665625572205, Loss 0.5048365376889705\n",
      "RNNA, rep: 2, epoch: 243, acc: 0.7699999809265137, Loss 0.5514017102122307\n",
      "RNNA, rep: 2, epoch: 244, acc: 0.8033332824707031, Loss 0.5257245013862848\n",
      "RNNA, rep: 2, epoch: 245, acc: 0.8100000023841858, Loss 0.5351290611922741\n",
      "RNNA, rep: 2, epoch: 246, acc: 0.84333336353302, Loss 0.4315173604339361\n",
      "RNNA, rep: 2, epoch: 247, acc: 0.8466665744781494, Loss 0.43522933274507525\n",
      "RNNA, rep: 2, epoch: 248, acc: 0.7899998426437378, Loss 0.4807712874561548\n",
      "RNNA, rep: 2, epoch: 249, acc: 0.8233332633972168, Loss 0.4800464492291212\n",
      "RNNA, rep: 2, epoch: 250, acc: 0.8433331847190857, Loss 0.43758694849908353\n",
      "RNNA, rep: 2, epoch: 251, acc: 0.8733334541320801, Loss 0.395519814491272\n",
      "RNNA, rep: 2, epoch: 252, acc: 0.8433331847190857, Loss 0.4593203680962324\n",
      "RNNA, rep: 2, epoch: 253, acc: 0.8433331251144409, Loss 0.4225838439911604\n",
      "RNNA, rep: 2, epoch: 254, acc: 0.8266667127609253, Loss 0.5184787017852068\n",
      "RNNA, rep: 2, epoch: 255, acc: 0.8433331847190857, Loss 0.44291411988437174\n",
      "RNNA, rep: 2, epoch: 256, acc: 0.8366667032241821, Loss 0.4506120591238141\n",
      "RNNA, rep: 2, epoch: 257, acc: 0.8166666626930237, Loss 0.5221303239092231\n",
      "RNNA, rep: 2, epoch: 258, acc: 0.8100000619888306, Loss 0.512942975834012\n",
      "RNNA, rep: 2, epoch: 259, acc: 0.8166664838790894, Loss 0.508358517549932\n",
      "RNNA, rep: 2, epoch: 260, acc: 0.8333331942558289, Loss 0.4398898780345917\n",
      "RNNA, rep: 2, epoch: 261, acc: 0.8233331441879272, Loss 0.5007189559563995\n",
      "RNNA, rep: 2, epoch: 262, acc: 0.8500001430511475, Loss 0.42863398268818853\n",
      "RNNA, rep: 2, epoch: 263, acc: 0.8599997758865356, Loss 0.4039376197382808\n",
      "RNNA, rep: 2, epoch: 264, acc: 0.8200000524520874, Loss 0.4956323287636042\n",
      "RNNA, rep: 2, epoch: 265, acc: 0.8733331561088562, Loss 0.42375625055283306\n",
      "RNNA, rep: 2, epoch: 266, acc: 0.8366667032241821, Loss 0.4787154518812895\n",
      "RNNA, rep: 2, epoch: 267, acc: 0.856666624546051, Loss 0.44566111635416744\n",
      "RNNA, rep: 2, epoch: 268, acc: 0.8133333325386047, Loss 0.48140996787697077\n",
      "RNNA, rep: 2, epoch: 269, acc: 0.8699999451637268, Loss 0.4251056074351072\n",
      "RNNA, rep: 2, epoch: 270, acc: 0.8433334827423096, Loss 0.4567996207624674\n",
      "RNNA, rep: 2, epoch: 271, acc: 0.8666666150093079, Loss 0.36779382683336737\n",
      "RNNA, rep: 2, epoch: 272, acc: 0.7866666316986084, Loss 0.5491999494656921\n",
      "RNNA, rep: 2, epoch: 273, acc: 0.8499999046325684, Loss 0.4367672897875309\n",
      "RNNA, rep: 2, epoch: 274, acc: 0.8700000047683716, Loss 0.38721648149192334\n",
      "RNNA, rep: 2, epoch: 275, acc: 0.8299997448921204, Loss 0.46518076419830323\n",
      "RNNA, rep: 2, epoch: 276, acc: 0.8266666531562805, Loss 0.45265863798558714\n",
      "RNNA, rep: 2, epoch: 277, acc: 0.8633333444595337, Loss 0.4344410258717835\n",
      "RNNA, rep: 2, epoch: 278, acc: 0.8700000047683716, Loss 0.3898182739689946\n",
      "RNNA, rep: 2, epoch: 279, acc: 0.8599998354911804, Loss 0.4394403898343444\n",
      "RNNA, rep: 2, epoch: 280, acc: 0.8266664743423462, Loss 0.5286221764609218\n",
      "RNNA, rep: 2, epoch: 281, acc: 0.8633333444595337, Loss 0.4240792627260089\n",
      "RNNA, rep: 2, epoch: 282, acc: 0.856666624546051, Loss 0.4127038346976042\n",
      "RNNA, rep: 2, epoch: 283, acc: 0.8366667032241821, Loss 0.4747700067609549\n",
      "RNNA, rep: 2, epoch: 284, acc: 0.8433334231376648, Loss 0.408555179592222\n",
      "RNNA, rep: 2, epoch: 285, acc: 0.8833333849906921, Loss 0.3640096633695066\n",
      "RNNA, rep: 2, epoch: 286, acc: 0.8400000929832458, Loss 0.5112862818688154\n",
      "RNNA, rep: 2, epoch: 287, acc: 0.8533333539962769, Loss 0.4624806658178568\n",
      "RNNA, rep: 2, epoch: 288, acc: 0.8499999046325684, Loss 0.36916755497455594\n",
      "RNNA, rep: 2, epoch: 289, acc: 0.893333375453949, Loss 0.3773431562632322\n",
      "RNNA, rep: 2, epoch: 290, acc: 0.8366667032241821, Loss 0.4941699838452041\n",
      "RNNA, rep: 2, epoch: 291, acc: 0.8799999952316284, Loss 0.3444356736913323\n",
      "RNNA, rep: 2, epoch: 292, acc: 0.893333375453949, Loss 0.3304510367102921\n",
      "RNNA, rep: 2, epoch: 293, acc: 0.893333375453949, Loss 0.34290727879852057\n",
      "RNNA, rep: 2, epoch: 294, acc: 0.8800000548362732, Loss 0.356561759673059\n",
      "RNNA, rep: 2, epoch: 295, acc: 0.8633333444595337, Loss 0.3781578621454537\n",
      "RNNA, rep: 2, epoch: 296, acc: 0.8833333849906921, Loss 0.3900757309049368\n",
      "RNNA, rep: 2, epoch: 297, acc: 0.90666663646698, Loss 0.3351503064855933\n",
      "RNNA, rep: 2, epoch: 298, acc: 0.9000000953674316, Loss 0.3224322724156082\n",
      "RNNA, rep: 2, epoch: 299, acc: 0.8600000739097595, Loss 0.3926155399531126\n",
      "RNNA, rep: 2, epoch: 300, acc: 0.8500001430511475, Loss 0.4012651198543608\n",
      "RNNA, rep: 2, epoch: 301, acc: 0.8366665840148926, Loss 0.46463413247838614\n",
      "RNNA, rep: 2, epoch: 302, acc: 0.8166665434837341, Loss 0.47972545694559815\n",
      "RNNA, rep: 2, epoch: 303, acc: 0.8733333349227905, Loss 0.3488568394817412\n",
      "RNNA, rep: 2, epoch: 304, acc: 0.893333375453949, Loss 0.3628407707810402\n",
      "RNNA, rep: 2, epoch: 305, acc: 0.8500000834465027, Loss 0.49365073051303626\n",
      "RNNA, rep: 2, epoch: 306, acc: 0.8600000739097595, Loss 0.417192736659199\n",
      "RNNA, rep: 2, epoch: 307, acc: 0.9000000953674316, Loss 0.34522153904661534\n",
      "RNNA, rep: 2, epoch: 308, acc: 0.8999999165534973, Loss 0.3475066774338484\n",
      "RNNA, rep: 2, epoch: 309, acc: 0.8399999737739563, Loss 0.4677273466810584\n",
      "RNNA, rep: 2, epoch: 310, acc: 0.8366665840148926, Loss 0.45158528672531245\n",
      "RNNA, rep: 2, epoch: 311, acc: 0.8199999332427979, Loss 0.4792682891245931\n",
      "RNNA, rep: 2, epoch: 312, acc: 0.8799999356269836, Loss 0.35333878416568043\n",
      "RNNA, rep: 2, epoch: 313, acc: 0.8566668033599854, Loss 0.38236046983860433\n",
      "RNNA, rep: 2, epoch: 314, acc: 0.8866667151451111, Loss 0.3483252795878798\n",
      "RNNA, rep: 2, epoch: 315, acc: 0.8966667652130127, Loss 0.294429965512827\n",
      "RNNA, rep: 2, epoch: 316, acc: 0.8699997663497925, Loss 0.34821469236165287\n",
      "RNNA, rep: 2, epoch: 317, acc: 0.9033334255218506, Loss 0.3136086842790246\n",
      "RNNA, rep: 2, epoch: 318, acc: 0.9100000262260437, Loss 0.2682438999041915\n",
      "RNNA, rep: 2, epoch: 319, acc: 0.8266667127609253, Loss 0.4849856141582131\n",
      "RNNA, rep: 2, epoch: 320, acc: 0.8600000739097595, Loss 0.42382711499929426\n",
      "RNNA, rep: 2, epoch: 321, acc: 0.84333336353302, Loss 0.44748290409334\n",
      "RNNA, rep: 2, epoch: 322, acc: 0.8366667032241821, Loss 0.4461299581453204\n",
      "RNNA, rep: 2, epoch: 323, acc: 0.9066666960716248, Loss 0.3059574807761237\n",
      "RNNA, rep: 2, epoch: 324, acc: 0.8066666126251221, Loss 0.5278364476282149\n",
      "RNNA, rep: 2, epoch: 325, acc: 0.9033334255218506, Loss 0.2764222959708422\n",
      "RNNA, rep: 2, epoch: 326, acc: 0.8666667342185974, Loss 0.3926717928284779\n",
      "RNNA, rep: 2, epoch: 327, acc: 0.8400000929832458, Loss 0.4086019594408572\n",
      "RNNA, rep: 2, epoch: 328, acc: 0.8533333539962769, Loss 0.4051507755694911\n",
      "RNNA, rep: 2, epoch: 329, acc: 0.9166666269302368, Loss 0.28224018999841066\n",
      "RNNA, rep: 2, epoch: 330, acc: 0.8666666150093079, Loss 0.3794831735640764\n",
      "RNNA, rep: 2, epoch: 331, acc: 0.9033333659172058, Loss 0.31047032280825076\n",
      "RNNA, rep: 2, epoch: 332, acc: 0.8266667723655701, Loss 0.5103593742847443\n",
      "RNNA, rep: 2, epoch: 333, acc: 0.830000102519989, Loss 0.45265109119005503\n",
      "RNNA, rep: 2, epoch: 334, acc: 0.8900001049041748, Loss 0.3903274124674499\n",
      "RNNA, rep: 2, epoch: 335, acc: 0.8433333039283752, Loss 0.4328834354598075\n",
      "RNNA, rep: 2, epoch: 336, acc: 0.8500000834465027, Loss 0.4226081724697724\n",
      "RNNA, rep: 2, epoch: 337, acc: 0.8366666436195374, Loss 0.44406613987870514\n",
      "RNNA, rep: 2, epoch: 338, acc: 0.8733334541320801, Loss 0.3444709314964712\n",
      "RNNA, rep: 2, epoch: 339, acc: 0.8633332848548889, Loss 0.38273595585953446\n",
      "RNNA, rep: 2, epoch: 340, acc: 0.8800000548362732, Loss 0.3567812664364465\n",
      "RNNA, rep: 2, epoch: 341, acc: 0.8766665458679199, Loss 0.3654119881801307\n",
      "RNNA, rep: 2, epoch: 342, acc: 0.846666693687439, Loss 0.40425068835727873\n",
      "RNNA, rep: 2, epoch: 343, acc: 0.8600000143051147, Loss 0.40813198872376233\n",
      "RNNA, rep: 2, epoch: 344, acc: 0.8700000047683716, Loss 0.3633583212224767\n",
      "RNNA, rep: 2, epoch: 345, acc: 0.9066666960716248, Loss 0.2642374400887638\n",
      "RNNA, rep: 2, epoch: 346, acc: 0.8733332753181458, Loss 0.3806543396413326\n",
      "RNNA, rep: 2, epoch: 347, acc: 0.8999999761581421, Loss 0.2951360119506717\n",
      "RNNA, rep: 2, epoch: 348, acc: 0.8900001049041748, Loss 0.3310130104608834\n",
      "RNNA, rep: 2, epoch: 349, acc: 0.8700000047683716, Loss 0.41037804757710544\n",
      "RNNA, rep: 2, epoch: 350, acc: 0.8599998950958252, Loss 0.3770581135014072\n",
      "RNNA, rep: 2, epoch: 351, acc: 0.8999999761581421, Loss 0.3182651983201504\n",
      "RNNA, rep: 2, epoch: 352, acc: 0.8799999952316284, Loss 0.3621871731057763\n",
      "RNNA, rep: 2, epoch: 353, acc: 0.8633333444595337, Loss 0.3809865138027817\n",
      "RNNA, rep: 2, epoch: 354, acc: 0.7966666221618652, Loss 0.5123018093267455\n",
      "RNNA, rep: 2, epoch: 355, acc: 0.8799999356269836, Loss 0.3311109331808984\n",
      "RNNA, rep: 2, epoch: 356, acc: 0.8766667246818542, Loss 0.33634151819627733\n",
      "RNNA, rep: 2, epoch: 357, acc: 0.8833334445953369, Loss 0.34699321070453154\n",
      "RNNA, rep: 2, epoch: 358, acc: 0.8733332753181458, Loss 0.34538801038172096\n",
      "RNNA, rep: 2, epoch: 359, acc: 0.8733332753181458, Loss 0.3422430839203298\n",
      "RNNA, rep: 2, epoch: 360, acc: 0.8800000548362732, Loss 0.3479479384259321\n",
      "RNNA, rep: 2, epoch: 361, acc: 0.8766667246818542, Loss 0.39950862092897294\n",
      "RNNA, rep: 2, epoch: 362, acc: 0.8533333539962769, Loss 0.4112827892228961\n",
      "RNNA, rep: 2, epoch: 363, acc: 0.8499999046325684, Loss 0.4253518266184255\n",
      "RNNA, rep: 2, epoch: 364, acc: 0.9033333659172058, Loss 0.3105293469270691\n",
      "RNNA, rep: 2, epoch: 365, acc: 0.9033333659172058, Loss 0.27981194254010916\n",
      "RNNA, rep: 2, epoch: 366, acc: 0.8600000739097595, Loss 0.42811774351168425\n",
      "RNNA, rep: 2, epoch: 367, acc: 0.903333306312561, Loss 0.28236668216530236\n",
      "RNNA, rep: 2, epoch: 368, acc: 0.893333375453949, Loss 0.3279158528195694\n",
      "RNNA, rep: 2, epoch: 369, acc: 0.8833333849906921, Loss 0.3446426308574155\n",
      "RNNA, rep: 2, epoch: 370, acc: 0.876666784286499, Loss 0.3644121922878549\n",
      "RNNA, rep: 2, epoch: 371, acc: 0.9000000953674316, Loss 0.29527124541811645\n",
      "RNNA, rep: 2, epoch: 372, acc: 0.8466665148735046, Loss 0.4004425069084391\n",
      "RNNA, rep: 2, epoch: 373, acc: 0.8933334350585938, Loss 0.34082187863532454\n",
      "RNNA, rep: 2, epoch: 374, acc: 0.8800000548362732, Loss 0.3561266528442502\n",
      "RNNA, rep: 2, epoch: 375, acc: 0.8566668629646301, Loss 0.3815037457831204\n",
      "RNNA, rep: 2, epoch: 376, acc: 0.8366668224334717, Loss 0.4507263199053705\n",
      "RNNA, rep: 2, epoch: 377, acc: 0.8633332848548889, Loss 0.3790245815180242\n",
      "RNNA, rep: 2, epoch: 378, acc: 0.8666666150093079, Loss 0.3696249898010865\n",
      "RNNA, rep: 2, epoch: 379, acc: 0.8933332562446594, Loss 0.31979248486924916\n",
      "RNNA, rep: 2, epoch: 380, acc: 0.856666624546051, Loss 0.4312139938841574\n",
      "RNNA, rep: 2, epoch: 381, acc: 0.9233334064483643, Loss 0.2464022816903889\n",
      "RNNA, rep: 2, epoch: 382, acc: 0.8666667938232422, Loss 0.3571729708928615\n",
      "RNNA, rep: 2, epoch: 383, acc: 0.8666666150093079, Loss 0.34982780761551113\n",
      "RNNA, rep: 2, epoch: 384, acc: 0.8799999952316284, Loss 0.3510510582639836\n",
      "RNNA, rep: 2, epoch: 385, acc: 0.8799999356269836, Loss 0.3299075433658436\n",
      "RNNA, rep: 2, epoch: 386, acc: 0.8400002121925354, Loss 0.45136864128522575\n",
      "RNNA, rep: 2, epoch: 387, acc: 0.8866667151451111, Loss 0.3435917725553736\n",
      "RNNA, rep: 2, epoch: 388, acc: 0.8966667056083679, Loss 0.32509548992617054\n",
      "RNNA, rep: 2, epoch: 389, acc: 0.9100000262260437, Loss 0.2991748888324946\n",
      "RNNA, rep: 2, epoch: 390, acc: 0.8600000739097595, Loss 0.39489535651635377\n",
      "RNNA, rep: 2, epoch: 391, acc: 0.8633333444595337, Loss 0.39226714281132447\n",
      "RNNA, rep: 2, epoch: 392, acc: 0.8933331966400146, Loss 0.30424400586867706\n",
      "RNNA, rep: 2, epoch: 393, acc: 0.8633334636688232, Loss 0.4542019684892148\n",
      "RNNA, rep: 2, epoch: 394, acc: 0.8900001645088196, Loss 0.27066809763899075\n",
      "RNNA, rep: 2, epoch: 395, acc: 0.8533332347869873, Loss 0.37569189578061923\n",
      "RNNA, rep: 2, epoch: 396, acc: 0.8366665840148926, Loss 0.4243803756404668\n",
      "RNNA, rep: 2, epoch: 397, acc: 0.8966666460037231, Loss 0.32370980000123384\n",
      "RNNA, rep: 2, epoch: 398, acc: 0.90666663646698, Loss 0.281298573249951\n",
      "RNNA, rep: 2, epoch: 399, acc: 0.8833332061767578, Loss 0.316164894704707\n",
      "RNNA, rep: 2, epoch: 400, acc: 0.8700000047683716, Loss 0.3462039349740371\n",
      "RNNA, rep: 2, epoch: 401, acc: 0.84333336353302, Loss 0.43584947078488767\n",
      "RNNA, rep: 2, epoch: 402, acc: 0.8533333539962769, Loss 0.43043444579001516\n",
      "RNNA, rep: 2, epoch: 403, acc: 0.8533333539962769, Loss 0.4232049760967493\n",
      "RNNA, rep: 2, epoch: 404, acc: 0.8833333849906921, Loss 0.3620781092182733\n",
      "RNNA, rep: 2, epoch: 405, acc: 0.8666665554046631, Loss 0.3668468847428448\n",
      "RNNA, rep: 2, epoch: 406, acc: 0.9033333659172058, Loss 0.29769284346606584\n",
      "RNNA, rep: 2, epoch: 407, acc: 0.8566667437553406, Loss 0.46172369259176776\n",
      "RNNA, rep: 2, epoch: 408, acc: 0.8833332657814026, Loss 0.31107712815050037\n",
      "RNNA, rep: 2, epoch: 409, acc: 0.8833332657814026, Loss 0.3708543527824804\n",
      "RNNA, rep: 2, epoch: 410, acc: 0.8966667056083679, Loss 0.30266745333326983\n",
      "RNNA, rep: 2, epoch: 411, acc: 0.8433333039283752, Loss 0.432385754736606\n",
      "RNNA, rep: 2, epoch: 412, acc: 0.8666667342185974, Loss 0.36944493431132286\n",
      "RNNA, rep: 2, epoch: 413, acc: 0.8866666555404663, Loss 0.3452772687585093\n",
      "RNNA, rep: 2, epoch: 414, acc: 0.8766667246818542, Loss 0.3414548979373649\n",
      "RNNA, rep: 2, epoch: 415, acc: 0.8833333849906921, Loss 0.3669103727815673\n",
      "RNNA, rep: 2, epoch: 416, acc: 0.8966667652130127, Loss 0.3518398516927846\n",
      "RNNA, rep: 2, epoch: 417, acc: 0.8866667747497559, Loss 0.32522658438654617\n",
      "RNNA, rep: 2, epoch: 418, acc: 0.8799999952316284, Loss 0.3531980365165509\n",
      "RNNA, rep: 2, epoch: 419, acc: 0.846666693687439, Loss 0.47611778709106145\n",
      "RNNA, rep: 2, epoch: 420, acc: 0.8266667127609253, Loss 0.44881949596107007\n",
      "RNNA, rep: 2, epoch: 421, acc: 0.8833334445953369, Loss 0.34183479050407184\n",
      "RNNA, rep: 2, epoch: 422, acc: 0.9166666269302368, Loss 0.21125614727847278\n",
      "RNNA, rep: 2, epoch: 423, acc: 0.8966667056083679, Loss 0.29052619930589574\n",
      "RNNA, rep: 2, epoch: 424, acc: 0.8733333349227905, Loss 0.3488725798111409\n",
      "RNNA, rep: 2, epoch: 425, acc: 0.8833333849906921, Loss 0.31577505173161624\n",
      "RNNA, rep: 2, epoch: 426, acc: 0.903333306312561, Loss 0.2958775195782073\n",
      "RNNA, rep: 2, epoch: 427, acc: 0.8766666650772095, Loss 0.35828396561089904\n",
      "RNNA, rep: 2, epoch: 428, acc: 0.893333375453949, Loss 0.31084999348502607\n",
      "RNNA, rep: 2, epoch: 429, acc: 0.8799999952316284, Loss 0.3438626164756715\n",
      "RNNA, rep: 2, epoch: 430, acc: 0.8833332657814026, Loss 0.30032283162232487\n",
      "RNNA, rep: 2, epoch: 431, acc: 0.8866667151451111, Loss 0.30047905365936456\n",
      "RNNA, rep: 2, epoch: 432, acc: 0.8566665053367615, Loss 0.4082672796375118\n",
      "RNNA, rep: 2, epoch: 433, acc: 0.8899999260902405, Loss 0.3320164756150916\n",
      "RNNA, rep: 2, epoch: 434, acc: 0.8600001335144043, Loss 0.41843895584577695\n",
      "RNNA, rep: 2, epoch: 435, acc: 0.8833332061767578, Loss 0.34173273362452167\n",
      "RNNA, rep: 2, epoch: 436, acc: 0.9233333468437195, Loss 0.2653058837470599\n",
      "RNNA, rep: 2, epoch: 437, acc: 0.8966667652130127, Loss 0.3169166904129088\n",
      "RNNA, rep: 2, epoch: 438, acc: 0.8766667246818542, Loss 0.3488251560344361\n",
      "RNNA, rep: 2, epoch: 439, acc: 0.809999942779541, Loss 0.4818658215855248\n",
      "RNNA, rep: 2, epoch: 440, acc: 0.8766667246818542, Loss 0.3475865217461251\n",
      "RNNA, rep: 2, epoch: 441, acc: 0.8900001049041748, Loss 0.31722626227419826\n",
      "RNNA, rep: 2, epoch: 442, acc: 0.9000000953674316, Loss 0.2916632992040832\n",
      "RNNA, rep: 2, epoch: 443, acc: 0.8766668438911438, Loss 0.35477606096072123\n",
      "RNNA, rep: 2, epoch: 444, acc: 0.876666784286499, Loss 0.32876835027942436\n",
      "RNNA, rep: 2, epoch: 445, acc: 0.8700000047683716, Loss 0.3362858334556222\n",
      "RNNA, rep: 2, epoch: 446, acc: 0.9033333659172058, Loss 0.27327242030762133\n",
      "RNNA, rep: 2, epoch: 447, acc: 0.9266666173934937, Loss 0.22613698632922025\n",
      "RNNA, rep: 2, epoch: 448, acc: 0.9066666960716248, Loss 0.2895708192815073\n",
      "RNNA, rep: 2, epoch: 449, acc: 0.8633333444595337, Loss 0.4036452928534709\n",
      "RNNA, rep: 2, epoch: 450, acc: 0.8966668844223022, Loss 0.2876366793597117\n",
      "RNNA, rep: 2, epoch: 451, acc: 0.8733335137367249, Loss 0.34155755264451726\n",
      "RNNA, rep: 2, epoch: 452, acc: 0.8500000238418579, Loss 0.41630273597431366\n",
      "RNNA, rep: 2, epoch: 453, acc: 0.8866667151451111, Loss 0.34028368078870697\n",
      "RNNA, rep: 2, epoch: 454, acc: 0.9000001549720764, Loss 0.31187417975859716\n",
      "RNNA, rep: 2, epoch: 455, acc: 0.8966665863990784, Loss 0.3091912551014684\n",
      "RNNA, rep: 2, epoch: 456, acc: 0.9100001454353333, Loss 0.291369775258936\n",
      "RNNA, rep: 2, epoch: 457, acc: 0.90666663646698, Loss 0.2759043398615904\n",
      "RNNA, rep: 2, epoch: 458, acc: 0.8866666555404663, Loss 0.3322527036466636\n",
      "RNNA, rep: 2, epoch: 459, acc: 0.8699999451637268, Loss 0.3902790042734705\n",
      "RNNA, rep: 2, epoch: 460, acc: 0.9200000166893005, Loss 0.221829091203399\n",
      "RNNA, rep: 2, epoch: 461, acc: 0.9366666674613953, Loss 0.21794297979446128\n",
      "RNNA, rep: 2, epoch: 462, acc: 0.893333375453949, Loss 0.2886619304679334\n",
      "RNNA, rep: 2, epoch: 463, acc: 0.8900001049041748, Loss 0.28756400859565473\n",
      "RNNA, rep: 2, epoch: 464, acc: 0.8533333539962769, Loss 0.38703496680711397\n",
      "RNNA, rep: 2, epoch: 465, acc: 0.8999999761581421, Loss 0.28529393716482443\n",
      "RNNA, rep: 2, epoch: 466, acc: 0.9433332681655884, Loss 0.23259476798935794\n",
      "RNNA, rep: 2, epoch: 467, acc: 0.8866666555404663, Loss 0.3358059420599602\n",
      "RNNA, rep: 2, epoch: 468, acc: 0.8500000834465027, Loss 0.45536291487747804\n",
      "RNNA, rep: 2, epoch: 469, acc: 0.8633332848548889, Loss 0.35335261180996896\n",
      "RNNA, rep: 2, epoch: 470, acc: 0.8733334541320801, Loss 0.3487844137765933\n",
      "RNNA, rep: 2, epoch: 471, acc: 0.9133334159851074, Loss 0.26312362565658987\n",
      "RNNA, rep: 2, epoch: 472, acc: 0.90666663646698, Loss 0.31217502618208526\n",
      "RNNA, rep: 2, epoch: 473, acc: 0.8500002026557922, Loss 0.3982398644159548\n",
      "RNNA, rep: 2, epoch: 474, acc: 0.8899999856948853, Loss 0.3354799344437197\n",
      "RNNA, rep: 2, epoch: 475, acc: 0.8733333349227905, Loss 0.32138333450304346\n",
      "RNNA, rep: 2, epoch: 476, acc: 0.8999999165534973, Loss 0.28767147194710563\n",
      "RNNA, rep: 2, epoch: 477, acc: 0.8800001740455627, Loss 0.33554225030587986\n",
      "RNNA, rep: 2, epoch: 478, acc: 0.8466666340827942, Loss 0.37228309714235364\n",
      "RNNA, rep: 2, epoch: 479, acc: 0.8900001049041748, Loss 0.31505052524618804\n",
      "RNNA, rep: 2, epoch: 480, acc: 0.9333333373069763, Loss 0.23761935388203712\n",
      "RNNA, rep: 2, epoch: 481, acc: 0.8666666150093079, Loss 0.40853911724174397\n",
      "RNNA, rep: 2, epoch: 482, acc: 0.8600002527236938, Loss 0.4039554793201387\n",
      "RNNA, rep: 2, epoch: 483, acc: 0.903333306312561, Loss 0.2603993095480837\n",
      "RNNA, rep: 2, epoch: 484, acc: 0.8666666150093079, Loss 0.3658714028215036\n",
      "RNNA, rep: 2, epoch: 485, acc: 0.830000102519989, Loss 0.4597429031063803\n",
      "RNNA, rep: 2, epoch: 486, acc: 0.8600000143051147, Loss 0.41870604009833184\n",
      "RNNA, rep: 2, epoch: 487, acc: 0.9233333468437195, Loss 0.24092672636033968\n",
      "RNNA, rep: 2, epoch: 488, acc: 0.8666667342185974, Loss 0.3959097441309132\n",
      "RNNA, rep: 2, epoch: 489, acc: 0.8466668725013733, Loss 0.4110731873172335\n",
      "RNNA, rep: 2, epoch: 490, acc: 0.8666666150093079, Loss 0.41164673485560344\n",
      "RNNA, rep: 2, epoch: 491, acc: 0.8833333849906921, Loss 0.35985594932921233\n",
      "RNNA, rep: 2, epoch: 492, acc: 0.9033334255218506, Loss 0.30871744349366054\n",
      "RNNA, rep: 2, epoch: 493, acc: 0.8866667747497559, Loss 0.31359636851004324\n",
      "RNNA, rep: 2, epoch: 494, acc: 0.8800000548362732, Loss 0.36908394102472813\n",
      "RNNA, rep: 2, epoch: 495, acc: 0.8566667437553406, Loss 0.38773110398091376\n",
      "RNNA, rep: 2, epoch: 496, acc: 0.8866667151451111, Loss 0.3910888035886455\n",
      "RNNA, rep: 2, epoch: 497, acc: 0.8633334636688232, Loss 0.3713280677422881\n",
      "RNNA, rep: 2, epoch: 498, acc: 0.8733333349227905, Loss 0.3438612829055637\n",
      "RNNA, rep: 2, epoch: 499, acc: 0.9033334255218506, Loss 0.279124101329362\n",
      "RNNA, rep: 2, epoch: 500, acc: 0.8533332347869873, Loss 0.4499929299368523\n",
      "RNNA, rep: 2, epoch: 501, acc: 0.8833333849906921, Loss 0.26185980191919955\n",
      "RNNA, rep: 2, epoch: 502, acc: 0.8666666150093079, Loss 0.38771285811439155\n",
      "RNNA, rep: 2, epoch: 503, acc: 0.856666624546051, Loss 0.43366739673074334\n",
      "RNNA, rep: 2, epoch: 504, acc: 0.9000000953674316, Loss 0.2862749741459265\n",
      "RNNA, rep: 2, epoch: 505, acc: 0.8833332657814026, Loss 0.32393458087462934\n",
      "RNNA, rep: 2, epoch: 506, acc: 0.9566666483879089, Loss 0.18703907774179243\n",
      "RNNA, rep: 2, epoch: 507, acc: 0.8999999165534973, Loss 0.3383970634045545\n",
      "RNNA, rep: 2, epoch: 508, acc: 0.8866665363311768, Loss 0.318334438779857\n",
      "RNNA, rep: 2, epoch: 509, acc: 0.8533334136009216, Loss 0.3930601011705585\n",
      "RNNA, rep: 2, epoch: 510, acc: 0.9100000858306885, Loss 0.29269651645445266\n",
      "RNNA, rep: 2, epoch: 511, acc: 0.8166666626930237, Loss 0.44277408584486694\n",
      "RNNA, rep: 2, epoch: 512, acc: 0.8633332848548889, Loss 0.39012017113622277\n",
      "RNNA, rep: 2, epoch: 513, acc: 0.830000102519989, Loss 0.4237587610841729\n",
      "RNNA, rep: 2, epoch: 514, acc: 0.9266667366027832, Loss 0.25434987954795363\n",
      "RNNA, rep: 2, epoch: 515, acc: 0.9033334255218506, Loss 0.2949403648870066\n",
      "RNNA, rep: 2, epoch: 516, acc: 0.8733334541320801, Loss 0.3746706058038399\n",
      "RNNA, rep: 2, epoch: 517, acc: 0.893333375453949, Loss 0.33752039812272416\n",
      "RNNA, rep: 2, epoch: 518, acc: 0.8766666650772095, Loss 0.400982574201189\n",
      "RNNA, rep: 2, epoch: 519, acc: 0.8899999856948853, Loss 0.2775264430418611\n",
      "RNNA, rep: 2, epoch: 520, acc: 0.9100000858306885, Loss 0.28931597668211906\n",
      "RNNA, rep: 2, epoch: 521, acc: 0.9000001549720764, Loss 0.2798926664912142\n",
      "RNNA, rep: 2, epoch: 522, acc: 0.8933332562446594, Loss 0.29555671924958005\n",
      "RNNA, rep: 2, epoch: 523, acc: 0.8733333349227905, Loss 0.3726660056831315\n",
      "RNNA, rep: 2, epoch: 524, acc: 0.9200000166893005, Loss 0.2007631118176505\n",
      "RNNA, rep: 2, epoch: 525, acc: 0.8733333349227905, Loss 0.35674662709701804\n",
      "RNNA, rep: 2, epoch: 526, acc: 0.8799999952316284, Loss 0.30940094322664663\n",
      "RNNA, rep: 2, epoch: 527, acc: 0.8700000047683716, Loss 0.3474894285504706\n",
      "RNNA, rep: 2, epoch: 528, acc: 0.8666666150093079, Loss 0.4011680039553903\n",
      "RNNA, rep: 2, epoch: 529, acc: 0.8966667056083679, Loss 0.30493099965620785\n",
      "RNNA, rep: 2, epoch: 530, acc: 0.9133334159851074, Loss 0.2559069375263061\n",
      "RNNA, rep: 2, epoch: 531, acc: 0.8866668939590454, Loss 0.3530501091759652\n",
      "RNNA, rep: 2, epoch: 532, acc: 0.9266666173934937, Loss 0.24742248482536525\n",
      "RNNA, rep: 2, epoch: 533, acc: 0.8866667151451111, Loss 0.32545294298324734\n",
      "RNNA, rep: 2, epoch: 534, acc: 0.863333523273468, Loss 0.38858475905610246\n",
      "RNNA, rep: 2, epoch: 535, acc: 0.9233334064483643, Loss 0.2553616716247052\n",
      "RNNA, rep: 2, epoch: 536, acc: 0.8800000548362732, Loss 0.318020183123881\n",
      "RNNA, rep: 2, epoch: 537, acc: 0.8733334541320801, Loss 0.3531433650641702\n",
      "RNNA, rep: 2, epoch: 538, acc: 0.8833333849906921, Loss 0.35257619511219673\n",
      "RNNA, rep: 2, epoch: 539, acc: 0.9033334851264954, Loss 0.301307447317522\n",
      "RNNA, rep: 2, epoch: 540, acc: 0.8866667151451111, Loss 0.34088742894120516\n",
      "RNNA, rep: 2, epoch: 541, acc: 0.8966667056083679, Loss 0.3314518360979855\n",
      "RNNA, rep: 2, epoch: 542, acc: 0.9000000953674316, Loss 0.264337060865364\n",
      "RNNA, rep: 2, epoch: 543, acc: 0.8933334350585938, Loss 0.32058011600049213\n",
      "RNNA, rep: 2, epoch: 544, acc: 0.9100000262260437, Loss 0.2671192033321131\n",
      "RNNA, rep: 2, epoch: 545, acc: 0.8733334541320801, Loss 0.3567346851795446\n",
      "RNNA, rep: 2, epoch: 546, acc: 0.8799999952316284, Loss 0.35405597773147746\n",
      "RNNA, rep: 2, epoch: 547, acc: 0.8933334350585938, Loss 0.31708309828885833\n",
      "RNNA, rep: 2, epoch: 548, acc: 0.8899999260902405, Loss 0.2959350606508087\n",
      "RNNA, rep: 2, epoch: 549, acc: 0.8666667342185974, Loss 0.3951153077342315\n",
      "RNNA, rep: 2, epoch: 550, acc: 0.8500000238418579, Loss 0.40047817546874287\n",
      "RNNA, rep: 2, epoch: 551, acc: 0.8500002026557922, Loss 0.4378931743046269\n",
      "RNNA, rep: 2, epoch: 552, acc: 0.820000171661377, Loss 0.49628648108802736\n",
      "RNNA, rep: 2, epoch: 553, acc: 0.9133333563804626, Loss 0.27969394644722345\n",
      "RNNA, rep: 2, epoch: 554, acc: 0.8699999451637268, Loss 0.3620625890023075\n",
      "RNNA, rep: 2, epoch: 555, acc: 0.8666667342185974, Loss 0.36406874461798\n",
      "RNNA, rep: 2, epoch: 556, acc: 0.8800001740455627, Loss 0.32675604424090127\n",
      "RNNA, rep: 2, epoch: 557, acc: 0.8500000238418579, Loss 0.4325264780409634\n",
      "RNNA, rep: 2, epoch: 558, acc: 0.8766667246818542, Loss 0.326397912549437\n",
      "RNNA, rep: 2, epoch: 559, acc: 0.9266667366027832, Loss 0.22871385302161798\n",
      "RNNA, rep: 2, epoch: 560, acc: 0.8633332848548889, Loss 0.35123938055941833\n",
      "RNNA, rep: 2, epoch: 561, acc: 0.8966664671897888, Loss 0.33822504182578994\n",
      "RNNA, rep: 2, epoch: 562, acc: 0.8733332753181458, Loss 0.33962937659583986\n",
      "RNNA, rep: 2, epoch: 563, acc: 0.8600000739097595, Loss 0.34394828682299705\n",
      "RNNA, rep: 2, epoch: 564, acc: 0.9066668152809143, Loss 0.2818406789470464\n",
      "RNNA, rep: 2, epoch: 565, acc: 0.9033334255218506, Loss 0.29116979037818963\n",
      "RNNA, rep: 2, epoch: 566, acc: 0.8399999737739563, Loss 0.44313721314887516\n",
      "RNNA, rep: 2, epoch: 567, acc: 0.9133333563804626, Loss 0.28189878243487326\n",
      "RNNA, rep: 2, epoch: 568, acc: 0.8833332061767578, Loss 0.3183200655184919\n",
      "RNNA, rep: 2, epoch: 569, acc: 0.8933334350585938, Loss 0.36927982037188484\n",
      "RNNA, rep: 2, epoch: 570, acc: 0.8266667127609253, Loss 0.44709842642536385\n",
      "RNNA, rep: 2, epoch: 571, acc: 0.9100000262260437, Loss 0.26181335929315536\n",
      "RNNA, rep: 2, epoch: 572, acc: 0.8899999856948853, Loss 0.31010510397842156\n",
      "RNNA, rep: 2, epoch: 573, acc: 0.8600001335144043, Loss 0.35166228071320804\n",
      "RNNA, rep: 2, epoch: 574, acc: 0.9166666269302368, Loss 0.2701291342382319\n",
      "RNNA, rep: 2, epoch: 575, acc: 0.8799999952316284, Loss 0.3168326647381764\n",
      "RNNA, rep: 2, epoch: 576, acc: 0.8566668033599854, Loss 0.38362185259349646\n",
      "RNNA, rep: 2, epoch: 577, acc: 0.9233333468437195, Loss 0.25829652656568214\n",
      "RNNA, rep: 2, epoch: 578, acc: 0.9033333659172058, Loss 0.297583070397377\n",
      "RNNA, rep: 2, epoch: 579, acc: 0.8966666460037231, Loss 0.27749169859685935\n",
      "RNNA, rep: 2, epoch: 580, acc: 0.926666796207428, Loss 0.242705562596675\n",
      "RNNA, rep: 2, epoch: 581, acc: 0.90666663646698, Loss 0.277470215234207\n",
      "RNNA, rep: 2, epoch: 582, acc: 0.903333306312561, Loss 0.29956267749657856\n",
      "RNNA, rep: 2, epoch: 583, acc: 0.8533335328102112, Loss 0.3768887594062835\n",
      "RNNA, rep: 2, epoch: 584, acc: 0.9233332872390747, Loss 0.22110233414685354\n",
      "RNNA, rep: 2, epoch: 585, acc: 0.8966666460037231, Loss 0.31079383867327126\n",
      "RNNA, rep: 2, epoch: 586, acc: 0.8666666150093079, Loss 0.3568545798584819\n",
      "RNNA, rep: 2, epoch: 587, acc: 0.8799999952316284, Loss 0.34125051511800847\n",
      "RNNA, rep: 2, epoch: 588, acc: 0.8800000548362732, Loss 0.30605022130068393\n",
      "RNNA, rep: 2, epoch: 589, acc: 0.9233333468437195, Loss 0.24983476633904503\n",
      "RNNA, rep: 2, epoch: 590, acc: 0.9099999070167542, Loss 0.26971472735051066\n",
      "RNNA, rep: 2, epoch: 591, acc: 0.8633332848548889, Loss 0.33522034344961865\n",
      "RNNA, rep: 2, epoch: 592, acc: 0.8933332562446594, Loss 0.3286536978860386\n",
      "RNNA, rep: 2, epoch: 593, acc: 0.84333336353302, Loss 0.4084057537931949\n",
      "RNNA, rep: 2, epoch: 594, acc: 0.8700000047683716, Loss 0.351585628669709\n",
      "RNNA, rep: 2, epoch: 595, acc: 0.9066666960716248, Loss 0.2957628341810778\n",
      "RNNA, rep: 2, epoch: 596, acc: 0.8433334231376648, Loss 0.3554595323745161\n",
      "RNNA, rep: 2, epoch: 597, acc: 0.8966666460037231, Loss 0.3168647758034058\n",
      "RNNA, rep: 2, epoch: 598, acc: 0.903333306312561, Loss 0.3464802979957312\n",
      "RNNA, rep: 2, epoch: 599, acc: 0.8733334541320801, Loss 0.3296895835199393\n",
      "RNNA, rep: 2, epoch: 600, acc: 0.93666672706604, Loss 0.212371697089402\n",
      "RNNA, rep: 2, epoch: 601, acc: 0.8800000548362732, Loss 0.3486277323460672\n",
      "RNNA, rep: 2, epoch: 602, acc: 0.9233332872390747, Loss 0.21858259232132696\n",
      "RNNA, rep: 2, epoch: 603, acc: 0.9133333563804626, Loss 0.27873229218646883\n",
      "RNNA, rep: 2, epoch: 604, acc: 0.9433333873748779, Loss 0.20376580022508278\n",
      "RNNA, rep: 2, epoch: 605, acc: 0.8899999260902405, Loss 0.3118106907350011\n",
      "RNNA, rep: 2, epoch: 606, acc: 0.8800000548362732, Loss 0.3427904930827208\n",
      "RNNA, rep: 2, epoch: 607, acc: 0.9166666269302368, Loss 0.25737367386813276\n",
      "RNNA, rep: 2, epoch: 608, acc: 0.8900001049041748, Loss 0.3182179075444583\n",
      "RNNA, rep: 2, epoch: 609, acc: 0.8766666650772095, Loss 0.31329984712530856\n",
      "RNNA, rep: 2, epoch: 610, acc: 0.8733333349227905, Loss 0.380599325299263\n",
      "RNNA, rep: 2, epoch: 611, acc: 0.8799999356269836, Loss 0.37158461327315306\n",
      "RNNA, rep: 2, epoch: 612, acc: 0.93666672706604, Loss 0.20616325194831006\n",
      "RNNA, rep: 2, epoch: 613, acc: 0.8600000143051147, Loss 0.40624255850794727\n",
      "RNNA, rep: 2, epoch: 614, acc: 0.8833332061767578, Loss 0.3431166800297797\n",
      "RNNA, rep: 2, epoch: 615, acc: 0.8633334636688232, Loss 0.3889227997767739\n",
      "RNNA, rep: 2, epoch: 616, acc: 0.8766667246818542, Loss 0.3305114764987957\n",
      "RNNA, rep: 2, epoch: 617, acc: 0.919999897480011, Loss 0.2408862812165171\n",
      "RNNA, rep: 2, epoch: 618, acc: 0.8833332657814026, Loss 0.3334253201889805\n",
      "RNNA, rep: 2, epoch: 619, acc: 0.9200000166893005, Loss 0.21830233251210301\n",
      "RNNA, rep: 2, epoch: 620, acc: 0.8866666555404663, Loss 0.33067392236669546\n",
      "RNNA, rep: 2, epoch: 621, acc: 0.9200000166893005, Loss 0.2585428091930225\n",
      "RNNA, rep: 2, epoch: 622, acc: 0.8533334136009216, Loss 0.3830928614875302\n",
      "RNNA, rep: 2, epoch: 623, acc: 0.8999999761581421, Loss 0.3071613421628717\n",
      "RNNA, rep: 2, epoch: 624, acc: 0.8833332657814026, Loss 0.31887909709475937\n",
      "RNNA, rep: 2, epoch: 625, acc: 0.8833333849906921, Loss 0.348616533074528\n",
      "RNNA, rep: 2, epoch: 626, acc: 0.9099999070167542, Loss 0.2539041780261323\n",
      "RNNA, rep: 2, epoch: 627, acc: 0.8600000739097595, Loss 0.3937190678226761\n",
      "RNNA, rep: 2, epoch: 628, acc: 0.8799998760223389, Loss 0.3289305445703212\n",
      "RNNA, rep: 2, epoch: 629, acc: 0.8733333349227905, Loss 0.34848279986006675\n",
      "RNNA, rep: 2, epoch: 630, acc: 0.8766667246818542, Loss 0.3652282801410183\n",
      "RNNA, rep: 2, epoch: 631, acc: 0.8700000047683716, Loss 0.3711074328562245\n",
      "RNNA, rep: 2, epoch: 632, acc: 0.8733335137367249, Loss 0.3490815539914183\n",
      "RNNA, rep: 2, epoch: 633, acc: 0.8766667246818542, Loss 0.3484849693533033\n",
      "RNNA, rep: 2, epoch: 634, acc: 0.9033333659172058, Loss 0.3505239610187709\n",
      "RNNA, rep: 2, epoch: 635, acc: 0.903333306312561, Loss 0.25650436089839784\n",
      "RNNA, rep: 2, epoch: 636, acc: 0.8833334445953369, Loss 0.33864736250252464\n",
      "RNNA, rep: 2, epoch: 637, acc: 0.8500001430511475, Loss 0.4232130888151005\n",
      "RNNA, rep: 2, epoch: 638, acc: 0.8700000643730164, Loss 0.36911354116396977\n",
      "RNNA, rep: 2, epoch: 639, acc: 0.8800000548362732, Loss 0.34079564405605195\n",
      "RNNA, rep: 2, epoch: 640, acc: 0.9033333659172058, Loss 0.30860383128456303\n",
      "RNNA, rep: 2, epoch: 641, acc: 0.8899999260902405, Loss 0.29633910436765293\n",
      "RNNA, rep: 2, epoch: 642, acc: 0.8633333444595337, Loss 0.42339899520156904\n",
      "RNNA, rep: 2, epoch: 643, acc: 0.8733333349227905, Loss 0.34545906890416517\n",
      "RNNA, rep: 2, epoch: 644, acc: 0.8799999356269836, Loss 0.3474155515432358\n",
      "RNNA, rep: 2, epoch: 645, acc: 0.913333535194397, Loss 0.26438376019184945\n",
      "RNNA, rep: 2, epoch: 646, acc: 0.919999897480011, Loss 0.2306375456182286\n",
      "RNNA, rep: 2, epoch: 647, acc: 0.90666663646698, Loss 0.3265575710730627\n",
      "RNNA, rep: 2, epoch: 648, acc: 0.9300000071525574, Loss 0.23148258282919415\n",
      "RNNA, rep: 2, epoch: 649, acc: 0.9100000262260437, Loss 0.29932361901272087\n",
      "RNNA, rep: 2, epoch: 650, acc: 0.8700000643730164, Loss 0.3294762988353614\n",
      "RNNA, rep: 2, epoch: 651, acc: 0.8666667342185974, Loss 0.3518661229801364\n",
      "RNNA, rep: 2, epoch: 652, acc: 0.8900001645088196, Loss 0.3660381146287546\n",
      "RNNA, rep: 2, epoch: 653, acc: 0.9133333563804626, Loss 0.2686692247726023\n",
      "RNNA, rep: 2, epoch: 654, acc: 0.9166668057441711, Loss 0.2866554071730934\n",
      "RNNA, rep: 2, epoch: 655, acc: 0.8800001740455627, Loss 0.32599146240972915\n",
      "RNNA, rep: 2, epoch: 656, acc: 0.8699999451637268, Loss 0.3373239030007971\n",
      "RNNA, rep: 2, epoch: 657, acc: 0.8833334445953369, Loss 0.34042072939220813\n",
      "RNNA, rep: 2, epoch: 658, acc: 0.8833332061767578, Loss 0.3692307313747005\n",
      "RNNA, rep: 2, epoch: 659, acc: 0.8800000548362732, Loss 0.30624318981776016\n",
      "RNNA, rep: 2, epoch: 660, acc: 0.8833335041999817, Loss 0.3383340962522198\n",
      "RNNA, rep: 2, epoch: 661, acc: 0.9133333563804626, Loss 0.27365559235448017\n",
      "RNNA, rep: 2, epoch: 662, acc: 0.8966666460037231, Loss 0.3324720622505993\n",
      "RNNA, rep: 2, epoch: 663, acc: 0.8999999761581421, Loss 0.31148549150442706\n",
      "RNNA, rep: 2, epoch: 664, acc: 0.8999999165534973, Loss 0.289282058600802\n",
      "RNNA, rep: 2, epoch: 665, acc: 0.9133332967758179, Loss 0.2638748520729132\n",
      "RNNA, rep: 2, epoch: 666, acc: 0.8533333539962769, Loss 0.41485944523883517\n",
      "RNNA, rep: 2, epoch: 667, acc: 0.9100001454353333, Loss 0.28961216023541053\n",
      "RNNA, rep: 2, epoch: 668, acc: 0.8866667151451111, Loss 0.3214725101320073\n",
      "RNNA, rep: 2, epoch: 669, acc: 0.9066666960716248, Loss 0.2683349414402619\n",
      "RNNA, rep: 2, epoch: 670, acc: 0.90666663646698, Loss 0.33642772331601006\n",
      "RNNA, rep: 2, epoch: 671, acc: 0.9133333563804626, Loss 0.3041833241272252\n",
      "RNNA, rep: 2, epoch: 672, acc: 0.893333375453949, Loss 0.31032732837600635\n",
      "RNNA, rep: 2, epoch: 673, acc: 0.8999999761581421, Loss 0.2632704544765875\n",
      "RNNA, rep: 2, epoch: 674, acc: 0.893333375453949, Loss 0.30295013706432655\n",
      "RNNA, rep: 2, epoch: 675, acc: 0.9033333659172058, Loss 0.2852760566037614\n",
      "RNNA, rep: 2, epoch: 676, acc: 0.9133334159851074, Loss 0.3015194148087176\n",
      "RNNA, rep: 2, epoch: 677, acc: 0.8833332657814026, Loss 0.34536800272762774\n",
      "RNNA, rep: 2, epoch: 678, acc: 0.90666663646698, Loss 0.2576904152322095\n",
      "RNNA, rep: 2, epoch: 679, acc: 0.903333306312561, Loss 0.2790346255444456\n",
      "RNNA, rep: 2, epoch: 680, acc: 0.8466665744781494, Loss 0.4037865409941878\n",
      "RNNA, rep: 2, epoch: 681, acc: 0.8833332657814026, Loss 0.33515459260670466\n",
      "RNNA, rep: 2, epoch: 682, acc: 0.9333332777023315, Loss 0.19173990654409864\n",
      "RNNA, rep: 2, epoch: 683, acc: 0.9433332681655884, Loss 0.17114260515663773\n",
      "RNNA, rep: 2, epoch: 684, acc: 0.9100000262260437, Loss 0.2796209688624367\n",
      "RNNA, rep: 2, epoch: 685, acc: 0.856666624546051, Loss 0.4041365632903762\n",
      "RNNA, rep: 2, epoch: 686, acc: 0.9266665577888489, Loss 0.23697865053429268\n",
      "RNNA, rep: 2, epoch: 687, acc: 0.856666624546051, Loss 0.38551314068492504\n",
      "RNNA, rep: 2, epoch: 688, acc: 0.8700000047683716, Loss 0.37704617786221206\n",
      "RNNA, rep: 2, epoch: 689, acc: 0.8933334350585938, Loss 0.3167971530975774\n",
      "RNNA, rep: 2, epoch: 690, acc: 0.9000000953674316, Loss 0.29235974833602085\n",
      "RNNA, rep: 2, epoch: 691, acc: 0.9100000858306885, Loss 0.2743831560155377\n",
      "RNNA, rep: 2, epoch: 692, acc: 0.8799999952316284, Loss 0.3170176136866212\n",
      "RNNA, rep: 2, epoch: 693, acc: 0.8633334636688232, Loss 0.35352551379124636\n",
      "RNNA, rep: 2, epoch: 694, acc: 0.893333375453949, Loss 0.3430179167794995\n",
      "RNNA, rep: 2, epoch: 695, acc: 0.9033333659172058, Loss 0.2582159515842795\n",
      "RNNA, rep: 2, epoch: 696, acc: 0.8933332562446594, Loss 0.314140668110922\n",
      "RNNA, rep: 2, epoch: 697, acc: 0.8566668629646301, Loss 0.4227275244751945\n",
      "RNNA, rep: 2, epoch: 698, acc: 0.8900001049041748, Loss 0.3068822700995952\n",
      "RNNA, rep: 2, epoch: 699, acc: 0.90666663646698, Loss 0.2525716178386938\n",
      "RNNA, rep: 2, epoch: 700, acc: 0.9033333659172058, Loss 0.276202971606981\n",
      "RNNA, rep: 2, epoch: 701, acc: 0.8999999761581421, Loss 0.28948690785910003\n",
      "RNNA, rep: 2, epoch: 702, acc: 0.8733332753181458, Loss 0.3847427310352214\n",
      "RNNA, rep: 2, epoch: 703, acc: 0.8633334636688232, Loss 0.4019120610243408\n",
      "RNNA, rep: 2, epoch: 704, acc: 0.9033333659172058, Loss 0.3107952819851926\n",
      "RNNA, rep: 2, epoch: 705, acc: 0.9200000166893005, Loss 0.2558814957656432\n",
      "RNNA, rep: 2, epoch: 706, acc: 0.8833333849906921, Loss 0.31912233618786556\n",
      "RNNA, rep: 2, epoch: 707, acc: 0.8899998664855957, Loss 0.28683053655317053\n",
      "RNNA, rep: 2, epoch: 708, acc: 0.8766666650772095, Loss 0.32440068291849455\n",
      "RNNA, rep: 2, epoch: 709, acc: 0.9233334064483643, Loss 0.2706225702195661\n",
      "RNNA, rep: 2, epoch: 710, acc: 0.8799999952316284, Loss 0.34861529000569136\n",
      "RNNA, rep: 2, epoch: 711, acc: 0.9000000953674316, Loss 0.29567825026926586\n",
      "RNNA, rep: 2, epoch: 712, acc: 0.8800000548362732, Loss 0.3740771609626245\n",
      "RNNA, rep: 2, epoch: 713, acc: 0.8899999856948853, Loss 0.27380185000423807\n",
      "RNNA, rep: 2, epoch: 714, acc: 0.9100000262260437, Loss 0.27638734951149674\n",
      "RNNA, rep: 2, epoch: 715, acc: 0.9133334159851074, Loss 0.26222828823607414\n",
      "RNNA, rep: 2, epoch: 716, acc: 0.8633332848548889, Loss 0.3788671421748586\n",
      "RNNA, rep: 2, epoch: 717, acc: 0.8766665458679199, Loss 0.3751918736146763\n",
      "RNNA, rep: 2, epoch: 718, acc: 0.9233334064483643, Loss 0.24529940166394226\n",
      "RNNA, rep: 2, epoch: 719, acc: 0.9300000667572021, Loss 0.23851258736336603\n",
      "RNNA, rep: 2, epoch: 720, acc: 0.9033333659172058, Loss 0.2716581283858977\n",
      "RNNA, rep: 2, epoch: 721, acc: 0.9133333563804626, Loss 0.25951765349134803\n",
      "RNNA, rep: 2, epoch: 722, acc: 0.9066666960716248, Loss 0.2914895056653768\n",
      "RNNA, rep: 2, epoch: 723, acc: 0.90666663646698, Loss 0.256637552913744\n",
      "RNNA, rep: 2, epoch: 724, acc: 0.9100000858306885, Loss 0.24011129009420984\n",
      "RNNA, rep: 2, epoch: 725, acc: 0.9066666960716248, Loss 0.290502552955877\n",
      "RNNA, rep: 2, epoch: 726, acc: 0.903333306312561, Loss 0.30596521952887995\n",
      "RNNA, rep: 2, epoch: 727, acc: 0.893333375453949, Loss 0.3277797195757739\n",
      "RNNA, rep: 2, epoch: 728, acc: 0.9000000953674316, Loss 0.3165178240579553\n",
      "RNNA, rep: 2, epoch: 729, acc: 0.903333306312561, Loss 0.27283565465011633\n",
      "RNNA, rep: 2, epoch: 730, acc: 0.8933334350585938, Loss 0.3079067356383894\n",
      "RNNA, rep: 2, epoch: 731, acc: 0.9333332777023315, Loss 0.21738699372042902\n",
      "RNNA, rep: 2, epoch: 732, acc: 0.8599998950958252, Loss 0.3704589100985322\n",
      "RNNA, rep: 2, epoch: 733, acc: 0.8933334350585938, Loss 0.30353849723760506\n",
      "RNNA, rep: 2, epoch: 734, acc: 0.90666663646698, Loss 0.2996277042559814\n",
      "RNNA, rep: 2, epoch: 735, acc: 0.8899999856948853, Loss 0.2956147858739132\n",
      "RNNA, rep: 2, epoch: 736, acc: 0.8999997973442078, Loss 0.311289044811856\n",
      "RNNA, rep: 2, epoch: 737, acc: 0.9100000262260437, Loss 0.2571361868554959\n",
      "RNNA, rep: 2, epoch: 738, acc: 0.9233332872390747, Loss 0.1982981862849556\n",
      "RNNA, rep: 2, epoch: 739, acc: 0.9200000762939453, Loss 0.2473485910409363\n",
      "RNNA, rep: 2, epoch: 740, acc: 0.8733333349227905, Loss 0.40636287060100584\n",
      "RNNA, rep: 2, epoch: 741, acc: 0.9166667461395264, Loss 0.2581352508056443\n",
      "RNNA, rep: 2, epoch: 742, acc: 0.8799999952316284, Loss 0.30842659480986184\n",
      "RNNA, rep: 2, epoch: 743, acc: 0.8799999952316284, Loss 0.3048754881910281\n",
      "RNNA, rep: 2, epoch: 744, acc: 0.8933332562446594, Loss 0.3213452889805194\n",
      "RNNA, rep: 2, epoch: 745, acc: 0.9333332777023315, Loss 0.21970262387185358\n",
      "RNNA, rep: 2, epoch: 746, acc: 0.8599998950958252, Loss 0.413291898627067\n",
      "RNNA, rep: 2, epoch: 747, acc: 0.8900001049041748, Loss 0.3228559172595851\n",
      "RNNA, rep: 2, epoch: 748, acc: 0.8266665935516357, Loss 0.4212184225511737\n",
      "RNNA, rep: 2, epoch: 749, acc: 0.9300000071525574, Loss 0.25735177839989776\n",
      "RNNA, rep: 2, epoch: 750, acc: 0.9033334255218506, Loss 0.289621669575572\n",
      "RNNA, rep: 2, epoch: 751, acc: 0.9433333873748779, Loss 0.20122166085639037\n",
      "RNNA, rep: 2, epoch: 752, acc: 0.9100000858306885, Loss 0.26419500161719045\n",
      "RNNA, rep: 2, epoch: 753, acc: 0.9366667866706848, Loss 0.22342751398915425\n",
      "RNNA, rep: 2, epoch: 754, acc: 0.9100000262260437, Loss 0.22673044284223579\n",
      "RNNA, rep: 2, epoch: 755, acc: 0.9100000858306885, Loss 0.2728287629806437\n",
      "RNNA, rep: 2, epoch: 756, acc: 0.9200000166893005, Loss 0.26114303097943775\n",
      "RNNA, rep: 2, epoch: 757, acc: 0.8866666555404663, Loss 0.3517485722200945\n",
      "RNNA, rep: 2, epoch: 758, acc: 0.8866667151451111, Loss 0.3497370949582546\n",
      "RNNA, rep: 2, epoch: 759, acc: 0.9066666960716248, Loss 0.2957202851737384\n",
      "RNNA, rep: 2, epoch: 760, acc: 0.8933334350585938, Loss 0.2748502710892353\n",
      "RNNA, rep: 2, epoch: 761, acc: 0.9300000071525574, Loss 0.23827854095725343\n",
      "RNNA, rep: 2, epoch: 762, acc: 0.8433333039283752, Loss 0.4009881044260692\n",
      "RNNA, rep: 2, epoch: 763, acc: 0.9333332777023315, Loss 0.21822903312626296\n",
      "RNNA, rep: 2, epoch: 764, acc: 0.9266666173934937, Loss 0.2663326432078611\n",
      "RNNA, rep: 2, epoch: 765, acc: 0.9133331775665283, Loss 0.26293218762846665\n",
      "RNNA, rep: 2, epoch: 766, acc: 0.8733332753181458, Loss 0.31723808962269684\n",
      "RNNA, rep: 2, epoch: 767, acc: 0.9200000762939453, Loss 0.23499939936911687\n",
      "RNNA, rep: 2, epoch: 768, acc: 0.8966667056083679, Loss 0.29727697785943746\n",
      "RNNA, rep: 2, epoch: 769, acc: 0.8766666650772095, Loss 0.3523038766067475\n",
      "RNNA, rep: 2, epoch: 770, acc: 0.9299999475479126, Loss 0.23225426351069473\n",
      "RNNA, rep: 2, epoch: 771, acc: 0.8833332657814026, Loss 0.3401618402509484\n",
      "RNNA, rep: 2, epoch: 772, acc: 0.9233332872390747, Loss 0.21431680851965212\n",
      "RNNA, rep: 2, epoch: 773, acc: 0.9366666674613953, Loss 0.21702503983105997\n",
      "RNNA, rep: 2, epoch: 774, acc: 0.9100000262260437, Loss 0.2886013804911636\n",
      "RNNA, rep: 2, epoch: 775, acc: 0.90666663646698, Loss 0.27625309364811984\n",
      "RNNA, rep: 2, epoch: 776, acc: 0.8866666555404663, Loss 0.3051773706532549\n",
      "RNNA, rep: 2, epoch: 777, acc: 0.949999988079071, Loss 0.1980757753783837\n",
      "RNNA, rep: 2, epoch: 778, acc: 0.9333334565162659, Loss 0.23100388451945036\n",
      "RNNA, rep: 2, epoch: 779, acc: 0.8966667652130127, Loss 0.3158738920924952\n",
      "RNNA, rep: 2, epoch: 780, acc: 0.8700000047683716, Loss 0.36860839735076295\n",
      "RNNA, rep: 2, epoch: 781, acc: 0.8966667056083679, Loss 0.29983852911391295\n",
      "RNNA, rep: 2, epoch: 782, acc: 0.919999897480011, Loss 0.24125801065121777\n",
      "RNNA, rep: 2, epoch: 783, acc: 0.9099998474121094, Loss 0.29356279503030236\n",
      "RNNA, rep: 2, epoch: 784, acc: 0.8833333849906921, Loss 0.3583709040354006\n",
      "RNNA, rep: 2, epoch: 785, acc: 0.9100000858306885, Loss 0.26088218395132573\n",
      "RNNA, rep: 2, epoch: 786, acc: 0.8999999165534973, Loss 0.2809752398141427\n",
      "RNNA, rep: 2, epoch: 787, acc: 0.893333375453949, Loss 0.2734453466060222\n",
      "RNNA, rep: 2, epoch: 788, acc: 0.9033333659172058, Loss 0.2622091557038948\n",
      "RNNA, rep: 2, epoch: 789, acc: 0.873333215713501, Loss 0.40304163012478966\n",
      "RNNA, rep: 2, epoch: 790, acc: 0.8733334541320801, Loss 0.34104737769579513\n",
      "RNNA, rep: 2, epoch: 791, acc: 0.81333327293396, Loss 0.5411958120088093\n",
      "RNNA, rep: 2, epoch: 792, acc: 0.893333375453949, Loss 0.3026143573084846\n",
      "RNNA, rep: 2, epoch: 793, acc: 0.9300000071525574, Loss 0.21263094762805848\n",
      "RNNA, rep: 2, epoch: 794, acc: 0.8799999952316284, Loss 0.31860147684346884\n",
      "RNNA, rep: 2, epoch: 795, acc: 0.9200000166893005, Loss 0.2642586788185872\n",
      "RNNA, rep: 2, epoch: 796, acc: 0.9100000262260437, Loss 0.27532913357106736\n",
      "RNNA, rep: 2, epoch: 797, acc: 0.9066666960716248, Loss 0.3088364588725381\n",
      "RNNA, rep: 2, epoch: 798, acc: 0.90666663646698, Loss 0.28219245252286784\n",
      "RNNA, rep: 2, epoch: 799, acc: 0.9100000262260437, Loss 0.25869639160344376\n",
      "RNNA, rep: 2, epoch: 800, acc: 0.8866665363311768, Loss 0.2995774730018457\n",
      "RNNA, rep: 2, epoch: 801, acc: 0.8533332943916321, Loss 0.40718979777710046\n",
      "RNNA, rep: 2, epoch: 802, acc: 0.8866667747497559, Loss 0.3197573502728483\n",
      "RNNA, rep: 2, epoch: 803, acc: 0.8933332562446594, Loss 0.31120598583947867\n",
      "RNNA, rep: 2, epoch: 804, acc: 0.9333334565162659, Loss 0.20939069787738845\n",
      "RNNA, rep: 2, epoch: 805, acc: 0.8700000047683716, Loss 0.3150911708915373\n",
      "RNNA, rep: 2, epoch: 806, acc: 0.9599999785423279, Loss 0.1458246051572496\n",
      "RNNA, rep: 2, epoch: 807, acc: 0.9133333563804626, Loss 0.24879075817938429\n",
      "RNNA, rep: 2, epoch: 808, acc: 0.8566667437553406, Loss 0.3403054168447852\n",
      "RNNA, rep: 2, epoch: 809, acc: 0.8766666650772095, Loss 0.3647084651107434\n",
      "RNNA, rep: 2, epoch: 810, acc: 0.8766667246818542, Loss 0.3214671227382496\n",
      "RNNA, rep: 2, epoch: 811, acc: 0.9200000166893005, Loss 0.2389058979548281\n",
      "RNNA, rep: 2, epoch: 812, acc: 0.8400000929832458, Loss 0.41372091066557914\n",
      "RNNA, rep: 2, epoch: 813, acc: 0.8700000643730164, Loss 0.34687506422837033\n",
      "RNNA, rep: 2, epoch: 814, acc: 0.9099999070167542, Loss 0.3099868933670223\n",
      "RNNA, rep: 2, epoch: 815, acc: 0.9166667461395264, Loss 0.2504243127326481\n",
      "RNNA, rep: 2, epoch: 816, acc: 0.8866667151451111, Loss 0.31292074004828463\n",
      "RNNA, rep: 2, epoch: 817, acc: 0.9066665768623352, Loss 0.2917694590060273\n",
      "RNNA, rep: 2, epoch: 818, acc: 0.8900001049041748, Loss 0.31043815607728903\n",
      "RNNA, rep: 2, epoch: 819, acc: 0.9266665577888489, Loss 0.2743237522861455\n",
      "RNNA, rep: 2, epoch: 820, acc: 0.9133333563804626, Loss 0.2661969734798186\n",
      "RNNA, rep: 2, epoch: 821, acc: 0.9033334255218506, Loss 0.2822631444327999\n",
      "RNNA, rep: 2, epoch: 822, acc: 0.893333375453949, Loss 0.34051093873451466\n",
      "RNNA, rep: 2, epoch: 823, acc: 0.8799999952316284, Loss 0.3045250855910126\n",
      "RNNA, rep: 2, epoch: 824, acc: 0.8833334445953369, Loss 0.31833274840028025\n",
      "RNNA, rep: 2, epoch: 825, acc: 0.8700000047683716, Loss 0.3527222818939481\n",
      "RNNA, rep: 2, epoch: 826, acc: 0.8733333349227905, Loss 0.35528262491803614\n",
      "RNNA, rep: 2, epoch: 827, acc: 0.893333375453949, Loss 0.28070263350789904\n",
      "RNNA, rep: 2, epoch: 828, acc: 0.8733332753181458, Loss 0.3537765482731629\n",
      "RNNA, rep: 2, epoch: 829, acc: 0.9033333659172058, Loss 0.26671311613521537\n",
      "RNNA, rep: 2, epoch: 830, acc: 0.9166667461395264, Loss 0.24119615325005725\n",
      "RNNA, rep: 2, epoch: 831, acc: 0.9633333683013916, Loss 0.17048167483648285\n",
      "RNNA, rep: 2, epoch: 832, acc: 0.8966667652130127, Loss 0.2932656458648853\n",
      "RNNA, rep: 2, epoch: 833, acc: 0.8666666150093079, Loss 0.3446272335323738\n",
      "RNNA, rep: 2, epoch: 834, acc: 0.8833333849906921, Loss 0.3690977832186036\n",
      "RNNA, rep: 2, epoch: 835, acc: 0.8799999356269836, Loss 0.3669763673748821\n",
      "RNNA, rep: 2, epoch: 836, acc: 0.9300000667572021, Loss 0.22828238002373838\n",
      "RNNA, rep: 2, epoch: 837, acc: 0.8966666460037231, Loss 0.2897301163035445\n",
      "RNNA, rep: 2, epoch: 838, acc: 0.893333375453949, Loss 0.2974767205584794\n",
      "RNNA, rep: 2, epoch: 839, acc: 0.9066668152809143, Loss 0.2576687790581491\n",
      "RNNA, rep: 2, epoch: 840, acc: 0.8833333849906921, Loss 0.33356415612623097\n",
      "RNNA, rep: 2, epoch: 841, acc: 0.9166667461395264, Loss 0.2533190057007596\n",
      "RNNA, rep: 2, epoch: 842, acc: 0.8866668939590454, Loss 0.3454869273403892\n",
      "RNNA, rep: 2, epoch: 843, acc: 0.940000057220459, Loss 0.1798136616172269\n",
      "RNNA, rep: 2, epoch: 844, acc: 0.8833335041999817, Loss 0.3061258329218253\n",
      "RNNA, rep: 2, epoch: 845, acc: 0.9266667366027832, Loss 0.21797362028155476\n",
      "RNNA, rep: 2, epoch: 846, acc: 0.9166666269302368, Loss 0.26720407807035373\n",
      "RNNA, rep: 2, epoch: 847, acc: 0.9100000858306885, Loss 0.24306540690886322\n",
      "RNNA, rep: 2, epoch: 848, acc: 0.8666667342185974, Loss 0.4201749372109771\n",
      "RNNA, rep: 2, epoch: 849, acc: 0.9133333563804626, Loss 0.25958017709082926\n",
      "RNNA, rep: 2, epoch: 850, acc: 0.8833333849906921, Loss 0.31532448324374857\n",
      "RNNA, rep: 2, epoch: 851, acc: 0.9200000762939453, Loss 0.26716157705406657\n",
      "RNNA, rep: 2, epoch: 852, acc: 0.9033334255218506, Loss 0.31159647382679395\n",
      "RNNA, rep: 2, epoch: 853, acc: 0.9000000953674316, Loss 0.28496297274832616\n",
      "RNNA, rep: 2, epoch: 854, acc: 0.8833333849906921, Loss 0.31368279618676753\n",
      "RNNA, rep: 2, epoch: 855, acc: 0.9433334469795227, Loss 0.17250102611258625\n",
      "RNNA, rep: 2, epoch: 856, acc: 0.9233335256576538, Loss 0.25079807732312476\n",
      "RNNA, rep: 2, epoch: 857, acc: 0.8700000047683716, Loss 0.40420648134080694\n",
      "RNNA, rep: 2, epoch: 858, acc: 0.919999897480011, Loss 0.26609792911913244\n",
      "RNNA, rep: 2, epoch: 859, acc: 0.9266667366027832, Loss 0.24910934596089646\n",
      "RNNA, rep: 2, epoch: 860, acc: 0.8866666555404663, Loss 0.3235760307975579\n",
      "RNNA, rep: 2, epoch: 861, acc: 0.9266665577888489, Loss 0.21461562767159192\n",
      "RNNA, rep: 2, epoch: 862, acc: 0.9166667461395264, Loss 0.2561297952191671\n",
      "RNNA, rep: 2, epoch: 863, acc: 0.8833332657814026, Loss 0.3502789182960987\n",
      "RNNA, rep: 2, epoch: 864, acc: 0.9033333659172058, Loss 0.25641604311065747\n",
      "RNNA, rep: 2, epoch: 865, acc: 0.8799999356269836, Loss 0.3276035632379353\n",
      "RNNA, rep: 2, epoch: 866, acc: 0.9166667461395264, Loss 0.2519450387940742\n",
      "RNNA, rep: 2, epoch: 867, acc: 0.9166667461395264, Loss 0.24062639496056362\n",
      "RNNA, rep: 2, epoch: 868, acc: 0.8833333849906921, Loss 0.3333007286884822\n",
      "RNNA, rep: 2, epoch: 869, acc: 0.8866668939590454, Loss 0.3086779697064776\n",
      "RNNA, rep: 2, epoch: 870, acc: 0.9333333373069763, Loss 0.18611880276934245\n",
      "RNNA, rep: 2, epoch: 871, acc: 0.8933334350585938, Loss 0.29303954101749696\n",
      "RNNA, rep: 2, epoch: 872, acc: 0.9000001549720764, Loss 0.302079289277317\n",
      "RNNA, rep: 2, epoch: 873, acc: 0.9233333468437195, Loss 0.24170889789122157\n",
      "RNNA, rep: 2, epoch: 874, acc: 0.8366668820381165, Loss 0.4766671082645189\n",
      "RNNA, rep: 2, epoch: 875, acc: 0.9166666269302368, Loss 0.2670761366351508\n",
      "RNNA, rep: 2, epoch: 876, acc: 0.9000000953674316, Loss 0.2742281618132256\n",
      "RNNA, rep: 2, epoch: 877, acc: 0.9166668057441711, Loss 0.23470245445729232\n",
      "RNNA, rep: 2, epoch: 878, acc: 0.8833332657814026, Loss 0.2994336933118757\n",
      "RNNA, rep: 2, epoch: 879, acc: 0.8666666150093079, Loss 0.37032112820073965\n",
      "RNNA, rep: 2, epoch: 880, acc: 0.9399999976158142, Loss 0.16563849535537883\n",
      "RNNA, rep: 2, epoch: 881, acc: 0.9000000953674316, Loss 0.27494158457964657\n",
      "RNNA, rep: 2, epoch: 882, acc: 0.8866665363311768, Loss 0.3166682736086659\n",
      "RNNA, rep: 2, epoch: 883, acc: 0.9333333373069763, Loss 0.21492621180659627\n",
      "RNNA, rep: 2, epoch: 884, acc: 0.8766667246818542, Loss 0.3186205307557248\n",
      "RNNA, rep: 2, epoch: 885, acc: 0.9133334159851074, Loss 0.2831416822230676\n",
      "RNNA, rep: 2, epoch: 886, acc: 0.9066666960716248, Loss 0.279039066536352\n",
      "RNNA, rep: 2, epoch: 887, acc: 0.93666672706604, Loss 0.22661625648732298\n",
      "RNNA, rep: 2, epoch: 888, acc: 0.9633333086967468, Loss 0.1652551012043841\n",
      "RNNA, rep: 2, epoch: 889, acc: 0.9300000071525574, Loss 0.19760126810055226\n",
      "RNNA, rep: 2, epoch: 890, acc: 0.8866667151451111, Loss 0.3192985873296857\n",
      "RNNA, rep: 2, epoch: 891, acc: 0.9133333563804626, Loss 0.2687508569832426\n",
      "RNNA, rep: 2, epoch: 892, acc: 0.9433332681655884, Loss 0.19233458148431964\n",
      "RNNA, rep: 2, epoch: 893, acc: 0.8899999856948853, Loss 0.30639369355980306\n",
      "RNNA, rep: 2, epoch: 894, acc: 0.9133333563804626, Loss 0.2488983457919676\n",
      "RNNA, rep: 2, epoch: 895, acc: 0.9266666173934937, Loss 0.21229953670292162\n",
      "RNNA, rep: 2, epoch: 896, acc: 0.9333333373069763, Loss 0.19672517892380711\n",
      "RNNA, rep: 2, epoch: 897, acc: 0.9133334159851074, Loss 0.2761788894137135\n",
      "RNNA, rep: 2, epoch: 898, acc: 0.9333333373069763, Loss 0.20698756848170888\n",
      "RNNA, rep: 2, epoch: 899, acc: 0.9133332967758179, Loss 0.2724569880485069\n",
      "RNNA, rep: 2, epoch: 900, acc: 0.9200000166893005, Loss 0.2527044574997854\n",
      "RNNA, rep: 2, epoch: 901, acc: 0.8733332753181458, Loss 0.3507736415753607\n",
      "RNNA, rep: 2, epoch: 902, acc: 0.9266667366027832, Loss 0.23811279485584236\n",
      "RNNA, rep: 2, epoch: 903, acc: 0.9399999976158142, Loss 0.19093332693330012\n",
      "RNNA, rep: 2, epoch: 904, acc: 0.8966666460037231, Loss 0.32235068041656634\n",
      "RNNA, rep: 2, epoch: 905, acc: 0.9200000166893005, Loss 0.2430942707986105\n",
      "RNNA, rep: 2, epoch: 906, acc: 0.8933334350585938, Loss 0.32132231681724077\n",
      "RNNA, rep: 2, epoch: 907, acc: 0.9166667461395264, Loss 0.23425679269479588\n",
      "RNNA, rep: 2, epoch: 908, acc: 0.8700000047683716, Loss 0.3708556875900831\n",
      "RNNA, rep: 2, epoch: 909, acc: 0.8733333349227905, Loss 0.3771983058063779\n",
      "RNNA, rep: 2, epoch: 910, acc: 0.9333332180976868, Loss 0.15860134874936194\n",
      "RNNA, rep: 2, epoch: 911, acc: 0.8899999260902405, Loss 0.3402159764751559\n",
      "RNNA, rep: 2, epoch: 912, acc: 0.9066668152809143, Loss 0.3120834170083981\n",
      "RNNA, rep: 2, epoch: 913, acc: 0.8766666650772095, Loss 0.33115776397520674\n",
      "RNNA, rep: 2, epoch: 914, acc: 0.9500000476837158, Loss 0.1506451688404195\n",
      "RNNA, rep: 2, epoch: 915, acc: 0.9433333873748779, Loss 0.15450125441537238\n",
      "RNNA, rep: 2, epoch: 916, acc: 0.8799999952316284, Loss 0.3823872886033496\n",
      "RNNA, rep: 2, epoch: 917, acc: 0.8899999260902405, Loss 0.26864228494814596\n",
      "RNNA, rep: 2, epoch: 918, acc: 0.903333306312561, Loss 0.2634504923026543\n",
      "RNNA, rep: 2, epoch: 919, acc: 0.9233334064483643, Loss 0.23024213010678068\n",
      "RNNA, rep: 2, epoch: 920, acc: 0.8933334350585938, Loss 0.25314308754110243\n",
      "RNNA, rep: 2, epoch: 921, acc: 0.9300000667572021, Loss 0.2213292734831339\n",
      "RNNA, rep: 2, epoch: 922, acc: 0.8833333849906921, Loss 0.35590150712785545\n",
      "RNNA, rep: 2, epoch: 923, acc: 0.9166666269302368, Loss 0.24041099342517555\n",
      "RNNA, rep: 2, epoch: 924, acc: 0.9033333659172058, Loss 0.26216773489199113\n",
      "RNNA, rep: 2, epoch: 925, acc: 0.9100000262260437, Loss 0.26554215154697886\n",
      "RNNA, rep: 2, epoch: 926, acc: 0.9166667461395264, Loss 0.2364260727603687\n",
      "RNNA, rep: 2, epoch: 927, acc: 0.8733332753181458, Loss 0.36620913940423633\n",
      "RNNA, rep: 2, epoch: 928, acc: 0.9533332586288452, Loss 0.16925599785638043\n",
      "RNNA, rep: 2, epoch: 929, acc: 0.9000000953674316, Loss 0.30968185294535944\n",
      "RNNA, rep: 2, epoch: 930, acc: 0.9000000953674316, Loss 0.3252237768875784\n",
      "RNNA, rep: 2, epoch: 931, acc: 0.9333334565162659, Loss 0.2174017566710245\n",
      "RNNA, rep: 2, epoch: 932, acc: 0.93666672706604, Loss 0.19837664297258015\n",
      "RNNA, rep: 2, epoch: 933, acc: 0.9166668057441711, Loss 0.22965852778230328\n",
      "RNNA, rep: 2, epoch: 934, acc: 0.8899999260902405, Loss 0.32509352755121657\n",
      "RNNA, rep: 2, epoch: 935, acc: 0.8833333849906921, Loss 0.3122548298473703\n",
      "RNNA, rep: 2, epoch: 936, acc: 0.8766667246818542, Loss 0.3122988694155356\n",
      "RNNA, rep: 2, epoch: 937, acc: 0.8866667747497559, Loss 0.31817816678259986\n",
      "RNNA, rep: 2, epoch: 938, acc: 0.8733332753181458, Loss 0.3168997460079845\n",
      "RNNA, rep: 2, epoch: 939, acc: 0.9100000858306885, Loss 0.24845934848068282\n",
      "RNNA, rep: 2, epoch: 940, acc: 0.9100000858306885, Loss 0.2603149677297915\n",
      "RNNA, rep: 2, epoch: 941, acc: 0.9466666579246521, Loss 0.16613579505472445\n",
      "RNNA, rep: 2, epoch: 942, acc: 0.90666663646698, Loss 0.24670446452801115\n",
      "RNNA, rep: 2, epoch: 943, acc: 0.8999999761581421, Loss 0.28310748367744965\n",
      "RNNA, rep: 2, epoch: 944, acc: 0.8966667056083679, Loss 0.2934728670062032\n",
      "RNNA, rep: 2, epoch: 945, acc: 0.9366666674613953, Loss 0.20675489824148827\n",
      "RNNA, rep: 2, epoch: 946, acc: 0.8966667056083679, Loss 0.27227308390196414\n",
      "RNNA, rep: 2, epoch: 947, acc: 0.8833334445953369, Loss 0.35464225462637844\n",
      "RNNA, rep: 2, epoch: 948, acc: 0.940000057220459, Loss 0.2087588246294763\n",
      "RNNA, rep: 2, epoch: 949, acc: 0.9166668057441711, Loss 0.2709432155219838\n",
      "RNNA, rep: 2, epoch: 950, acc: 0.8933334350585938, Loss 0.2949745385139249\n",
      "RNNA, rep: 2, epoch: 951, acc: 0.8866667747497559, Loss 0.35168499877268916\n",
      "RNNA, rep: 2, epoch: 952, acc: 0.9099999070167542, Loss 0.23901218191720544\n",
      "RNNA, rep: 2, epoch: 953, acc: 0.93666672706604, Loss 0.22354990475432715\n",
      "RNNA, rep: 2, epoch: 954, acc: 0.8800001740455627, Loss 0.3313296957255807\n",
      "RNNA, rep: 2, epoch: 955, acc: 0.913333535194397, Loss 0.25247475125535856\n",
      "RNNA, rep: 2, epoch: 956, acc: 0.8899999856948853, Loss 0.27219761817133986\n",
      "RNNA, rep: 2, epoch: 957, acc: 0.8966667652130127, Loss 0.2989015219733119\n",
      "RNNA, rep: 2, epoch: 958, acc: 0.9133332967758179, Loss 0.26765360869816507\n",
      "RNNA, rep: 2, epoch: 959, acc: 0.9133334159851074, Loss 0.25085597941128074\n",
      "RNNA, rep: 2, epoch: 960, acc: 0.8733333349227905, Loss 0.35699371061928103\n",
      "RNNA, rep: 2, epoch: 961, acc: 0.8799999952316284, Loss 0.3338449641584884\n",
      "RNNA, rep: 2, epoch: 962, acc: 0.9100000262260437, Loss 0.24770375301130115\n",
      "RNNA, rep: 2, epoch: 963, acc: 0.9033334851264954, Loss 0.24344047539518215\n",
      "RNNA, rep: 2, epoch: 964, acc: 0.8799999952316284, Loss 0.31540591674624013\n",
      "RNNA, rep: 2, epoch: 965, acc: 0.8966667652130127, Loss 0.3005053428688552\n",
      "RNNA, rep: 2, epoch: 966, acc: 0.9233332872390747, Loss 0.23701012225064916\n",
      "RNNA, rep: 2, epoch: 967, acc: 0.9300000071525574, Loss 0.19321712092205418\n",
      "RNNA, rep: 2, epoch: 968, acc: 0.9133332967758179, Loss 0.2339081755228108\n",
      "RNNA, rep: 2, epoch: 969, acc: 0.903333306312561, Loss 0.2677159452758497\n",
      "RNNA, rep: 2, epoch: 970, acc: 0.8900001049041748, Loss 0.32254366082604974\n",
      "RNNA, rep: 2, epoch: 971, acc: 0.9333333373069763, Loss 0.17890098772651983\n",
      "RNNA, rep: 2, epoch: 972, acc: 0.9333334565162659, Loss 0.20400655747915153\n",
      "RNNA, rep: 2, epoch: 973, acc: 0.9100000858306885, Loss 0.2822912816875032\n",
      "RNNA, rep: 2, epoch: 974, acc: 0.8666666150093079, Loss 0.33154392344295047\n",
      "RNNA, rep: 2, epoch: 975, acc: 0.9266667366027832, Loss 0.22471572533657308\n",
      "RNNA, rep: 2, epoch: 976, acc: 0.8866667151451111, Loss 0.3222932955255965\n",
      "RNNA, rep: 2, epoch: 977, acc: 0.8899999856948853, Loss 0.33592803525039927\n",
      "RNNA, rep: 2, epoch: 978, acc: 0.9133332967758179, Loss 0.2540384073310997\n",
      "RNNA, rep: 2, epoch: 979, acc: 0.9366666674613953, Loss 0.19631591255194508\n",
      "RNNA, rep: 2, epoch: 980, acc: 0.9200000762939453, Loss 0.2548834843857912\n",
      "RNNA, rep: 2, epoch: 981, acc: 0.9100000858306885, Loss 0.25084691019263117\n",
      "RNNA, rep: 2, epoch: 982, acc: 0.93666672706604, Loss 0.21180969358072615\n",
      "RNNA, rep: 2, epoch: 983, acc: 0.929999828338623, Loss 0.1973309852043167\n",
      "RNNA, rep: 2, epoch: 984, acc: 0.8900001049041748, Loss 0.3314201252831845\n",
      "RNNA, rep: 2, epoch: 985, acc: 0.9266665577888489, Loss 0.22319841329474002\n",
      "RNNA, rep: 2, epoch: 986, acc: 0.876666784286499, Loss 0.35390338254976084\n",
      "RNNA, rep: 2, epoch: 987, acc: 0.9499998688697815, Loss 0.14107045335520524\n",
      "RNNA, rep: 2, epoch: 988, acc: 0.9466667175292969, Loss 0.16891939028428168\n",
      "RNNA, rep: 2, epoch: 989, acc: 0.8833334445953369, Loss 0.3112409629463218\n",
      "RNNA, rep: 2, epoch: 990, acc: 0.9000001549720764, Loss 0.30385142081533556\n",
      "RNNA, rep: 2, epoch: 991, acc: 0.9399999976158142, Loss 0.2091985052730888\n",
      "RNNA, rep: 2, epoch: 992, acc: 0.8999999165534973, Loss 0.2757626565697137\n",
      "RNNA, rep: 2, epoch: 993, acc: 0.9099999070167542, Loss 0.30036149343592117\n",
      "RNNA, rep: 2, epoch: 994, acc: 0.8966667056083679, Loss 0.2906635751691647\n",
      "RNNA, rep: 2, epoch: 995, acc: 0.9300000071525574, Loss 0.21394315561745317\n",
      "RNNA, rep: 2, epoch: 996, acc: 0.8900001645088196, Loss 0.31913157109287565\n",
      "RNNA, rep: 2, epoch: 997, acc: 0.9133333563804626, Loss 0.24031389701878653\n",
      "RNNA, rep: 2, epoch: 998, acc: 0.9000000953674316, Loss 0.282883029577788\n",
      "RNNA, rep: 2, epoch: 999, acc: 0.9166668057441711, Loss 0.2159885489300359\n",
      "RNNA, rep: 2, epoch: 1000, acc: 0.9300000667572021, Loss 0.21188722316350322\n",
      "RNNA, rep: 2, epoch: 1001, acc: 0.9166666269302368, Loss 0.2686439380404772\n",
      "RNNA, rep: 2, epoch: 1002, acc: 0.8999998569488525, Loss 0.29608896290417763\n",
      "RNNA, rep: 2, epoch: 1003, acc: 0.9200000166893005, Loss 0.23802156048768666\n",
      "RNNA, rep: 2, epoch: 1004, acc: 0.8933334350585938, Loss 0.301257058432966\n",
      "RNNA, rep: 2, epoch: 1005, acc: 0.9300000071525574, Loss 0.24104392834880856\n",
      "RNNA, rep: 2, epoch: 1006, acc: 0.893333375453949, Loss 0.30652791368484034\n",
      "RNNA, rep: 2, epoch: 1007, acc: 0.9133333563804626, Loss 0.2697916331974557\n",
      "RNNA, rep: 2, epoch: 1008, acc: 0.9100000262260437, Loss 0.2833777061966248\n",
      "RNNA, rep: 2, epoch: 1009, acc: 0.9233334064483643, Loss 0.22951074011289166\n",
      "RNNA, rep: 2, epoch: 1010, acc: 0.8866667151451111, Loss 0.27240607474319406\n",
      "RNNA, rep: 2, epoch: 1011, acc: 0.9433335065841675, Loss 0.2257404865184799\n",
      "RNNA, rep: 2, epoch: 1012, acc: 0.9366667866706848, Loss 0.19511866303742864\n",
      "RNNA, rep: 2, epoch: 1013, acc: 0.9066668152809143, Loss 0.2640383013244718\n",
      "RNNA, rep: 2, epoch: 1014, acc: 0.8833334445953369, Loss 0.33025688604975584\n",
      "RNNA, rep: 2, epoch: 1015, acc: 0.9166666269302368, Loss 0.2532389673445141\n",
      "RNNA, rep: 2, epoch: 1016, acc: 0.8999999165534973, Loss 0.2864781455544289\n",
      "RNNA, rep: 2, epoch: 1017, acc: 0.8933332562446594, Loss 0.2887757736805361\n",
      "RNNA, rep: 2, epoch: 1018, acc: 0.9399999976158142, Loss 0.17398353520489762\n",
      "RNNA, rep: 2, epoch: 1019, acc: 0.9100001454353333, Loss 0.30572993480484\n",
      "RNNA, rep: 2, epoch: 1020, acc: 0.9266667366027832, Loss 0.20826662154519\n",
      "RNNA, rep: 2, epoch: 1021, acc: 0.9166666269302368, Loss 0.24282969015446723\n",
      "RNNA, rep: 2, epoch: 1022, acc: 0.9399999976158142, Loss 0.19328297950385603\n",
      "RNNA, rep: 2, epoch: 1023, acc: 0.9066665768623352, Loss 0.2510658417234663\n",
      "RNNA, rep: 2, epoch: 1024, acc: 0.8900001049041748, Loss 0.3024233549358905\n",
      "RNNA, rep: 2, epoch: 1025, acc: 0.8899999856948853, Loss 0.33249792632414027\n",
      "RNNA, rep: 2, epoch: 1026, acc: 0.9266668558120728, Loss 0.21480906369455624\n",
      "RNNA, rep: 2, epoch: 1027, acc: 0.9533333778381348, Loss 0.16352502327412366\n",
      "RNNA, rep: 2, epoch: 1028, acc: 0.9333333373069763, Loss 0.19675328746408924\n",
      "RNNA, rep: 2, epoch: 1029, acc: 0.9399999976158142, Loss 0.18553360929625343\n",
      "RNNA, rep: 2, epoch: 1030, acc: 0.9433332085609436, Loss 0.1864457483717706\n",
      "RNNA, rep: 2, epoch: 1031, acc: 0.9333332777023315, Loss 0.19458264248154591\n",
      "RNNA, rep: 2, epoch: 1032, acc: 0.9033333659172058, Loss 0.27785496955300915\n",
      "RNNA, rep: 2, epoch: 1033, acc: 0.9333334565162659, Loss 0.21286867863847875\n",
      "RNNA, rep: 2, epoch: 1034, acc: 0.93666672706604, Loss 0.2029810092592379\n",
      "RNNA, rep: 2, epoch: 1035, acc: 0.8833332061767578, Loss 0.339863091702573\n",
      "RNNA, rep: 2, epoch: 1036, acc: 0.90666663646698, Loss 0.270916572228889\n",
      "RNNA, rep: 2, epoch: 1037, acc: 0.9200000762939453, Loss 0.22970872831443556\n",
      "RNNA, rep: 2, epoch: 1038, acc: 0.9399999976158142, Loss 0.19408465884858742\n",
      "RNNA, rep: 2, epoch: 1039, acc: 0.9300000071525574, Loss 0.21449524362571537\n",
      "RNNA, rep: 2, epoch: 1040, acc: 0.8866667151451111, Loss 0.2790347922203364\n",
      "RNNA, rep: 2, epoch: 1041, acc: 0.9133332967758179, Loss 0.247306113592349\n",
      "RNNA, rep: 2, epoch: 1042, acc: 0.9100000858306885, Loss 0.22709813940920867\n",
      "RNNA, rep: 2, epoch: 1043, acc: 0.9100000858306885, Loss 0.2568318043270847\n",
      "RNNA, rep: 2, epoch: 1044, acc: 0.9399999380111694, Loss 0.22958209640928545\n",
      "RNNA, rep: 2, epoch: 1045, acc: 0.9266667366027832, Loss 0.2476496921031503\n",
      "RNNA, rep: 2, epoch: 1046, acc: 0.9333332777023315, Loss 0.19238758934428915\n",
      "RNNA, rep: 2, epoch: 1047, acc: 0.9133333563804626, Loss 0.25323694130871444\n",
      "RNNA, rep: 2, epoch: 1048, acc: 0.9100000858306885, Loss 0.24561661430314416\n",
      "RNNA, rep: 2, epoch: 1049, acc: 0.8766666650772095, Loss 0.30915097875578795\n",
      "RNNA, rep: 2, epoch: 1050, acc: 0.8799999952316284, Loss 0.3113531673551188\n",
      "RNNA, rep: 2, epoch: 1051, acc: 0.9300000071525574, Loss 0.18414722876623274\n",
      "RNNA, rep: 2, epoch: 1052, acc: 0.8933332562446594, Loss 0.2809587850124808\n",
      "RNNA, rep: 2, epoch: 1053, acc: 0.9399999976158142, Loss 0.19129473570152186\n",
      "RNNA, rep: 2, epoch: 1054, acc: 0.8999999761581421, Loss 0.2997912753646961\n",
      "RNNA, rep: 2, epoch: 1055, acc: 0.8933334350585938, Loss 0.3348719704826362\n",
      "RNNA, rep: 2, epoch: 1056, acc: 0.9166666269302368, Loss 0.2261909920416656\n",
      "RNNA, rep: 2, epoch: 1057, acc: 0.8933334350585938, Loss 0.27774552498885896\n",
      "RNNA, rep: 2, epoch: 1058, acc: 0.8766666650772095, Loss 0.35271547721262325\n",
      "RNNA, rep: 2, epoch: 1059, acc: 0.8933334350585938, Loss 0.32840920664515577\n",
      "RNNA, rep: 2, epoch: 1060, acc: 0.9466666579246521, Loss 0.1696408625281765\n",
      "RNNA, rep: 2, epoch: 1061, acc: 0.8866667151451111, Loss 0.2805246056130272\n",
      "RNNA, rep: 2, epoch: 1062, acc: 0.9433333873748779, Loss 0.1321085900250182\n",
      "RNNA, rep: 2, epoch: 1063, acc: 0.9200000166893005, Loss 0.1984564082103316\n",
      "RNNA, rep: 2, epoch: 1064, acc: 0.9100000858306885, Loss 0.31169720915684596\n",
      "RNNA, rep: 2, epoch: 1065, acc: 0.9333333373069763, Loss 0.1786424420896219\n",
      "RNNA, rep: 2, epoch: 1066, acc: 0.8599998950958252, Loss 0.3756805504520889\n",
      "RNNA, rep: 2, epoch: 1067, acc: 0.8866666555404663, Loss 0.30976368722505865\n",
      "RNNA, rep: 2, epoch: 1068, acc: 0.9333333373069763, Loss 0.1774823067811667\n",
      "RNNA, rep: 2, epoch: 1069, acc: 0.9166666269302368, Loss 0.21670935085683596\n",
      "RNNA, rep: 2, epoch: 1070, acc: 0.93666672706604, Loss 0.210146485940204\n",
      "RNNA, rep: 2, epoch: 1071, acc: 0.9033334255218506, Loss 0.25873774668085386\n",
      "RNNA, rep: 2, epoch: 1072, acc: 0.9266666173934937, Loss 0.2243313133524498\n",
      "RNNA, rep: 2, epoch: 1073, acc: 0.9100000858306885, Loss 0.25919674220611344\n",
      "RNNA, rep: 2, epoch: 1074, acc: 0.9033333659172058, Loss 0.22694869773142273\n",
      "RNNA, rep: 2, epoch: 1075, acc: 0.90666663646698, Loss 0.2979972756741336\n",
      "RNNA, rep: 2, epoch: 1076, acc: 0.9100000262260437, Loss 0.2855388162774034\n",
      "RNNA, rep: 2, epoch: 1077, acc: 0.9166667461395264, Loss 0.2548577661928721\n",
      "RNNA, rep: 2, epoch: 1078, acc: 0.9333333373069763, Loss 0.18326514519372722\n",
      "RNNA, rep: 2, epoch: 1079, acc: 0.9100001454353333, Loss 0.25887892615777675\n",
      "RNNA, rep: 2, epoch: 1080, acc: 0.8999999761581421, Loss 0.2640990575659089\n",
      "RNNA, rep: 2, epoch: 1081, acc: 0.9166666269302368, Loss 0.23051634359406306\n",
      "RNNA, rep: 2, epoch: 1082, acc: 0.9100000262260437, Loss 0.28449963850318455\n",
      "RNNA, rep: 2, epoch: 1083, acc: 0.9233332276344299, Loss 0.20956998208304867\n",
      "RNNA, rep: 2, epoch: 1084, acc: 0.9099999070167542, Loss 0.25838245104823726\n",
      "RNNA, rep: 2, epoch: 1085, acc: 0.9200000166893005, Loss 0.2640029346558731\n",
      "RNNA, rep: 2, epoch: 1086, acc: 0.903333306312561, Loss 0.27161454902321563\n",
      "RNNA, rep: 2, epoch: 1087, acc: 0.9466667175292969, Loss 0.14129949104302797\n",
      "RNNA, rep: 2, epoch: 1088, acc: 0.9533333778381348, Loss 0.17872698508173926\n",
      "RNNA, rep: 2, epoch: 1089, acc: 0.9133332967758179, Loss 0.2616818234798848\n",
      "RNNA, rep: 2, epoch: 1090, acc: 0.9100000262260437, Loss 0.26457413537107644\n",
      "RNNA, rep: 2, epoch: 1091, acc: 0.9366666674613953, Loss 0.21890607296867529\n",
      "RNNA, rep: 2, epoch: 1092, acc: 0.9399999976158142, Loss 0.18182222879229812\n",
      "RNNA, rep: 2, epoch: 1093, acc: 0.8633334636688232, Loss 0.33062963694828795\n",
      "RNNA, rep: 2, epoch: 1094, acc: 0.8700001239776611, Loss 0.38969577746029244\n",
      "RNNA, rep: 2, epoch: 1095, acc: 0.9166667461395264, Loss 0.1966331654964597\n",
      "RNNA, rep: 2, epoch: 1096, acc: 0.8933334350585938, Loss 0.2809354281662672\n",
      "RNNA, rep: 2, epoch: 1097, acc: 0.9433332681655884, Loss 0.14397066721969168\n",
      "RNNA, rep: 2, epoch: 1098, acc: 0.9533333778381348, Loss 0.1912365043911268\n",
      "RNNA, rep: 2, epoch: 1099, acc: 0.9333333373069763, Loss 0.22388994764274683\n",
      "RNNA, rep: 2, epoch: 1100, acc: 0.9133334159851074, Loss 0.23981848791940139\n",
      "RNNA, rep: 2, epoch: 1101, acc: 0.9233332872390747, Loss 0.2095554250542773\n",
      "RNNA, rep: 2, epoch: 1102, acc: 0.9299999475479126, Loss 0.21031616617983673\n",
      "RNNA, rep: 2, epoch: 1103, acc: 0.940000057220459, Loss 0.1923734816547949\n",
      "RNNA, rep: 2, epoch: 1104, acc: 0.9233333468437195, Loss 0.23376933212974108\n",
      "RNNA, rep: 2, epoch: 1105, acc: 0.9233332276344299, Loss 0.24854223200498382\n",
      "RNNA, rep: 2, epoch: 1106, acc: 0.926666796207428, Loss 0.21045936495516798\n",
      "RNNA, rep: 2, epoch: 1107, acc: 0.903333306312561, Loss 0.295812445669435\n",
      "RNNA, rep: 2, epoch: 1108, acc: 0.9433333873748779, Loss 0.18778044062317348\n",
      "RNNA, rep: 2, epoch: 1109, acc: 0.9366666674613953, Loss 0.23986068858706858\n",
      "RNNA, rep: 2, epoch: 1110, acc: 0.9333334565162659, Loss 0.18738995140462067\n",
      "RNNA, rep: 2, epoch: 1111, acc: 0.9566667079925537, Loss 0.15442505354469177\n",
      "RNNA, rep: 2, epoch: 1112, acc: 0.8966666460037231, Loss 0.29822708624007643\n",
      "RNNA, rep: 2, epoch: 1113, acc: 0.8999999761581421, Loss 0.29341493245156014\n",
      "RNNA, rep: 2, epoch: 1114, acc: 0.8833332657814026, Loss 0.33323564772261305\n",
      "RNNA, rep: 2, epoch: 1115, acc: 0.9166666269302368, Loss 0.2362207860336639\n",
      "RNNA, rep: 2, epoch: 1116, acc: 0.9233333468437195, Loss 0.22732436874008272\n",
      "RNNA, rep: 2, epoch: 1117, acc: 0.9533333778381348, Loss 0.1547342044240213\n",
      "RNNA, rep: 2, epoch: 1118, acc: 0.9233332872390747, Loss 0.23075706522795372\n",
      "RNNA, rep: 2, epoch: 1119, acc: 0.940000057220459, Loss 0.19898195245245007\n",
      "RNNA, rep: 2, epoch: 1120, acc: 0.9399999976158142, Loss 0.21400294761435362\n",
      "RNNA, rep: 2, epoch: 1121, acc: 0.9133332967758179, Loss 0.23167472023560548\n",
      "RNNA, rep: 2, epoch: 1122, acc: 0.9100000262260437, Loss 0.25828658550104594\n",
      "RNNA, rep: 2, epoch: 1123, acc: 0.9233332276344299, Loss 0.24693405768164667\n",
      "RNNA, rep: 2, epoch: 1124, acc: 0.8899999856948853, Loss 0.3311085322272265\n",
      "RNNA, rep: 2, epoch: 1125, acc: 0.903333306312561, Loss 0.30533668698364635\n",
      "RNNA, rep: 2, epoch: 1126, acc: 0.9266666173934937, Loss 0.23020243839593604\n",
      "RNNA, rep: 2, epoch: 1127, acc: 0.8966667056083679, Loss 0.2522756081314583\n",
      "RNNA, rep: 2, epoch: 1128, acc: 0.90666663646698, Loss 0.25777188304520676\n",
      "RNNA, rep: 2, epoch: 1129, acc: 0.949999988079071, Loss 0.16439355968468589\n",
      "RNNA, rep: 2, epoch: 1130, acc: 0.9100000262260437, Loss 0.2443350691741216\n",
      "RNNA, rep: 2, epoch: 1131, acc: 0.9499998688697815, Loss 0.19068317984383612\n",
      "RNNA, rep: 2, epoch: 1132, acc: 0.9166666269302368, Loss 0.2212616423415602\n",
      "RNNA, rep: 2, epoch: 1133, acc: 0.9333333373069763, Loss 0.17264685417700093\n",
      "RNNA, rep: 2, epoch: 1134, acc: 0.9233333468437195, Loss 0.2310752943827538\n",
      "RNNA, rep: 2, epoch: 1135, acc: 0.9100001454353333, Loss 0.2583532472507795\n",
      "RNNA, rep: 2, epoch: 1136, acc: 0.9466665387153625, Loss 0.1990310356375994\n",
      "RNNA, rep: 2, epoch: 1137, acc: 0.8933331966400146, Loss 0.33062525059678594\n",
      "RNNA, rep: 2, epoch: 1138, acc: 0.9066665768623352, Loss 0.31940487289684827\n",
      "RNNA, rep: 2, epoch: 1139, acc: 0.9100000262260437, Loss 0.2831876253784867\n",
      "RNNA, rep: 2, epoch: 1140, acc: 0.9300000667572021, Loss 0.21554545491555474\n",
      "RNNA, rep: 2, epoch: 1141, acc: 0.8966667056083679, Loss 0.30230507267202483\n",
      "RNNA, rep: 2, epoch: 1142, acc: 0.8600000739097595, Loss 0.3895370948454365\n",
      "RNNA, rep: 2, epoch: 1143, acc: 0.9066666960716248, Loss 0.27780118308379315\n",
      "RNNA, rep: 2, epoch: 1144, acc: 0.8899999260902405, Loss 0.3215672944209655\n",
      "RNNA, rep: 2, epoch: 1145, acc: 0.9433333873748779, Loss 0.17856014288714506\n",
      "RNNA, rep: 2, epoch: 1146, acc: 0.893333375453949, Loss 0.2957965011024498\n",
      "RNNA, rep: 2, epoch: 1147, acc: 0.9100000858306885, Loss 0.24242290670110378\n",
      "RNNA, rep: 2, epoch: 1148, acc: 0.8699999451637268, Loss 0.31821985380069234\n",
      "RNNA, rep: 2, epoch: 1149, acc: 0.9099999070167542, Loss 0.2902131642200402\n",
      "RNNA, rep: 2, epoch: 1150, acc: 0.9200000166893005, Loss 0.22464335827608012\n",
      "RNNA, rep: 2, epoch: 1151, acc: 0.9000000953674316, Loss 0.28499948484532067\n",
      "RNNA, rep: 2, epoch: 1152, acc: 0.9233334064483643, Loss 0.24897945929551496\n",
      "RNNA, rep: 2, epoch: 1153, acc: 0.8966666460037231, Loss 0.29479191358201207\n",
      "RNNA, rep: 2, epoch: 1154, acc: 0.8666666150093079, Loss 0.35519343710155227\n",
      "RNNA, rep: 2, epoch: 1155, acc: 0.8966666460037231, Loss 0.2586898784592631\n",
      "RNNA, rep: 2, epoch: 1156, acc: 0.9333332777023315, Loss 0.17988780463318108\n",
      "RNNA, rep: 2, epoch: 1157, acc: 0.9300001263618469, Loss 0.19132454535953003\n",
      "RNNA, rep: 2, epoch: 1158, acc: 0.9133334159851074, Loss 0.22687784972280498\n",
      "RNNA, rep: 2, epoch: 1159, acc: 0.9133333563804626, Loss 0.26304841574281457\n",
      "RNNA, rep: 2, epoch: 1160, acc: 0.9333333373069763, Loss 0.17986011332570342\n",
      "RNNA, rep: 2, epoch: 1161, acc: 0.9433333873748779, Loss 0.18280899581790436\n",
      "RNNA, rep: 2, epoch: 1162, acc: 0.8766667246818542, Loss 0.3296701990213478\n",
      "RNNA, rep: 2, epoch: 1163, acc: 0.9166667461395264, Loss 0.18527116643555927\n",
      "RNNA, rep: 2, epoch: 1164, acc: 0.93666672706604, Loss 0.17680453085660702\n",
      "RNNA, rep: 2, epoch: 1165, acc: 0.9266665577888489, Loss 0.22324828724464169\n",
      "RNNA, rep: 2, epoch: 1166, acc: 0.9133334159851074, Loss 0.26117498472885925\n",
      "RNNA, rep: 2, epoch: 1167, acc: 0.9466666579246521, Loss 0.1500367256658501\n",
      "RNNA, rep: 2, epoch: 1168, acc: 0.9333333373069763, Loss 0.20756171217391967\n",
      "RNNA, rep: 2, epoch: 1169, acc: 0.9200000166893005, Loss 0.21097114956472068\n",
      "RNNA, rep: 2, epoch: 1170, acc: 0.873333215713501, Loss 0.3345671859536378\n",
      "RNNA, rep: 2, epoch: 1171, acc: 0.90666663646698, Loss 0.26549075342918516\n",
      "RNNA, rep: 2, epoch: 1172, acc: 0.8966666460037231, Loss 0.30290091377712086\n",
      "RNNA, rep: 2, epoch: 1173, acc: 0.9333333373069763, Loss 0.20711204598192126\n",
      "RNNA, rep: 2, epoch: 1174, acc: 0.8800000548362732, Loss 0.3118803136100178\n",
      "RNNA, rep: 2, epoch: 1175, acc: 0.8799999356269836, Loss 0.3073429677748936\n",
      "RNNA, rep: 2, epoch: 1176, acc: 0.8699999451637268, Loss 0.3837257558503188\n",
      "RNNA, rep: 2, epoch: 1177, acc: 0.940000057220459, Loss 0.19927019037175342\n",
      "RNNA, rep: 2, epoch: 1178, acc: 0.9399999976158142, Loss 0.18739512146406923\n",
      "RNNA, rep: 2, epoch: 1179, acc: 0.9300000071525574, Loss 0.1876651479312568\n",
      "RNNA, rep: 2, epoch: 1180, acc: 0.9233333468437195, Loss 0.1765456668823026\n",
      "RNNA, rep: 2, epoch: 1181, acc: 0.9233332872390747, Loss 0.24823989896132845\n",
      "RNNA, rep: 2, epoch: 1182, acc: 0.8900001049041748, Loss 0.2773611242818879\n",
      "RNNA, rep: 2, epoch: 1183, acc: 0.9299999475479126, Loss 0.24380549896799494\n",
      "RNNA, rep: 2, epoch: 1184, acc: 0.9466667175292969, Loss 0.15908048977202272\n",
      "RNNA, rep: 2, epoch: 1185, acc: 0.926666796207428, Loss 0.21507355336390901\n",
      "RNNA, rep: 2, epoch: 1186, acc: 0.9333332777023315, Loss 0.18563727714383277\n",
      "RNNA, rep: 2, epoch: 1187, acc: 0.9033333659172058, Loss 0.2867520236136625\n",
      "RNNA, rep: 2, epoch: 1188, acc: 0.8799999952316284, Loss 0.34541046914528123\n",
      "RNNA, rep: 2, epoch: 1189, acc: 0.9099999070167542, Loss 0.26967356733453923\n",
      "RNNA, rep: 2, epoch: 1190, acc: 0.9233334064483643, Loss 0.20212836210645035\n",
      "RNNA, rep: 2, epoch: 1191, acc: 0.9100000262260437, Loss 0.2616418259663624\n",
      "RNNA, rep: 2, epoch: 1192, acc: 0.9599999785423279, Loss 0.1562530684801459\n",
      "RNNA, rep: 2, epoch: 1193, acc: 0.9333332777023315, Loss 0.18169868913057144\n",
      "RNNA, rep: 2, epoch: 1194, acc: 0.9066668152809143, Loss 0.25531023721501694\n",
      "RNNA, rep: 2, epoch: 1195, acc: 0.9066666960716248, Loss 0.2646968380347243\n",
      "RNNA, rep: 2, epoch: 1196, acc: 0.903333306312561, Loss 0.29222164581064136\n",
      "RNNA, rep: 2, epoch: 1197, acc: 0.9333332777023315, Loss 0.18679216534364967\n",
      "RNNA, rep: 2, epoch: 1198, acc: 0.9333332777023315, Loss 0.19057022702530957\n",
      "RNNA, rep: 2, epoch: 1199, acc: 0.9333333373069763, Loss 0.1864888332472765\n",
      "RNNA, rep: 2, epoch: 1200, acc: 0.8866665363311768, Loss 0.31811418929893986\n",
      "RNNA, rep: 2, epoch: 1201, acc: 0.8700000047683716, Loss 0.3418698708894954\n",
      "RNNA, rep: 2, epoch: 1202, acc: 0.9000000953674316, Loss 0.26525252061197535\n",
      "RNNA, rep: 2, epoch: 1203, acc: 0.9200000762939453, Loss 0.23922213978046783\n",
      "RNNA, rep: 2, epoch: 1204, acc: 0.9300000667572021, Loss 0.2384141850634478\n",
      "RNNA, rep: 2, epoch: 1205, acc: 0.9166666269302368, Loss 0.23162779194768518\n",
      "RNNA, rep: 2, epoch: 1206, acc: 0.9233332276344299, Loss 0.23306025746220257\n",
      "RNNA, rep: 2, epoch: 1207, acc: 0.9266664981842041, Loss 0.2078270343434997\n",
      "RNNA, rep: 2, epoch: 1208, acc: 0.916666567325592, Loss 0.2410364639761974\n",
      "RNNA, rep: 2, epoch: 1209, acc: 0.9500001668930054, Loss 0.20366932548044134\n",
      "RNNA, rep: 2, epoch: 1210, acc: 0.9033334255218506, Loss 0.24858711353415855\n",
      "RNNA, rep: 2, epoch: 1211, acc: 0.9433333873748779, Loss 0.1727905405306956\n",
      "RNNA, rep: 2, epoch: 1212, acc: 0.9333334565162659, Loss 0.19766467979585287\n",
      "RNNA, rep: 2, epoch: 1213, acc: 0.9433333873748779, Loss 0.17846732996433276\n",
      "RNNA, rep: 2, epoch: 1214, acc: 0.8966667056083679, Loss 0.3336860159084608\n",
      "RNNA, rep: 2, epoch: 1215, acc: 0.9499999284744263, Loss 0.19663732050539692\n",
      "RNNA, rep: 2, epoch: 1216, acc: 0.8833333849906921, Loss 0.37292877404419417\n",
      "RNNA, rep: 2, epoch: 1217, acc: 0.9233332872390747, Loss 0.19671535711328034\n",
      "RNNA, rep: 2, epoch: 1218, acc: 0.8633332848548889, Loss 0.3845534575216152\n",
      "RNNA, rep: 2, epoch: 1219, acc: 0.9200000762939453, Loss 0.23003922660063836\n",
      "RNNA, rep: 2, epoch: 1220, acc: 0.9200000166893005, Loss 0.1922311881725909\n",
      "RNNA, rep: 2, epoch: 1221, acc: 0.9366666674613953, Loss 0.24637538050534202\n",
      "RNNA, rep: 2, epoch: 1222, acc: 0.9366665482521057, Loss 0.19345497410395182\n",
      "RNNA, rep: 2, epoch: 1223, acc: 0.9200000166893005, Loss 0.19939518948973273\n",
      "RNNA, rep: 2, epoch: 1224, acc: 0.9766666889190674, Loss 0.08358540442088269\n",
      "RNNA                 Rep: 2   Epoch: 1     Acc: 0.9767 _min_10_max_10 Time: 96.90 sec\n"
     ]
    }
   ],
   "source": [
    "collectorA = dict()\n",
    "#min_lengths = [5, 10, 10, 20, 20, 40, 40, 70,100]\n",
    "#max_lengths = [5, 10, 15, 20, 25, 40, 45,75,140]\n",
    "num_samples = 100\n",
    "min_lengths = [10]\n",
    "max_lengths = [10]\n",
    "focus = 'Top'\n",
    "for rep in range(3):  # Number of repetitions\n",
    "    for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "        # Select the model based on 'kind'\n",
    "        #for kind in [\"RNN\", \"RNNA\",\"LSTM\", \"LSTMA\", \"GRU\", \"GRUA\"]:\n",
    "        for kind in [\"RNNA\"]:\n",
    "        # Select the model based on 'kind'\n",
    "            if kind == \"RNN\":\n",
    "                model = NetRNN(hidden_dim=12, inp=3)\n",
    "            elif kind == \"RNNA\":\n",
    "                model = NetRNNWithAttention(hidden_dim=12, inp=3)\n",
    "            elif kind == \"RNNE\":\n",
    "                model = NetRNNWithAttentionExpFirst(hidden_dim=12, inp=3)\n",
    "            elif kind == \"LSTM\":\n",
    "                model = NetLSTM(hidden_dim=12, inp=3)\n",
    "            elif kind == \"LSTMA\":\n",
    "                model = NetLSTMWithAttention(hidden_dim=12, inp=3)\n",
    "            elif kind == \"LSTME\":\n",
    "                model = NetLSTMWithAttentionExpFirst(hidden_dim=12, inp=3)\n",
    "            elif kind == \"GRU\":\n",
    "                model = NetGRU(hidden_dim=12, inp=3)\n",
    "            elif kind == \"GRUA\":\n",
    "                model = NetGRUMWithAttention(hidden_dim=12, inp=3)\n",
    "            elif kind == \"GRUE\":\n",
    "                model = NetGRUMWithAttentionExpFirst(hidden_dim=12, inp=3)\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters())\n",
    "            criterion = nn.MSELoss()\n",
    "            acc = 0.0\n",
    "            W = []\n",
    "            AC = []\n",
    "            start_time = time.time()  # Start time of the epoch\n",
    "\n",
    "            while True:\n",
    "                sequences, targets = generateTrainDataNoiseTop(num_samples, [min_len,max_len],0.5)\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                count = 0\n",
    "\n",
    "                for seq, target in zip(sequences, targets):\n",
    "                    optimizer.zero_grad()\n",
    "                    seq_tensor = torch.Tensor([seq])  # Add an extra dimension for batch\n",
    "                    target_tensor = torch.Tensor([target])\n",
    "\n",
    "                    output = model(seq_tensor)\n",
    "                    loss = criterion(output, target_tensor)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    A = 1.0 * (target_tensor.detach() > 0.0)\n",
    "                    B = 1.0 * (output.detach() > 0.0)\n",
    "                    acc = (1.0 * (A.flatten() == B.flatten())).mean()\n",
    "                    total_acc += acc\n",
    "                    count += 1\n",
    "\n",
    "                avg_loss = total_loss / count\n",
    "                avg_acc = total_acc / count\n",
    "                W.append(avg_loss)\n",
    "                AC.append(avg_acc)\n",
    "                print(f\"{kind}, rep: {rep}, epoch: {len(AC)}, acc: {avg_acc}, Loss {avg_loss}\")\n",
    "\n",
    "                # Check for stopping condition\n",
    "                if avg_acc >= 0.97 or len(W)>2000:\n",
    "                    break\n",
    "\n",
    "            end_time = time.time()  # End time of the epoch\n",
    "            epoch_duration = end_time - start_time  # Calculate duration\n",
    "            collectorA[f\"{kind}_rep_{rep}_min_{min_len} max_{max_len}\"] = AC\n",
    "            torch.save(model, f'./ModelsWithNoise/model_{kind}_{focus}_min_{min_len}_max_{max_len}_rep_{rep}.model')\n",
    "            print(f\"{kind:<20} Rep: {rep:<3} Epoch: {len(A):<5} Acc: {avg_acc:.4f} \" f\"_min_{min_len}_max_{max_len} Time: {epoch_duration:.2f} sec\")\n",
    "            df=pd.DataFrame()\n",
    "            df[\"accuracy\"]=AC\n",
    "            df[\"loss\"]=W\n",
    "            df.to_csv(f'./ModelsWithNoise/score_{kind}_{focus}_min_{min_len}_max_{max_len}_rep_{rep}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize model based on 'kind'\n",
    "def initialize_model(kind, hidden_dim, inp):\n",
    "    if kind == \"RNN\":\n",
    "        return NetRNN(hidden_dim=hidden_dim, inp=inp)\n",
    "    elif kind == \"RNNA\":\n",
    "        return NetRNNWithAttention(hidden_dim=hidden_dim, inp=inp)\n",
    "    # Add other initializations as needed\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(target_tensor, output):\n",
    "    A = 1.0 * (target_tensor.detach() > 0.0)\n",
    "    B = 1.0 * (output.detach() > 0.0)\n",
    "    return (1.0 * (A.flatten() == B.flatten())).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collector\n",
    "collectorA = dict()\n",
    "num_samples = 100\n",
    "min_lengths = [10]\n",
    "max_lengths = [10]\n",
    "focus = 'Top'\n",
    "\n",
    "for rep in range(1):  # Number of repetitions\n",
    "    for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "        for kind in [\"RNNA\"]:  # Use the specific models you're interested in\n",
    "            # Initialize the appropriate model based on 'kind'\n",
    "            model = initialize_model(kind, hidden_dim=12, inp=3)\n",
    "\n",
    "            # Save the uninitialized model\n",
    "            torch.save(model, f'./ModelsWithNoise/untrained_model_{kind}_{focus}_min_{min_len}_max_{max_len}_rep_{rep}.model')\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters())\n",
    "            criterion = nn.MSELoss()\n",
    "            \n",
    "            # Generate the same dataset for training comparison\n",
    "            sequences, targets = generateTrainDataNoiseTop(num_samples, [min_len,max_len])\n",
    "\n",
    "            W = []\n",
    "            AC = []\n",
    "            start_time = time.time()  # Start time of the epoch\n",
    "\n",
    "            epoch = 0  # Initialize epoch counter\n",
    "            while True:\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                count = 0\n",
    "\n",
    "                for seq, target in zip(sequences, targets):\n",
    "                    optimizer.zero_grad()\n",
    "                    seq_tensor = torch.Tensor([seq])  # Add an extra dimension for batch\n",
    "                    target_tensor = torch.Tensor([target])\n",
    "\n",
    "                    output = model(seq_tensor)\n",
    "                    loss = criterion(output, target_tensor)\n",
    "                    total_loss += loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    acc = calculate_accuracy(target_tensor, output)\n",
    "                    total_acc += acc\n",
    "                    count += 1\n",
    "\n",
    "                avg_loss = total_loss / count\n",
    "                avg_acc = total_acc / count\n",
    "                W.append(avg_loss)\n",
    "                AC.append(avg_acc)\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    # Save the model every 5 epochs\n",
    "                    torch.save(model, f'./ModelsWithNoise/model_{kind}_{focus}_min_{min_len}_max_{max_len}_rep_{rep}_epoch_{epoch}.model')\n",
    "\n",
    "                epoch += 1  # Increment epoch counter\n",
    "\n",
    "                if avg_acc >= 0.97 or epoch > 2000:\n",
    "                    break\n",
    "\n",
    "            # Save the final model\n",
    "            torch.save(model, f'./ModelsWithNoise/final_model_{kind}_{focus}_min_{min_len}_max_{max_len}_rep_{rep}.model')\n",
    "\n",
    "            end_time = time.time()  # End time of the epoch\n",
    "            collectorA[f\"{kind}_rep_{rep}_min_{min_len} max_{max_len}\"] = AC\n",
    "\n",
    "            # Save the training statistics\n",
    "            df = pd.DataFrame({'accuracy': AC, 'loss': W})\n",
    "            df.to_csv(f'./ModelsWithNoise/score_{kind}_{focus}_min_{min_len}_max_{max_len}_rep_{rep}.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        scores = torch.bmm(Q, K.transpose(1, 2)) / np.sqrt(self.input_dim)\n",
    "        attention = self.softmax(scores)\n",
    "        output = torch.bmm(attention, V)\n",
    "\n",
    "        if return_attention:\n",
    "            return output, Q, K, scores, attention, V\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './ModelsWithNoise/model_RNNA_Top_min_10_max_10_rep_0.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[251], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ModelsWithNoise/model_RNNA_Top_min_10_max_10_rep_0.model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m modelRNNWithAttention \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './ModelsWithNoise/model_RNNA_Top_min_10_max_10_rep_0.model'"
     ]
    }
   ],
   "source": [
    "model_path = \"./ModelsWithNoise/model_RNNA_Top_min_10_max_10_rep_0.model\"\n",
    "modelRNNWithAttention = torch.load(model_path, map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of the modified SelfAttention\n",
    "input_dim = modelRNNWithAttention.attention.input_dim  # You need to ensure this attribute exists or adjust accordingly\n",
    "new_attention_layer = SelfAttention(input_dim)\n",
    "\n",
    "\n",
    "# Replace the old attention layer with the new one\n",
    "modelRNNWithAttention.attention = new_attention_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "s,t = generateTrainDataNoiseTop(100, [10,10]) \n",
    "s_tensor = torch.Tensor(s)\n",
    "weighted_output, Q, K, scores, attention, V = modelRNNWithAttention.attention(s_tensor, return_attention=True)\n",
    "\n",
    "weighted_output_np = [sequence.detach().numpy() for sequence in weighted_output]\n",
    "# Plot preparation\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Updated color labels to reflect the logic more accurately\n",
    "color_labels = {\n",
    "    'green': 'Pos. value in A & Neg. value in B',\n",
    "    'black': 'Neg. value in A & Pos. value in B',\n",
    "    'blue': 'Pos. values in both A and B',\n",
    "    'red': 'Neg. values in both A and B'\n",
    "}\n",
    "\n",
    "# Iterate through each sequence and its corresponding weighted output\n",
    "for seq, weighted_output in zip(s, weighted_output_np):\n",
    "    # Initially, set colors to an empty list to catch errors if none of the conditions below are met\n",
    "    colors = []\n",
    "\n",
    "    # Check the specific values at index 0 for Array A and index 1 for Array B\n",
    "    value_A = seq[0, 0]  # Value at index 0 in Array A\n",
    "    value_B = seq[1, 1]  # Value at index 1 in Array B\n",
    "\n",
    "    # Assign colors based on the values in A[0] and B[1]\n",
    "    if value_A > 0 and value_B < 0:\n",
    "        colors = ['green', 'green']  # Pos. in A & Neg. in B\n",
    "    elif value_A < 0 and value_B > 0:\n",
    "        colors = ['black', 'black']  # Neg. in A & Pos. in B\n",
    "    elif value_A > 0 and value_B > 0:\n",
    "        colors = ['blue', 'blue']  # Pos. in both A and B\n",
    "    elif value_A < 0 and value_B < 0:\n",
    "        colors = ['red', 'red']  # Neg. in both A and B\n",
    "\n",
    "    last_two_weighted_output = weighted_output[-2:, :2]\n",
    "    # Plot the last two weighted output values with determined colors\n",
    "    for point, color in zip(last_two_weighted_output, colors):\n",
    "        plt.scatter(point[0], point[1], alpha=0.6, color=color, label=color_labels[color] if color_labels[color] not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "plt.title('Scatter Plot for Top focus labels with Color Coding Based on Values in A[0] and B[1]')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./ModelsWithNoise/untrained_model_RNNA_Top_min_10_max_10_rep_0.model\"\n",
    "modelRNNWithAttention = torch.load(model_path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "s,t = generateTrainDataNoiseTop(100, [10,10]) \n",
    "s_tensor = torch.Tensor(s)\n",
    "weighted_output, Q, K, scores, attention, V = modelRNNWithAttention.attention(s_tensor, return_attention=True)\n",
    "\n",
    "weighted_output_np = [sequence.detach().numpy() for sequence in weighted_output]\n",
    "# Plot preparation\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Updated color labels to reflect the logic more accurately\n",
    "color_labels = {\n",
    "    'green': 'Pos. value in A & Neg. value in B',\n",
    "    'black': 'Neg. value in A & Pos. value in B',\n",
    "    'blue': 'Pos. values in both A and B',\n",
    "    'red': 'Neg. values in both A and B'\n",
    "}\n",
    "\n",
    "# Iterate through each sequence and its corresponding weighted output\n",
    "for seq, weighted_output in zip(s, weighted_output_np):\n",
    "    # Initially, set colors to an empty list to catch errors if none of the conditions below are met\n",
    "    colors = []\n",
    "\n",
    "    # Check the specific values at index 0 for Array A and index 1 for Array B\n",
    "    value_A = seq[0, 0]  # Value at index 0 in Array A\n",
    "    value_B = seq[1, 1]  # Value at index 1 in Array B\n",
    "\n",
    "    # Assign colors based on the values in A[0] and B[1]\n",
    "    if value_A > 0 and value_B < 0:\n",
    "        colors = ['green', 'green']  # Pos. in A & Neg. in B\n",
    "    elif value_A < 0 and value_B > 0:\n",
    "        colors = ['black', 'black']  # Neg. in A & Pos. in B\n",
    "    elif value_A > 0 and value_B > 0:\n",
    "        colors = ['blue', 'blue']  # Pos. in both A and B\n",
    "    elif value_A < 0 and value_B < 0:\n",
    "        colors = ['red', 'red']  # Neg. in both A and B\n",
    "\n",
    "    last_two_weighted_output = weighted_output[-2:, :2]\n",
    "    # Plot the last two weighted output values with determined colors\n",
    "    for point, color in zip(last_two_weighted_output, colors):\n",
    "        plt.scatter(point[0], point[1], alpha=0.6, color=color, label=color_labels[color] if color_labels[color] not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "plt.title('Scatter Plot for Top focus labels with Color Coding Based on Values in A and B for untrained RNNA model')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig(f\"./ModelsWithNoise/untrained_model_RNNA_Top_min_10_max_10_rep_0.png\")\n",
    "plt.close()  # Close the plot to free memory for the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [10,100,200 ,300,400,500,700,840]:\n",
    "    model_path = f\"./ModelsWithNoise/model_RNNA_Top_min_10_max_10_rep_0_epoch_{i}.model\"\n",
    "    modelRNNWithAttention = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    s,t = generateTrainDataNoiseTop(100, [10,10]) \n",
    "    s_tensor = torch.Tensor(s)\n",
    "    weighted_output, Q, K, scores, attention, V = modelRNNWithAttention.attention(s_tensor, return_attention=True)\n",
    "\n",
    "    weighted_output_np = [sequence.detach().numpy() for sequence in weighted_output]\n",
    "    # Plot preparation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Updated color labels to reflect the logic more accurately\n",
    "    color_labels = {\n",
    "        'green': 'Pos. value in A & Neg. value in B',\n",
    "        'black': 'Neg. value in A & Pos. value in B',\n",
    "        'blue': 'Pos. values in both A and B',\n",
    "        'red': 'Neg. values in both A and B'\n",
    "    }\n",
    "\n",
    "    # Iterate through each sequence and its corresponding weighted output\n",
    "    for seq, weighted_output in zip(s, weighted_output_np):\n",
    "        # Initially, set colors to an empty list to catch errors if none of the conditions below are met\n",
    "        colors = []\n",
    "\n",
    "        # Check the specific values at index 0 for Array A and index 1 for Array B\n",
    "        value_A = seq[0, 0]  # Value at index 0 in Array A\n",
    "        value_B = seq[1, 1]  # Value at index 1 in Array B\n",
    "\n",
    "        # Assign colors based on the values in A[0] and B[1]\n",
    "        if value_A > 0 and value_B < 0:\n",
    "            colors = ['green', 'green']  # Pos. in A & Neg. in B\n",
    "        elif value_A < 0 and value_B > 0:\n",
    "            colors = ['black', 'black']  # Neg. in A & Pos. in B\n",
    "        elif value_A > 0 and value_B > 0:\n",
    "            colors = ['blue', 'blue']  # Pos. in both A and B\n",
    "        elif value_A < 0 and value_B < 0:\n",
    "            colors = ['red', 'red']  # Neg. in both A and B\n",
    "\n",
    "        last_two_weighted_output = weighted_output[-2:, :2]\n",
    "        # Plot the last two weighted output values with determined colors\n",
    "        for point, color in zip(last_two_weighted_output, colors):\n",
    "            plt.scatter(point[0], point[1], alpha=0.6, color=color, label=color_labels[color] if color_labels[color] not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "    plt.title(f'Scatter Plot for Top focus labels with Color Coding Based on Values in A and B for trained RNNA epoch {i}')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    #plt.show()\n",
    "    plt.savefig(f\"./ModelsWithNoise/RNNA_epoch_{i}_scatter_plot.png\")\n",
    "    plt.close()  # Close the plot to free memory for the next iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
