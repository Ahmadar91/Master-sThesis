{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:31.277897100Z",
     "start_time": "2023-12-06T17:00:30.656125100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\magics\\pylab.py:162: UserWarning: pylab import has clobbered these variables: ['mean', 'copy']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:39.317492Z",
     "start_time": "2023-12-06T17:00:34.720993Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import numpy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib.pyplot import figure, subplots, imshow, xticks, yticks, title\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.cluster import mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "from statistics import mean\n",
    "from scipy.stats import entropy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:40.939651100Z",
     "start_time": "2023-12-06T17:00:40.913651900Z"
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim**0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted\n",
    "\n",
    "\n",
    "class NetRNNWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3, out = 3):\n",
    "        super(NetRNNWithAttention, self).__init__()\n",
    "        self.attention = SelfAttention(inp)  # Attention layer with input dimension\n",
    "        self.rnnLayer = nn.RNN(inp, hidden_dim, batch_first=True)\n",
    "        self.outputLayer = nn.Linear(hidden_dim, out)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.resetHidden()\n",
    "        self.inp = inp\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applying attention first\n",
    "        attn_out = self.attention(x)\n",
    "        # Feeding the output of the attention layer into the RNN\n",
    "        h0 = torch.zeros(1, x.shape[0], self.hidden_dim)\n",
    "        rnn_out, _ = self.rnnLayer(attn_out, h0)\n",
    "        rnn_out = torch.tanh(rnn_out)\n",
    "\n",
    "        # Applying the final output layer\n",
    "        out = torch.sigmoid(self.outputLayer(rnn_out[:, -1, :])).squeeze()\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                # Feeding the output of the attention layer into the RNN\n",
    "                out, h0 = self.rnnLayer(attn_out, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)\n",
    "                out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "\n",
    "\n",
    "model = NetRNNWithAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:41.605650400Z",
     "start_time": "2023-12-06T17:00:41.594151100Z"
    }
   },
   "outputs": [],
   "source": [
    "class NetRNN(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3, out = 3):\n",
    "        super(NetRNN, self).__init__()\n",
    "        self.rnnLayer = nn.RNN(inp, hidden_dim, batch_first=True)\n",
    "        self.outputLayer = nn.Linear(hidden_dim, out)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.resetHidden()\n",
    "        self.inp = inp\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h0 = torch.Tensor(numpy.zeros((1, x.shape[0], self.hidden_dim)))\n",
    "        out, self.h0 = self.rnnLayer(x, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "        out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.Tensor(numpy.zeros((1, 1, self.hidden_dim)))\n",
    "            for i in range(x.shape[1]):\n",
    "                out, h0 = self.rnnLayer(x[l][i].reshape((1, 1, self.inp)), h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "                # print(out.detach().numpy().flatten().shape)\n",
    "            out = torch.tanh(out)\n",
    "            out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "            for i in range(x.shape[1]):\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "                # print(out.detach().numpy().flatten().shape)\n",
    "        return numpy.array(O), numpy.array(H)\n",
    "\n",
    "\n",
    "model = NetRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:23.990361100Z",
     "start_time": "2023-12-06T18:19:23.969358500Z"
    }
   },
   "outputs": [],
   "source": [
    "def generateTrainData(num_samples, params):\n",
    "    s = []  # Sequences\n",
    "    t = []  # Labels\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        common_length = np.random.randint(params[\"min_length\"], params[\"max_length\"] + 1)\n",
    "\n",
    "        array_A = np.full(common_length, params[\"fill\"])\n",
    "        array_B = np.full(common_length, params[\"fill\"])\n",
    "        array_C = np.full(common_length, params[\"fill\"])\n",
    "\n",
    "        # Exclude the last two indices\n",
    "        possible_indices = np.arange(common_length - 2)\n",
    "\n",
    "        index_A = np.random.choice(possible_indices)\n",
    "        value_A = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_A[index_A] = value_A\n",
    "\n",
    "        # Update possible indices for array B to also exclude index_A\n",
    "        possible_indices_B = np.delete(possible_indices, np.where(possible_indices == index_A))\n",
    "        index_B = np.random.choice(possible_indices_B)\n",
    "        value_B = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_B[index_B] = value_B\n",
    "\n",
    "        value_C = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_C[-1] = value_C\n",
    "        array_C[-2] = value_C\n",
    "\n",
    "        mapped_value_A = 1 if value_A == params[\"value_2\"] else 0\n",
    "        mapped_value_B = 1 if value_B == params[\"value_2\"] else 0\n",
    "        result = int((mapped_value_A != mapped_value_B) if value_C == params[\"value_1\"] else (mapped_value_A == mapped_value_B))\n",
    "\n",
    "        # Mapping back to original value_1 and value_2 for the label\n",
    "        label_value_A = params[\"value_2\"] if mapped_value_A == 1 else params[\"value_1\"]\n",
    "        label_value_B = params[\"value_2\"] if mapped_value_B == 1 else params[\"value_1\"]\n",
    "\n",
    "        label_arr = [mapped_value_A, mapped_value_B, result]  # Label array with value_A, value_B, and result\n",
    "\n",
    "        combined_array = np.vstack([array_A, array_B, array_C]).T\n",
    "        s.append(combined_array)\n",
    "        t.append(label_arr)\n",
    "\n",
    "    return s, np.array(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:26.878858500Z",
     "start_time": "2023-12-06T18:19:26.864361100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "[array([[ 0,  0,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 1,  0,  0],\n",
      "       [ 0, -1,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0, -1],\n",
      "       [ 0,  0, -1]]), array([[ 1,  0,  0],\n",
      "       [ 0,  1,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0, -1],\n",
      "       [ 0,  0, -1]])]\n",
      "[[1 0 1]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "num_seq = 2\n",
    "# Example dictionary with parameters\n",
    "parameters = {\"min_length\": 5, \"max_length\": 10, \"fill\": 0, \"value_1\": -1, \"value_2\": 1}\n",
    "\n",
    "sequences, labels = generateTrainData(num_seq, parameters)\n",
    "print(sequences)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:28.905358900Z",
     "start_time": "2023-12-06T18:19:28.879361600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "tensor([[[ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.],\n",
      "         [ 0.,  0.,  1.],\n",
      "         [ 0.,  0.,  1.]],\n",
      "\n",
      "        [[-1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0., -1.],\n",
      "         [ 0.,  0., -1.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [-1.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  1.],\n",
      "         [ 0.,  0.,  1.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0., -1.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 1.,  0.,  0.],\n",
      "         [ 0.,  0.,  1.],\n",
      "         [ 0.,  0.,  1.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [-1.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  1.],\n",
      "         [ 0.,  0.,  1.]]])\n",
      "torch.Size([5, 3])\n",
      "NetRNN(\n",
      "  (rnnLayer): RNN(3, 12, batch_first=True)\n",
      "  (outputLayer): Linear(in_features=12, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"min_length\": 10,\n",
    "    \"max_length\": 10,\n",
    "    \"fill\": 0,\n",
    "    \"value_1\": -1,\n",
    "    \"value_2\": 1,\n",
    "}\n",
    "sequences, labels = generateTrainData(5, parameters)\n",
    "model = NetRNN()\n",
    "output=model(torch.Tensor(sequences))\n",
    "print(torch.Tensor(sequences))\n",
    "print(output.shape)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:03:38.917270600Z",
     "start_time": "2023-12-06T18:03:36.540272600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_25736\\1604298959.py:31: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0, 0.9, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_sequences(sequences, labels):\n",
    "    num_samples = len(sequences)  # Number of samples to display\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(3 * num_samples, 15))\n",
    "\n",
    "    for i, (seq, label) in enumerate(zip(sequences, labels)):\n",
    "        reshaped_sequence = seq  # Use the sequence as it is\n",
    "\n",
    "        ax = plt.subplot(1, num_samples, i + 1)\n",
    "        img = ax.imshow(\n",
    "            reshaped_sequence, cmap=\"gray\", vmin=-1.0, vmax=1.0\n",
    "        )  # Adjusted vmin and vmax\n",
    "\n",
    "        operation_title = \"XOR\" if label[2] == 0 else \"XNOR\"\n",
    "        ax.set_title(f\"Operation: {operation_title}, {label}\")\n",
    "\n",
    "        ax.set_xlabel(\"Arrays (A, B, C)\")\n",
    "        ax.set_ylabel(\"Time Points\")\n",
    "        ax.set_xticks(range(3))\n",
    "        ax.set_xticklabels([\"A\", \"B\", \"C\"])\n",
    "        ax.set_yticks(range(reshaped_sequence.shape[0]))\n",
    "        ax.set_yticklabels([f\"{j+1}\" for j in range(reshaped_sequence.shape[0])])\n",
    "\n",
    "    # Adjusted positioning of colorbar\n",
    "    cbar_ax = plt.gcf().add_axes([0.93, 0.15, 0.02, 0.7])\n",
    "    cbar = plt.colorbar(img, cax=cbar_ax)\n",
    "    cbar.set_ticks([-1, 0, 1])\n",
    "    cbar.set_ticklabels([\"-1\", \"0\", \"1\"])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming sequences and labels are already generated using generateTrainData\n",
    "plot_sequences(sequences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:01:02.144838900Z",
     "start_time": "2023-12-06T17:01:02.130460400Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters_list = []\n",
    "\n",
    "min_lengths = [5, 10, 10, 20, 20, 40, 40]\n",
    "max_lengths = [5, 10, 15, 20, 25, 40, 45]\n",
    "\n",
    "for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "    parameters_list.append(\n",
    "        {\n",
    "            \"min_length\": min_len,\n",
    "            \"max_length\": max_len,\n",
    "            \"fill\": 0,\n",
    "            \"value_1\": -1,\n",
    "            \"value_2\": 1,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:44.759873Z",
     "start_time": "2023-12-06T18:19:42.281374100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN, rep: 0, epoch: 1, acc: 0.503180809020996\n",
      "RNN, rep: 0, epoch: 2, acc: 0.5065082842111588\n",
      "RNN, rep: 0, epoch: 3, acc: 0.5084999871253967\n",
      "RNN, rep: 0, epoch: 4, acc: 0.5308729633688927\n",
      "RNN, rep: 0, epoch: 5, acc: 0.588187882900238\n",
      "RNN, rep: 0, epoch: 6, acc: 0.6232257491350174\n",
      "RNN, rep: 0, epoch: 7, acc: 0.6554333120584488\n",
      "RNN, rep: 0, epoch: 8, acc: 0.6952180881798268\n",
      "RNN, rep: 0, epoch: 9, acc: 0.7304376837611198\n",
      "RNN, rep: 0, epoch: 10, acc: 0.747788907289505\n",
      "RNN, rep: 0, epoch: 11, acc: 0.76658497184515\n",
      "RNN, rep: 0, epoch: 12, acc: 0.7718523472547532\n",
      "RNN, rep: 0, epoch: 13, acc: 0.7806057235598565\n",
      "RNN, rep: 0, epoch: 14, acc: 0.784082232862711\n",
      "RNN, rep: 0, epoch: 15, acc: 0.7888004972040653\n",
      "RNN, rep: 0, epoch: 16, acc: 0.7933720628917217\n",
      "RNN, rep: 0, epoch: 17, acc: 0.8009723310172557\n",
      "RNN, rep: 0, epoch: 18, acc: 0.8032749511301518\n",
      "RNN, rep: 0, epoch: 19, acc: 0.8088079704344273\n",
      "RNN, rep: 0, epoch: 20, acc: 0.8105133919417858\n",
      "RNN, rep: 0, epoch: 21, acc: 0.8168782261013985\n",
      "RNN, rep: 0, epoch: 22, acc: 0.8203486460447311\n",
      "RNN, rep: 0, epoch: 23, acc: 0.8276168555021286\n",
      "RNN, rep: 0, epoch: 24, acc: 0.8324326127767563\n",
      "RNN, rep: 0, epoch: 25, acc: 0.8344928394258022\n",
      "RNN, rep: 0, epoch: 26, acc: 0.843521098792553\n",
      "RNN, rep: 0, epoch: 27, acc: 0.83985197275877\n",
      "RNN, rep: 0, epoch: 28, acc: 0.8473588314652443\n",
      "RNN, rep: 0, epoch: 29, acc: 0.847655917853117\n",
      "RNN, rep: 0, epoch: 30, acc: 0.8557376995682716\n",
      "RNN, rep: 0, epoch: 31, acc: 0.8619339445978403\n",
      "RNN, rep: 0, epoch: 32, acc: 0.8662962409853935\n",
      "RNN, rep: 0, epoch: 33, acc: 0.867113112732768\n",
      "RNN, rep: 0, epoch: 34, acc: 0.8752617682516575\n",
      "RNN, rep: 0, epoch: 35, acc: 0.8798989792913199\n",
      "RNN, rep: 0, epoch: 36, acc: 0.8858685922622681\n",
      "RNN, rep: 0, epoch: 37, acc: 0.8899026957154273\n",
      "RNN, rep: 0, epoch: 38, acc: 0.8932274753600359\n",
      "RNN, rep: 0, epoch: 39, acc: 0.8978384897857904\n",
      "RNN, rep: 0, epoch: 40, acc: 0.8974372297525406\n",
      "RNN, rep: 0, epoch: 41, acc: 0.9022883795946837\n",
      "RNN, rep: 0, epoch: 42, acc: 0.9074741882085801\n",
      "RNN, rep: 0, epoch: 43, acc: 0.9108135069906712\n",
      "RNN, rep: 0, epoch: 44, acc: 0.9134836605191231\n",
      "RNN, rep: 0, epoch: 45, acc: 0.9175953605026007\n",
      "RNN, rep: 0, epoch: 46, acc: 0.9198972466960549\n",
      "RNN, rep: 0, epoch: 47, acc: 0.9236766368895769\n",
      "RNN, rep: 0, epoch: 48, acc: 0.9265017116069794\n",
      "RNN, rep: 0, epoch: 49, acc: 0.9274995346739888\n",
      "RNN, rep: 0, epoch: 50, acc: 0.9301469260826707\n",
      "RNN, rep: 0, epoch: 51, acc: 0.9329471341148019\n",
      "RNN, rep: 0, epoch: 52, acc: 0.9338537470251321\n",
      "RNN, rep: 0, epoch: 53, acc: 0.936448057629168\n",
      "RNN, rep: 0, epoch: 54, acc: 0.9394386487454176\n",
      "RNN, rep: 0, epoch: 55, acc: 0.9394988592341542\n",
      "RNN, rep: 0, epoch: 56, acc: 0.9414111694693565\n",
      "RNN, rep: 0, epoch: 57, acc: 0.9453997817635537\n",
      "RNN, rep: 0, epoch: 58, acc: 0.9452228132635355\n",
      "RNN, rep: 0, epoch: 59, acc: 0.9479310734197497\n",
      "RNN, rep: 0, epoch: 60, acc: 0.9504715367406606\n",
      "RNN, rep: 0, epoch: 61, acc: 0.9508604531362653\n",
      "RNN, rep: 0, epoch: 62, acc: 0.9518951144814491\n",
      "RNN, rep: 0, epoch: 63, acc: 0.9550159139558673\n",
      "RNN, rep: 0, epoch: 64, acc: 0.9562743007019162\n",
      "RNN, rep: 0, epoch: 65, acc: 0.9578768450766801\n",
      "RNN, rep: 0, epoch: 66, acc: 0.9593749818578362\n",
      "RNN, rep: 0, epoch: 67, acc: 0.9613646125048398\n",
      "RNN, rep: 0, epoch: 68, acc: 0.962131157014519\n",
      "RNN, rep: 0, epoch: 69, acc: 0.964059018753469\n",
      "RNN, rep: 0, epoch: 70, acc: 0.9655886971391737\n",
      "RNN, rep: 0, epoch: 71, acc: 0.9660819043405354\n",
      "RNN, rep: 0, epoch: 72, acc: 0.9672819131426513\n",
      "RNN, rep: 0, epoch: 73, acc: 0.9680556359887124\n",
      "RNN, rep: 0, epoch: 74, acc: 0.970068261101842\n",
      "RNN                  Rep: 0   Epoch: 74    Acc: 0.9701 Params: min_length: 5, max_length: 5, fill: 0, value_1: -1, value_2: 1 Time: 12.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 3])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.5005306303501129\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.5015003344416619\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5009187537431717\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5121772193908691\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.5248491096496583\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.5573169723153114\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.5886885371804237\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.6162151029706001\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.6307820463180542\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.6444100394845009\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.6522547274827957\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.666816184669733\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.6850617635250091\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.6949652384221554\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.7163514028489589\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.7243616093695163\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.7310868082940578\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.7575875125825405\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.7640805676579475\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.7723158998787403\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.7835855265706777\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.7810210276395082\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.8066571616381407\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.8067457453906536\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.8103011630475521\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.8212228524684906\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.8281526611745358\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.8264735201746225\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.8331498125195503\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.84162338770926\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.8452405273169279\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.8472861608862877\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.8547201581299305\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.8512392023950816\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.8515049397945404\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.852039006575942\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.8600630658119917\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.8648767764866352\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.8692666909843684\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.8590426772087812\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.8584938763827086\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.8736286623030901\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.879205762706697\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.8755018423497677\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.8629367489740253\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.8820011411979795\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.8735370364412666\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.8745454140752554\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.8815505591779947\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.8801480563357472\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.8802241231501102\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.8792485897243023\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.8865883561596274\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.8837523253262043\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.8965994000807405\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.8797158898413181\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.8830811563879252\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.8861845936626196\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.897755883820355\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.8937936409562827\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.8785313792526722\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.8899547715857625\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.8811228927969933\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.8928535590693355\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.8879812451824546\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.8791550311818719\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.8789617793634534\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.8916641733050347\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.889583436474204\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.8911688295006752\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.9014180175960064\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.8993845377862454\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.8939765461534261\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.8866589262709021\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.8901062914729119\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.8925675409287215\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.8953400606662035\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.8958399972692132\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.8981823592633009\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.9062211962789297\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.9058122515305876\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.9252729745022953\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.932635897360742\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.9376044418103993\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.9427359609492123\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.9513974411599339\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.9508308029174805\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.9558020974695682\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.9554905351810157\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.9605401182547212\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.9629011448100209\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.9643199260532856\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.9677743900753558\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.9669848418049515\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.9699813743308187\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.9693460140936077\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.9713374871294945\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 97    Acc: 0.9713 Params: min_length: 5, max_length: 5, fill: 0, value_1: -1, value_2: 1 Time: 23.86 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.49803338885307313\n",
      "RNN, rep: 0, epoch: 2, acc: 0.503658972978592\n",
      "RNN, rep: 0, epoch: 3, acc: 0.5036845079064369\n",
      "RNN, rep: 0, epoch: 4, acc: 0.5010328009724617\n",
      "RNN, rep: 0, epoch: 5, acc: 0.513105026781559\n",
      "RNN, rep: 0, epoch: 6, acc: 0.5627352347970009\n",
      "RNN, rep: 0, epoch: 7, acc: 0.6005557355284691\n",
      "RNN, rep: 0, epoch: 8, acc: 0.6176805847883224\n",
      "RNN, rep: 0, epoch: 9, acc: 0.6311809763312339\n",
      "RNN, rep: 0, epoch: 10, acc: 0.6365366458892823\n",
      "RNN, rep: 0, epoch: 11, acc: 0.6445594525337219\n",
      "RNN, rep: 0, epoch: 12, acc: 0.6385790008306503\n",
      "RNN, rep: 0, epoch: 13, acc: 0.6455300796031952\n",
      "RNN, rep: 0, epoch: 14, acc: 0.6453956857323646\n",
      "RNN, rep: 0, epoch: 15, acc: 0.6478460296988487\n",
      "RNN, rep: 0, epoch: 16, acc: 0.6489592677354813\n",
      "RNN, rep: 0, epoch: 17, acc: 0.6467975032329559\n",
      "RNN, rep: 0, epoch: 18, acc: 0.6480350911617279\n",
      "RNN, rep: 0, epoch: 19, acc: 0.6557600092887879\n",
      "RNN, rep: 0, epoch: 20, acc: 0.6526103603839875\n",
      "RNN, rep: 0, epoch: 21, acc: 0.6572761401534081\n",
      "RNN, rep: 0, epoch: 22, acc: 0.6576785585284233\n",
      "RNN, rep: 0, epoch: 23, acc: 0.6458704680204391\n",
      "RNN, rep: 0, epoch: 24, acc: 0.6551227301359177\n",
      "RNN, rep: 0, epoch: 25, acc: 0.6568881154060364\n",
      "RNN, rep: 0, epoch: 26, acc: 0.6553836825489998\n",
      "RNN, rep: 0, epoch: 27, acc: 0.6563791006803512\n",
      "RNN, rep: 0, epoch: 28, acc: 0.6589897421002388\n",
      "RNN, rep: 0, epoch: 29, acc: 0.6581023815274238\n",
      "RNN, rep: 0, epoch: 30, acc: 0.657950239777565\n",
      "RNN, rep: 0, epoch: 31, acc: 0.660251475572586\n",
      "RNN, rep: 0, epoch: 32, acc: 0.6553956604003907\n",
      "RNN, rep: 0, epoch: 33, acc: 0.6619235354661942\n",
      "RNN, rep: 0, epoch: 34, acc: 0.6660488346219062\n",
      "RNN, rep: 0, epoch: 35, acc: 0.659539215862751\n",
      "RNN, rep: 0, epoch: 36, acc: 0.6696982565522194\n",
      "RNN, rep: 0, epoch: 37, acc: 0.6611333963274956\n",
      "RNN, rep: 0, epoch: 38, acc: 0.6692570734024048\n",
      "RNN, rep: 0, epoch: 39, acc: 0.677541012763977\n",
      "RNN, rep: 0, epoch: 40, acc: 0.676654748916626\n",
      "RNN, rep: 0, epoch: 41, acc: 0.6850853675603866\n",
      "RNN, rep: 0, epoch: 42, acc: 0.6873031839728355\n",
      "RNN, rep: 0, epoch: 43, acc: 0.6828071638941765\n",
      "RNN, rep: 0, epoch: 44, acc: 0.7018231356143951\n",
      "RNN, rep: 0, epoch: 45, acc: 0.7049047201871872\n",
      "RNN, rep: 0, epoch: 46, acc: 0.7106913673877716\n",
      "RNN, rep: 0, epoch: 47, acc: 0.7163908021152019\n",
      "RNN, rep: 0, epoch: 48, acc: 0.7191126927733421\n",
      "RNN, rep: 0, epoch: 49, acc: 0.7312204846739769\n",
      "RNN, rep: 0, epoch: 50, acc: 0.7396772621572018\n",
      "RNN, rep: 0, epoch: 51, acc: 0.7448932482302189\n",
      "RNN, rep: 0, epoch: 52, acc: 0.755549633204937\n",
      "RNN, rep: 0, epoch: 53, acc: 0.7640686552226543\n",
      "RNN, rep: 0, epoch: 54, acc: 0.7716410353779792\n",
      "RNN, rep: 0, epoch: 55, acc: 0.7844825419783592\n",
      "RNN, rep: 0, epoch: 56, acc: 0.79559719607234\n",
      "RNN, rep: 0, epoch: 57, acc: 0.7974903154373169\n",
      "RNN, rep: 0, epoch: 58, acc: 0.8120453986525535\n",
      "RNN, rep: 0, epoch: 59, acc: 0.8135002267360687\n",
      "RNN, rep: 0, epoch: 60, acc: 0.818348902463913\n",
      "RNN, rep: 0, epoch: 61, acc: 0.8287929791212082\n",
      "RNN, rep: 0, epoch: 62, acc: 0.8337374958395958\n",
      "RNN, rep: 0, epoch: 63, acc: 0.8353408966213465\n",
      "RNN, rep: 0, epoch: 64, acc: 0.8394559157639742\n",
      "RNN, rep: 0, epoch: 65, acc: 0.8433871202170848\n",
      "RNN, rep: 0, epoch: 66, acc: 0.853820476680994\n",
      "RNN, rep: 0, epoch: 67, acc: 0.8451493126153946\n",
      "RNN, rep: 0, epoch: 68, acc: 0.8579206675291061\n",
      "RNN, rep: 0, epoch: 69, acc: 0.8617716301977635\n",
      "RNN, rep: 0, epoch: 70, acc: 0.8631401837617159\n",
      "RNN, rep: 0, epoch: 71, acc: 0.868554912507534\n",
      "RNN, rep: 0, epoch: 72, acc: 0.8722716276347637\n",
      "RNN, rep: 0, epoch: 73, acc: 0.876032124683261\n",
      "RNN, rep: 0, epoch: 74, acc: 0.8797340204566717\n",
      "RNN, rep: 0, epoch: 75, acc: 0.8819815866649151\n",
      "RNN, rep: 0, epoch: 76, acc: 0.8867506273090839\n",
      "RNN, rep: 0, epoch: 77, acc: 0.8919663763046265\n",
      "RNN, rep: 0, epoch: 78, acc: 0.8953431398421526\n",
      "RNN, rep: 0, epoch: 79, acc: 0.8996670228987932\n",
      "RNN, rep: 0, epoch: 80, acc: 0.9078186525404454\n",
      "RNN, rep: 0, epoch: 81, acc: 0.9083721015974879\n",
      "RNN, rep: 0, epoch: 82, acc: 0.9168717897310853\n",
      "RNN, rep: 0, epoch: 83, acc: 0.9177164103090764\n",
      "RNN, rep: 0, epoch: 84, acc: 0.919970340617001\n",
      "RNN, rep: 0, epoch: 85, acc: 0.9234702380746603\n",
      "RNN, rep: 0, epoch: 86, acc: 0.9269969428703189\n",
      "RNN, rep: 0, epoch: 87, acc: 0.9315726451575757\n",
      "RNN, rep: 0, epoch: 88, acc: 0.9345230686664582\n",
      "RNN, rep: 0, epoch: 89, acc: 0.9389784028753638\n",
      "RNN, rep: 0, epoch: 90, acc: 0.9371980008855462\n",
      "RNN, rep: 0, epoch: 91, acc: 0.9426555711776018\n",
      "RNN, rep: 0, epoch: 92, acc: 0.9452354737371206\n",
      "RNN, rep: 0, epoch: 93, acc: 0.9476092656515539\n",
      "RNN, rep: 0, epoch: 94, acc: 0.9479174147360027\n",
      "RNN, rep: 0, epoch: 95, acc: 0.9516337386891246\n",
      "RNN, rep: 0, epoch: 96, acc: 0.9518750297650694\n",
      "RNN, rep: 0, epoch: 97, acc: 0.9551414843089878\n",
      "RNN, rep: 0, epoch: 98, acc: 0.9570429526828229\n",
      "RNN, rep: 0, epoch: 99, acc: 0.9584572027064859\n",
      "RNN, rep: 0, epoch: 100, acc: 0.9601406086049974\n",
      "RNN, rep: 0, epoch: 101, acc: 0.9620438845828175\n",
      "RNN, rep: 0, epoch: 102, acc: 0.9624267164058984\n",
      "RNN, rep: 0, epoch: 103, acc: 0.9651110792160034\n",
      "RNN, rep: 0, epoch: 104, acc: 0.9656766315922141\n",
      "RNN, rep: 0, epoch: 105, acc: 0.965665314681828\n",
      "RNN, rep: 0, epoch: 106, acc: 0.9682807379215955\n",
      "RNN, rep: 0, epoch: 107, acc: 0.9693027576245368\n",
      "RNN, rep: 0, epoch: 108, acc: 0.9686520679853856\n",
      "RNN, rep: 0, epoch: 109, acc: 0.9716380393318832\n",
      "RNN                  Rep: 0   Epoch: 109   Acc: 0.9716 Params: min_length: 10, max_length: 10, fill: 0, value_1: -1, value_2: 1 Time: 22.52 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.5020456886291504\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.5045629474520683\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5319650027155877\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5839257082343101\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.5977899116277695\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.613918409049511\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.6209039500355721\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.6424469301104545\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.635503543317318\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.6402914649248124\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.6435714545845985\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.6517280101776123\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.6409020651876927\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.6385885670781135\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.6354013672471046\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.5967523114383221\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.6377891927957535\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.6242127807438373\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.617729689925909\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.6056670343875885\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.61824902176857\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.6530425679683686\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.6628241375088691\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.6926104837656021\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.7132513283193112\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.7312666967511177\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.7453576318919659\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.7609212864935399\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.7678912852704525\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.7761643296480178\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.779503530561924\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.7849992276728153\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.7894111220538617\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.7937904866039753\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.7272519175708294\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.7841073347628117\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.7903861877322197\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.7972227960824967\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.7968242467939853\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.7989802019298077\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.8009959708154202\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.8026156997680665\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.8006233654916286\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.8035590256750583\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.8040594917535782\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.803493857383728\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.8062840697169303\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.8038980622589588\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.7946854110062123\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.8106895135343075\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.8121711218357086\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.8086238498985767\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.8171640430390835\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.8169136011600494\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.8190819464623929\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.8238498730957509\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.8280471982061863\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.8183735667914153\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.767904684394598\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.6197212760150432\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.6468971054255962\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.6067776881158352\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.6108908894658088\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.5883882373571396\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.5905989184975624\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.5994396176934242\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.6098062226176262\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.5838920041918755\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.5860778015851974\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.6103463593125343\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.6228493742644787\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.5930192272365093\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.6129447036981582\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.6107907146215439\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.6137874653935432\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.600030103623867\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.6382840146124363\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.6967408718168735\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.7258016522228717\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.7799828133732081\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.8063663494586945\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.8363009134680033\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.8307705304771662\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.8287414145469666\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.8316043236851692\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.8364744468778372\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.8464290841668844\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.8459063387662172\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.8594722281396389\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.8487208399176598\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.8583143216371536\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.8574203516542912\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.8632730840891599\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.8411434894055128\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.874003065302968\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.864327400252223\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.8597189539670944\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.8551532910764217\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.8585897376388312\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.8452365136891603\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.8668698056787253\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.8805120673775673\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.8714954008534551\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.8751991736143827\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.8664811111614108\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.8723855614289642\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.8821429568529129\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.8836451622471213\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.8798933766037226\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.8767997701093555\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.8806600692868233\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.8760530726611614\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.8809453526511788\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.8757208130136133\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.886599464416504\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.8852677193656564\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.8811901269108057\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.880825813934207\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.8869275664165616\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.8821116762980818\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.8888307658582926\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.8917781384661794\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.8847252275794745\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.8873959312215447\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.8808584832400084\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.7787388203665614\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.7579138853400945\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.7556190814822912\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.8739113653451205\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.8817816412448883\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.8831673803552985\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.8795449899882078\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.8799353140965104\n",
      "NetRNNWithAttention, rep: 0, epoch: 134, acc: 0.8832810908555985\n",
      "NetRNNWithAttention, rep: 0, epoch: 135, acc: 0.8757726691663266\n",
      "NetRNNWithAttention, rep: 0, epoch: 136, acc: 0.8996366790309548\n",
      "NetRNNWithAttention, rep: 0, epoch: 137, acc: 0.8757656566798687\n",
      "NetRNNWithAttention, rep: 0, epoch: 138, acc: 0.8907546697929501\n",
      "NetRNNWithAttention, rep: 0, epoch: 139, acc: 0.8970072596892714\n",
      "NetRNNWithAttention, rep: 0, epoch: 140, acc: 0.8995931100845337\n",
      "NetRNNWithAttention, rep: 0, epoch: 141, acc: 0.8883369418233633\n",
      "NetRNNWithAttention, rep: 0, epoch: 142, acc: 0.8894139027968049\n",
      "NetRNNWithAttention, rep: 0, epoch: 143, acc: 0.9054902507923543\n",
      "NetRNNWithAttention, rep: 0, epoch: 144, acc: 0.8955293374694884\n",
      "NetRNNWithAttention, rep: 0, epoch: 145, acc: 0.9000687207840383\n",
      "NetRNNWithAttention, rep: 0, epoch: 146, acc: 0.8938157965429128\n",
      "NetRNNWithAttention, rep: 0, epoch: 147, acc: 0.8897753327339888\n",
      "NetRNNWithAttention, rep: 0, epoch: 148, acc: 0.9001720825955272\n",
      "NetRNNWithAttention, rep: 0, epoch: 149, acc: 0.8983232549205422\n",
      "NetRNNWithAttention, rep: 0, epoch: 150, acc: 0.9136871033720673\n",
      "NetRNNWithAttention, rep: 0, epoch: 151, acc: 0.9147077395021915\n",
      "NetRNNWithAttention, rep: 0, epoch: 152, acc: 0.901359888613224\n",
      "NetRNNWithAttention, rep: 0, epoch: 153, acc: 0.9129791385494173\n",
      "NetRNNWithAttention, rep: 0, epoch: 154, acc: 0.9119583120569587\n",
      "NetRNNWithAttention, rep: 0, epoch: 155, acc: 0.9227150424197316\n",
      "NetRNNWithAttention, rep: 0, epoch: 156, acc: 0.9269289115443826\n",
      "NetRNNWithAttention, rep: 0, epoch: 157, acc: 0.9341683903150261\n",
      "NetRNNWithAttention, rep: 0, epoch: 158, acc: 0.922608207166195\n",
      "NetRNNWithAttention, rep: 0, epoch: 159, acc: 0.933130353204906\n",
      "NetRNNWithAttention, rep: 0, epoch: 160, acc: 0.9299632148630917\n",
      "NetRNNWithAttention, rep: 0, epoch: 161, acc: 0.9241058709099889\n",
      "NetRNNWithAttention, rep: 0, epoch: 162, acc: 0.9308710777014494\n",
      "NetRNNWithAttention, rep: 0, epoch: 163, acc: 0.932748173866421\n",
      "NetRNNWithAttention, rep: 0, epoch: 164, acc: 0.9388030424341559\n",
      "NetRNNWithAttention, rep: 0, epoch: 165, acc: 0.941144043970853\n",
      "NetRNNWithAttention, rep: 0, epoch: 166, acc: 0.9386048043053598\n",
      "NetRNNWithAttention, rep: 0, epoch: 167, acc: 0.9402798296976834\n",
      "NetRNNWithAttention, rep: 0, epoch: 168, acc: 0.9478485836181789\n",
      "NetRNNWithAttention, rep: 0, epoch: 169, acc: 0.9463186202663928\n",
      "NetRNNWithAttention, rep: 0, epoch: 170, acc: 0.9420590614154934\n",
      "NetRNNWithAttention, rep: 0, epoch: 171, acc: 0.9486358281690628\n",
      "NetRNNWithAttention, rep: 0, epoch: 172, acc: 0.9472605053335428\n",
      "NetRNNWithAttention, rep: 0, epoch: 173, acc: 0.9513810907490552\n",
      "NetRNNWithAttention, rep: 0, epoch: 174, acc: 0.9496510464511811\n",
      "NetRNNWithAttention, rep: 0, epoch: 175, acc: 0.9510077177546918\n",
      "NetRNNWithAttention, rep: 0, epoch: 176, acc: 0.9576036238577217\n",
      "NetRNNWithAttention, rep: 0, epoch: 177, acc: 0.9562616134993732\n",
      "NetRNNWithAttention, rep: 0, epoch: 178, acc: 0.9517988023534417\n",
      "NetRNNWithAttention, rep: 0, epoch: 179, acc: 0.9533758178446442\n",
      "NetRNNWithAttention, rep: 0, epoch: 180, acc: 0.9568424871750176\n",
      "NetRNNWithAttention, rep: 0, epoch: 181, acc: 0.9539410317409783\n",
      "NetRNNWithAttention, rep: 0, epoch: 182, acc: 0.9578765870351345\n",
      "NetRNNWithAttention, rep: 0, epoch: 183, acc: 0.9584870264399796\n",
      "NetRNNWithAttention, rep: 0, epoch: 184, acc: 0.9584711678046733\n",
      "NetRNNWithAttention, rep: 0, epoch: 185, acc: 0.9572458214126527\n",
      "NetRNNWithAttention, rep: 0, epoch: 186, acc: 0.9623053917195648\n",
      "NetRNNWithAttention, rep: 0, epoch: 187, acc: 0.9636039431486279\n",
      "NetRNNWithAttention, rep: 0, epoch: 188, acc: 0.9616277159564197\n",
      "NetRNNWithAttention, rep: 0, epoch: 189, acc: 0.9625558462366461\n",
      "NetRNNWithAttention, rep: 0, epoch: 190, acc: 0.9661545177269727\n",
      "NetRNNWithAttention, rep: 0, epoch: 191, acc: 0.9669828047975898\n",
      "NetRNNWithAttention, rep: 0, epoch: 192, acc: 0.9688519765995443\n",
      "NetRNNWithAttention, rep: 0, epoch: 193, acc: 0.9670746732736006\n",
      "NetRNNWithAttention, rep: 0, epoch: 194, acc: 0.9714438375085592\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 194   Acc: 0.9714 Params: min_length: 10, max_length: 10, fill: 0, value_1: -1, value_2: 1 Time: 53.56 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.4995464414358139\n",
      "RNN, rep: 0, epoch: 2, acc: 0.5000291487574577\n",
      "RNN, rep: 0, epoch: 3, acc: 0.5012980633974076\n",
      "RNN, rep: 0, epoch: 4, acc: 0.5037454569339752\n",
      "RNN, rep: 0, epoch: 5, acc: 0.5354476389288902\n",
      "RNN, rep: 0, epoch: 6, acc: 0.5681747171282768\n",
      "RNN, rep: 0, epoch: 7, acc: 0.5869902223348618\n",
      "RNN, rep: 0, epoch: 8, acc: 0.6058231869339943\n",
      "RNN, rep: 0, epoch: 9, acc: 0.6129762864112854\n",
      "RNN, rep: 0, epoch: 10, acc: 0.6264757978916168\n",
      "RNN, rep: 0, epoch: 11, acc: 0.6324890190362931\n",
      "RNN, rep: 0, epoch: 12, acc: 0.6284292483329773\n",
      "RNN, rep: 0, epoch: 13, acc: 0.6315541711449623\n",
      "RNN, rep: 0, epoch: 14, acc: 0.6381105557084084\n",
      "RNN, rep: 0, epoch: 15, acc: 0.6413703688979149\n",
      "RNN, rep: 0, epoch: 16, acc: 0.6349651771783829\n",
      "RNN, rep: 0, epoch: 17, acc: 0.6420767962932586\n",
      "RNN, rep: 0, epoch: 18, acc: 0.6422672310471534\n",
      "RNN, rep: 0, epoch: 19, acc: 0.6451295912265778\n",
      "RNN, rep: 0, epoch: 20, acc: 0.6518616870045661\n",
      "RNN, rep: 0, epoch: 21, acc: 0.6514390975236892\n",
      "RNN, rep: 0, epoch: 22, acc: 0.649008564054966\n",
      "RNN, rep: 0, epoch: 23, acc: 0.6563314345479011\n",
      "RNN, rep: 0, epoch: 24, acc: 0.6570799332857132\n",
      "RNN, rep: 0, epoch: 25, acc: 0.6757618269324303\n",
      "RNN, rep: 0, epoch: 26, acc: 0.6928263688087464\n",
      "RNN, rep: 0, epoch: 27, acc: 0.7148205590248108\n",
      "RNN, rep: 0, epoch: 28, acc: 0.7188125896453857\n",
      "RNN, rep: 0, epoch: 29, acc: 0.7337138330936432\n",
      "RNN, rep: 0, epoch: 30, acc: 0.7521403308212757\n",
      "RNN, rep: 0, epoch: 31, acc: 0.763946475982666\n",
      "RNN, rep: 0, epoch: 32, acc: 0.7774529722332955\n",
      "RNN, rep: 0, epoch: 33, acc: 0.7845389470458031\n",
      "RNN, rep: 0, epoch: 34, acc: 0.7829311619699001\n",
      "RNN, rep: 0, epoch: 35, acc: 0.7830094169080257\n",
      "RNN, rep: 0, epoch: 36, acc: 0.7806437788903713\n",
      "RNN, rep: 0, epoch: 37, acc: 0.7990434500575065\n",
      "RNN, rep: 0, epoch: 38, acc: 0.8094968074560165\n",
      "RNN, rep: 0, epoch: 39, acc: 0.8213990940153599\n",
      "RNN, rep: 0, epoch: 40, acc: 0.8066956682503224\n",
      "RNN, rep: 0, epoch: 41, acc: 0.8186308628320694\n",
      "RNN, rep: 0, epoch: 42, acc: 0.8315143922716379\n",
      "RNN, rep: 0, epoch: 43, acc: 0.8372850469499826\n",
      "RNN, rep: 0, epoch: 44, acc: 0.8372326610982418\n",
      "RNN, rep: 0, epoch: 45, acc: 0.8465359050780534\n",
      "RNN, rep: 0, epoch: 46, acc: 0.8579080621898174\n",
      "RNN, rep: 0, epoch: 47, acc: 0.8650005513429642\n",
      "RNN, rep: 0, epoch: 48, acc: 0.8722337597608566\n",
      "RNN, rep: 0, epoch: 49, acc: 0.880492915213108\n",
      "RNN, rep: 0, epoch: 50, acc: 0.880606426820159\n",
      "RNN, rep: 0, epoch: 51, acc: 0.8779907621443271\n",
      "RNN, rep: 0, epoch: 52, acc: 0.8677136304229498\n",
      "RNN, rep: 0, epoch: 53, acc: 0.8894986651092768\n",
      "RNN, rep: 0, epoch: 54, acc: 0.8982248149067164\n",
      "RNN, rep: 0, epoch: 55, acc: 0.9014735659211874\n",
      "RNN, rep: 0, epoch: 56, acc: 0.9069310434907675\n",
      "RNN, rep: 0, epoch: 57, acc: 0.9119538541138172\n",
      "RNN, rep: 0, epoch: 58, acc: 0.9147737611085176\n",
      "RNN, rep: 0, epoch: 59, acc: 0.9184449590742588\n",
      "RNN, rep: 0, epoch: 60, acc: 0.9199298537522554\n",
      "RNN, rep: 0, epoch: 61, acc: 0.9203221248835325\n",
      "RNN, rep: 0, epoch: 62, acc: 0.9278835327923298\n",
      "RNN, rep: 0, epoch: 63, acc: 0.929867483638227\n",
      "RNN, rep: 0, epoch: 64, acc: 0.9326657784730196\n",
      "RNN, rep: 0, epoch: 65, acc: 0.9344682586938142\n",
      "RNN, rep: 0, epoch: 66, acc: 0.936984968483448\n",
      "RNN, rep: 0, epoch: 67, acc: 0.9399217533692718\n",
      "RNN, rep: 0, epoch: 68, acc: 0.942071581222117\n",
      "RNN, rep: 0, epoch: 69, acc: 0.9439310149475932\n",
      "RNN, rep: 0, epoch: 70, acc: 0.9463761646673083\n",
      "RNN, rep: 0, epoch: 71, acc: 0.947654391489923\n",
      "RNN, rep: 0, epoch: 72, acc: 0.9500193442031741\n",
      "RNN, rep: 0, epoch: 73, acc: 0.9496082218363882\n",
      "RNN, rep: 0, epoch: 74, acc: 0.9359548987820745\n",
      "RNN, rep: 0, epoch: 75, acc: 0.9434057466685772\n",
      "RNN, rep: 0, epoch: 76, acc: 0.9500619284808636\n",
      "RNN, rep: 0, epoch: 77, acc: 0.9529469216242432\n",
      "RNN, rep: 0, epoch: 78, acc: 0.9554831011220812\n",
      "RNN, rep: 0, epoch: 79, acc: 0.957463801652193\n",
      "RNN, rep: 0, epoch: 80, acc: 0.9587262066826224\n",
      "RNN, rep: 0, epoch: 81, acc: 0.9590876511111855\n",
      "RNN, rep: 0, epoch: 82, acc: 0.961179535984993\n",
      "RNN, rep: 0, epoch: 83, acc: 0.9628244045935571\n",
      "RNN, rep: 0, epoch: 84, acc: 0.9635203034803271\n",
      "RNN, rep: 0, epoch: 85, acc: 0.9644867070391774\n",
      "RNN, rep: 0, epoch: 86, acc: 0.9623823995329439\n",
      "RNN, rep: 0, epoch: 87, acc: 0.9353518630564213\n",
      "RNN, rep: 0, epoch: 88, acc: 0.9605731502734125\n",
      "RNN, rep: 0, epoch: 89, acc: 0.9643186894245446\n",
      "RNN, rep: 0, epoch: 90, acc: 0.9658006737753749\n",
      "RNN, rep: 0, epoch: 91, acc: 0.9677322694659233\n",
      "RNN, rep: 0, epoch: 92, acc: 0.968685305006802\n",
      "RNN, rep: 0, epoch: 93, acc: 0.970146118272096\n",
      "RNN                  Rep: 0   Epoch: 93    Acc: 0.9701 Params: min_length: 10, max_length: 15, fill: 0, value_1: -1, value_2: 1 Time: 20.80 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.499655022919178\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.5067768839001655\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5115880706906318\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5824079468846322\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.6203702944517135\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.6363012006878853\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.5676397901773452\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.5043343126773834\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.5164388194680214\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.5422523775696755\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.5835182270407677\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.6130928200483322\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.6222738555073738\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.6347889924049377\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.6554748821258545\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.661254090666771\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.681487253010273\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.7078950910270214\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.727020183056593\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.7391352222859859\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.7488998706638813\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.757594042122364\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.7657425047457218\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.769648758172989\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.775091177970171\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.7760536752641201\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.7810255953669548\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.785057076215744\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.787144563049078\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.7923157423734665\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.7924173970520496\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.700237921923399\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.7700710828602314\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.7940427203476429\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.7962680143117905\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.7976824353635311\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.7993036937713623\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.80237061470747\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.8043223080039025\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.8071938546001911\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.805182589739561\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.8087041668593884\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.810813987404108\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.8137668639421463\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.8137989430129529\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.8140982395410538\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.6511856783926487\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.6613943572342396\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.6347747869789601\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.6606319543719291\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.6465527060627937\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.6526743513345719\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.6541475602984428\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.6583876845240593\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.6556044274568558\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.6542447036504746\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.6531640139222145\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.657011878490448\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.6561925438046455\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.662292275428772\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.649275683760643\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.664504651427269\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.6638949072360992\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.664281932413578\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.6319632512331009\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.6603544965386391\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.6621510797739029\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.6631424039602279\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.6651037079095841\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.6670699340105056\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.6689583715796471\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.6699692171812057\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.6720121589303016\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.6703762444853782\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.6585040873289109\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.6629730820655823\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.6587864220142364\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.6653752431273461\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.6652779212594032\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.6659445226192474\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.6626554173231125\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.6629790306091309\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.663580673635006\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.6641785454750061\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.6698053163290024\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.6682465720176697\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.6671817561984063\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.6714206784963608\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.6676939183473587\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.6409144240617752\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.6460423454642296\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.7141708375513554\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.7496987092494964\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.761220912784338\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.7775402447581291\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.7813090193271637\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.7953664109110832\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.8019203002750873\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.8039434619247914\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.8107944849133492\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.8145909345149994\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.8214916068315506\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.8248120513558388\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.8282099425792694\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.8386019958555698\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.8426528113335371\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.8470035719126463\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.8558978021144867\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.8573607610911131\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.8610254099965096\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.8650220748782158\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.8736647489666939\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.8813281864672899\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.8816339106857777\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.8860000164806843\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.8897557881101966\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.8929046253487468\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.8962034426257014\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.9060378218814731\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.9056561619788408\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.905876842699945\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.911728984452784\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.9115931485593319\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.9171024989336729\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.921176381148398\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.9259213815256954\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.9258382351696491\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.931126190982759\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.9344004740193487\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.9327260044403374\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.9399150069989264\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.9422808750905096\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.9437565180659294\n",
      "NetRNNWithAttention, rep: 0, epoch: 134, acc: 0.946483070962131\n",
      "NetRNNWithAttention, rep: 0, epoch: 135, acc: 0.9465235766209662\n",
      "NetRNNWithAttention, rep: 0, epoch: 136, acc: 0.9499044656194746\n",
      "NetRNNWithAttention, rep: 0, epoch: 137, acc: 0.9528525463491678\n",
      "NetRNNWithAttention, rep: 0, epoch: 138, acc: 0.9542109220661223\n",
      "NetRNNWithAttention, rep: 0, epoch: 139, acc: 0.953772389292717\n",
      "NetRNNWithAttention, rep: 0, epoch: 140, acc: 0.9584405051730573\n",
      "NetRNNWithAttention, rep: 0, epoch: 141, acc: 0.9598465836048127\n",
      "NetRNNWithAttention, rep: 0, epoch: 142, acc: 0.9602537978440523\n",
      "NetRNNWithAttention, rep: 0, epoch: 143, acc: 0.9614412228390574\n",
      "NetRNNWithAttention, rep: 0, epoch: 144, acc: 0.9628475882671773\n",
      "NetRNNWithAttention, rep: 0, epoch: 145, acc: 0.9638362709619105\n",
      "NetRNNWithAttention, rep: 0, epoch: 146, acc: 0.9654596956446767\n",
      "NetRNNWithAttention, rep: 0, epoch: 147, acc: 0.9659122951701283\n",
      "NetRNNWithAttention, rep: 0, epoch: 148, acc: 0.9667834325134754\n",
      "NetRNNWithAttention, rep: 0, epoch: 149, acc: 0.9675764570012688\n",
      "NetRNNWithAttention, rep: 0, epoch: 150, acc: 0.9683364974148572\n",
      "NetRNNWithAttention, rep: 0, epoch: 151, acc: 0.9684976747725159\n",
      "NetRNNWithAttention, rep: 0, epoch: 152, acc: 0.9712512971367687\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 152   Acc: 0.9713 Params: min_length: 10, max_length: 15, fill: 0, value_1: -1, value_2: 1 Time: 46.99 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.4992616790533066\n",
      "RNN, rep: 0, epoch: 2, acc: 0.49973344534635544\n",
      "RNN, rep: 0, epoch: 3, acc: 0.4995386099815369\n",
      "RNN, rep: 0, epoch: 4, acc: 0.4990179431438446\n",
      "RNN, rep: 0, epoch: 5, acc: 0.49890165001153947\n",
      "RNN, rep: 0, epoch: 6, acc: 0.499388764500618\n",
      "RNN, rep: 0, epoch: 7, acc: 0.5010220924019814\n",
      "RNN, rep: 0, epoch: 8, acc: 0.49996903985738755\n",
      "RNN, rep: 0, epoch: 9, acc: 0.49889968901872633\n",
      "RNN, rep: 0, epoch: 10, acc: 0.49937075436115264\n",
      "RNN, rep: 0, epoch: 11, acc: 0.5006179514527321\n",
      "RNN, rep: 0, epoch: 12, acc: 0.500266360938549\n",
      "RNN, rep: 0, epoch: 13, acc: 0.5026684346795082\n",
      "RNN, rep: 0, epoch: 14, acc: 0.5042572739720345\n",
      "RNN, rep: 0, epoch: 15, acc: 0.5059853059053421\n",
      "RNN, rep: 0, epoch: 16, acc: 0.5126039430499076\n",
      "RNN, rep: 0, epoch: 17, acc: 0.5552697092294693\n",
      "RNN, rep: 0, epoch: 18, acc: 0.5805184331536293\n",
      "RNN, rep: 0, epoch: 19, acc: 0.5960052585601807\n",
      "RNN, rep: 0, epoch: 20, acc: 0.6163703313469887\n",
      "RNN, rep: 0, epoch: 21, acc: 0.6274761939048767\n",
      "RNN, rep: 0, epoch: 22, acc: 0.6295766204595565\n",
      "RNN, rep: 0, epoch: 23, acc: 0.6375216433405876\n",
      "RNN, rep: 0, epoch: 24, acc: 0.6426149469614029\n",
      "RNN, rep: 0, epoch: 25, acc: 0.6426264080405235\n",
      "RNN, rep: 0, epoch: 26, acc: 0.6400989198684692\n",
      "RNN, rep: 0, epoch: 27, acc: 0.6412196445465088\n",
      "RNN, rep: 0, epoch: 28, acc: 0.6506300202012062\n",
      "RNN, rep: 0, epoch: 29, acc: 0.6547688528895378\n",
      "RNN, rep: 0, epoch: 30, acc: 0.6515829205513001\n",
      "RNN, rep: 0, epoch: 31, acc: 0.6488505992293357\n",
      "RNN, rep: 0, epoch: 32, acc: 0.653298311829567\n",
      "RNN, rep: 0, epoch: 33, acc: 0.655109530389309\n",
      "RNN, rep: 0, epoch: 34, acc: 0.6445563960075379\n",
      "RNN, rep: 0, epoch: 35, acc: 0.6534493100643158\n",
      "RNN, rep: 0, epoch: 36, acc: 0.6553996160626412\n",
      "RNN, rep: 0, epoch: 37, acc: 0.654801858663559\n",
      "RNN, rep: 0, epoch: 38, acc: 0.6436691224575043\n",
      "RNN, rep: 0, epoch: 39, acc: 0.6543818634748458\n",
      "RNN, rep: 0, epoch: 40, acc: 0.6449913203716278\n",
      "RNN, rep: 0, epoch: 41, acc: 0.6533546188473701\n",
      "RNN, rep: 0, epoch: 42, acc: 0.6551985636353492\n",
      "RNN, rep: 0, epoch: 43, acc: 0.6555459013581276\n",
      "RNN, rep: 0, epoch: 44, acc: 0.6558279797434807\n",
      "RNN, rep: 0, epoch: 45, acc: 0.6552810433506966\n",
      "RNN, rep: 0, epoch: 46, acc: 0.6577856341004371\n",
      "RNN, rep: 0, epoch: 47, acc: 0.6598315152525902\n",
      "RNN, rep: 0, epoch: 48, acc: 0.6602041515707969\n",
      "RNN, rep: 0, epoch: 49, acc: 0.6607732173800468\n",
      "RNN, rep: 0, epoch: 50, acc: 0.6639153823256493\n",
      "RNN, rep: 0, epoch: 51, acc: 0.6557489785552025\n",
      "RNN, rep: 0, epoch: 52, acc: 0.6615079659223556\n",
      "RNN, rep: 0, epoch: 53, acc: 0.6655798065662384\n",
      "RNN, rep: 0, epoch: 54, acc: 0.662440941631794\n",
      "RNN, rep: 0, epoch: 55, acc: 0.6508787921071053\n",
      "RNN, rep: 0, epoch: 56, acc: 0.6626313343644142\n",
      "RNN, rep: 0, epoch: 57, acc: 0.6500557011365891\n",
      "RNN, rep: 0, epoch: 58, acc: 0.6565210053324699\n",
      "RNN, rep: 0, epoch: 59, acc: 0.6590817868709564\n",
      "RNN, rep: 0, epoch: 60, acc: 0.6615783450007439\n",
      "RNN, rep: 0, epoch: 61, acc: 0.6594136860966683\n",
      "RNN, rep: 0, epoch: 62, acc: 0.6643458810448647\n",
      "RNN, rep: 0, epoch: 63, acc: 0.6649742510914802\n",
      "RNN, rep: 0, epoch: 64, acc: 0.6639127606153488\n",
      "RNN, rep: 0, epoch: 65, acc: 0.6610335549712181\n",
      "RNN, rep: 0, epoch: 66, acc: 0.6602907827496529\n",
      "RNN, rep: 0, epoch: 67, acc: 0.6658150684833527\n",
      "RNN, rep: 0, epoch: 68, acc: 0.6640672338008881\n",
      "RNN, rep: 0, epoch: 69, acc: 0.6612008261680603\n",
      "RNN, rep: 0, epoch: 70, acc: 0.6665009579062462\n",
      "RNN, rep: 0, epoch: 71, acc: 0.6612110963463783\n",
      "RNN, rep: 0, epoch: 72, acc: 0.6597410526871681\n",
      "RNN, rep: 0, epoch: 73, acc: 0.6618831980228425\n",
      "RNN, rep: 0, epoch: 74, acc: 0.6657546752691269\n",
      "RNN, rep: 0, epoch: 75, acc: 0.6705055677890778\n",
      "RNN, rep: 0, epoch: 76, acc: 0.6652761629223823\n",
      "RNN, rep: 0, epoch: 77, acc: 0.6648578646779061\n",
      "RNN, rep: 0, epoch: 78, acc: 0.6632182425260544\n",
      "RNN, rep: 0, epoch: 79, acc: 0.6688176333904267\n",
      "RNN, rep: 0, epoch: 80, acc: 0.667052728831768\n",
      "RNN, rep: 0, epoch: 81, acc: 0.6649533599615097\n",
      "RNN, rep: 0, epoch: 82, acc: 0.6695112156867981\n",
      "RNN, rep: 0, epoch: 83, acc: 0.6669321998953819\n",
      "RNN, rep: 0, epoch: 84, acc: 0.6675854107737541\n",
      "RNN, rep: 0, epoch: 85, acc: 0.6666229382157326\n",
      "RNN, rep: 0, epoch: 86, acc: 0.6724197801947593\n",
      "RNN, rep: 0, epoch: 87, acc: 0.6682218715548516\n",
      "RNN, rep: 0, epoch: 88, acc: 0.6678639793395996\n",
      "RNN, rep: 0, epoch: 89, acc: 0.6252456140518189\n",
      "RNN, rep: 0, epoch: 90, acc: 0.6740863814949989\n",
      "RNN, rep: 0, epoch: 91, acc: 0.6654838901758194\n",
      "RNN, rep: 0, epoch: 92, acc: 0.669346579015255\n",
      "RNN, rep: 0, epoch: 93, acc: 0.6711407122015953\n",
      "RNN, rep: 0, epoch: 94, acc: 0.6693084093928338\n",
      "RNN, rep: 0, epoch: 95, acc: 0.67196776419878\n",
      "RNN, rep: 0, epoch: 96, acc: 0.6732387408614159\n",
      "RNN, rep: 0, epoch: 97, acc: 0.6753538727760315\n",
      "RNN, rep: 0, epoch: 98, acc: 0.6855386793613434\n",
      "RNN, rep: 0, epoch: 99, acc: 0.6803919059038163\n",
      "RNN, rep: 0, epoch: 100, acc: 0.6261513583362103\n",
      "RNN, rep: 0, epoch: 101, acc: 0.7027799223363399\n",
      "RNN, rep: 0, epoch: 102, acc: 0.7257166573405266\n",
      "RNN, rep: 0, epoch: 103, acc: 0.7457370646297932\n",
      "RNN, rep: 0, epoch: 104, acc: 0.7414367173612118\n",
      "RNN, rep: 0, epoch: 105, acc: 0.7635527744889259\n",
      "RNN, rep: 0, epoch: 106, acc: 0.7696885229647159\n",
      "RNN, rep: 0, epoch: 107, acc: 0.778790024369955\n",
      "RNN, rep: 0, epoch: 108, acc: 0.7880938442051411\n",
      "RNN, rep: 0, epoch: 109, acc: 0.8017998979985714\n",
      "RNN, rep: 0, epoch: 110, acc: 0.776277638822794\n",
      "RNN, rep: 0, epoch: 111, acc: 0.8078485198318959\n",
      "RNN, rep: 0, epoch: 112, acc: 0.817584827542305\n",
      "RNN, rep: 0, epoch: 113, acc: 0.8033725051581859\n",
      "RNN, rep: 0, epoch: 114, acc: 0.8075074936449528\n",
      "RNN, rep: 0, epoch: 115, acc: 0.8212964764982462\n",
      "RNN, rep: 0, epoch: 116, acc: 0.8182830765843392\n",
      "RNN, rep: 0, epoch: 117, acc: 0.8366669371724129\n",
      "RNN, rep: 0, epoch: 118, acc: 0.8439795423299075\n",
      "RNN, rep: 0, epoch: 119, acc: 0.8508597691357136\n",
      "RNN, rep: 0, epoch: 120, acc: 0.8435574787855148\n",
      "RNN, rep: 0, epoch: 121, acc: 0.8601076410710812\n",
      "RNN, rep: 0, epoch: 122, acc: 0.8669557488709688\n",
      "RNN, rep: 0, epoch: 123, acc: 0.8703761721402407\n",
      "RNN, rep: 0, epoch: 124, acc: 0.8732325138896704\n",
      "RNN, rep: 0, epoch: 125, acc: 0.8702813173085451\n",
      "RNN, rep: 0, epoch: 126, acc: 0.8830616296082735\n",
      "RNN, rep: 0, epoch: 127, acc: 0.8883449027687311\n",
      "RNN, rep: 0, epoch: 128, acc: 0.8877804783731699\n",
      "RNN, rep: 0, epoch: 129, acc: 0.8948288485407829\n",
      "RNN, rep: 0, epoch: 130, acc: 0.8978921138495207\n",
      "RNN, rep: 0, epoch: 131, acc: 0.9029719618707895\n",
      "RNN, rep: 0, epoch: 132, acc: 0.9055236785113812\n",
      "RNN, rep: 0, epoch: 133, acc: 0.9091365329921246\n",
      "RNN, rep: 0, epoch: 134, acc: 0.9149026395380497\n",
      "RNN, rep: 0, epoch: 135, acc: 0.9138501009345055\n",
      "RNN, rep: 0, epoch: 136, acc: 0.9113693857938051\n",
      "RNN, rep: 0, epoch: 137, acc: 0.9200218115374446\n",
      "RNN, rep: 0, epoch: 138, acc: 0.9255124302953481\n",
      "RNN, rep: 0, epoch: 139, acc: 0.930091461315751\n",
      "RNN, rep: 0, epoch: 140, acc: 0.9331765708327293\n",
      "RNN, rep: 0, epoch: 141, acc: 0.9302954144403338\n",
      "RNN, rep: 0, epoch: 142, acc: 0.9338045451045036\n",
      "RNN, rep: 0, epoch: 143, acc: 0.9403418534994126\n",
      "RNN, rep: 0, epoch: 144, acc: 0.9428309364616871\n",
      "RNN, rep: 0, epoch: 145, acc: 0.9458307560533286\n",
      "RNN, rep: 0, epoch: 146, acc: 0.9480272301658988\n",
      "RNN, rep: 0, epoch: 147, acc: 0.9468573373183609\n",
      "RNN, rep: 0, epoch: 148, acc: 0.9528533677011728\n",
      "RNN, rep: 0, epoch: 149, acc: 0.9530535422265529\n",
      "RNN, rep: 0, epoch: 150, acc: 0.9391847633756697\n",
      "RNN, rep: 0, epoch: 151, acc: 0.9509729512222111\n",
      "RNN, rep: 0, epoch: 152, acc: 0.9538747055269777\n",
      "RNN, rep: 0, epoch: 153, acc: 0.9552606490813196\n",
      "RNN, rep: 0, epoch: 154, acc: 0.947692439518869\n",
      "RNN, rep: 0, epoch: 155, acc: 0.9536048847809434\n",
      "RNN, rep: 0, epoch: 156, acc: 0.9573117908276617\n",
      "RNN, rep: 0, epoch: 157, acc: 0.9615014225803316\n",
      "RNN, rep: 0, epoch: 158, acc: 0.9646329863555729\n",
      "RNN, rep: 0, epoch: 159, acc: 0.9626902789063752\n",
      "RNN, rep: 0, epoch: 160, acc: 0.9632107241265476\n",
      "RNN, rep: 0, epoch: 161, acc: 0.9643779731355607\n",
      "RNN, rep: 0, epoch: 162, acc: 0.9672871970012784\n",
      "RNN, rep: 0, epoch: 163, acc: 0.9644446069560945\n",
      "RNN, rep: 0, epoch: 164, acc: 0.9691170127317309\n",
      "RNN, rep: 0, epoch: 165, acc: 0.9725012089312076\n",
      "RNN                  Rep: 0   Epoch: 165   Acc: 0.9725 Params: min_length: 20, max_length: 20, fill: 0, value_1: -1, value_2: 1 Time: 45.35 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.5011499857902527\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.498548099398613\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5009145537018775\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5055115923285485\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.5036117121577263\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.5171655207872391\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.503159818649292\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.5060735595226288\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.536222227215767\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.5907738772034645\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.6148438715934753\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.629592696428299\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.6346245735883713\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.6384447541832924\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.6380489325523376\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.6420388698577881\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.6436012476682663\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.6496968194842339\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.6532531177997589\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.6477755174040795\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.6514661908149719\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.6566737368702888\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.6523567226529121\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.6520570796728135\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.6551701471209526\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.6534143468737602\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.6533856251835823\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.6555683001875877\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.655028080046177\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.6558051693439484\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.6549208721518517\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.6567724084854126\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.6565312668681145\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.656631374657154\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.6578393632173538\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.6615025520324707\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.6587799036502838\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.6581908041238784\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.6604486486315727\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.6614287981390953\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.6620081421732903\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.6632019555568696\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.6622983762621879\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.6606496676802636\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.6600463879108429\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.6602775821089745\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.662038478255272\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.6590702012181282\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.6626976037025452\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.6598044148087502\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.6609586235880852\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.6617978990077973\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.6625164747238159\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.6147809323668479\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.49766550213098526\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.5146267154812812\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.5199662396311759\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.5228317895531654\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.5354082328081131\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.5415814501047135\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.5390680262446403\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.562120672762394\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.5638961279392243\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.5846966281533241\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.5822434014081955\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.5949498006701469\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.6180640514194965\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.6052145387232304\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.6296908433735371\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.6307248245179653\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.6540873935818672\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.6528057888150215\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.6412001812458038\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.6493982964754105\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.6624370668828488\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.6617538620531559\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.6736363123357296\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.6668276190757751\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.6577837659418583\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.6633873291313648\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.6650658921897411\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.6828760369122029\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.6686604943871498\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.6913994142413139\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.7102355419099331\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.7212579450011254\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.7378559087216854\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.7373900049924851\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.7440293200314045\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.7596906150877476\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.7683694592118263\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.7768205949664115\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.7750489398837089\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.7781217700242996\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.7879004554450512\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.7825676786899567\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.7917450205981731\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.7892087283730507\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.7945593529939652\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.7965171855688095\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.8023728904128075\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.7991879282891751\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.8048094433546066\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.8056673654913902\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.8130103123188018\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.8185634487867355\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.8149144016206264\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.8240921570360661\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.8233199106156825\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.8268399687856436\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.8289625567942858\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.8356784076988697\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.842012119665742\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.849634516313672\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.8593566198647022\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.8587414205074311\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.8316567140817642\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.8622214359045028\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.8653255317360162\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.8712156196683645\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.8725056809186935\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.8764692908525467\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.8849487564712762\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.8812318504601717\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.8914459280669689\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.8932326552644372\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.8847308004274964\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.8925594558939338\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.8887717342749238\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.8991176095604897\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.8947548432275653\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.9018913777172566\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.8955400827154517\n",
      "NetRNNWithAttention, rep: 0, epoch: 134, acc: 0.9022275739535689\n",
      "NetRNNWithAttention, rep: 0, epoch: 135, acc: 0.9076997344568372\n",
      "NetRNNWithAttention, rep: 0, epoch: 136, acc: 0.8752210927382111\n",
      "NetRNNWithAttention, rep: 0, epoch: 137, acc: 0.8976804043352604\n",
      "NetRNNWithAttention, rep: 0, epoch: 138, acc: 0.897853192165494\n",
      "NetRNNWithAttention, rep: 0, epoch: 139, acc: 0.9080716429278255\n",
      "NetRNNWithAttention, rep: 0, epoch: 140, acc: 0.898210342079401\n",
      "NetRNNWithAttention, rep: 0, epoch: 141, acc: 0.901266359500587\n",
      "NetRNNWithAttention, rep: 0, epoch: 142, acc: 0.8982560382038355\n",
      "NetRNNWithAttention, rep: 0, epoch: 143, acc: 0.9024081022851169\n",
      "NetRNNWithAttention, rep: 0, epoch: 144, acc: 0.9101515629701317\n",
      "NetRNNWithAttention, rep: 0, epoch: 145, acc: 0.9023961909301579\n",
      "NetRNNWithAttention, rep: 0, epoch: 146, acc: 0.9107499750517308\n",
      "NetRNNWithAttention, rep: 0, epoch: 147, acc: 0.9087428891658783\n",
      "NetRNNWithAttention, rep: 0, epoch: 148, acc: 0.913880762886256\n",
      "NetRNNWithAttention, rep: 0, epoch: 149, acc: 0.8950412488728762\n",
      "NetRNNWithAttention, rep: 0, epoch: 150, acc: 0.9103988278657198\n",
      "NetRNNWithAttention, rep: 0, epoch: 151, acc: 0.9142613999545575\n",
      "NetRNNWithAttention, rep: 0, epoch: 152, acc: 0.92758739490062\n",
      "NetRNNWithAttention, rep: 0, epoch: 153, acc: 0.9269823809154332\n",
      "NetRNNWithAttention, rep: 0, epoch: 154, acc: 0.9264275883324444\n",
      "NetRNNWithAttention, rep: 0, epoch: 155, acc: 0.9349026892893016\n",
      "NetRNNWithAttention, rep: 0, epoch: 156, acc: 0.9345799858681858\n",
      "NetRNNWithAttention, rep: 0, epoch: 157, acc: 0.9355121334642171\n",
      "NetRNNWithAttention, rep: 0, epoch: 158, acc: 0.9336408245936036\n",
      "NetRNNWithAttention, rep: 0, epoch: 159, acc: 0.9401567253656685\n",
      "NetRNNWithAttention, rep: 0, epoch: 160, acc: 0.938197818081826\n",
      "NetRNNWithAttention, rep: 0, epoch: 161, acc: 0.9378110671043396\n",
      "NetRNNWithAttention, rep: 0, epoch: 162, acc: 0.9420351419597864\n",
      "NetRNNWithAttention, rep: 0, epoch: 163, acc: 0.9490650503709912\n",
      "NetRNNWithAttention, rep: 0, epoch: 164, acc: 0.9455275929532945\n",
      "NetRNNWithAttention, rep: 0, epoch: 165, acc: 0.9504197607934475\n",
      "NetRNNWithAttention, rep: 0, epoch: 166, acc: 0.9484655402693898\n",
      "NetRNNWithAttention, rep: 0, epoch: 167, acc: 0.9526121727749706\n",
      "NetRNNWithAttention, rep: 0, epoch: 168, acc: 0.9501822618767619\n",
      "NetRNNWithAttention, rep: 0, epoch: 169, acc: 0.9474119203723967\n",
      "NetRNNWithAttention, rep: 0, epoch: 170, acc: 0.9490967273339629\n",
      "NetRNNWithAttention, rep: 0, epoch: 171, acc: 0.9508032754808664\n",
      "NetRNNWithAttention, rep: 0, epoch: 172, acc: 0.9565543155744671\n",
      "NetRNNWithAttention, rep: 0, epoch: 173, acc: 0.9538365584146231\n",
      "NetRNNWithAttention, rep: 0, epoch: 174, acc: 0.9486260568257421\n",
      "NetRNNWithAttention, rep: 0, epoch: 175, acc: 0.9527883352991193\n",
      "NetRNNWithAttention, rep: 0, epoch: 176, acc: 0.9530142838601023\n",
      "NetRNNWithAttention, rep: 0, epoch: 177, acc: 0.960444074086845\n",
      "NetRNNWithAttention, rep: 0, epoch: 178, acc: 0.9604210244305432\n",
      "NetRNNWithAttention, rep: 0, epoch: 179, acc: 0.9600486131571233\n",
      "NetRNNWithAttention, rep: 0, epoch: 180, acc: 0.9614079032279551\n",
      "NetRNNWithAttention, rep: 0, epoch: 181, acc: 0.9609056326374411\n",
      "NetRNNWithAttention, rep: 0, epoch: 182, acc: 0.963498508175835\n",
      "NetRNNWithAttention, rep: 0, epoch: 183, acc: 0.9585561468638479\n",
      "NetRNNWithAttention, rep: 0, epoch: 184, acc: 0.9673218070436269\n",
      "NetRNNWithAttention, rep: 0, epoch: 185, acc: 0.9655136139411479\n",
      "NetRNNWithAttention, rep: 0, epoch: 186, acc: 0.9662900682259351\n",
      "NetRNNWithAttention, rep: 0, epoch: 187, acc: 0.9662176837725565\n",
      "NetRNNWithAttention, rep: 0, epoch: 188, acc: 0.9696021356666461\n",
      "NetRNNWithAttention, rep: 0, epoch: 189, acc: 0.9678098428435624\n",
      "NetRNNWithAttention, rep: 0, epoch: 190, acc: 0.9662405597791076\n",
      "NetRNNWithAttention, rep: 0, epoch: 191, acc: 0.971115996572189\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 191   Acc: 0.9711 Params: min_length: 20, max_length: 20, fill: 0, value_1: -1, value_2: 1 Time: 66.04 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.5002744713425636\n",
      "RNN, rep: 0, epoch: 2, acc: 0.5022939124703407\n",
      "RNN, rep: 0, epoch: 3, acc: 0.49870285838842393\n",
      "RNN, rep: 0, epoch: 4, acc: 0.4976062050461769\n",
      "RNN, rep: 0, epoch: 5, acc: 0.5005916371941567\n",
      "RNN, rep: 0, epoch: 6, acc: 0.5027583968639374\n",
      "RNN, rep: 0, epoch: 7, acc: 0.500046453177929\n",
      "RNN, rep: 0, epoch: 8, acc: 0.4994653743505478\n",
      "RNN, rep: 0, epoch: 9, acc: 0.49993479639291766\n",
      "RNN, rep: 0, epoch: 10, acc: 0.5015658289194107\n",
      "RNN, rep: 0, epoch: 11, acc: 0.500773330628872\n",
      "RNN, rep: 0, epoch: 12, acc: 0.5004284280538559\n",
      "RNN, rep: 0, epoch: 13, acc: 0.4990772596001625\n",
      "RNN, rep: 0, epoch: 14, acc: 0.49955330431461337\n",
      "RNN, rep: 0, epoch: 15, acc: 0.5008637368679046\n",
      "RNN, rep: 0, epoch: 16, acc: 0.49923527866601947\n",
      "RNN, rep: 0, epoch: 17, acc: 0.5006448763608933\n",
      "RNN, rep: 0, epoch: 18, acc: 0.501349735558033\n",
      "RNN, rep: 0, epoch: 19, acc: 0.4999836447834969\n",
      "RNN, rep: 0, epoch: 20, acc: 0.5008867502212524\n",
      "RNN, rep: 0, epoch: 21, acc: 0.5034712183475495\n",
      "RNN, rep: 0, epoch: 22, acc: 0.5034747821092606\n",
      "RNN, rep: 0, epoch: 23, acc: 0.5159771677851677\n",
      "RNN, rep: 0, epoch: 24, acc: 0.5436111503839492\n",
      "RNN, rep: 0, epoch: 25, acc: 0.5781939285993576\n",
      "RNN, rep: 0, epoch: 26, acc: 0.6088845270872116\n",
      "RNN, rep: 0, epoch: 27, acc: 0.6141738441586494\n",
      "RNN, rep: 0, epoch: 28, acc: 0.6221492686867713\n",
      "RNN, rep: 0, epoch: 29, acc: 0.6308737942576408\n",
      "RNN, rep: 0, epoch: 30, acc: 0.6335791686177253\n",
      "RNN, rep: 0, epoch: 31, acc: 0.6410508280992508\n",
      "RNN, rep: 0, epoch: 32, acc: 0.6494834378361702\n",
      "RNN, rep: 0, epoch: 33, acc: 0.6447384956479073\n",
      "RNN, rep: 0, epoch: 34, acc: 0.6410216745734215\n",
      "RNN, rep: 0, epoch: 35, acc: 0.6511989519000053\n",
      "RNN, rep: 0, epoch: 36, acc: 0.6542169705033303\n",
      "RNN, rep: 0, epoch: 37, acc: 0.6712395170331001\n",
      "RNN, rep: 0, epoch: 38, acc: 0.6874468043446541\n",
      "RNN, rep: 0, epoch: 39, acc: 0.7057132914662361\n",
      "RNN, rep: 0, epoch: 40, acc: 0.7109764698147774\n",
      "RNN, rep: 0, epoch: 41, acc: 0.7235364833474159\n",
      "RNN, rep: 0, epoch: 42, acc: 0.7378151711821556\n",
      "RNN, rep: 0, epoch: 43, acc: 0.740107992887497\n",
      "RNN, rep: 0, epoch: 44, acc: 0.7426384332776069\n",
      "RNN, rep: 0, epoch: 45, acc: 0.7596889553964138\n",
      "RNN, rep: 0, epoch: 46, acc: 0.7649387614428997\n",
      "RNN, rep: 0, epoch: 47, acc: 0.7743821120262147\n",
      "RNN, rep: 0, epoch: 48, acc: 0.7754728202521801\n",
      "RNN, rep: 0, epoch: 49, acc: 0.7810839802026749\n",
      "RNN, rep: 0, epoch: 50, acc: 0.7927182693779469\n",
      "RNN, rep: 0, epoch: 51, acc: 0.7870581659674645\n",
      "RNN, rep: 0, epoch: 52, acc: 0.7982008761167526\n",
      "RNN, rep: 0, epoch: 53, acc: 0.7949839769303799\n",
      "RNN, rep: 0, epoch: 54, acc: 0.7886514472961426\n",
      "RNN, rep: 0, epoch: 55, acc: 0.8032850590348244\n",
      "RNN, rep: 0, epoch: 56, acc: 0.8003555808961391\n",
      "RNN, rep: 0, epoch: 57, acc: 0.8099330985546112\n",
      "RNN, rep: 0, epoch: 58, acc: 0.8111130644381046\n",
      "RNN, rep: 0, epoch: 59, acc: 0.8184620693325997\n",
      "RNN, rep: 0, epoch: 60, acc: 0.8172992147505284\n",
      "RNN, rep: 0, epoch: 61, acc: 0.8283311082422733\n",
      "RNN, rep: 0, epoch: 62, acc: 0.8349703246355057\n",
      "RNN, rep: 0, epoch: 63, acc: 0.80089688539505\n",
      "RNN, rep: 0, epoch: 64, acc: 0.822661893516779\n",
      "RNN, rep: 0, epoch: 65, acc: 0.8396084805577994\n",
      "RNN, rep: 0, epoch: 66, acc: 0.8470277885347605\n",
      "RNN, rep: 0, epoch: 67, acc: 0.8453549952805042\n",
      "RNN, rep: 0, epoch: 68, acc: 0.8530126169323922\n",
      "RNN, rep: 0, epoch: 69, acc: 0.8614519181847572\n",
      "RNN, rep: 0, epoch: 70, acc: 0.8683685456961393\n",
      "RNN, rep: 0, epoch: 71, acc: 0.8778997153788805\n",
      "RNN, rep: 0, epoch: 72, acc: 0.8825848292559385\n",
      "RNN, rep: 0, epoch: 73, acc: 0.8801965612918139\n",
      "RNN, rep: 0, epoch: 74, acc: 0.8902291125059127\n",
      "RNN, rep: 0, epoch: 75, acc: 0.8952623285353184\n",
      "RNN, rep: 0, epoch: 76, acc: 0.8974750454723835\n",
      "RNN, rep: 0, epoch: 77, acc: 0.9025802608579397\n",
      "RNN, rep: 0, epoch: 78, acc: 0.9013409095257521\n",
      "RNN, rep: 0, epoch: 79, acc: 0.9031034216284752\n",
      "RNN, rep: 0, epoch: 80, acc: 0.9100925217941404\n",
      "RNN, rep: 0, epoch: 81, acc: 0.9179099909961224\n",
      "RNN, rep: 0, epoch: 82, acc: 0.9123072269931436\n",
      "RNN, rep: 0, epoch: 83, acc: 0.9196487686410546\n",
      "RNN, rep: 0, epoch: 84, acc: 0.8676758679002523\n",
      "RNN, rep: 0, epoch: 85, acc: 0.9239420571550727\n",
      "RNN, rep: 0, epoch: 86, acc: 0.9257972131669522\n",
      "RNN, rep: 0, epoch: 87, acc: 0.9337691223621368\n",
      "RNN, rep: 0, epoch: 88, acc: 0.9112363553792239\n",
      "RNN, rep: 0, epoch: 89, acc: 0.7246800884231925\n",
      "RNN, rep: 0, epoch: 90, acc: 0.8164041812345385\n",
      "RNN, rep: 0, epoch: 91, acc: 0.9158350441977382\n",
      "RNN, rep: 0, epoch: 92, acc: 0.9246622014790773\n",
      "RNN, rep: 0, epoch: 93, acc: 0.9282815395668149\n",
      "RNN, rep: 0, epoch: 94, acc: 0.9347283488139511\n",
      "RNN, rep: 0, epoch: 95, acc: 0.9367151051014662\n",
      "RNN, rep: 0, epoch: 96, acc: 0.9401436555385589\n",
      "RNN, rep: 0, epoch: 97, acc: 0.9420575971528887\n",
      "RNN, rep: 0, epoch: 98, acc: 0.9448694777488709\n",
      "RNN, rep: 0, epoch: 99, acc: 0.9459039196372032\n",
      "RNN, rep: 0, epoch: 100, acc: 0.9509360364452004\n",
      "RNN, rep: 0, epoch: 101, acc: 0.9510221203789115\n",
      "RNN, rep: 0, epoch: 102, acc: 0.9499929486960172\n",
      "RNN, rep: 0, epoch: 103, acc: 0.9524393508210778\n",
      "RNN, rep: 0, epoch: 104, acc: 0.9564248063787818\n",
      "RNN, rep: 0, epoch: 105, acc: 0.9273455661162734\n",
      "RNN, rep: 0, epoch: 106, acc: 0.9567944866977632\n",
      "RNN, rep: 0, epoch: 107, acc: 0.9576388422586024\n",
      "RNN, rep: 0, epoch: 108, acc: 0.9588138088583946\n",
      "RNN, rep: 0, epoch: 109, acc: 0.9597953338548542\n",
      "RNN, rep: 0, epoch: 110, acc: 0.9610318657569588\n",
      "RNN, rep: 0, epoch: 111, acc: 0.9644477178342641\n",
      "RNN, rep: 0, epoch: 112, acc: 0.9630782960169018\n",
      "RNN, rep: 0, epoch: 113, acc: 0.9661192934028804\n",
      "RNN, rep: 0, epoch: 114, acc: 0.9670012119784951\n",
      "RNN, rep: 0, epoch: 115, acc: 0.966676107365638\n",
      "RNN, rep: 0, epoch: 116, acc: 0.9705089267157018\n",
      "RNN                  Rep: 0   Epoch: 116   Acc: 0.9705 Params: min_length: 20, max_length: 25, fill: 0, value_1: -1, value_2: 1 Time: 33.43 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.4990851223468781\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.49867344230413435\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5028025734424592\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5013648340106011\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.5009078419208527\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.5029377830028534\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.5080391117930412\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.5348141235113144\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.5994339936971664\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.6288682374358178\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.6783167234063149\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.7160033077001572\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.7373887540400028\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.7544441515207291\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.7642152954638004\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.7683399085700512\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.7784093365073204\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.7818867550790309\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.7858299109339714\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.7902402894198894\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.7950014606118202\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.7955566608905792\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.7990261982381344\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.8028959590196609\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.8036152918636799\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.809032516181469\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.8119058512151242\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.8155471527576447\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.817196619361639\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.8184890995919705\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.8296027828752994\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.8333536092936993\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.8384639070928097\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.8438460354506969\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.847061862796545\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.8548113565891982\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.8551642280817032\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.8597283385694027\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.8675924079865217\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.8625949159264564\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.8660099545866251\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.8696415484696627\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.8756014450639487\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.8693463423848152\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.8754512482136488\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.874386514723301\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.8798874061182141\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.877810508236289\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.8760669378936291\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.881258566826582\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.8854020984098315\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.8848221602290869\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.8881492449343205\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.8901719046756625\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.8165204123780131\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.7800866113603115\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.8091701332107186\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.7980301509797573\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.8343603303283453\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.8170159048959613\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.8373165699467063\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.8245051854103803\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.8380024046078325\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.8157227806001902\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.8344636169448495\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.7702283631265163\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.7164257690310478\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.6902847007289529\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.6720492165908217\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.7144443549960852\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.7479099094495177\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.7300305180624127\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.7732775595784187\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.8061091520264745\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.8124027112498879\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.8153668718412519\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.8394818121939898\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.8349317945167423\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.8453309446200729\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.8425657644495368\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.8459962319582701\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.8702889600023628\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.8690445802733302\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.8746326732635498\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.8651807075738907\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.8830751929804683\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.8691102457791566\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.8661186246946454\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.8854334897920489\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.8814061176031828\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.8894717794284225\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.8772572733461856\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.8910671906545758\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.8961032285168767\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.805291792601347\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.7511937732994557\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.8301984807476401\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.8736933454498649\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.879152138158679\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.8832912643998861\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.8920809483155608\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.8912377183884382\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.8979573093168437\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.9085364743322134\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.9030106965824962\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.9100834222510457\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.9151218781620264\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.9143375737965107\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.8934518518298864\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.9072117050178349\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.9128470168821514\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.9105046316608787\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.9134629214182496\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.9250306965224445\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.9248059110343456\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.9270996252261102\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.9279693179018795\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.9341546942293644\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.9313283863663674\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.9324890168942511\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.9339931095205247\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.9311453224532307\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.9349080905504524\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.9346801140345633\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.9365660371817648\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.9400069083273411\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.9392011278122664\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.947261170707643\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.950233096703887\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.95558912537992\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.9475037069432437\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.9383145796135068\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.9462951905280351\n",
      "NetRNNWithAttention, rep: 0, epoch: 134, acc: 0.9462907655909657\n",
      "NetRNNWithAttention, rep: 0, epoch: 135, acc: 0.9477838859148323\n",
      "NetRNNWithAttention, rep: 0, epoch: 136, acc: 0.9520792120229453\n",
      "NetRNNWithAttention, rep: 0, epoch: 137, acc: 0.9500052824988962\n",
      "NetRNNWithAttention, rep: 0, epoch: 138, acc: 0.9509477593749761\n",
      "NetRNNWithAttention, rep: 0, epoch: 139, acc: 0.953291270583868\n",
      "NetRNNWithAttention, rep: 0, epoch: 140, acc: 0.9571104953903705\n",
      "NetRNNWithAttention, rep: 0, epoch: 141, acc: 0.9553679189365357\n",
      "NetRNNWithAttention, rep: 0, epoch: 142, acc: 0.9589075103402138\n",
      "NetRNNWithAttention, rep: 0, epoch: 143, acc: 0.9564500194694847\n",
      "NetRNNWithAttention, rep: 0, epoch: 144, acc: 0.9552965964004397\n",
      "NetRNNWithAttention, rep: 0, epoch: 145, acc: 0.960224232133478\n",
      "NetRNNWithAttention, rep: 0, epoch: 146, acc: 0.9586840546689928\n",
      "NetRNNWithAttention, rep: 0, epoch: 147, acc: 0.9625025192648172\n",
      "NetRNNWithAttention, rep: 0, epoch: 148, acc: 0.9619049722421914\n",
      "NetRNNWithAttention, rep: 0, epoch: 149, acc: 0.9627728844434023\n",
      "NetRNNWithAttention, rep: 0, epoch: 150, acc: 0.9599543147906661\n",
      "NetRNNWithAttention, rep: 0, epoch: 151, acc: 0.9634566068835556\n",
      "NetRNNWithAttention, rep: 0, epoch: 152, acc: 0.9650235659908504\n",
      "NetRNNWithAttention, rep: 0, epoch: 153, acc: 0.9636100328620523\n",
      "NetRNNWithAttention, rep: 0, epoch: 154, acc: 0.9663833283446729\n",
      "NetRNNWithAttention, rep: 0, epoch: 155, acc: 0.9669242665916681\n",
      "NetRNNWithAttention, rep: 0, epoch: 156, acc: 0.9660390255972743\n",
      "NetRNNWithAttention, rep: 0, epoch: 157, acc: 0.9697102563269436\n",
      "NetRNNWithAttention, rep: 0, epoch: 158, acc: 0.970109069654718\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 158   Acc: 0.9701 Params: min_length: 20, max_length: 25, fill: 0, value_1: -1, value_2: 1 Time: 57.29 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.5017749226093292\n",
      "RNN, rep: 0, epoch: 2, acc: 0.49910952657461166\n",
      "RNN, rep: 0, epoch: 3, acc: 0.5030312052369118\n",
      "RNN, rep: 0, epoch: 4, acc: 0.4986340856552124\n",
      "RNN, rep: 0, epoch: 5, acc: 0.5008370414376259\n",
      "RNN, rep: 0, epoch: 6, acc: 0.5015659448504448\n",
      "RNN, rep: 0, epoch: 7, acc: 0.5012052887678147\n",
      "RNN, rep: 0, epoch: 8, acc: 0.5094753775000572\n",
      "RNN, rep: 0, epoch: 9, acc: 0.49917309045791625\n",
      "RNN, rep: 0, epoch: 10, acc: 0.4999641051888466\n",
      "RNN, rep: 0, epoch: 11, acc: 0.5122809886932373\n",
      "RNN, rep: 0, epoch: 12, acc: 0.5155018067359924\n",
      "RNN, rep: 0, epoch: 13, acc: 0.5133991539478302\n",
      "RNN, rep: 0, epoch: 14, acc: 0.5333459633588791\n",
      "RNN, rep: 0, epoch: 15, acc: 0.5061813068389892\n",
      "RNN, rep: 0, epoch: 16, acc: 0.5435883110761642\n",
      "RNN, rep: 0, epoch: 17, acc: 0.5770545390248298\n",
      "RNN, rep: 0, epoch: 18, acc: 0.5947854951024055\n",
      "RNN, rep: 0, epoch: 19, acc: 0.6139366582036019\n",
      "RNN, rep: 0, epoch: 20, acc: 0.6239811298251152\n",
      "RNN, rep: 0, epoch: 21, acc: 0.6281899121403695\n",
      "RNN, rep: 0, epoch: 22, acc: 0.6351102790236474\n",
      "RNN, rep: 0, epoch: 23, acc: 0.6348635452985764\n",
      "RNN, rep: 0, epoch: 24, acc: 0.6398024266958237\n",
      "RNN, rep: 0, epoch: 25, acc: 0.6414629596471787\n",
      "RNN, rep: 0, epoch: 26, acc: 0.6228837397694588\n",
      "RNN, rep: 0, epoch: 27, acc: 0.5001686254143715\n",
      "RNN, rep: 0, epoch: 28, acc: 0.49052044451236726\n",
      "RNN, rep: 0, epoch: 29, acc: 0.5055426186323166\n",
      "RNN, rep: 0, epoch: 30, acc: 0.5039465582370758\n",
      "RNN, rep: 0, epoch: 31, acc: 0.5033775001764298\n",
      "RNN, rep: 0, epoch: 32, acc: 0.5070920377969742\n",
      "RNN, rep: 0, epoch: 33, acc: 0.5061150446534157\n",
      "RNN, rep: 0, epoch: 34, acc: 0.5109899634122849\n",
      "RNN, rep: 0, epoch: 35, acc: 0.5339765620231628\n",
      "RNN, rep: 0, epoch: 36, acc: 0.5593415433168412\n",
      "RNN, rep: 0, epoch: 37, acc: 0.5972819915413856\n",
      "RNN, rep: 0, epoch: 38, acc: 0.6105061969161034\n",
      "RNN, rep: 0, epoch: 39, acc: 0.6231844088435173\n",
      "RNN, rep: 0, epoch: 40, acc: 0.6280266731977463\n",
      "RNN, rep: 0, epoch: 41, acc: 0.6308653268218041\n",
      "RNN, rep: 0, epoch: 42, acc: 0.6330786156654358\n",
      "RNN, rep: 0, epoch: 43, acc: 0.6388954409956932\n",
      "RNN, rep: 0, epoch: 44, acc: 0.6411595445871353\n",
      "RNN, rep: 0, epoch: 45, acc: 0.6415399158000946\n",
      "RNN, rep: 0, epoch: 46, acc: 0.6433197012543679\n",
      "RNN, rep: 0, epoch: 47, acc: 0.6476952701807022\n",
      "RNN, rep: 0, epoch: 48, acc: 0.643951047360897\n",
      "RNN, rep: 0, epoch: 49, acc: 0.6444961163401604\n",
      "RNN, rep: 0, epoch: 50, acc: 0.64756571829319\n",
      "RNN, rep: 0, epoch: 51, acc: 0.6487062212824821\n",
      "RNN, rep: 0, epoch: 52, acc: 0.6517692840099335\n",
      "RNN, rep: 0, epoch: 53, acc: 0.649168761074543\n",
      "RNN, rep: 0, epoch: 54, acc: 0.6508814671635628\n",
      "RNN, rep: 0, epoch: 55, acc: 0.6503039327263832\n",
      "RNN, rep: 0, epoch: 56, acc: 0.6529923096299172\n",
      "RNN, rep: 0, epoch: 57, acc: 0.6526463922858238\n",
      "RNN, rep: 0, epoch: 58, acc: 0.6534766665101052\n",
      "RNN, rep: 0, epoch: 59, acc: 0.6554898032546044\n",
      "RNN, rep: 0, epoch: 60, acc: 0.6547372302412987\n",
      "RNN, rep: 0, epoch: 61, acc: 0.6523753482103348\n",
      "RNN, rep: 0, epoch: 62, acc: 0.5411689895391464\n",
      "RNN, rep: 0, epoch: 63, acc: 0.5669886898994446\n",
      "RNN, rep: 0, epoch: 64, acc: 0.5754527547955512\n",
      "RNN, rep: 0, epoch: 65, acc: 0.6433342105150223\n",
      "RNN, rep: 0, epoch: 66, acc: 0.6490033593773842\n",
      "RNN, rep: 0, epoch: 67, acc: 0.6520833787322045\n",
      "RNN, rep: 0, epoch: 68, acc: 0.6539917829632759\n",
      "RNN, rep: 0, epoch: 69, acc: 0.6551306828856468\n",
      "RNN, rep: 0, epoch: 70, acc: 0.6547055625915528\n",
      "RNN, rep: 0, epoch: 71, acc: 0.6562189090251923\n",
      "RNN, rep: 0, epoch: 72, acc: 0.6038861206173897\n",
      "RNN, rep: 0, epoch: 73, acc: 0.5371119344234466\n",
      "RNN, rep: 0, epoch: 74, acc: 0.567601065337658\n",
      "RNN, rep: 0, epoch: 75, acc: 0.5516026365756989\n",
      "RNN, rep: 0, epoch: 76, acc: 0.5365613442659378\n",
      "RNN, rep: 0, epoch: 77, acc: 0.5302043387293816\n",
      "RNN, rep: 0, epoch: 78, acc: 0.5383150151371956\n",
      "RNN, rep: 0, epoch: 79, acc: 0.5462849798798561\n",
      "RNN, rep: 0, epoch: 80, acc: 0.5458114752173424\n",
      "RNN, rep: 0, epoch: 81, acc: 0.5533912122249603\n",
      "RNN, rep: 0, epoch: 82, acc: 0.5507046073675156\n",
      "RNN, rep: 0, epoch: 83, acc: 0.5499972152709961\n",
      "RNN, rep: 0, epoch: 84, acc: 0.5320519107580185\n",
      "RNN, rep: 0, epoch: 85, acc: 0.543136422932148\n",
      "RNN, rep: 0, epoch: 86, acc: 0.546534349322319\n",
      "RNN, rep: 0, epoch: 87, acc: 0.5422760218381881\n",
      "RNN, rep: 0, epoch: 88, acc: 0.5815327227115631\n",
      "RNN, rep: 0, epoch: 89, acc: 0.6017000621557236\n",
      "RNN, rep: 0, epoch: 90, acc: 0.6153023126721382\n",
      "RNN, rep: 0, epoch: 91, acc: 0.6220523890852928\n",
      "RNN, rep: 0, epoch: 92, acc: 0.6248698058724403\n",
      "RNN, rep: 0, epoch: 93, acc: 0.6310215544700623\n",
      "RNN, rep: 0, epoch: 94, acc: 0.6356075009703637\n",
      "RNN, rep: 0, epoch: 95, acc: 0.6401578450202942\n",
      "RNN, rep: 0, epoch: 96, acc: 0.6383281034231186\n",
      "RNN, rep: 0, epoch: 97, acc: 0.6422019621729851\n",
      "RNN, rep: 0, epoch: 98, acc: 0.6423801037669182\n",
      "RNN, rep: 0, epoch: 99, acc: 0.6448336815834046\n",
      "RNN, rep: 0, epoch: 100, acc: 0.645615399479866\n",
      "RNN, rep: 0, epoch: 101, acc: 0.6464609694480896\n",
      "RNN, rep: 0, epoch: 102, acc: 0.6506253188848495\n",
      "RNN, rep: 0, epoch: 103, acc: 0.653961321413517\n",
      "RNN, rep: 0, epoch: 104, acc: 0.655574160516262\n",
      "RNN, rep: 0, epoch: 105, acc: 0.6499565836787223\n",
      "RNN, rep: 0, epoch: 106, acc: 0.6538498818874359\n",
      "RNN, rep: 0, epoch: 107, acc: 0.6525435674190522\n",
      "RNN, rep: 0, epoch: 108, acc: 0.6513789823651314\n",
      "RNN, rep: 0, epoch: 109, acc: 0.6524533650279045\n",
      "RNN, rep: 0, epoch: 110, acc: 0.6566937312483787\n",
      "RNN, rep: 0, epoch: 111, acc: 0.6554517656564712\n",
      "RNN, rep: 0, epoch: 112, acc: 0.6533903867006302\n",
      "RNN, rep: 0, epoch: 113, acc: 0.6532335248589516\n",
      "RNN, rep: 0, epoch: 114, acc: 0.6553952303528786\n",
      "RNN, rep: 0, epoch: 115, acc: 0.6568678948283195\n",
      "RNN, rep: 0, epoch: 116, acc: 0.653581708073616\n",
      "RNN, rep: 0, epoch: 117, acc: 0.6561634528636933\n",
      "RNN, rep: 0, epoch: 118, acc: 0.6569407486915588\n",
      "RNN, rep: 0, epoch: 119, acc: 0.6598699903488159\n",
      "RNN, rep: 0, epoch: 120, acc: 0.6615779349207878\n",
      "RNN, rep: 0, epoch: 121, acc: 0.6555727919936181\n",
      "RNN, rep: 0, epoch: 122, acc: 0.6560961654782296\n",
      "RNN, rep: 0, epoch: 123, acc: 0.6584205639362335\n",
      "RNN, rep: 0, epoch: 124, acc: 0.6586109736561775\n",
      "RNN, rep: 0, epoch: 125, acc: 0.6597593295574188\n",
      "RNN, rep: 0, epoch: 126, acc: 0.6603974214196205\n",
      "RNN, rep: 0, epoch: 127, acc: 0.6592116954922677\n",
      "RNN, rep: 0, epoch: 128, acc: 0.659160032570362\n",
      "RNN, rep: 0, epoch: 129, acc: 0.6589014774560928\n",
      "RNN, rep: 0, epoch: 130, acc: 0.6615667575597763\n",
      "RNN, rep: 0, epoch: 131, acc: 0.6601381060481072\n",
      "RNN, rep: 0, epoch: 132, acc: 0.6602991080284119\n",
      "RNN, rep: 0, epoch: 133, acc: 0.6599582916498185\n",
      "RNN, rep: 0, epoch: 134, acc: 0.6601107409596443\n",
      "RNN, rep: 0, epoch: 135, acc: 0.6602354842424393\n",
      "RNN, rep: 0, epoch: 136, acc: 0.6603059154748917\n",
      "RNN, rep: 0, epoch: 137, acc: 0.6609622293710709\n",
      "RNN, rep: 0, epoch: 138, acc: 0.661190120279789\n",
      "RNN, rep: 0, epoch: 139, acc: 0.6613195702433586\n",
      "RNN, rep: 0, epoch: 140, acc: 0.6624094584584236\n",
      "RNN, rep: 0, epoch: 141, acc: 0.6607447475194931\n",
      "RNN, rep: 0, epoch: 142, acc: 0.6634706640243531\n",
      "RNN, rep: 0, epoch: 143, acc: 0.6626165297627449\n",
      "RNN, rep: 0, epoch: 144, acc: 0.6629202806949616\n",
      "RNN, rep: 0, epoch: 145, acc: 0.6617050194740295\n",
      "RNN, rep: 0, epoch: 146, acc: 0.6623324525356292\n",
      "RNN, rep: 0, epoch: 147, acc: 0.6618467870354653\n",
      "RNN, rep: 0, epoch: 148, acc: 0.663585880100727\n",
      "RNN, rep: 0, epoch: 149, acc: 0.6639482334256173\n",
      "RNN, rep: 0, epoch: 150, acc: 0.6653510275483131\n",
      "RNN, rep: 0, epoch: 151, acc: 0.6619710010290146\n",
      "RNN, rep: 0, epoch: 152, acc: 0.6646351584792137\n",
      "RNN, rep: 0, epoch: 153, acc: 0.6627799615263938\n",
      "RNN, rep: 0, epoch: 154, acc: 0.6621956449747085\n",
      "RNN, rep: 0, epoch: 155, acc: 0.6623872578144073\n",
      "RNN, rep: 0, epoch: 156, acc: 0.664707114994526\n",
      "RNN, rep: 0, epoch: 157, acc: 0.6641347971558571\n",
      "RNN, rep: 0, epoch: 158, acc: 0.6622344562411309\n",
      "RNN, rep: 0, epoch: 159, acc: 0.6636910262703896\n",
      "RNN, rep: 0, epoch: 160, acc: 0.6632362660765648\n",
      "RNN, rep: 0, epoch: 161, acc: 0.6632920810580254\n",
      "RNN, rep: 0, epoch: 162, acc: 0.6643901827931404\n",
      "RNN, rep: 0, epoch: 163, acc: 0.6638926517963409\n",
      "RNN, rep: 0, epoch: 164, acc: 0.665309508740902\n",
      "RNN, rep: 0, epoch: 165, acc: 0.6634548366069793\n",
      "RNN, rep: 0, epoch: 166, acc: 0.6632976427674293\n",
      "RNN, rep: 0, epoch: 167, acc: 0.6660044759511947\n",
      "RNN, rep: 0, epoch: 168, acc: 0.661810465157032\n",
      "RNN, rep: 0, epoch: 169, acc: 0.6652800107002258\n",
      "RNN, rep: 0, epoch: 170, acc: 0.6613977074623107\n",
      "RNN, rep: 0, epoch: 171, acc: 0.6649855515360832\n",
      "RNN, rep: 0, epoch: 172, acc: 0.6646726238727569\n",
      "RNN, rep: 0, epoch: 173, acc: 0.6649809423089027\n",
      "RNN, rep: 0, epoch: 174, acc: 0.6639204892516136\n",
      "RNN, rep: 0, epoch: 175, acc: 0.6634030708670616\n",
      "RNN, rep: 0, epoch: 176, acc: 0.6644081825017929\n",
      "RNN, rep: 0, epoch: 177, acc: 0.669052283167839\n",
      "RNN, rep: 0, epoch: 178, acc: 0.6665056920051575\n",
      "RNN, rep: 0, epoch: 179, acc: 0.6666130810976029\n",
      "RNN, rep: 0, epoch: 180, acc: 0.6643207824230194\n",
      "RNN, rep: 0, epoch: 181, acc: 0.6625440242886543\n",
      "RNN, rep: 0, epoch: 182, acc: 0.6645592376589775\n",
      "RNN, rep: 0, epoch: 183, acc: 0.6658180212974548\n",
      "RNN, rep: 0, epoch: 184, acc: 0.6659188610315323\n",
      "RNN, rep: 0, epoch: 185, acc: 0.6094756332039833\n",
      "RNN, rep: 0, epoch: 186, acc: 0.6622305768728256\n",
      "RNN, rep: 0, epoch: 187, acc: 0.6654400697350502\n",
      "RNN, rep: 0, epoch: 188, acc: 0.6597413527965545\n",
      "RNN, rep: 0, epoch: 189, acc: 0.6638397151231765\n",
      "RNN, rep: 0, epoch: 190, acc: 0.6613200634717942\n",
      "RNN, rep: 0, epoch: 191, acc: 0.6654332500696182\n",
      "RNN, rep: 0, epoch: 192, acc: 0.6317998948693275\n",
      "RNN, rep: 0, epoch: 193, acc: 0.6480210834741592\n",
      "RNN, rep: 0, epoch: 194, acc: 0.6620055863261223\n",
      "RNN, rep: 0, epoch: 195, acc: 0.6610701459646225\n",
      "RNN, rep: 0, epoch: 196, acc: 0.6622968816757202\n",
      "RNN, rep: 0, epoch: 197, acc: 0.6623101007938385\n",
      "RNN, rep: 0, epoch: 198, acc: 0.6646929213404655\n",
      "RNN, rep: 0, epoch: 199, acc: 0.6616553294658661\n",
      "RNN, rep: 0, epoch: 200, acc: 0.6630459922552109\n",
      "RNN, rep: 0, epoch: 201, acc: 0.6626839181780815\n",
      "RNN, rep: 0, epoch: 202, acc: 0.6629665920138359\n",
      "RNN, rep: 0, epoch: 203, acc: 0.6639820477366447\n",
      "RNN, rep: 0, epoch: 204, acc: 0.662966416478157\n",
      "RNN, rep: 0, epoch: 205, acc: 0.6640740609169007\n",
      "RNN, rep: 0, epoch: 206, acc: 0.6628538939356804\n",
      "RNN, rep: 0, epoch: 207, acc: 0.6629333221912384\n",
      "RNN, rep: 0, epoch: 208, acc: 0.6638165441155434\n",
      "RNN, rep: 0, epoch: 209, acc: 0.6629504564404488\n",
      "RNN, rep: 0, epoch: 210, acc: 0.663914101421833\n",
      "RNN, rep: 0, epoch: 211, acc: 0.664966451227665\n",
      "RNN, rep: 0, epoch: 212, acc: 0.6684055042266845\n",
      "RNN, rep: 0, epoch: 213, acc: 0.6633182889223099\n",
      "RNN, rep: 0, epoch: 214, acc: 0.664688848555088\n",
      "RNN, rep: 0, epoch: 215, acc: 0.6637647747993469\n",
      "RNN, rep: 0, epoch: 216, acc: 0.664285096526146\n",
      "RNN, rep: 0, epoch: 217, acc: 0.6648209270834923\n",
      "RNN, rep: 0, epoch: 218, acc: 0.6632210105657578\n",
      "RNN, rep: 0, epoch: 219, acc: 0.6643985572457314\n",
      "RNN, rep: 0, epoch: 220, acc: 0.6661703976988792\n",
      "RNN, rep: 0, epoch: 221, acc: 0.6632474389672279\n",
      "RNN, rep: 0, epoch: 222, acc: 0.6669966676831245\n",
      "RNN, rep: 0, epoch: 223, acc: 0.6667878246307373\n",
      "RNN, rep: 0, epoch: 224, acc: 0.6638539296388626\n",
      "RNN, rep: 0, epoch: 225, acc: 0.6632718712091445\n",
      "RNN, rep: 0, epoch: 226, acc: 0.6599448883533477\n",
      "RNN, rep: 0, epoch: 227, acc: 0.66437768638134\n",
      "RNN, rep: 0, epoch: 228, acc: 0.6665696406364441\n",
      "RNN, rep: 0, epoch: 229, acc: 0.6642522239685058\n",
      "RNN, rep: 0, epoch: 230, acc: 0.666944353878498\n",
      "RNN, rep: 0, epoch: 231, acc: 0.6563050156831741\n",
      "RNN, rep: 0, epoch: 232, acc: 0.6338614067435264\n",
      "RNN, rep: 0, epoch: 233, acc: 0.6628161084651947\n",
      "RNN, rep: 0, epoch: 234, acc: 0.6623569652438164\n",
      "RNN, rep: 0, epoch: 235, acc: 0.6540229570865631\n",
      "RNN, rep: 0, epoch: 236, acc: 0.662596028149128\n",
      "RNN, rep: 0, epoch: 237, acc: 0.6635119014978409\n",
      "RNN, rep: 0, epoch: 238, acc: 0.6626128649711609\n",
      "RNN, rep: 0, epoch: 239, acc: 0.6649245595932007\n",
      "RNN, rep: 0, epoch: 240, acc: 0.6653541502356529\n",
      "RNN, rep: 0, epoch: 241, acc: 0.6656347748637199\n",
      "RNN, rep: 0, epoch: 242, acc: 0.6656324630975723\n",
      "RNN, rep: 0, epoch: 243, acc: 0.6633745357394218\n",
      "RNN, rep: 0, epoch: 244, acc: 0.6630855724215508\n",
      "RNN, rep: 0, epoch: 245, acc: 0.664288323521614\n",
      "RNN, rep: 0, epoch: 246, acc: 0.6641518488526345\n",
      "RNN, rep: 0, epoch: 247, acc: 0.6642740282416344\n",
      "RNN, rep: 0, epoch: 248, acc: 0.6640595200657845\n",
      "RNN, rep: 0, epoch: 249, acc: 0.6662522131204605\n",
      "RNN, rep: 0, epoch: 250, acc: 0.6619546368718148\n",
      "RNN, rep: 0, epoch: 251, acc: 0.6627468284964562\n",
      "RNN, rep: 0, epoch: 252, acc: 0.6669188529253006\n",
      "RNN, rep: 0, epoch: 253, acc: 0.6618414998054505\n",
      "RNN, rep: 0, epoch: 254, acc: 0.6633564558625221\n",
      "RNN, rep: 0, epoch: 255, acc: 0.6659564745426177\n",
      "RNN, rep: 0, epoch: 256, acc: 0.6639750891923905\n",
      "RNN, rep: 0, epoch: 257, acc: 0.6663700070977211\n",
      "RNN, rep: 0, epoch: 258, acc: 0.6647732371091842\n",
      "RNN, rep: 0, epoch: 259, acc: 0.6633388653397561\n",
      "RNN, rep: 0, epoch: 260, acc: 0.6665635025501251\n",
      "RNN, rep: 0, epoch: 261, acc: 0.6657707211375237\n",
      "RNN, rep: 0, epoch: 262, acc: 0.6631749173998833\n",
      "RNN, rep: 0, epoch: 263, acc: 0.665005368590355\n",
      "RNN, rep: 0, epoch: 264, acc: 0.6663395720720291\n",
      "RNN, rep: 0, epoch: 265, acc: 0.6647072196006775\n",
      "RNN, rep: 0, epoch: 266, acc: 0.6679650095105171\n",
      "RNN, rep: 0, epoch: 267, acc: 0.6657146647572517\n",
      "RNN, rep: 0, epoch: 268, acc: 0.6688108000159264\n",
      "RNN, rep: 0, epoch: 269, acc: 0.667769564986229\n",
      "RNN, rep: 0, epoch: 270, acc: 0.6649434465169907\n",
      "RNN, rep: 0, epoch: 271, acc: 0.6648306435346604\n",
      "RNN, rep: 0, epoch: 272, acc: 0.6636431124806405\n",
      "RNN, rep: 0, epoch: 273, acc: 0.6653867945075035\n",
      "RNN, rep: 0, epoch: 274, acc: 0.667250047326088\n",
      "RNN, rep: 0, epoch: 275, acc: 0.6674545267224312\n",
      "RNN, rep: 0, epoch: 276, acc: 0.6656723529100418\n",
      "RNN, rep: 0, epoch: 277, acc: 0.6714338010549545\n",
      "RNN, rep: 0, epoch: 278, acc: 0.6674496680498123\n",
      "RNN, rep: 0, epoch: 279, acc: 0.6698938521742821\n",
      "RNN, rep: 0, epoch: 280, acc: 0.66104940533638\n",
      "RNN, rep: 0, epoch: 281, acc: 0.6710745120048522\n",
      "RNN, rep: 0, epoch: 282, acc: 0.6686161965131759\n",
      "RNN, rep: 0, epoch: 283, acc: 0.6684245032072067\n",
      "RNN, rep: 0, epoch: 284, acc: 0.6640605971217155\n",
      "RNN, rep: 0, epoch: 285, acc: 0.6650726607441902\n",
      "RNN, rep: 0, epoch: 286, acc: 0.6660308107733727\n",
      "RNN, rep: 0, epoch: 287, acc: 0.6677024719119072\n",
      "RNN, rep: 0, epoch: 288, acc: 0.666660028398037\n",
      "RNN, rep: 0, epoch: 289, acc: 0.6638606768846512\n",
      "RNN, rep: 0, epoch: 290, acc: 0.6676939460635185\n",
      "RNN, rep: 0, epoch: 291, acc: 0.6674552083015441\n",
      "RNN, rep: 0, epoch: 292, acc: 0.6668544319272042\n",
      "RNN, rep: 0, epoch: 293, acc: 0.6678468069434166\n",
      "RNN, rep: 0, epoch: 294, acc: 0.6697542625665664\n",
      "RNN, rep: 0, epoch: 295, acc: 0.6675496900081634\n",
      "RNN, rep: 0, epoch: 296, acc: 0.6677748915553093\n",
      "RNN, rep: 0, epoch: 297, acc: 0.6692067596316338\n",
      "RNN, rep: 0, epoch: 298, acc: 0.6663946431875228\n",
      "RNN, rep: 0, epoch: 299, acc: 0.6684603714942932\n",
      "RNN, rep: 0, epoch: 300, acc: 0.671174248456955\n",
      "RNN, rep: 0, epoch: 301, acc: 0.6722517436742783\n",
      "RNN, rep: 0, epoch: 302, acc: 0.67977728754282\n",
      "RNN, rep: 0, epoch: 303, acc: 0.6721470065414905\n",
      "RNN, rep: 0, epoch: 304, acc: 0.5889081779122353\n",
      "RNN, rep: 0, epoch: 305, acc: 0.6971261455118656\n",
      "RNN, rep: 0, epoch: 306, acc: 0.697509800195694\n",
      "RNN, rep: 0, epoch: 307, acc: 0.692910622805357\n",
      "RNN, rep: 0, epoch: 308, acc: 0.6911959518492222\n",
      "RNN, rep: 0, epoch: 309, acc: 0.688553617745638\n",
      "RNN, rep: 0, epoch: 310, acc: 0.683011663556099\n",
      "RNN, rep: 0, epoch: 311, acc: 0.6864972157776356\n",
      "RNN, rep: 0, epoch: 312, acc: 0.6836925615370274\n",
      "RNN, rep: 0, epoch: 313, acc: 0.690048623085022\n",
      "RNN, rep: 0, epoch: 314, acc: 0.7045509006083012\n",
      "RNN, rep: 0, epoch: 315, acc: 0.6957993388175965\n",
      "RNN, rep: 0, epoch: 316, acc: 0.688360376060009\n",
      "RNN, rep: 0, epoch: 317, acc: 0.6906872078776359\n",
      "RNN, rep: 0, epoch: 318, acc: 0.6906662841141223\n",
      "RNN, rep: 0, epoch: 319, acc: 0.6959525299072266\n",
      "RNN, rep: 0, epoch: 320, acc: 0.6921579915285111\n",
      "RNN, rep: 0, epoch: 321, acc: 0.7000566707551479\n",
      "RNN, rep: 0, epoch: 322, acc: 0.6807913281023502\n",
      "RNN, rep: 0, epoch: 323, acc: 0.7193532864749431\n",
      "RNN, rep: 0, epoch: 324, acc: 0.7497192105650902\n",
      "RNN, rep: 0, epoch: 325, acc: 0.7671365106105804\n",
      "RNN, rep: 0, epoch: 326, acc: 0.7774510152637959\n",
      "RNN, rep: 0, epoch: 327, acc: 0.7889302361011505\n",
      "RNN, rep: 0, epoch: 328, acc: 0.797808790653944\n",
      "RNN, rep: 0, epoch: 329, acc: 0.8084769108891487\n",
      "RNN, rep: 0, epoch: 330, acc: 0.8126732966303826\n",
      "RNN, rep: 0, epoch: 331, acc: 0.8182122388482094\n",
      "RNN, rep: 0, epoch: 332, acc: 0.8230816254019737\n",
      "RNN, rep: 0, epoch: 333, acc: 0.8306895545125008\n",
      "RNN, rep: 0, epoch: 334, acc: 0.8347557316720485\n",
      "RNN, rep: 0, epoch: 335, acc: 0.8387498427927494\n",
      "RNN, rep: 0, epoch: 336, acc: 0.8507371068000793\n",
      "RNN, rep: 0, epoch: 337, acc: 0.8459381783753633\n",
      "RNN, rep: 0, epoch: 338, acc: 0.8539295304566622\n",
      "RNN, rep: 0, epoch: 339, acc: 0.8563320219516755\n",
      "RNN, rep: 0, epoch: 340, acc: 0.8611820520460606\n",
      "RNN, rep: 0, epoch: 341, acc: 0.8668609252572059\n",
      "RNN, rep: 0, epoch: 342, acc: 0.8705055461823941\n",
      "RNN, rep: 0, epoch: 343, acc: 0.8704969783872366\n",
      "RNN, rep: 0, epoch: 344, acc: 0.8786250951141119\n",
      "RNN, rep: 0, epoch: 345, acc: 0.877372223213315\n",
      "RNN, rep: 0, epoch: 346, acc: 0.8329821456223726\n",
      "RNN, rep: 0, epoch: 347, acc: 0.8812314072251319\n",
      "RNN, rep: 0, epoch: 348, acc: 0.889560526534915\n",
      "RNN, rep: 0, epoch: 349, acc: 0.8943174076080322\n",
      "RNN, rep: 0, epoch: 350, acc: 0.8872047835588455\n",
      "RNN, rep: 0, epoch: 351, acc: 0.8933536606281995\n",
      "RNN, rep: 0, epoch: 352, acc: 0.8975021082907915\n",
      "RNN, rep: 0, epoch: 353, acc: 0.9029909930378198\n",
      "RNN, rep: 0, epoch: 354, acc: 0.9052329067140817\n",
      "RNN, rep: 0, epoch: 355, acc: 0.903919977247715\n",
      "RNN, rep: 0, epoch: 356, acc: 0.9116186269372701\n",
      "RNN, rep: 0, epoch: 357, acc: 0.9131859783828259\n",
      "RNN, rep: 0, epoch: 358, acc: 0.9165733844786882\n",
      "RNN, rep: 0, epoch: 359, acc: 0.9194864276051521\n",
      "RNN, rep: 0, epoch: 360, acc: 0.9219558401405812\n",
      "RNN, rep: 0, epoch: 361, acc: 0.926062436401844\n",
      "RNN, rep: 0, epoch: 362, acc: 0.9156447960808873\n",
      "RNN, rep: 0, epoch: 363, acc: 0.926058048941195\n",
      "RNN, rep: 0, epoch: 364, acc: 0.9220430551469326\n",
      "RNN, rep: 0, epoch: 365, acc: 0.9308067945018411\n",
      "RNN, rep: 0, epoch: 366, acc: 0.9287702603638173\n",
      "RNN, rep: 0, epoch: 367, acc: 0.9342112378403544\n",
      "RNN, rep: 0, epoch: 368, acc: 0.936919258236885\n",
      "RNN, rep: 0, epoch: 369, acc: 0.9380064728483558\n",
      "RNN, rep: 0, epoch: 370, acc: 0.9402441798895598\n",
      "RNN, rep: 0, epoch: 371, acc: 0.9441767711937428\n",
      "RNN, rep: 0, epoch: 372, acc: 0.9453338675945997\n",
      "RNN, rep: 0, epoch: 373, acc: 0.945696260407567\n",
      "RNN, rep: 0, epoch: 374, acc: 0.9479900070279836\n",
      "RNN, rep: 0, epoch: 375, acc: 0.9492146223038435\n",
      "RNN, rep: 0, epoch: 376, acc: 0.9513414206728339\n",
      "RNN, rep: 0, epoch: 377, acc: 0.9516470719873905\n",
      "RNN, rep: 0, epoch: 378, acc: 0.9543409666791558\n",
      "RNN, rep: 0, epoch: 379, acc: 0.9552137455344201\n",
      "RNN, rep: 0, epoch: 380, acc: 0.9576303018629551\n",
      "RNN, rep: 0, epoch: 381, acc: 0.9562369014695287\n",
      "RNN, rep: 0, epoch: 382, acc: 0.9595480305701494\n",
      "RNN, rep: 0, epoch: 383, acc: 0.9607510194927454\n",
      "RNN, rep: 0, epoch: 384, acc: 0.9595620562881231\n",
      "RNN, rep: 0, epoch: 385, acc: 0.9632940794155002\n",
      "RNN, rep: 0, epoch: 386, acc: 0.9642237177491189\n",
      "RNN, rep: 0, epoch: 387, acc: 0.964203217048198\n",
      "RNN, rep: 0, epoch: 388, acc: 0.965636587664485\n",
      "RNN, rep: 0, epoch: 389, acc: 0.9668013799190521\n",
      "RNN, rep: 0, epoch: 390, acc: 0.9669038033485413\n",
      "RNN, rep: 0, epoch: 391, acc: 0.9673486807569861\n",
      "RNN, rep: 0, epoch: 392, acc: 0.9686610165797174\n",
      "RNN, rep: 0, epoch: 393, acc: 0.9703078394383192\n",
      "RNN                  Rep: 0   Epoch: 393   Acc: 0.9703 Params: min_length: 40, max_length: 40, fill: 0, value_1: -1, value_2: 1 Time: 160.07 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.496453133225441\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.4991662389039993\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.49974868386983873\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.4991649055480957\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.5020005229115486\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.4996316087245941\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.5000153079628944\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.5025078052282334\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.507090273797512\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.5528081408143044\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.6092083159089089\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.6054544985294342\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.6302848160266876\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.5466312572360039\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.5072481730580329\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.5007649526000023\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.5467323958873749\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.579028409421444\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.6135677614808083\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.6277953159809112\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.6357980540394783\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.6285224947333335\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.6423838499188423\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.642063675224781\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.6427278950810432\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.6471694704890251\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.6530293416976929\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.6504728034138679\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.6499910780787468\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.6462942951917648\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.6459717550873756\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.6523294427990913\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.6574546906352043\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.6558867865800857\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.6570988911390304\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.6346329626441002\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.6238074293732643\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.6531251966953278\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.6486177888512611\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.6546615380048751\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.6552612990140915\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.6545669972896576\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.6543759769201278\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.6562355506420136\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.6568085673451424\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.6564463394880294\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.6567726224660874\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.6577720686793327\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.659834906756878\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.6577575451135635\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.659169131219387\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.6611293521523476\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.6581927165389061\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.6594944855570793\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.6574666202068329\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.6589362666010856\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.6600535854697227\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.6499989286065102\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.5674966642260552\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.5951580995321274\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.6510893845558167\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.6553908628225327\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.6676005440950393\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.6202773001790046\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.629519273340702\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.6162515294551849\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.61095434948802\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.5352511900663376\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.6274018800258636\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.6422849434614182\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.6429992580413818\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.6373005267977715\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.6595125544071198\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.6603902480006218\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.648106344640255\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.6687922725081443\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.6631445705890655\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.6575235521793366\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.6760278108716011\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.6710629418492318\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.6794912910461426\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.687875714302063\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.7042930789291859\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.6957906433939933\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.7087213134765625\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.7237824712693691\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.737482195198536\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.753274749815464\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.7131664343178272\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.6593259631097317\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.7522531048953534\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.7558256204426289\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.7740621587634087\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.7736682485044003\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.7844243286550046\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.7805828437209129\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.7674442121386528\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.7920050996541977\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.7965365287661552\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.7929880970716476\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.7205001482367516\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.6337524704635144\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.7771519441902638\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.7943681482970715\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.7939134468138218\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.7966514053940773\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.7994795997440814\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.7999716447293759\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.8012730184197426\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.806647891998291\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.8027486081421376\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.8077737854421139\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.8112762561440467\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.8122005292773247\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.8088192796707153\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.8140024785697461\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.8161166325211525\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.8163496223092079\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.8171767555177212\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.8209835587441922\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.6693048760294914\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.6835275559127331\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.6776609200239182\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.7028364896774292\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.6728437235951423\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.6992068393528461\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.644300300180912\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.649488076120615\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.6786203925311566\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.6965636353194714\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.651913094818592\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.6747685547173023\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.6702392165362835\n",
      "NetRNNWithAttention, rep: 0, epoch: 134, acc: 0.6588903823494912\n",
      "NetRNNWithAttention, rep: 0, epoch: 135, acc: 0.6746455179154873\n",
      "NetRNNWithAttention, rep: 0, epoch: 136, acc: 0.6755922804772854\n",
      "NetRNNWithAttention, rep: 0, epoch: 137, acc: 0.6821679911762476\n",
      "NetRNNWithAttention, rep: 0, epoch: 138, acc: 0.7032986410707235\n",
      "NetRNNWithAttention, rep: 0, epoch: 139, acc: 0.6971067553013564\n",
      "NetRNNWithAttention, rep: 0, epoch: 140, acc: 0.7118828217685222\n",
      "NetRNNWithAttention, rep: 0, epoch: 141, acc: 0.6744138738512993\n",
      "NetRNNWithAttention, rep: 0, epoch: 142, acc: 0.7066798452287912\n",
      "NetRNNWithAttention, rep: 0, epoch: 143, acc: 0.7178352935612202\n",
      "NetRNNWithAttention, rep: 0, epoch: 144, acc: 0.7108985048532486\n",
      "NetRNNWithAttention, rep: 0, epoch: 145, acc: 0.7080320061743259\n",
      "NetRNNWithAttention, rep: 0, epoch: 146, acc: 0.7252072113752365\n",
      "NetRNNWithAttention, rep: 0, epoch: 147, acc: 0.7146290171891451\n",
      "NetRNNWithAttention, rep: 0, epoch: 148, acc: 0.723645088672638\n",
      "NetRNNWithAttention, rep: 0, epoch: 149, acc: 0.7064765024185181\n",
      "NetRNNWithAttention, rep: 0, epoch: 150, acc: 0.7211578300595284\n",
      "NetRNNWithAttention, rep: 0, epoch: 151, acc: 0.7114520542323589\n",
      "NetRNNWithAttention, rep: 0, epoch: 152, acc: 0.7292803616821766\n",
      "NetRNNWithAttention, rep: 0, epoch: 153, acc: 0.7213809771835804\n",
      "NetRNNWithAttention, rep: 0, epoch: 154, acc: 0.7364133286476136\n",
      "NetRNNWithAttention, rep: 0, epoch: 155, acc: 0.7340961672365666\n",
      "NetRNNWithAttention, rep: 0, epoch: 156, acc: 0.720173394382\n",
      "NetRNNWithAttention, rep: 0, epoch: 157, acc: 0.7317041428387165\n",
      "NetRNNWithAttention, rep: 0, epoch: 158, acc: 0.7185890655219555\n",
      "NetRNNWithAttention, rep: 0, epoch: 159, acc: 0.7347250555455684\n",
      "NetRNNWithAttention, rep: 0, epoch: 160, acc: 0.7280544885993003\n",
      "NetRNNWithAttention, rep: 0, epoch: 161, acc: 0.752572753727436\n",
      "NetRNNWithAttention, rep: 0, epoch: 162, acc: 0.7479024685919284\n",
      "NetRNNWithAttention, rep: 0, epoch: 163, acc: 0.7449262553453445\n",
      "NetRNNWithAttention, rep: 0, epoch: 164, acc: 0.7705733219534159\n",
      "NetRNNWithAttention, rep: 0, epoch: 165, acc: 0.749815685749054\n",
      "NetRNNWithAttention, rep: 0, epoch: 166, acc: 0.7595311399549246\n",
      "NetRNNWithAttention, rep: 0, epoch: 167, acc: 0.7629515707492829\n",
      "NetRNNWithAttention, rep: 0, epoch: 168, acc: 0.646354620307684\n",
      "NetRNNWithAttention, rep: 0, epoch: 169, acc: 0.7023428022861481\n",
      "NetRNNWithAttention, rep: 0, epoch: 170, acc: 0.6961593709141016\n",
      "NetRNNWithAttention, rep: 0, epoch: 171, acc: 0.7166968850046396\n",
      "NetRNNWithAttention, rep: 0, epoch: 172, acc: 0.7274874438345432\n",
      "NetRNNWithAttention, rep: 0, epoch: 173, acc: 0.7297419554740191\n",
      "NetRNNWithAttention, rep: 0, epoch: 174, acc: 0.7543374449014664\n",
      "NetRNNWithAttention, rep: 0, epoch: 175, acc: 0.7440025176852941\n",
      "NetRNNWithAttention, rep: 0, epoch: 176, acc: 0.7364273319393396\n",
      "NetRNNWithAttention, rep: 0, epoch: 177, acc: 0.7403231524676085\n",
      "NetRNNWithAttention, rep: 0, epoch: 178, acc: 0.728375175446272\n",
      "NetRNNWithAttention, rep: 0, epoch: 179, acc: 0.7549227234721184\n",
      "NetRNNWithAttention, rep: 0, epoch: 180, acc: 0.8225458067655563\n",
      "NetRNNWithAttention, rep: 0, epoch: 181, acc: 0.8091502530127763\n",
      "NetRNNWithAttention, rep: 0, epoch: 182, acc: 0.7915420914441347\n",
      "NetRNNWithAttention, rep: 0, epoch: 183, acc: 0.7911303806304931\n",
      "NetRNNWithAttention, rep: 0, epoch: 184, acc: 0.7957273673266172\n",
      "NetRNNWithAttention, rep: 0, epoch: 185, acc: 0.8004587315022945\n",
      "NetRNNWithAttention, rep: 0, epoch: 186, acc: 0.7866558265686036\n",
      "NetRNNWithAttention, rep: 0, epoch: 187, acc: 0.79639240026474\n",
      "NetRNNWithAttention, rep: 0, epoch: 188, acc: 0.7882598046958447\n",
      "NetRNNWithAttention, rep: 0, epoch: 189, acc: 0.8052518376708031\n",
      "NetRNNWithAttention, rep: 0, epoch: 190, acc: 0.803664060011506\n",
      "NetRNNWithAttention, rep: 0, epoch: 191, acc: 0.775794088691473\n",
      "NetRNNWithAttention, rep: 0, epoch: 192, acc: 0.6678569650650025\n",
      "NetRNNWithAttention, rep: 0, epoch: 193, acc: 0.7591338697075843\n",
      "NetRNNWithAttention, rep: 0, epoch: 194, acc: 0.802631753012538\n",
      "NetRNNWithAttention, rep: 0, epoch: 195, acc: 0.771951749548316\n",
      "NetRNNWithAttention, rep: 0, epoch: 196, acc: 0.779050658121705\n",
      "NetRNNWithAttention, rep: 0, epoch: 197, acc: 0.8159833136200905\n",
      "NetRNNWithAttention, rep: 0, epoch: 198, acc: 0.8266701506823302\n",
      "NetRNNWithAttention, rep: 0, epoch: 199, acc: 0.8448328026384115\n",
      "NetRNNWithAttention, rep: 0, epoch: 200, acc: 0.838085094988346\n",
      "NetRNNWithAttention, rep: 0, epoch: 201, acc: 0.8102596773952245\n",
      "NetRNNWithAttention, rep: 0, epoch: 202, acc: 0.8509874562174082\n",
      "NetRNNWithAttention, rep: 0, epoch: 203, acc: 0.8700754693895578\n",
      "NetRNNWithAttention, rep: 0, epoch: 204, acc: 0.8615210063010454\n",
      "NetRNNWithAttention, rep: 0, epoch: 205, acc: 0.8728226458281279\n",
      "NetRNNWithAttention, rep: 0, epoch: 206, acc: 0.865590525791049\n",
      "NetRNNWithAttention, rep: 0, epoch: 207, acc: 0.8372708988189698\n",
      "NetRNNWithAttention, rep: 0, epoch: 208, acc: 0.8466882327944041\n",
      "NetRNNWithAttention, rep: 0, epoch: 209, acc: 0.8372265706956387\n",
      "NetRNNWithAttention, rep: 0, epoch: 210, acc: 0.8364377197250724\n",
      "NetRNNWithAttention, rep: 0, epoch: 211, acc: 0.8425804968178272\n",
      "NetRNNWithAttention, rep: 0, epoch: 212, acc: 0.8496068424358963\n",
      "NetRNNWithAttention, rep: 0, epoch: 213, acc: 0.8316622722521424\n",
      "NetRNNWithAttention, rep: 0, epoch: 214, acc: 0.8549775965511799\n",
      "NetRNNWithAttention, rep: 0, epoch: 215, acc: 0.8760061502829194\n",
      "NetRNNWithAttention, rep: 0, epoch: 216, acc: 0.8649736019968987\n",
      "NetRNNWithAttention, rep: 0, epoch: 217, acc: 0.867810800075531\n",
      "NetRNNWithAttention, rep: 0, epoch: 218, acc: 0.8958850137516856\n",
      "NetRNNWithAttention, rep: 0, epoch: 219, acc: 0.8830551133304835\n",
      "NetRNNWithAttention, rep: 0, epoch: 220, acc: 0.8803803497180342\n",
      "NetRNNWithAttention, rep: 0, epoch: 221, acc: 0.8953761802241206\n",
      "NetRNNWithAttention, rep: 0, epoch: 222, acc: 0.8969810717180371\n",
      "NetRNNWithAttention, rep: 0, epoch: 223, acc: 0.872925905175507\n",
      "NetRNNWithAttention, rep: 0, epoch: 224, acc: 0.9045678523741663\n",
      "NetRNNWithAttention, rep: 0, epoch: 225, acc: 0.9141439404338598\n",
      "NetRNNWithAttention, rep: 0, epoch: 226, acc: 0.913568374607712\n",
      "NetRNNWithAttention, rep: 0, epoch: 227, acc: 0.9123264643922449\n",
      "NetRNNWithAttention, rep: 0, epoch: 228, acc: 0.9138952518068254\n",
      "NetRNNWithAttention, rep: 0, epoch: 229, acc: 0.900260930750519\n",
      "NetRNNWithAttention, rep: 0, epoch: 230, acc: 0.9109024690464139\n",
      "NetRNNWithAttention, rep: 0, epoch: 231, acc: 0.9097312377765775\n",
      "NetRNNWithAttention, rep: 0, epoch: 232, acc: 0.9229818718321622\n",
      "NetRNNWithAttention, rep: 0, epoch: 233, acc: 0.9097154131904245\n",
      "NetRNNWithAttention, rep: 0, epoch: 234, acc: 0.8932325757853686\n",
      "NetRNNWithAttention, rep: 0, epoch: 235, acc: 0.9105837815068663\n",
      "NetRNNWithAttention, rep: 0, epoch: 236, acc: 0.8854143241420388\n",
      "NetRNNWithAttention, rep: 0, epoch: 237, acc: 0.8233596087247134\n",
      "NetRNNWithAttention, rep: 0, epoch: 238, acc: 0.7645728204399347\n",
      "NetRNNWithAttention, rep: 0, epoch: 239, acc: 0.8048407374322415\n",
      "NetRNNWithAttention, rep: 0, epoch: 240, acc: 0.783713507913053\n",
      "NetRNNWithAttention, rep: 0, epoch: 241, acc: 0.8043109016120433\n",
      "NetRNNWithAttention, rep: 0, epoch: 242, acc: 0.7933292722702027\n",
      "NetRNNWithAttention, rep: 0, epoch: 243, acc: 0.813330027833581\n",
      "NetRNNWithAttention, rep: 0, epoch: 244, acc: 0.8239822905883193\n",
      "NetRNNWithAttention, rep: 0, epoch: 245, acc: 0.822360760346055\n",
      "NetRNNWithAttention, rep: 0, epoch: 246, acc: 0.8169683143869042\n",
      "NetRNNWithAttention, rep: 0, epoch: 247, acc: 0.8344588736817241\n",
      "NetRNNWithAttention, rep: 0, epoch: 248, acc: 0.8420779120922088\n",
      "NetRNNWithAttention, rep: 0, epoch: 249, acc: 0.8388671229779721\n",
      "NetRNNWithAttention, rep: 0, epoch: 250, acc: 0.8652614956349134\n",
      "NetRNNWithAttention, rep: 0, epoch: 251, acc: 0.8625819448381662\n",
      "NetRNNWithAttention, rep: 0, epoch: 252, acc: 0.8752402364462614\n",
      "NetRNNWithAttention, rep: 0, epoch: 253, acc: 0.8840546518191695\n",
      "NetRNNWithAttention, rep: 0, epoch: 254, acc: 0.8859022317454219\n",
      "NetRNNWithAttention, rep: 0, epoch: 255, acc: 0.883231345936656\n",
      "NetRNNWithAttention, rep: 0, epoch: 256, acc: 0.8967418795078993\n",
      "NetRNNWithAttention, rep: 0, epoch: 257, acc: 0.8805750340595841\n",
      "NetRNNWithAttention, rep: 0, epoch: 258, acc: 0.8916573854535819\n",
      "NetRNNWithAttention, rep: 0, epoch: 259, acc: 0.8774557421728969\n",
      "NetRNNWithAttention, rep: 0, epoch: 260, acc: 0.8821089991182088\n",
      "NetRNNWithAttention, rep: 0, epoch: 261, acc: 0.8730250640213489\n",
      "NetRNNWithAttention, rep: 0, epoch: 262, acc: 0.8501689770445228\n",
      "NetRNNWithAttention, rep: 0, epoch: 263, acc: 0.8600814709998668\n",
      "NetRNNWithAttention, rep: 0, epoch: 264, acc: 0.8992081375420093\n",
      "NetRNNWithAttention, rep: 0, epoch: 265, acc: 0.8869480118341744\n",
      "NetRNNWithAttention, rep: 0, epoch: 266, acc: 0.9156029431149364\n",
      "NetRNNWithAttention, rep: 0, epoch: 267, acc: 0.8897104317322373\n",
      "NetRNNWithAttention, rep: 0, epoch: 268, acc: 0.8665828060172498\n",
      "NetRNNWithAttention, rep: 0, epoch: 269, acc: 0.9044060382433236\n",
      "NetRNNWithAttention, rep: 0, epoch: 270, acc: 0.9026909304223955\n",
      "NetRNNWithAttention, rep: 0, epoch: 271, acc: 0.8948917434550822\n",
      "NetRNNWithAttention, rep: 0, epoch: 272, acc: 0.907931058332324\n",
      "NetRNNWithAttention, rep: 0, epoch: 273, acc: 0.9083876767009497\n",
      "NetRNNWithAttention, rep: 0, epoch: 274, acc: 0.9068822187744081\n",
      "NetRNNWithAttention, rep: 0, epoch: 275, acc: 0.9066403436288237\n",
      "NetRNNWithAttention, rep: 0, epoch: 276, acc: 0.8663648436404765\n",
      "NetRNNWithAttention, rep: 0, epoch: 277, acc: 0.8974565578997136\n",
      "NetRNNWithAttention, rep: 0, epoch: 278, acc: 0.9071148833818734\n",
      "NetRNNWithAttention, rep: 0, epoch: 279, acc: 0.8979181168228387\n",
      "NetRNNWithAttention, rep: 0, epoch: 280, acc: 0.8997453268431127\n",
      "NetRNNWithAttention, rep: 0, epoch: 281, acc: 0.9054931003972888\n",
      "NetRNNWithAttention, rep: 0, epoch: 282, acc: 0.9160613382421434\n",
      "NetRNNWithAttention, rep: 0, epoch: 283, acc: 0.9125002234056592\n",
      "NetRNNWithAttention, rep: 0, epoch: 284, acc: 0.9078732502274215\n",
      "NetRNNWithAttention, rep: 0, epoch: 285, acc: 0.8996732027456165\n",
      "NetRNNWithAttention, rep: 0, epoch: 286, acc: 0.903018638137728\n",
      "NetRNNWithAttention, rep: 0, epoch: 287, acc: 0.9212480133026838\n",
      "NetRNNWithAttention, rep: 0, epoch: 288, acc: 0.9152164760045707\n",
      "NetRNNWithAttention, rep: 0, epoch: 289, acc: 0.9270756838843226\n",
      "NetRNNWithAttention, rep: 0, epoch: 290, acc: 0.9075689632445574\n",
      "NetRNNWithAttention, rep: 0, epoch: 291, acc: 0.914928008876741\n",
      "NetRNNWithAttention, rep: 0, epoch: 292, acc: 0.9251021427009255\n",
      "NetRNNWithAttention, rep: 0, epoch: 293, acc: 0.9248541313316673\n",
      "NetRNNWithAttention, rep: 0, epoch: 294, acc: 0.9079754785820842\n",
      "NetRNNWithAttention, rep: 0, epoch: 295, acc: 0.9169902225863189\n",
      "NetRNNWithAttention, rep: 0, epoch: 296, acc: 0.9285277642682195\n",
      "NetRNNWithAttention, rep: 0, epoch: 297, acc: 0.9326385179162026\n",
      "NetRNNWithAttention, rep: 0, epoch: 298, acc: 0.9293096478655934\n",
      "NetRNNWithAttention, rep: 0, epoch: 299, acc: 0.9289988051913679\n",
      "NetRNNWithAttention, rep: 0, epoch: 300, acc: 0.934174896189943\n",
      "NetRNNWithAttention, rep: 0, epoch: 301, acc: 0.937565436353907\n",
      "NetRNNWithAttention, rep: 0, epoch: 302, acc: 0.9324785740859807\n",
      "NetRNNWithAttention, rep: 0, epoch: 303, acc: 0.8358806168101728\n",
      "NetRNNWithAttention, rep: 0, epoch: 304, acc: 0.9257133994158357\n",
      "NetRNNWithAttention, rep: 0, epoch: 305, acc: 0.9367458064667881\n",
      "NetRNNWithAttention, rep: 0, epoch: 306, acc: 0.9370486113522202\n",
      "NetRNNWithAttention, rep: 0, epoch: 307, acc: 0.9306209850497544\n",
      "NetRNNWithAttention, rep: 0, epoch: 308, acc: 0.9412342618405819\n",
      "NetRNNWithAttention, rep: 0, epoch: 309, acc: 0.9361638662591577\n",
      "NetRNNWithAttention, rep: 0, epoch: 310, acc: 0.9390061492472888\n",
      "NetRNNWithAttention, rep: 0, epoch: 311, acc: 0.9367139092646539\n",
      "NetRNNWithAttention, rep: 0, epoch: 312, acc: 0.938269790187478\n",
      "NetRNNWithAttention, rep: 0, epoch: 313, acc: 0.9285236044693739\n",
      "NetRNNWithAttention, rep: 0, epoch: 314, acc: 0.9422883750125766\n",
      "NetRNNWithAttention, rep: 0, epoch: 315, acc: 0.9447826319001615\n",
      "NetRNNWithAttention, rep: 0, epoch: 316, acc: 0.9408971470873803\n",
      "NetRNNWithAttention, rep: 0, epoch: 317, acc: 0.9426034379098565\n",
      "NetRNNWithAttention, rep: 0, epoch: 318, acc: 0.9427112704422325\n",
      "NetRNNWithAttention, rep: 0, epoch: 319, acc: 0.9441513438336551\n",
      "NetRNNWithAttention, rep: 0, epoch: 320, acc: 0.9288244560733437\n",
      "NetRNNWithAttention, rep: 0, epoch: 321, acc: 0.9443933932157234\n",
      "NetRNNWithAttention, rep: 0, epoch: 322, acc: 0.9472144522704184\n",
      "NetRNNWithAttention, rep: 0, epoch: 323, acc: 0.9496761078247801\n",
      "NetRNNWithAttention, rep: 0, epoch: 324, acc: 0.949065852900967\n",
      "NetRNNWithAttention, rep: 0, epoch: 325, acc: 0.951638842835091\n",
      "NetRNNWithAttention, rep: 0, epoch: 326, acc: 0.950879752621986\n",
      "NetRNNWithAttention, rep: 0, epoch: 327, acc: 0.9479535171110183\n",
      "NetRNNWithAttention, rep: 0, epoch: 328, acc: 0.9490477803070099\n",
      "NetRNNWithAttention, rep: 0, epoch: 329, acc: 0.9425696611776948\n",
      "NetRNNWithAttention, rep: 0, epoch: 330, acc: 0.9581237134058028\n",
      "NetRNNWithAttention, rep: 0, epoch: 331, acc: 0.9595244319643825\n",
      "NetRNNWithAttention, rep: 0, epoch: 332, acc: 0.9581538982596248\n",
      "NetRNNWithAttention, rep: 0, epoch: 333, acc: 0.9604418207053095\n",
      "NetRNNWithAttention, rep: 0, epoch: 334, acc: 0.9595658673159778\n",
      "NetRNNWithAttention, rep: 0, epoch: 335, acc: 0.9614586212392896\n",
      "NetRNNWithAttention, rep: 0, epoch: 336, acc: 0.9625966911530122\n",
      "NetRNNWithAttention, rep: 0, epoch: 337, acc: 0.9417823611898348\n",
      "NetRNNWithAttention, rep: 0, epoch: 338, acc: 0.7990587316639721\n",
      "NetRNNWithAttention, rep: 0, epoch: 339, acc: 0.8932300090044737\n",
      "NetRNNWithAttention, rep: 0, epoch: 340, acc: 0.9090074740489945\n",
      "NetRNNWithAttention, rep: 0, epoch: 341, acc: 0.8853669747198001\n",
      "NetRNNWithAttention, rep: 0, epoch: 342, acc: 0.8892333432566375\n",
      "NetRNNWithAttention, rep: 0, epoch: 343, acc: 0.9507840548548847\n",
      "NetRNNWithAttention, rep: 0, epoch: 344, acc: 0.9360648668557405\n",
      "NetRNNWithAttention, rep: 0, epoch: 345, acc: 0.9487708156649023\n",
      "NetRNNWithAttention, rep: 0, epoch: 346, acc: 0.9537541869934648\n",
      "NetRNNWithAttention, rep: 0, epoch: 347, acc: 0.9538755507254973\n",
      "NetRNNWithAttention, rep: 0, epoch: 348, acc: 0.9545249128574506\n",
      "NetRNNWithAttention, rep: 0, epoch: 349, acc: 0.9542408518306911\n",
      "NetRNNWithAttention, rep: 0, epoch: 350, acc: 0.9532629862241447\n",
      "NetRNNWithAttention, rep: 0, epoch: 351, acc: 0.942422346570529\n",
      "NetRNNWithAttention, rep: 0, epoch: 352, acc: 0.9569531796406955\n",
      "NetRNNWithAttention, rep: 0, epoch: 353, acc: 0.9620234555425122\n",
      "NetRNNWithAttention, rep: 0, epoch: 354, acc: 0.9597540022386238\n",
      "NetRNNWithAttention, rep: 0, epoch: 355, acc: 0.9618204718269407\n",
      "NetRNNWithAttention, rep: 0, epoch: 356, acc: 0.962281514192\n",
      "NetRNNWithAttention, rep: 0, epoch: 357, acc: 0.9590407716901973\n",
      "NetRNNWithAttention, rep: 0, epoch: 358, acc: 0.9647023512469605\n",
      "NetRNNWithAttention, rep: 0, epoch: 359, acc: 0.9647826855024323\n",
      "NetRNNWithAttention, rep: 0, epoch: 360, acc: 0.9628836068650708\n",
      "NetRNNWithAttention, rep: 0, epoch: 361, acc: 0.9655176497623325\n",
      "NetRNNWithAttention, rep: 0, epoch: 362, acc: 0.9675351234478876\n",
      "NetRNNWithAttention, rep: 0, epoch: 363, acc: 0.9693579823104664\n",
      "NetRNNWithAttention, rep: 0, epoch: 364, acc: 0.9660375680215657\n",
      "NetRNNWithAttention, rep: 0, epoch: 365, acc: 0.9689358509285375\n",
      "NetRNNWithAttention, rep: 0, epoch: 366, acc: 0.9714223780855537\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 366   Acc: 0.9714 Params: min_length: 40, max_length: 40, fill: 0, value_1: -1, value_2: 1 Time: 175.82 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.49954628348350527\n",
      "RNN, rep: 0, epoch: 2, acc: 0.49920103907585145\n",
      "RNN, rep: 0, epoch: 3, acc: 0.4990070700645447\n",
      "RNN, rep: 0, epoch: 4, acc: 0.5037135517597199\n",
      "RNN, rep: 0, epoch: 5, acc: 0.49998405784368516\n",
      "RNN, rep: 0, epoch: 6, acc: 0.4986700910329819\n",
      "RNN, rep: 0, epoch: 7, acc: 0.4993318152427673\n",
      "RNN, rep: 0, epoch: 8, acc: 0.500931139588356\n",
      "RNN, rep: 0, epoch: 9, acc: 0.5046389281749726\n",
      "RNN, rep: 0, epoch: 10, acc: 0.5009442156553269\n",
      "RNN, rep: 0, epoch: 11, acc: 0.4977236333489418\n",
      "RNN, rep: 0, epoch: 12, acc: 0.49807765036821366\n",
      "RNN, rep: 0, epoch: 13, acc: 0.4996061435341835\n",
      "RNN, rep: 0, epoch: 14, acc: 0.501282112300396\n",
      "RNN, rep: 0, epoch: 15, acc: 0.49880609959363936\n",
      "RNN, rep: 0, epoch: 16, acc: 0.5013381946086883\n",
      "RNN, rep: 0, epoch: 17, acc: 0.5037977442145347\n",
      "RNN, rep: 0, epoch: 18, acc: 0.4991266432404518\n",
      "RNN, rep: 0, epoch: 19, acc: 0.49960500091314314\n",
      "RNN, rep: 0, epoch: 20, acc: 0.5034435260295868\n",
      "RNN, rep: 0, epoch: 21, acc: 0.496981635093689\n",
      "RNN, rep: 0, epoch: 22, acc: 0.49895481556653976\n",
      "RNN, rep: 0, epoch: 23, acc: 0.4988211718201637\n",
      "RNN, rep: 0, epoch: 24, acc: 0.5028491267561912\n",
      "RNN, rep: 0, epoch: 25, acc: 0.4999560329318047\n",
      "RNN, rep: 0, epoch: 26, acc: 0.4990354189276695\n",
      "RNN, rep: 0, epoch: 27, acc: 0.5015352064371109\n",
      "RNN, rep: 0, epoch: 28, acc: 0.5020346522331238\n",
      "RNN, rep: 0, epoch: 29, acc: 0.5009217193722725\n",
      "RNN, rep: 0, epoch: 30, acc: 0.5016705146431923\n",
      "RNN, rep: 0, epoch: 31, acc: 0.49798814743757247\n",
      "RNN, rep: 0, epoch: 32, acc: 0.4996605974435806\n",
      "RNN, rep: 0, epoch: 33, acc: 0.5006823858618736\n",
      "RNN, rep: 0, epoch: 34, acc: 0.5015501156449318\n",
      "RNN, rep: 0, epoch: 35, acc: 0.49878867596387866\n",
      "RNN, rep: 0, epoch: 36, acc: 0.5006977275013924\n",
      "RNN, rep: 0, epoch: 37, acc: 0.49954606682062147\n",
      "RNN, rep: 0, epoch: 38, acc: 0.5008123680949211\n",
      "RNN, rep: 0, epoch: 39, acc: 0.498881573677063\n",
      "RNN, rep: 0, epoch: 40, acc: 0.5005206471681595\n",
      "RNN, rep: 0, epoch: 41, acc: 0.5001510137319565\n",
      "RNN, rep: 0, epoch: 42, acc: 0.5002786281704903\n",
      "RNN, rep: 0, epoch: 43, acc: 0.49999491423368453\n",
      "RNN, rep: 0, epoch: 44, acc: 0.5006917694211006\n",
      "RNN, rep: 0, epoch: 45, acc: 0.5006796160340309\n",
      "RNN, rep: 0, epoch: 46, acc: 0.49966912150382997\n",
      "RNN, rep: 0, epoch: 47, acc: 0.5007338646054268\n",
      "RNN, rep: 0, epoch: 48, acc: 0.501831850707531\n",
      "RNN, rep: 0, epoch: 49, acc: 0.5022033980488777\n",
      "RNN, rep: 0, epoch: 50, acc: 0.5000174874067307\n",
      "RNN, rep: 0, epoch: 51, acc: 0.4991976100206375\n",
      "RNN, rep: 0, epoch: 52, acc: 0.5031042966246605\n",
      "RNN, rep: 0, epoch: 53, acc: 0.5018040800094604\n",
      "RNN, rep: 0, epoch: 54, acc: 0.5033052110671997\n",
      "RNN, rep: 0, epoch: 55, acc: 0.5005460843443871\n",
      "RNN, rep: 0, epoch: 56, acc: 0.4994336885213852\n",
      "RNN, rep: 0, epoch: 57, acc: 0.5007972264289856\n",
      "RNN, rep: 0, epoch: 58, acc: 0.4997501116991043\n",
      "RNN, rep: 0, epoch: 59, acc: 0.49937735348939893\n",
      "RNN, rep: 0, epoch: 60, acc: 0.5014389106631278\n",
      "RNN, rep: 0, epoch: 61, acc: 0.5007341742515564\n",
      "RNN, rep: 0, epoch: 62, acc: 0.49950856626033785\n",
      "RNN, rep: 0, epoch: 63, acc: 0.4999746748805046\n",
      "RNN, rep: 0, epoch: 64, acc: 0.5001161623001099\n",
      "RNN, rep: 0, epoch: 65, acc: 0.5021171897649765\n",
      "RNN, rep: 0, epoch: 66, acc: 0.49981124460697174\n",
      "RNN, rep: 0, epoch: 67, acc: 0.4997041437029839\n",
      "RNN, rep: 0, epoch: 68, acc: 0.5013015741109847\n",
      "RNN, rep: 0, epoch: 69, acc: 0.502447517812252\n",
      "RNN, rep: 0, epoch: 70, acc: 0.5099944984912872\n",
      "RNN, rep: 0, epoch: 71, acc: 0.5192621928453446\n",
      "RNN, rep: 0, epoch: 72, acc: 0.5613245421648025\n",
      "RNN, rep: 0, epoch: 73, acc: 0.5214190077781677\n",
      "RNN, rep: 0, epoch: 74, acc: 0.5790326833724976\n",
      "RNN, rep: 0, epoch: 75, acc: 0.6064423674345016\n",
      "RNN, rep: 0, epoch: 76, acc: 0.6216217029094696\n",
      "RNN, rep: 0, epoch: 77, acc: 0.6279201939702034\n",
      "RNN, rep: 0, epoch: 78, acc: 0.6315227809548378\n",
      "RNN, rep: 0, epoch: 79, acc: 0.6372680589556694\n",
      "RNN, rep: 0, epoch: 80, acc: 0.5943206089735031\n",
      "RNN, rep: 0, epoch: 81, acc: 0.6365582323074341\n",
      "RNN, rep: 0, epoch: 82, acc: 0.6419921877980233\n",
      "RNN, rep: 0, epoch: 83, acc: 0.6501486855745315\n",
      "RNN, rep: 0, epoch: 84, acc: 0.6435303246974945\n",
      "RNN, rep: 0, epoch: 85, acc: 0.6469765841960907\n",
      "RNN, rep: 0, epoch: 86, acc: 0.649971817433834\n",
      "RNN, rep: 0, epoch: 87, acc: 0.6483181822299957\n",
      "RNN, rep: 0, epoch: 88, acc: 0.6496096450090408\n",
      "RNN, rep: 0, epoch: 89, acc: 0.6498326662182808\n",
      "RNN, rep: 0, epoch: 90, acc: 0.6512575268745422\n",
      "RNN, rep: 0, epoch: 91, acc: 0.6514079597592354\n",
      "RNN, rep: 0, epoch: 92, acc: 0.6539221876859664\n",
      "RNN, rep: 0, epoch: 93, acc: 0.6531057325005531\n",
      "RNN, rep: 0, epoch: 94, acc: 0.6541436767578125\n",
      "RNN, rep: 0, epoch: 95, acc: 0.6562895998358727\n",
      "RNN, rep: 0, epoch: 96, acc: 0.6568229478597641\n",
      "RNN, rep: 0, epoch: 97, acc: 0.6560526722669602\n",
      "RNN, rep: 0, epoch: 98, acc: 0.6579775360226631\n",
      "RNN, rep: 0, epoch: 99, acc: 0.6577270141243935\n",
      "RNN, rep: 0, epoch: 100, acc: 0.6590504232048988\n",
      "RNN, rep: 0, epoch: 101, acc: 0.6581112995743752\n",
      "RNN, rep: 0, epoch: 102, acc: 0.657223818898201\n",
      "RNN, rep: 0, epoch: 103, acc: 0.5573018521070481\n",
      "RNN, rep: 0, epoch: 104, acc: 0.5488893276453018\n",
      "RNN, rep: 0, epoch: 105, acc: 0.5333545613288879\n",
      "RNN, rep: 0, epoch: 106, acc: 0.6051914197206497\n",
      "RNN, rep: 0, epoch: 107, acc: 0.6341117143630981\n",
      "RNN, rep: 0, epoch: 108, acc: 0.6400827151536942\n",
      "RNN, rep: 0, epoch: 109, acc: 0.6427792000770569\n",
      "RNN, rep: 0, epoch: 110, acc: 0.6457770305871964\n",
      "RNN, rep: 0, epoch: 111, acc: 0.6459029629826546\n",
      "RNN, rep: 0, epoch: 112, acc: 0.6504332318902015\n",
      "RNN, rep: 0, epoch: 113, acc: 0.6517514330148697\n",
      "RNN, rep: 0, epoch: 114, acc: 0.6538733685016632\n",
      "RNN, rep: 0, epoch: 115, acc: 0.649508541226387\n",
      "RNN, rep: 0, epoch: 116, acc: 0.6542452102899552\n",
      "RNN, rep: 0, epoch: 117, acc: 0.6542076712846756\n",
      "RNN, rep: 0, epoch: 118, acc: 0.6555232208967209\n",
      "RNN, rep: 0, epoch: 119, acc: 0.6532177618145942\n",
      "RNN, rep: 0, epoch: 120, acc: 0.6568049409985542\n",
      "RNN, rep: 0, epoch: 121, acc: 0.6560525333881378\n",
      "RNN, rep: 0, epoch: 122, acc: 0.6560617622733116\n",
      "RNN, rep: 0, epoch: 123, acc: 0.6561101448535919\n",
      "RNN, rep: 0, epoch: 124, acc: 0.6561556053161621\n",
      "RNN, rep: 0, epoch: 125, acc: 0.6562333911657333\n",
      "RNN, rep: 0, epoch: 126, acc: 0.6581192162632942\n",
      "RNN, rep: 0, epoch: 127, acc: 0.6575446262955665\n",
      "RNN, rep: 0, epoch: 128, acc: 0.6583737328648567\n",
      "RNN, rep: 0, epoch: 129, acc: 0.6599675783514977\n",
      "RNN, rep: 0, epoch: 130, acc: 0.659591659605503\n",
      "RNN, rep: 0, epoch: 131, acc: 0.6603876912593841\n",
      "RNN, rep: 0, epoch: 132, acc: 0.6612045380473137\n",
      "RNN, rep: 0, epoch: 133, acc: 0.6603009593486786\n",
      "RNN, rep: 0, epoch: 134, acc: 0.6610098561644554\n",
      "RNN, rep: 0, epoch: 135, acc: 0.6616567462682724\n",
      "RNN, rep: 0, epoch: 136, acc: 0.661744669675827\n",
      "RNN, rep: 0, epoch: 137, acc: 0.6639915603399277\n",
      "RNN, rep: 0, epoch: 138, acc: 0.6615806537866592\n",
      "RNN, rep: 0, epoch: 139, acc: 0.6619901770353317\n",
      "RNN, rep: 0, epoch: 140, acc: 0.660385408103466\n",
      "RNN, rep: 0, epoch: 141, acc: 0.6616431209445\n",
      "RNN, rep: 0, epoch: 142, acc: 0.6625978076457977\n",
      "RNN, rep: 0, epoch: 143, acc: 0.6624100464582443\n",
      "RNN, rep: 0, epoch: 144, acc: 0.6612335142493247\n",
      "RNN, rep: 0, epoch: 145, acc: 0.659131502211094\n",
      "RNN, rep: 0, epoch: 146, acc: 0.6623383542895317\n",
      "RNN, rep: 0, epoch: 147, acc: 0.6638898396492005\n",
      "RNN, rep: 0, epoch: 148, acc: 0.6624526441097259\n",
      "RNN, rep: 0, epoch: 149, acc: 0.6629108107089996\n",
      "RNN, rep: 0, epoch: 150, acc: 0.6606255042552948\n",
      "RNN, rep: 0, epoch: 151, acc: 0.6619521233439446\n",
      "RNN, rep: 0, epoch: 152, acc: 0.6634846964478492\n",
      "RNN, rep: 0, epoch: 153, acc: 0.6631322649121284\n",
      "RNN, rep: 0, epoch: 154, acc: 0.6627475309371949\n",
      "RNN, rep: 0, epoch: 155, acc: 0.662865543961525\n",
      "RNN, rep: 0, epoch: 156, acc: 0.6632380798459053\n",
      "RNN, rep: 0, epoch: 157, acc: 0.6646563175320626\n",
      "RNN, rep: 0, epoch: 158, acc: 0.6628071197867393\n",
      "RNN, rep: 0, epoch: 159, acc: 0.6656973877549172\n",
      "RNN, rep: 0, epoch: 160, acc: 0.662541811466217\n",
      "RNN, rep: 0, epoch: 161, acc: 0.6626998671889305\n",
      "RNN, rep: 0, epoch: 162, acc: 0.6639505338668823\n",
      "RNN, rep: 0, epoch: 163, acc: 0.663897587954998\n",
      "RNN, rep: 0, epoch: 164, acc: 0.6636137217283249\n",
      "RNN, rep: 0, epoch: 165, acc: 0.662210765182972\n",
      "RNN, rep: 0, epoch: 166, acc: 0.6629871565103531\n",
      "RNN, rep: 0, epoch: 167, acc: 0.6678238245844841\n",
      "RNN, rep: 0, epoch: 168, acc: 0.6671502676606178\n",
      "RNN, rep: 0, epoch: 169, acc: 0.6695350849628449\n",
      "RNN, rep: 0, epoch: 170, acc: 0.6691630885004998\n",
      "RNN, rep: 0, epoch: 171, acc: 0.664062095284462\n",
      "RNN, rep: 0, epoch: 172, acc: 0.6627658742666245\n",
      "RNN, rep: 0, epoch: 173, acc: 0.6640354132652283\n",
      "RNN, rep: 0, epoch: 174, acc: 0.6681903213262558\n",
      "RNN, rep: 0, epoch: 175, acc: 0.6663469392061233\n",
      "RNN, rep: 0, epoch: 176, acc: 0.6666816821694375\n",
      "RNN, rep: 0, epoch: 177, acc: 0.6637646788358689\n",
      "RNN, rep: 0, epoch: 178, acc: 0.6652910777926445\n",
      "RNN, rep: 0, epoch: 179, acc: 0.6607657328248024\n",
      "RNN, rep: 0, epoch: 180, acc: 0.6643310663104057\n",
      "RNN, rep: 0, epoch: 181, acc: 0.6669189295172692\n",
      "RNN, rep: 0, epoch: 182, acc: 0.6634824472665787\n",
      "RNN, rep: 0, epoch: 183, acc: 0.6658211243152619\n",
      "RNN, rep: 0, epoch: 184, acc: 0.6647678443789482\n",
      "RNN, rep: 0, epoch: 185, acc: 0.6648793497681618\n",
      "RNN, rep: 0, epoch: 186, acc: 0.6644202914834022\n",
      "RNN, rep: 0, epoch: 187, acc: 0.6658468151092529\n",
      "RNN, rep: 0, epoch: 188, acc: 0.659053663611412\n",
      "RNN, rep: 0, epoch: 189, acc: 0.6653694093227387\n",
      "RNN, rep: 0, epoch: 190, acc: 0.6629449611902237\n",
      "RNN, rep: 0, epoch: 191, acc: 0.6677697932720185\n",
      "RNN, rep: 0, epoch: 192, acc: 0.6673374959826469\n",
      "RNN, rep: 0, epoch: 193, acc: 0.6665163663029671\n",
      "RNN, rep: 0, epoch: 194, acc: 0.6650511875748635\n",
      "RNN, rep: 0, epoch: 195, acc: 0.667515658736229\n",
      "RNN, rep: 0, epoch: 196, acc: 0.6664002838730813\n",
      "RNN, rep: 0, epoch: 197, acc: 0.6675311243534088\n",
      "RNN, rep: 0, epoch: 198, acc: 0.6658401307463646\n",
      "RNN, rep: 0, epoch: 199, acc: 0.667021258175373\n",
      "RNN, rep: 0, epoch: 200, acc: 0.6645151114463806\n",
      "RNN, rep: 0, epoch: 201, acc: 0.666619111597538\n",
      "RNN, rep: 0, epoch: 202, acc: 0.664872477054596\n",
      "RNN, rep: 0, epoch: 203, acc: 0.6425516280531883\n",
      "RNN, rep: 0, epoch: 204, acc: 0.6626890307664871\n",
      "RNN, rep: 0, epoch: 205, acc: 0.6670434042811394\n",
      "RNN, rep: 0, epoch: 206, acc: 0.6649091830849647\n",
      "RNN, rep: 0, epoch: 207, acc: 0.667002614736557\n",
      "RNN, rep: 0, epoch: 208, acc: 0.6647145077586174\n",
      "RNN, rep: 0, epoch: 209, acc: 0.6661168643832207\n",
      "RNN, rep: 0, epoch: 210, acc: 0.6661318632960319\n",
      "RNN, rep: 0, epoch: 211, acc: 0.665653301179409\n",
      "RNN, rep: 0, epoch: 212, acc: 0.6655030080676079\n",
      "RNN, rep: 0, epoch: 213, acc: 0.6656737479567528\n",
      "RNN, rep: 0, epoch: 214, acc: 0.665597605407238\n",
      "RNN, rep: 0, epoch: 215, acc: 0.6675136747956276\n",
      "RNN, rep: 0, epoch: 216, acc: 0.6670784199237824\n",
      "RNN, rep: 0, epoch: 217, acc: 0.6650976505875588\n",
      "RNN, rep: 0, epoch: 218, acc: 0.6648264425992966\n",
      "RNN, rep: 0, epoch: 219, acc: 0.6674200969934464\n",
      "RNN, rep: 0, epoch: 220, acc: 0.6637787073850632\n",
      "RNN, rep: 0, epoch: 221, acc: 0.6640436673164367\n",
      "RNN, rep: 0, epoch: 222, acc: 0.6662641993165016\n",
      "RNN, rep: 0, epoch: 223, acc: 0.6656756287813187\n",
      "RNN, rep: 0, epoch: 224, acc: 0.6648794391751289\n",
      "RNN, rep: 0, epoch: 225, acc: 0.6658241847157478\n",
      "RNN, rep: 0, epoch: 226, acc: 0.6666248181462288\n",
      "RNN, rep: 0, epoch: 227, acc: 0.663673291504383\n",
      "RNN, rep: 0, epoch: 228, acc: 0.6663332307338714\n",
      "RNN, rep: 0, epoch: 229, acc: 0.6651876851916313\n",
      "RNN, rep: 0, epoch: 230, acc: 0.6652130025625229\n",
      "RNN, rep: 0, epoch: 231, acc: 0.6652155363559723\n",
      "RNN, rep: 0, epoch: 232, acc: 0.6672897359728813\n",
      "RNN, rep: 0, epoch: 233, acc: 0.665993258357048\n",
      "RNN, rep: 0, epoch: 234, acc: 0.6682529392838478\n",
      "RNN, rep: 0, epoch: 235, acc: 0.6532435488700866\n",
      "RNN, rep: 0, epoch: 236, acc: 0.6674234107136726\n",
      "RNN, rep: 0, epoch: 237, acc: 0.6671347627043724\n",
      "RNN, rep: 0, epoch: 238, acc: 0.6642906621098519\n",
      "RNN, rep: 0, epoch: 239, acc: 0.6678952518105506\n",
      "RNN, rep: 0, epoch: 240, acc: 0.6665876817703247\n",
      "RNN, rep: 0, epoch: 241, acc: 0.6702441155910492\n",
      "RNN, rep: 0, epoch: 242, acc: 0.6663653892278671\n",
      "RNN, rep: 0, epoch: 243, acc: 0.6659260275959968\n",
      "RNN, rep: 0, epoch: 244, acc: 0.6646233612298965\n",
      "RNN, rep: 0, epoch: 245, acc: 0.6664253640174865\n",
      "RNN, rep: 0, epoch: 246, acc: 0.6665597978234291\n",
      "RNN, rep: 0, epoch: 247, acc: 0.6650855436921119\n",
      "RNN, rep: 0, epoch: 248, acc: 0.6670784661173821\n",
      "RNN, rep: 0, epoch: 249, acc: 0.6651405146718026\n",
      "RNN, rep: 0, epoch: 250, acc: 0.6616226986050606\n",
      "RNN, rep: 0, epoch: 251, acc: 0.6664825835824013\n",
      "RNN, rep: 0, epoch: 252, acc: 0.6659915235638618\n",
      "RNN, rep: 0, epoch: 253, acc: 0.6688894566893577\n",
      "RNN, rep: 0, epoch: 254, acc: 0.6710206151008606\n",
      "RNN, rep: 0, epoch: 255, acc: 0.6670787200331688\n",
      "RNN, rep: 0, epoch: 256, acc: 0.6679762101173401\n",
      "RNN, rep: 0, epoch: 257, acc: 0.6686471775174141\n",
      "RNN, rep: 0, epoch: 258, acc: 0.6672350877523422\n",
      "RNN, rep: 0, epoch: 259, acc: 0.6630448845028877\n",
      "RNN, rep: 0, epoch: 260, acc: 0.6707423302531242\n",
      "RNN, rep: 0, epoch: 261, acc: 0.6737052497267723\n",
      "RNN, rep: 0, epoch: 262, acc: 0.6701856353878974\n",
      "RNN, rep: 0, epoch: 263, acc: 0.6762051673233509\n",
      "RNN, rep: 0, epoch: 264, acc: 0.6618404716253281\n",
      "RNN, rep: 0, epoch: 265, acc: 0.6757376706600189\n",
      "RNN, rep: 0, epoch: 266, acc: 0.6689017930626869\n",
      "RNN, rep: 0, epoch: 267, acc: 0.675336044728756\n",
      "RNN, rep: 0, epoch: 268, acc: 0.6753694048523903\n",
      "RNN, rep: 0, epoch: 269, acc: 0.6761068466305733\n",
      "RNN, rep: 0, epoch: 270, acc: 0.6842243087291717\n",
      "RNN, rep: 0, epoch: 271, acc: 0.7243583241105079\n",
      "RNN, rep: 0, epoch: 272, acc: 0.7497197137773037\n",
      "RNN, rep: 0, epoch: 273, acc: 0.7372355955839157\n",
      "RNN, rep: 0, epoch: 274, acc: 0.6744790785014629\n",
      "RNN, rep: 0, epoch: 275, acc: 0.7146684464812278\n",
      "RNN, rep: 0, epoch: 276, acc: 0.7277872188389302\n",
      "RNN, rep: 0, epoch: 277, acc: 0.7365688641369342\n",
      "RNN, rep: 0, epoch: 278, acc: 0.7340059208869935\n",
      "RNN, rep: 0, epoch: 279, acc: 0.7223917376995087\n",
      "RNN, rep: 0, epoch: 280, acc: 0.7195303012430668\n",
      "RNN, rep: 0, epoch: 281, acc: 0.7430087268352509\n",
      "RNN, rep: 0, epoch: 282, acc: 0.7492072302103042\n",
      "RNN, rep: 0, epoch: 283, acc: 0.7573118329048156\n",
      "RNN, rep: 0, epoch: 284, acc: 0.768100840896368\n",
      "RNN, rep: 0, epoch: 285, acc: 0.7702345672249794\n",
      "RNN, rep: 0, epoch: 286, acc: 0.7894030953943729\n",
      "RNN, rep: 0, epoch: 287, acc: 0.7996089930832386\n",
      "RNN, rep: 0, epoch: 288, acc: 0.7778607040643692\n",
      "RNN, rep: 0, epoch: 289, acc: 0.7992103971540928\n",
      "RNN, rep: 0, epoch: 290, acc: 0.8105549328029156\n",
      "RNN, rep: 0, epoch: 291, acc: 0.8169246409833432\n",
      "RNN, rep: 0, epoch: 292, acc: 0.8200237528979778\n",
      "RNN, rep: 0, epoch: 293, acc: 0.8245506335794925\n",
      "RNN, rep: 0, epoch: 294, acc: 0.8270576621592045\n",
      "RNN, rep: 0, epoch: 295, acc: 0.8224939674139022\n",
      "RNN, rep: 0, epoch: 296, acc: 0.8300873413681984\n",
      "RNN, rep: 0, epoch: 297, acc: 0.8351273506879806\n",
      "RNN, rep: 0, epoch: 298, acc: 0.8406736552715302\n",
      "RNN, rep: 0, epoch: 299, acc: 0.8397354310750962\n",
      "RNN, rep: 0, epoch: 300, acc: 0.8398090600967407\n",
      "RNN, rep: 0, epoch: 301, acc: 0.843066180050373\n",
      "RNN, rep: 0, epoch: 302, acc: 0.8486128696799278\n",
      "RNN, rep: 0, epoch: 303, acc: 0.8586339325457811\n",
      "RNN, rep: 0, epoch: 304, acc: 0.85885428160429\n",
      "RNN, rep: 0, epoch: 305, acc: 0.8590799807012082\n",
      "RNN, rep: 0, epoch: 306, acc: 0.861345746293664\n",
      "RNN, rep: 0, epoch: 307, acc: 0.8711158512532711\n",
      "RNN, rep: 0, epoch: 308, acc: 0.8700740154087544\n",
      "RNN, rep: 0, epoch: 309, acc: 0.8756883543729782\n",
      "RNN, rep: 0, epoch: 310, acc: 0.8771673416346312\n",
      "RNN, rep: 0, epoch: 311, acc: 0.8755818931013346\n",
      "RNN, rep: 0, epoch: 312, acc: 0.883274593576789\n",
      "RNN, rep: 0, epoch: 313, acc: 0.8857492356747388\n",
      "RNN, rep: 0, epoch: 314, acc: 0.8912426602095366\n",
      "RNN, rep: 0, epoch: 315, acc: 0.8644349618256092\n",
      "RNN, rep: 0, epoch: 316, acc: 0.816234835088253\n",
      "RNN, rep: 0, epoch: 317, acc: 0.8390630802512169\n",
      "RNN, rep: 0, epoch: 318, acc: 0.8907673205435276\n",
      "RNN, rep: 0, epoch: 319, acc: 0.8936074053496122\n",
      "RNN, rep: 0, epoch: 320, acc: 0.8968857696652413\n",
      "RNN, rep: 0, epoch: 321, acc: 0.9028734082728624\n",
      "RNN, rep: 0, epoch: 322, acc: 0.9022142273187638\n",
      "RNN, rep: 0, epoch: 323, acc: 0.9030121161043644\n",
      "RNN, rep: 0, epoch: 324, acc: 0.9080247769504786\n",
      "RNN, rep: 0, epoch: 325, acc: 0.9092026430368424\n",
      "RNN, rep: 0, epoch: 326, acc: 0.9111890882253647\n",
      "RNN, rep: 0, epoch: 327, acc: 0.9141053010895849\n",
      "RNN, rep: 0, epoch: 328, acc: 0.9177354304119945\n",
      "RNN, rep: 0, epoch: 329, acc: 0.9168548400327563\n",
      "RNN, rep: 0, epoch: 330, acc: 0.9229007415473461\n",
      "RNN, rep: 0, epoch: 331, acc: 0.9260085308924317\n",
      "RNN, rep: 0, epoch: 332, acc: 0.928346615768969\n",
      "RNN, rep: 0, epoch: 333, acc: 0.9305363181605935\n",
      "RNN, rep: 0, epoch: 334, acc: 0.9305666434392333\n",
      "RNN, rep: 0, epoch: 335, acc: 0.9341421405225993\n",
      "RNN, rep: 0, epoch: 336, acc: 0.9328090188279748\n",
      "RNN, rep: 0, epoch: 337, acc: 0.9352387952432036\n",
      "RNN, rep: 0, epoch: 338, acc: 0.9382499795034528\n",
      "RNN, rep: 0, epoch: 339, acc: 0.9430298858508468\n",
      "RNN, rep: 0, epoch: 340, acc: 0.9417526188492775\n",
      "RNN, rep: 0, epoch: 341, acc: 0.9443650292232633\n",
      "RNN, rep: 0, epoch: 342, acc: 0.9481914453580975\n",
      "RNN, rep: 0, epoch: 343, acc: 0.9468341550044715\n",
      "RNN, rep: 0, epoch: 344, acc: 0.9484181717783212\n",
      "RNN, rep: 0, epoch: 345, acc: 0.951940210480243\n",
      "RNN, rep: 0, epoch: 346, acc: 0.9532265034876763\n",
      "RNN, rep: 0, epoch: 347, acc: 0.9554342459514737\n",
      "RNN, rep: 0, epoch: 348, acc: 0.9561441146023572\n",
      "RNN, rep: 0, epoch: 349, acc: 0.958952367901802\n",
      "RNN, rep: 0, epoch: 350, acc: 0.9602298265881837\n",
      "RNN, rep: 0, epoch: 351, acc: 0.9608583161234856\n",
      "RNN, rep: 0, epoch: 352, acc: 0.9614371139369905\n",
      "RNN, rep: 0, epoch: 353, acc: 0.9641375637799502\n",
      "RNN, rep: 0, epoch: 354, acc: 0.9655836475640536\n",
      "RNN, rep: 0, epoch: 355, acc: 0.963886602949351\n",
      "RNN, rep: 0, epoch: 356, acc: 0.965925498791039\n",
      "RNN, rep: 0, epoch: 357, acc: 0.9676280329190194\n",
      "RNN, rep: 0, epoch: 358, acc: 0.9674650429561734\n",
      "RNN, rep: 0, epoch: 359, acc: 0.9683362199552357\n",
      "RNN, rep: 0, epoch: 360, acc: 0.9711861707270145\n",
      "RNN                  Rep: 0   Epoch: 360   Acc: 0.9712 Params: min_length: 40, max_length: 45, fill: 0, value_1: -1, value_2: 1 Time: 152.58 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.49899990260601046\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.5024422782659531\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5008130034804344\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5023353749513626\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.4977352759242058\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.5032163807749748\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.4977256774902344\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.5008684176206589\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.49896735697984695\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.5009483754634857\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.5001843744516372\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.5002623224258422\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.5018734583258628\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.5053912806510925\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.5131307062506676\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.5782296949625015\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.6082836112380028\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.6072872871160507\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.6207420417666435\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.6256693440675736\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.6331688389182091\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.6589975926280022\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.7063240399956703\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.7309630660712719\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.746455372273922\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.7553745594620704\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.7668761737644673\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.7698883083462715\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.7733122588694096\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.7811156685650349\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.782222638130188\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.7857259938120842\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.7904821889102459\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.7944307704269886\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.7903559592366218\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.7972142370045185\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.7992717912793159\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.8004620614647865\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.8042112445831299\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.7742572198808193\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.7869864593446255\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.8153435190021991\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.8157282653450966\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.8233302707970143\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.8363325123488903\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.8251398758590222\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.8390620478987694\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.8420104676485062\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.8468289773911237\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.8464460384100676\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.8491824293136596\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.853799816519022\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.8562165396660566\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.8589546722918748\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.8562761336565018\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.8669161786139011\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.8668175844848156\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.8668513082712889\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.8690438183397055\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.8759567610174418\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.8643377400934696\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.871505921408534\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.8723546697944403\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.8823014618828893\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.8805220982059836\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.885145956017077\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.8752926805615425\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.8529874312132597\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.8875646265968681\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.8739220487698912\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.8954525814577937\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.8873382100462913\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.8795569424703717\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.8801919276639819\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.8858862871676684\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.886573373824358\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.8935946459323167\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.8950789013504982\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.884999586418271\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.8998910891637206\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.809830229319632\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.7473994842544198\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.7856506348401308\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.7988164748623967\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.7460894811153412\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.7753139160946012\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.765683185607195\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.8041710016876459\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.8560054346174002\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.8661343264579773\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.8757038725167513\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.8827254941314459\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.8812233149260282\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.8878886636719108\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.8982174724340439\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.8983154626935721\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.8958509005978703\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.9036333149671555\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.9119612320885062\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.9159594750031829\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.9113128892704845\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.9187338788062334\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.9199660920724273\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.9215060902386903\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.9231093366071582\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.925008382089436\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.9302845526859165\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.9320884690433741\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.9359252644330263\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.9396352906525135\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.9399953234568238\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.9398339519277215\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.9386424440145492\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.9346289430931211\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.9428985414840281\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.9438532631844282\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.9476063302718103\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.9495677566714584\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.9464206047914923\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.9506650280766189\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.9537802746146917\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.9559334718622268\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.9559919714741408\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.9575467003136873\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.9556391174346208\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.9588982970640063\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.9607333996333182\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.9628511891700328\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.9576979598775506\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.9623759750276804\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.965286343600601\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.9639352423883975\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.960843660812825\n",
      "NetRNNWithAttention, rep: 0, epoch: 134, acc: 0.9658405641466379\n",
      "NetRNNWithAttention, rep: 0, epoch: 135, acc: 0.9658464901708066\n",
      "NetRNNWithAttention, rep: 0, epoch: 136, acc: 0.9668359766155481\n",
      "NetRNNWithAttention, rep: 0, epoch: 137, acc: 0.9689443396776914\n",
      "NetRNNWithAttention, rep: 0, epoch: 138, acc: 0.9698667722195387\n",
      "NetRNNWithAttention, rep: 0, epoch: 139, acc: 0.9707293065451086\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 139   Acc: 0.9707 Params: min_length: 40, max_length: 45, fill: 0, value_1: -1, value_2: 1 Time: 68.90 sec\n"
     ]
    }
   ],
   "source": [
    "collectorA = dict()\n",
    "num_samples = 100\n",
    "for rep in range(1):\n",
    "    for params in parameters_list:\n",
    "        params_str = \", \".join([f\"{key}: {value}\" for key, value in params.items()])\n",
    "        for kind in [\"RNN\",\"NetRNNWithAttention\" ]:\n",
    "            if kind == \"RNN\":\n",
    "                model = NetRNN(hidden_dim=12, inp=3)\n",
    "            if kind == \"NetRNNWithAttention\":\n",
    "                model = NetRNNWithAttention(hidden_dim=12, inp=3)\n",
    "            optimizer = optim.Adam(model.parameters())\n",
    "            error = nn.MSELoss()\n",
    "            acc = 0.0\n",
    "            W = []\n",
    "            A = []\n",
    "            start_time = time.time()  # Start time of the epoch\n",
    "            while acc < 0.97:\n",
    "                model.resetHidden()\n",
    "                sequences, targets = generateTrainData(num_samples, params)\n",
    "\n",
    "                divs = []\n",
    "                for seq, target in zip(sequences, targets):\n",
    "                #for i, seq in enumerate(sequences):\n",
    "                    optimizer.zero_grad()\n",
    "                    seq_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "                    target_tensor = torch.tensor(target, dtype=torch.float32).unsqueeze(0)  # Add batch dimension to target as well\n",
    "                    \n",
    "                    \n",
    "                    output = model(seq_tensor)\n",
    "                    #if output.shape != target_tensor.shape:\n",
    "                    #    output = output.squeeze()  # Adjusting the output shape\n",
    "                    loss = error(output, target_tensor)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    div = output.detach().numpy() - target_tensor.numpy()\n",
    "                    divs.append(1.0 - abs(div).mean())\n",
    "\n",
    "                W.append(loss.item())\n",
    "                acc = mean(divs)\n",
    "                A.append(acc)\n",
    "                print(f\"{kind}, rep: {rep}, epoch: {len(A) }, acc: {acc}\")\n",
    "\n",
    "                # Restart training if not converging\n",
    "                # if acc < 0.97 and len(A) > 2000:\n",
    "                #     if kind == \"RNN\":\n",
    "                #         model = NetRNN(hidden_dim=12, inp=3)\n",
    "                #     if kind == \"NetRNNWithAttention\":\n",
    "                #         model = NetRNNWithAttention(hidden_dim=12, inp=3)\n",
    "                #     optimizer = optim.Adam(model.parameters())\n",
    "                #     acc = 0.0\n",
    "                #     W = []\n",
    "                #     A = []\n",
    "                #     print(\"repeat\")\n",
    "            end_time = time.time()  # End time of the epoch\n",
    "            epoch_duration = end_time - start_time  # Calculate duration\n",
    "            collectorA[\"{0} {1}\".format(kind, rep)] = A\n",
    "            params_save_str = \" \".join([f\"{key}_{value}\" for key, value in params.items()])\n",
    "            torch.save(model, f'models/model_{kind}_{params_save_str}.model')\n",
    "            print(f\"{kind:<20} Rep: {rep:<3} Epoch: {len(A):<5} Acc: {acc:.4f} \" f\"Params: {params_str:<40} Time: {epoch_duration:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:01:12.582743400Z",
     "start_time": "2023-12-06T17:01:12.545745700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./models/model_RNN_min_length_3 max_length_3 fill_0 value_1_-1 value_2_1.model\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "params = {\n",
    "            'min_length': 3, \n",
    "            'max_length': 3, \n",
    "            'fill': 0, \n",
    "            'value_1': -1, \n",
    "            'value_2': 1\n",
    "        }\n",
    "\n",
    "    \n",
    "def calculate_accuracy(predictions, targets):\n",
    "    # Ensure predictions and targets are the same shape\n",
    "    predictions = predictions.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    # Round predictions to the nearest integer (0 or 1)\n",
    "    predictions = predictions.round()\n",
    "\n",
    "    # Calculate the number of correct predictions\n",
    "    correct = (predictions == targets).float()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = correct.sum() / correct.numel()  # Use numel() instead of len()\n",
    "    return acc.item()\n",
    "\n",
    "\n",
    "# Generate test data\n",
    "sequences, targets = generateTrainDataOnlyResultsLabel(100, params)  # You can use a different function for test data\n",
    "\n",
    "# Convert sequences and targets to tensors and pad sequences\n",
    "seq_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "padded_seq_tensors = pad_sequence(seq_tensors, batch_first=True)\n",
    "target_tensors = torch.tensor(targets, dtype=torch.float32).squeeze()\n",
    "\n",
    "# Evaluate the model on test data\n",
    "with torch.no_grad():\n",
    "    total_acc = 0.0\n",
    "    for seq_tensor, target_tensor in zip(padded_seq_tensors, target_tensors):\n",
    "        output = model(seq_tensor.unsqueeze(0))  # Add batch dimension\n",
    "        acc = calculate_accuracy(output, target_tensor)\n",
    "        total_acc += acc\n",
    "\n",
    "    # Calculate average accuracy\n",
    "    avg_acc = total_acc / len(padded_seq_tensors)\n",
    "    print(f\"Average Test Accuracy: {avg_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:50:29.630985800Z",
     "start_time": "2023-12-06T17:50:29.440984500Z"
    }
   },
   "outputs": [],
   "source": [
    "def symbolEntropy(D,base=2):\n",
    "    value,counts = numpy.unique(D, return_counts=True)\n",
    "    return entropy(counts,base=base)\n",
    "\n",
    "def computeTransmissionHfast(I,H,O,maskC,maskNC,iMult=2,oMult=2):\n",
    "    #print(\"I H O\",I.shape,H.shape,O.shape)\n",
    "    B=numpy.bitwise_and(H,maskNC)\n",
    "    IB=(B*iMult)+I\n",
    "    AB=H#numpy.bitwise_and(H,maskC+maskNC)\n",
    "    BO=(B*oMult)+O\n",
    "    IAB=(AB*iMult)+I\n",
    "    IBO=(B*(iMult*oMult))+(I*oMult)+O\n",
    "    ABO=(AB*oMult)+O\n",
    "    IABO=(AB*(iMult*oMult))+(I*oMult)+O\n",
    "    hB=symbolEntropy(B, base=2)\n",
    "    hIB=symbolEntropy(IB, base=2)\n",
    "    hAB=symbolEntropy(AB, base=2)\n",
    "    hBO=symbolEntropy(BO, base=2)\n",
    "    hIAB=symbolEntropy(IAB, base=2)\n",
    "    hIBO=symbolEntropy(IBO, base=2)\n",
    "    hABO=symbolEntropy(ABO, base=2)\n",
    "    hIABO=symbolEntropy(IABO, base=2)\n",
    "    #-H(B)+H(IB)+H(AB)+H(BO)-H(IAB)-H(IBO)-H(ABO)+H(IABO)\n",
    "    #print(hB,hIB,hAB,hBO,hIAB,hIBO,hABO,hIABO)\n",
    "    return-hB+hIB+hAB+hBO-hIAB-hIBO-hABO+hIABO\n",
    "\n",
    "def singleShrinkingDecompositionInformation(I,H,O,width,iMult=2,oMult=2):\n",
    "    nodes=list(range(width))\n",
    "    cols=[]\n",
    "    colh=[]\n",
    "    while len(nodes)>0:\n",
    "        infos=[]\n",
    "        for node in nodes:\n",
    "            subset=copy.deepcopy(nodes)\n",
    "            subset.remove(node)\n",
    "            maskA=0\n",
    "            for s in subset:\n",
    "                maskA+=1*(2**s)\n",
    "            maskA=int(maskA)\n",
    "            maskB=numpy.bitwise_and(numpy.bitwise_not(maskA),((2**width)-1))\n",
    "            h=computeTransmissionHfast(I,H,O,maskA,maskB,iMult=iMult,oMult=oMult)\n",
    "            infos.append(h)\n",
    "        nodeToDrop=nodes[infos.index(max(infos))]\n",
    "        nodes.remove(nodeToDrop)\n",
    "        cols.append(copy.deepcopy(nodes))\n",
    "        colh.append(max(infos))\n",
    "    return cols,colh\n",
    "\n",
    "def getOutTaH(model,dataSet):\n",
    "    O,H=model.step(torch.Tensor(dataSet))\n",
    "    #print(H.shape,H.min(),H.max())\n",
    "    #figure()\n",
    "    #hist(H.flatten())\n",
    "    H=H.transpose()\n",
    "    O=O.transpose()\n",
    "    B=numpy.zeros(H.shape)\n",
    "    clusterNr=2\n",
    "    for i in range(B.shape[0]):\n",
    "        a=H[i].reshape(-1, 1)\n",
    "        if len(numpy.unique(a))==1:\n",
    "            who=numpy.random.randint(len(a))\n",
    "            a[who]=1-a[who]\n",
    "        kmeans = KMeans(n_clusters=clusterNr).fit(a)\n",
    "        B[i]=kmeans.labels_\n",
    "        #B[i]=1.0*(H[i]>numpy.median(H[i]))\n",
    "\n",
    "\n",
    "    H=numpy.zeros((H.shape))\n",
    "    for i in range(12):\n",
    "        H+=B[i]*(clusterNr**i)\n",
    "    H=H.astype((int))\n",
    "    return O,H\n",
    "\n",
    "def shrinkingDecompositionInformation(model,width,dataSet,target,numbers=[0,1,2],whichTS=5,dsLength=8):\n",
    "    output,H=getOutTaH(model,dataSet)\n",
    "    output=output.transpose()[whichTS::dsLength].transpose()\n",
    "    #print(\"target.shape\",target.shape,\"output.shape\",output.shape,\"H.shape\",H.shape,\"dataset.shape\",dataSet.shape)\n",
    "    H=H.transpose()[whichTS::dsLength].transpose()\n",
    "    #target=target.transpose()[whichTS::dsLength].transpose()\n",
    "    #print(H.shape,target.shape,numpy.array(range(512))[whichTS::dsLength])\n",
    "    collectorSet=dict()\n",
    "    collectorH=dict()\n",
    "    for number in numbers:\n",
    "        I=target[number].astype(int)\n",
    "        O=(1.0*(output[number]>0.5)).astype(int)\n",
    "        #print(\"O\",O,\"T\",target[number])\n",
    "        #print(number,\"I.shape\",I.shape,\"O.shape\",O.shape,\"H.shape\",H.shape)\n",
    "        s,h=singleShrinkingDecompositionInformation(I,H,O,width)\n",
    "        collectorSet[number]=s\n",
    "        collectorH[number]=h\n",
    "    return collectorSet,collectorH\n",
    "\n",
    "def removalIntoVec(res,width,H):\n",
    "    V=numpy.zeros(width)\n",
    "    #for i,r in enumerate(res):\n",
    "    #    for e in r:\n",
    "    #        V[e]+=H[0]-H[i]\n",
    "    fullSet=list(range(width))\n",
    "    nRes=copy.deepcopy(res)\n",
    "    nRes.insert(0,fullSet)\n",
    "    nodeList=[]\n",
    "    for i in range(width):\n",
    "        removedNode=list(set(nRes[i])-set(nRes[i+1]))[0]\n",
    "        nodeList.append(removedNode)\n",
    "    for i,node in enumerate(nodeList):\n",
    "        V[node]=H[0]-H[i]\n",
    "    #V=sqrt(V)\n",
    "    if V.sum()==0:\n",
    "        return V\n",
    "    return V#/V.max()\n",
    "\n",
    "def removalIntoMatrix(res,width,H):\n",
    "    M=[]\n",
    "    for i in range(len(res)):\n",
    "        M.append(removalIntoVec(res[i],width,H[i]))\n",
    "    return numpy.array(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:20:00.458639400Z",
     "start_time": "2023-12-06T18:19:58.361140500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 2.220446049250313e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 8.881784197001252e-16\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./Models/model_NetRNNWithAttention_min_length_3 max_length_3 fill_0 value_1_-1 value_2_1.model\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "params = {\n",
    "            'min_length': 3, \n",
    "            'max_length': 3, \n",
    "            'fill': 0, \n",
    "            'value_1': -1, \n",
    "            'value_2': 1\n",
    "        }\n",
    "\n",
    "# Run the analysis\n",
    "for i in range(3):\n",
    "    s, t = generateTrainData(100, params)  # Use your generateTrainData function\n",
    "    # Visualize the results\n",
    "    S,H=shrinkingDecompositionInformation(model,12,s,t.transpose(),numbers=[0],whichTS=i,dsLength=3)\n",
    "    subplot(6,2,i+1)\n",
    "    M=removalIntoMatrix(S,12,H)\n",
    "    imshow(M)\n",
    "    print(M.min(),M.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.220446049250313e-16 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 4.440892098500626e-16\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAACDCAYAAABFlfv4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdb0lEQVR4nO3dfUxUZ9oG8GsQGEpXBhFhGEVQu4ifqCgjuFZSJ1JxjbSbFltTP0q1baDRYBNls0rVrFM/aozUVLtbJVlr1WarttZoEMWmimKBRqSWCqtouwzUpXyqiHOe9w9fpo7MIINzZubg9UtOwpy5n8M9T+bc3g5nnqMSQggQERERKYSXuxMgIiIicgSbFyIiIlIUNi9ERESkKGxeiIiISFHYvBAREZGisHkhIiIiRWHzQkRERIrC5oWIiIgUhc0LERERKQqbFyIiIlIU2ZqX+vp6zJs3DwEBAQgMDERaWhpaWlq6HJOYmAiVSmW1vfXWW3KlSEQeiLWDiB5FJde9jWbOnImamhrs3LkT7e3tWLRoESZNmoS9e/faHZOYmIioqCisXbvWss/f3x8BAQFypEhEHoi1g4gexVuOg16+fBnHjh3DhQsXMHHiRABATk4OkpOTsXnzZuh0Ortj/f39odVq5UiLiDwcawcRdYcszUthYSECAwMtxQcADAYDvLy8cP78ebzwwgt2x3766afYs2cPtFotZs+ejVWrVsHf399ufFtbG9ra2iyPJUlCfX09+vfvD5VK5ZwXREQOEUKgubkZOp0OXl7d/+s0awfRk8uRuiFL82IymRASEmL9i7y9ERQUBJPJZHfcq6++ioiICOh0Oly8eBErVqxARUUFvvjiC7tjjEYj1qxZ47Tcich5bty4gUGDBnU7nrWDiLpTNxxqXlauXIkNGzZ0GXP58mVHDmllyZIllp/HjBmDsLAwTJ8+HVVVVRg2bJjNMVlZWcjMzLQ8bmxsxODBg1FdEomAP/DLVK70QtQYd6dAj+ngT2VOOU5Ti4SICdfQt29fAMqqHX9CMrzh0+NciKhn7qEd3+KopW50xaHmZfny5Vi4cGGXMUOHDoVWq0VdXZ11Uvfuob6+3qG/Sev1egBAZWWl3QKkVquhVqs77Q/4gxcC+rJ5cSVvFQu+0jn7nOn484uSaoc3fPheJnKH///6UHf+bOtQ8zJgwAAMGDDgkXHx8fFoaGhAcXExYmNjAQAnT56EJEmWotId33//PQAgLCzMkTSJyMOwdhCRM8ny0cSIESPw/PPPY/HixSgqKsKZM2eQkZGBuXPnWr4t8MsvvyA6OhpFRUUAgKqqKqxbtw7FxcW4du0avvzyS8yfPx/PPvssxo4dK0eaRORhWDuIqDtkuWAXuH/lf0ZGBqZPn4579+7B29sbX3zxBfR6PXJychASEoKKigrcunULAODr64sTJ05g69ataGpqgkqlgiRJ8PPzw9GjR5GcnCxXqkTkQTpqx9SpU9He3g4vLy9oNBoUFRUhLi4O7e3tNmvHxo0b0draCgDo378/3nnnHXe+DCKSkWwXhQQFBWHv3r345z//CUmSsG3bNpSUlCAmJgZJSUnw9/eHEAKJiYkAgPDwcJw+fRpfffUVhBD4+9//jrKyMrz44otISUnBpUuX5EqViDxIUFAQ5syZAwD45JNPcPHiRYwfPx5JSUmoq6tDZGRkp9phNBpx+/ZtbNiwAeXl5Xjrrbfw6quvsm4Q9VKyrbDbQa/XY9KkSfjwww8B3F9LITw8HO+88w5WrlzZKT41NRWtra04cuSIZd/kyZMxbtw47Nix45G/r6mpCRqNBr/9NJQX7LpYkm6cu1Ogx3T8v9875ThNzRL6Rf0HjY2NPVrl1tV1A/i9diRiDi/YJXKDe6IdBTjcrboh67/ud+/eRXFxMQwGw++/0MsLBoMBhYWFNscUFhZaxQNAUlKS3Xgi6l1YN4joUWS75gUAbt68CbPZjNDQUKv9oaGh+PHHH22OMZlMNuPtLVD18CqZTU1Nj5k1EbmTK+oGwNpBpGSK/7uK0WiERqOxbOHh4e5OiYgUgLWDSLlkbV6Cg4PRp08f1NbWWu2vra21u+CUVqt1KD4rKwuNjY2W7caNG85JnojcwhV1A2DtIFIyWZsXX19fxMbGIj8/37JPkiTk5+cjPj7e5pj4+HireADIy8uzG69WqxEQEGC1EZFyuaJuAKwdREom6zUvAJCZmYkFCxZg4sSJiIuLw9atW9Ha2opFixYBAObPn4+BAwfCaDQCAJYuXYpp06bhgw8+wKxZs7Bv3z589913+Pjjj+VOlYg8BOsGEXVF9uYlNTUVv/76K1avXg2TyYRx48bh2LFjlovrrl+/bnXr64SEBOzduxd/+9vf8Ne//hV//OMfcejQIYwePVruVInIQ7BuEFFXZG9egPs3WerYOh53KCgosIrNzc21/O8KAMrLy/Hiiy/izp07rkiViDwE6wYR2SP7t43279+PzMxMZGdnW62w+/CdYx8UEBCAmpoay1ZdXS13mkTkQVg3iKgrsjcvW7ZsweLFi7Fo0SKMHDkSO3bsgL+/P3bt2mV3jEqlglartWwPr99ARL0b6wYRdUXWPxt1rJSZlZVl2feolTIBoKWlBREREZAkCRMmTMD69esxatQom7EPLzTV2NgIAGhqkZz0Kqi77ol2d6dAj6mp2TnnTcf515O7j7iibgD2a8c9tAOy3jSFiGy5h/v/hnSnbnjcCrvDhw/Hrl27MHbsWDQ2NmLz5s1ISEhAeXk5Bg0a1CneaDRizZo1nfZHTLjmlNdAjviPuxOgx9QvyrnHa25uhkajcWiMK+oGYL92fIujDuVLRM7Vnbrhkgt2HREfH2+1NkNCQgJGjBiBnTt3Yt26dZ3is7KykJmZaXksSRLq6+vRv39/qwv8HtbU1ITw8HDcuHFDces7MHf3UGru7shbCIHm5mbodDqX/D5H6wbQs9qh1PcAwNzdhbl3nyN1Q9bmpScrZT7Mx8cH48ePR2Vlpc3n1Wo11Gq11b7AwMBu56jkxamYu3soNXdX5+3oJy4dXFE3gMerHUp9DwDM3V2Ye/d0t2543Aq7DzObzSgrK0NYWJhcaRKRB2HdIKJH8bgVdteuXYvJkyfjmWeeQUNDAzZt2oTq6mq88cYbcqdKRB6CdYOIuuJxK+z+9ttvWLx4MUwmE/r164fY2FicPXsWI0eOdGpearUa2dnZnT42VgLm7h5KzV2JebNuOB9zdw/mLg+V6Ml3GYmIiIjcRPZF6oiIiIicic0LERERKQqbFyIiIlIUNi9ERESkKL26edm+fTsiIyPh5+cHvV6PoqKiLuM///xzREdHw8/PD2PGjMHRo65fJtxoNGLSpEno27cvQkJCkJKSgoqKii7H5ObmQqVSWW1+fn4uyvh37733Xqc8oqOjuxzjCXMOAJGRkZ1yV6lUSE9Ptxnvzjn/5ptvMHv2bOh0OqhUKhw6dMjqeSEEVq9ejbCwMDz11FMwGAy4cuXKI4/r6PnSW7FuuBbrButGT/Ta5mX//v3IzMxEdnY2SkpKEBMTg6SkJNTV1dmMP3v2LF555RWkpaWhtLQUKSkpSElJwaVLl1ya9+nTp5Geno5z584hLy8P7e3tmDFjBlpbW7scFxAQgJqaGstWXV3tooytjRo1yiqPb7/91m6sp8w5AFy4cMEq77y8PADASy+9ZHeMu+a8tbUVMTEx2L59u83nN27ciG3btmHHjh04f/48nn76aSQlJeHOnTt2j+no+dJbsW6wbjiCdcONdUP0UnFxcSI9Pd3y2Gw2C51OJ4xGo834l19+WcyaNctqn16vF2+++aaseT5KXV2dACBOnz5tN2b37t1Co9G4Lik7srOzRUxMTLfjPXXOhRBi6dKlYtiwYUKSJJvPe8qcAxAHDx60PJYkSWi1WrFp0ybLvoaGBqFWq8Vnn31m9ziOni+9FeuG67FuuF5vqBu98pOXu3fvori4GAaDwbLPy8sLBoMBhYWFNscUFhZaxQNAUlKS3XhXaWxsBAAEBQV1GdfS0oKIiAiEh4djzpw5KC8vd0V6nVy5cgU6nQ5Dhw7FvHnzcP36dbuxnjrnd+/exZ49e/D66693eXNPT5nzB129ehUmk8lqXjUaDfR6vd157cn50huxbrBuPA7WDdfWjV7ZvNy8eRNms9myGmeH0NBQmEwmm2NMJpND8a4gSRKWLVuGKVOmYPTo0Xbjhg8fjl27duHw4cPYs2cPJElCQkICfv75ZxdmC+j1euTm5uLYsWP46KOPcPXqVUydOhXNzc024z1xzgHg0KFDaGhowMKFC+3GeMqcP6xj7hyZ156cL70R6wbrxuNg3Xj0GGeS/fYA1HPp6em4dOlSl3//BYD4+HirG9YlJCRgxIgR2LlzJ9atWyd3mhYzZ860/Dx27Fjo9XpERETgwIEDSEtLc1kej+uTTz7BzJkzu7wtu6fMOdHDWDfcg3XDtXrlJy/BwcHo06cPamtrrfbX1tZCq9XaHKPVah2Kl1tGRgaOHDmCU6dOYdCgQQ6N9fHxwfjx41FZWSlTdt0TGBiIqKgou3l42pwDQHV1NU6cOOHwDf08Zc475s6Ree3J+dIbsW54xnuYdcP1lFg3emXz4uvri9jYWOTn51v2SZKE/Px8q673QfHx8VbxAJCXl2c3Xi5CCGRkZODgwYM4efIkhgwZ4vAxzGYzysrKEBYWJkOG3dfS0oKqqiq7eXjKnD9o9+7dCAkJwaxZsxwa5ylzPmTIEGi1Wqt5bWpqwvnz5+3Oa0/Ol96IdcMz3sOsG66nyLoh+yXBbrJv3z6hVqtFbm6u+OGHH8SSJUtEYGCgMJlMQgghXnvtNbFy5UpL/JkzZ4S3t7fYvHmzuHz5ssjOzhY+Pj6irKzMpXm//fbbQqPRiIKCAlFTU2PZbt26ZYl5OPc1a9aI48ePi6qqKlFcXCzmzp0r/Pz8RHl5uUtzX758uSgoKBBXr14VZ86cEQaDQQQHB4u6ujqbeXvKnHcwm81i8ODBYsWKFZ2e86Q5b25uFqWlpaK0tFQAEFu2bBGlpaWiurpaCCHE+++/LwIDA8Xhw4fFxYsXxZw5c8SQIUPE7du3Lcd47rnnRE5OjuXxo86XJwXrBuuGo1g33FM3em3zIoQQOTk5YvDgwcLX11fExcWJc+fOWZ6bNm2aWLBggVX8gQMHRFRUlPD19RWjRo0SX3/9tYszvv8VNlvb7t27LTEP575s2TLL6wwNDRXJycmipKTE5bmnpqaKsLAw4evrKwYOHChSU1NFZWWl3byF8Iw573D8+HEBQFRUVHR6zpPm/NSpUzbfIx35SZIkVq1aJUJDQ4VarRbTp0/v9JoiIiJEdna21b6uzpcnCeuGa7FusG70hEoIIeT/fIeIiIjIOXrlNS9ERETUe7F5ISIiIkVh80JERESKwuaFiIiIFIXNCxERESkKmxciIiJSFDYvREREpChsXoiIiEhR2LwQERGRorB5ISIiIkVh80JERESKwuaFiIiIFIXNCxERESkKmxciIiJSFDYvREREpChsXoiIiEhR2LwQERGRorB5ISIiIkWRrXmpr6/HvHnzEBAQgMDAQKSlpaGlpaXLMYmJiVCpVFbbW2+9JVeKREREpEAqIYSQ48AzZ85ETU0Ndu7cifb2dixatAiTJk3C3r177Y5JTExEVFQU1q5da9nn7++PgIAAOVIkIiIiBfKW46CXL1/GsWPHcOHCBUycOBEAkJOTg+TkZGzevBk6nc7uWH9/f2i1WjnSIiIiol5AlualsLAQgYGBlsYFAAwGA7y8vHD+/Hm88MILdsd++umn2LNnD7RaLWbPno1Vq1bB39/fbnxbWxva2tosjyVJQn19Pfr37w+VSuWcF0REDhFCoLm5GTqdDl5evLSOiJxLlubFZDIhJCTE+hd5eyMoKAgmk8nuuFdffRURERHQ6XS4ePEiVqxYgYqKCnzxxRd2xxiNRqxZs8ZpuROR89y4cQODBg1ydxpE1Ms41LysXLkSGzZs6DLm8uXLPU5myZIllp/HjBmDsLAwTJ8+HVVVVRg2bJjNMVlZWcjMzLQ8bmxsxODBg1FdEomAPzz+//heiBrz2MfocPCnMqccx5k5ET3IWe/RphYJEROuoW/fvk45HhHRgxxqXpYvX46FCxd2GTN06FBotVrU1dVZ7b937x7q6+sdup5Fr9cDACorK+02L2q1Gmq1utP+gD94IaDv4zcv3iqfxz5GB2fkAzg3J6IHOes92oF/uiUiOTjUvAwYMAADBgx4ZFx8fDwaGhpQXFyM2NhYAMDJkychSZKlIemO77//HgAQFhbmSJpERETUi8lyJd2IESPw/PPPY/HixSgqKsKZM2eQkZGBuXPnWr5p9MsvvyA6OhpFRUUAgKqqKqxbtw7FxcW4du0avvzyS8yfPx/PPvssxo4dK0eaREREpECyfQ3g008/RXR0NKZPn47k5GT86U9/wscff2x5vr29HRUVFbh16xYAwNfXFydOnMCMGTMQHR2N5cuX4y9/+Qu++uoruVIkIiIiBZKteQkKCsLevXvR3NyM9evX4+TJkwgODoZer0dRUREiIyMhhEBiYiIAIDw8HKdPn8b//vc//Otf/0KfPn2wbds2TJkyBUePHpUrTSIiIlIY2Rdg2L9/PzIzM5GdnY2SkhLExMQgKSmp0wW9Hc6ePYtXXnkFaWlpKC0tRUpKClJSUnDp0iW5UyUiIiIFkO32AB30ej0mTZqEDz/8EMD9ReTCw8PxzjvvYOXKlZ3iU1NT0draiiNHjlj2TZ48GePGjcOOHTse+fuampqg0Wjw209DnfLNiSTduMc+Rofj//3eKcdxZk5ED3LWe7SpWUK/qP+gsbGRt/cgIqeT9ZOXu3fvori4GAaD4fdf6OUFg8GAwsJCm2MKCwut4gEgKSnJbnxbWxuampqsNiIiIuq9ZG1ebt68CbPZjNDQUKv9oaGhdlfaNZlMDsUbjUZoNBrLFh4e7pzkiYiIyCMp/qYjWVlZaGxstGw3btxwd0pEREQkI1nubdQhODgYffr0QW1trdX+2tpauyvtarVah+LtrbBLREREvZOsn7z4+voiNjYW+fn5ln2SJCE/Px/x8fE2x8THx1vFA0BeXp7deCIiInqyyPrJCwBkZmZiwYIFmDhxIuLi4rB161a0trZi0aJFAID58+dj4MCBMBqNAIClS5di2rRp+OCDDzBr1izs27cP3333ndUCd0RERPTkkr15SU1Nxb///W+kpaXBbDbj6aefxrZt2ywX5V6/fh1eXr9/APTTTz/h3r17ePfdd/Huu+8CAHx8fDB69Gi5UyUiIiIFkL152b9/Pw4fPox//OMf0Ov12Lp1K5YvX44///nPCAkJQUFBQacxAQEBqKiosDzmnWmJiIiog+zfNtqyZQsWL16MRYsWYeTIkdixYwf8/f2xa9cuu2NUKhW0Wq1le/ir00RERPTkkvWTl45F6rKysiz7HrVIHQC0tLQgIiICkiRhwoQJWL9+PUaNGmUztq2tDW1tbZbHjY2NAICmFskpr+GeaHfKcYD7q446gzNzInqQs96jHeefzAt4E9ETStbmpatF6n788UebY4YPH45du3Zh7NixaGxsxObNm5GQkIDy8nIMGjSoU7zRaMSaNWs67Y+YcM0prwH4j5OOA/SLctaRnJcT0YOc9x69r7m5GRqNxrkHJaInnuzXvDgqPj7e6mvRCQkJGDFiBHbu3Il169Z1is/KykJmZqblsSRJqK+vR//+/bu8VqapqQnh4eG4ceOG4u69wtzdQ6m5uyNvIQSam5uh0+lc8vuI6MnicYvUPczHxwfjx49HZWWlzedtLVIXGBjY7RwDAgIU9Q/Rg5i7eyg1d1fnzU9ciEguHrdI3cPMZjPKysoQFhYmV5pERESkIB63SN3atWsxefJkPPPMM2hoaMCmTZtQXV2NN954Q+5UiYiISAFcskjdr7/+itWrV8NkMmHcuHE4duyY3UXqfvvtNyxevBgmkwn9+vVDbGwszp49i5EjRzo1L7VajezsbEXeF4m5u4dSc1dq3kRE9qgEv8tIRERECiL7InVEREREzsTmhYiIiBSFzQsREREpCpsXIiIiUpRe3bxs374dkZGR8PPzg16vR1FRUZfxn3/+OaKjo+Hn54cxY8bg6NGjLsr0d0ajEZMmTULfvn0REhKClJQUqzts25KbmwuVSmW1+fn5uSjj37333nud8oiOju5yjCfMOQBERkZ2yl2lUiE9Pd1mvDvn/JtvvsHs2bOh0+mgUqlw6NAhq+eFEFi9ejXCwsLw1FNPwWAw4MqVK488rqPnCxGRu/Ta5mX//v3IzMxEdnY2SkpKEBMTg6SkJNTV1dmMP3v2LF555RWkpaWhtLQUKSkpSElJwaVLl1ya9+nTp5Geno5z584hLy8P7e3tmDFjBlpbW7scFxAQgJqaGstWXV3tooytjRo1yiqPb7/91m6sp8w5AFy4cMEq77y8PADASy+9ZHeMu+a8tbUVMTEx2L59u83nN27ciG3btmHHjh04f/48nn76aSQlJeHOnTt2j+no+UJE5Fail4qLixPp6emWx2azWeh0OmE0Gm3Gv/zyy2LWrFlW+/R6vXjzzTdlzfNR6urqBABx+vRpuzG7d+8WGo3GdUnZkZ2dLWJiYrod76lzLoQQS5cuFcOGDROSJNl83lPmHIA4ePCg5bEkSUKr1YpNmzZZ9jU0NAi1Wi0+++wzu8dx9HwhInKnXvnJy927d1FcXAyDwWDZ5+XlBYPBgMLCQptjCgsLreIBICkpyW68qzQ2NgIAgoKCuoxraWlBREQEwsPDMWfOHJSXl7sivU6uXLkCnU6HoUOHYt68ebh+/brdWE+d87t372LPnj14/fXXu7y5p6fM+YOuXr0Kk8lkNa8ajQZ6vd7uvPbkfCEicqde2bzcvHkTZrPZsopvh9DQUJhMJptjTCaTQ/GuIEkSli1bhilTpmD06NF244YPH45du3bh8OHD2LNnDyRJQkJCAn7++WcXZgvo9Xrk5ubi2LFj+Oijj3D16lVMnToVzc3NNuM9cc4B4NChQ2hoaMDChQvtxnjKnD+sY+4cmdeenC9ERO4k++0BqOfS09Nx6dKlLq8bAYD4+HirG10mJCRgxIgR2LlzJ9atWyd3mhYzZ860/Dx27Fjo9XpERETgwIEDSEtLc1kej+uTTz7BzJkzodPp7MZ4ypwTET2JeuUnL8HBwejTpw9qa2ut9tfW1kKr1doco9VqHYqXW0ZGBo4cOYJTp05h0KBBDo318fHB+PHjUVlZKVN23RMYGIioqCi7eXjanANAdXU1Tpw44fCNQD1lzjvmzpF57cn5QkTkTr2yefH19UVsbCzy8/Mt+yRJQn5+vtX/lh8UHx9vFQ8AeXl5duPlIoRARkYGDh48iJMnT2LIkCEOH8NsNqOsrAxhYWEyZNh9LS0tqKqqspuHp8z5g3bv3o2QkBDMmjXLoXGeMudDhgyBVqu1mtempiacP3/e7rz25HwhInIrd18xLJd9+/YJtVotcnNzxQ8//CCWLFkiAgMDhclkEkII8dprr4mVK1da4s+cOSO8vb3F5s2bxeXLl0V2drbw8fERZWVlLs377bffFhqNRhQUFIiamhrLduvWLUvMw7mvWbNGHD9+XFRVVYni4mIxd+5c4efnJ8rLy12a+/Lly0VBQYG4evWqOHPmjDAYDCI4OFjU1dXZzNtT5ryD2WwWgwcPFitWrOj0nCfNeXNzsygtLRWlpaUCgNiyZYsoLS0V1dXVQggh3n//fREYGCgOHz4sLl68KObMmSOGDBkibt++bTnGc889J3JyciyPH3W+EBF5kl7bvAghRE5Ojhg8eLDw9fUVcXFx4ty5c5bnpk2bJhYsWGAVf+DAAREVFSV8fX3FqFGjxNdff+3ijO9/9dXWtnv3bkvMw7kvW7bM8jpDQ0NFcnKyKCkpcXnuqampIiwsTPj6+oqBAweK1NRUUVlZaTdvITxjzjscP35cABAVFRWdnvOkOT916pTN90hHfpIkiVWrVonQ0FChVqvF9OnTO72miIgIkZ2dbbWvq/OFiMiTqIQQwi0f+RARERH1QK+85oWIiIh6LzYvREREpChsXoiIiEhR2LwQERGRorB5ISIiIkVh80JERESKwuaFiIiIFIXNCxERESkKmxciIiJSFDYvREREpChsXoiIiEhR2LwQERGRovwf2F+XJ63tPE0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./Models/model_NetRNNWithAttention_min_length_3 max_length_3 fill_0 value_1_-1 value_2_1.model\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "params = {\n",
    "            'min_length': 3, \n",
    "            'max_length': 3, \n",
    "            'fill': 0, \n",
    "            'value_1': -1, \n",
    "            'value_2': 1\n",
    "        }\n",
    "\n",
    "# Run the analysis\n",
    "for i in range(3):\n",
    "    s, t = generateTrainData(100, params)  # Use your generateTrainData function\n",
    "    # Visualize the results\n",
    "    S,H=shrinkingDecompositionInformation(model,12,s,t.transpose(),numbers=[0],whichTS=i,dsLength=3)\n",
    "    subplot(6,2,i+1)\n",
    "    M=removalIntoMatrix(S,12,H)\n",
    "    imshow(M)\n",
    "    print(M.min(),M.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1580e8fc450>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "modelRNN=torch.load(\"models/model_RNN_min_length_3 max_length_3 fill_0 value_1_-1 value_2_1.model\")\n",
    "modelRNNWithAttention=torch.load(\"models/model_NetRNNWithAttention_min_length_3 max_length_3 fill_0 value_1_-1 value_2_1.model\")\n",
    "params = {\n",
    "            'min_length': 3, \n",
    "            'max_length': 3, \n",
    "            'fill': 0, \n",
    "            'value_1': -1, \n",
    "            'value_2': 1\n",
    "        }\n",
    "s, t = generateTrainData(100, params)  \n",
    "S,H=shrinkingDecompositionInformation(modelRNN,12,s,t.transpose(),numbers=[0],whichTS=i,dsLength=3)\n",
    "figure()\n",
    "imshow(removalIntoMatrix(S,12,H))\n",
    "S,H=shrinkingDecompositionInformation(modelRNNWithAttention,12,s,t.transpose(),numbers=[0],whichTS=i,dsLength=3)\n",
    "figure()\n",
    "imshow(removalIntoMatrix(S,12,H))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
