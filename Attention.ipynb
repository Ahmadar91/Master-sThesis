{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:31.277897100Z",
     "start_time": "2023-12-06T17:00:30.656125100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\magics\\pylab.py:162: UserWarning: pylab import has clobbered these variables: ['mean', 'copy']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:39.317492Z",
     "start_time": "2023-12-06T17:00:34.720993Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import numpy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib.pyplot import figure, subplots, imshow, xticks, yticks, title\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.cluster import mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "from statistics import mean\n",
    "from scipy.stats import entropy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim**0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:40.939651100Z",
     "start_time": "2023-12-06T17:00:40.913651900Z"
    }
   },
   "outputs": [],
   "source": [
    "class NetRNNWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetRNNWithAttention, self).__init__()\n",
    "        self.attention = SelfAttention(inp)  # Attention layer with input dimension\n",
    "        self.rnnLayer = nn.RNN(inp, hidden_dim, batch_first=True)\n",
    "        self.outputLayer = nn.Linear(hidden_dim, 3)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.resetHidden()\n",
    "        self.inp = inp\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applying attention first\n",
    "        attn_out = self.attention(x)\n",
    "        # Feeding the output of the attention layer into the RNN\n",
    "        h0 = torch.zeros(1, x.shape[0], self.hidden_dim)\n",
    "        rnn_out, _ = self.rnnLayer(attn_out, h0)\n",
    "        rnn_out = torch.tanh(rnn_out)\n",
    "\n",
    "        # Applying the final output layer\n",
    "        out = torch.sigmoid(self.outputLayer(rnn_out[:, -1, :])).squeeze()\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                # Feeding the output of the attention layer into the RNN\n",
    "                out, h0 = self.rnnLayer(attn_out, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)\n",
    "                out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "\n",
    "\n",
    "model = NetRNNWithAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:41.605650400Z",
     "start_time": "2023-12-06T17:00:41.594151100Z"
    }
   },
   "outputs": [],
   "source": [
    "class NetRNN(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetRNN, self).__init__()\n",
    "        self.rnnLayer = nn.RNN(inp, hidden_dim, batch_first=True)\n",
    "        self.outputLayer = nn.Linear(hidden_dim, 3)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.resetHidden()\n",
    "        self.inp = inp\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h0 = torch.Tensor(numpy.zeros((1, x.shape[0], self.hidden_dim)))\n",
    "        out, self.h0 = self.rnnLayer(x, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "        out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.Tensor(numpy.zeros((1, 1, self.hidden_dim)))\n",
    "            for i in range(x.shape[1]):\n",
    "                out, h0 = self.rnnLayer(x[l][i].reshape((1, 1, self.inp)), h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "                # print(out.detach().numpy().flatten().shape)\n",
    "            out = torch.tanh(out)\n",
    "            out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "            for i in range(x.shape[1]):\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "                # print(out.detach().numpy().flatten().shape)\n",
    "        return numpy.array(O), numpy.array(H)\n",
    "\n",
    "\n",
    "model = NetRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=12,inp=3):\n",
    "        super(NetLSTM, self).__init__()\n",
    "        self.lstmLayer = nn.LSTM(inp, int(hidden_dim/2), 1,batch_first=True)\n",
    "        self.outputLayer = nn.Linear(int(hidden_dim/2), 3)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.resetHidden()\n",
    "        self.inp=inp\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h0 = (torch.zeros(1, x.shape[0], int(self.hidden_dim/2)),\n",
    "                   torch.zeros(1, x.shape[0], int(self.hidden_dim/2)))\n",
    "        out, self.h0 = self.lstmLayer(x, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "        hh=numpy.concatenate((self.h0[0].detach().numpy(),self.h0[1].detach().numpy()),2)\n",
    "        self.hidden.append(hh)\n",
    "        out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = (torch.Tensor(numpy.zeros((1, 1, int(self.hidden_dim/2)))),\n",
    "                  torch.Tensor(numpy.zeros((1, 1, int(self.hidden_dim/2)))))\n",
    "            for i in range(x.shape[1]):\n",
    "                out, h0 = self.lstmLayer(x[l][i].reshape((1, 1, self.inp)), h0)\n",
    "                hh=numpy.concatenate((h0[0].detach().numpy().flatten(),h0[1].detach().numpy().flatten()))\n",
    "                H.append(hh.flatten())\n",
    "                #print(hh.flatten().shape)\n",
    "            out = torch.tanh(out)\n",
    "            out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "            for i in range(x.shape[1]):\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "                #print(out.detach().numpy().flatten().shape)\n",
    "        return numpy.array(O),numpy.array(H)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "model=NetLSTM(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12,inp=3):\n",
    "        super(NetLSTMWithAttention, self).__init__()\n",
    "        self.lstmLayer = nn.LSTM(inp, int(hidden_dim/2), 1,batch_first=True)\n",
    "        self.outputLayer = nn.Linear(int(hidden_dim/2), 3)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.resetHidden()\n",
    "        self.inp=inp\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applying attention first\n",
    "        attn_out = self.attention(x)\n",
    "\n",
    "        self.h0 = (torch.zeros(1, attn_out.shape[0], int(self.hidden_dim/2)),\n",
    "                   torch.zeros(1, attn_out.shape[0], int(self.hidden_dim/2)))\n",
    "        out, self.h0 = self.lstmLayer(attn_out, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "        hh = torch.cat((self.h0[0].detach(), self.h0[1].detach()), 2)\n",
    "        self.hidden.append(hh.numpy())\n",
    "        out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = (torch.zeros(1, 1, int(self.hidden_dim/2)),\n",
    "                  torch.zeros(1, 1, int(self.hidden_dim/2)))\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                out, h0 = self.lstmLayer(attn_out, h0)\n",
    "                hh = torch.cat((h0[0].detach(), h0[1].detach()), 2).numpy().flatten()\n",
    "                H.append(hh)\n",
    "                \n",
    "                out = torch.tanh(out)\n",
    "                out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return numpy.array(O), numpy.array(H)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "model=NetLSTMWithAttention(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetGRU(nn.Module):\n",
    "    def __init__(self, hidden_dim=12,inp=3):\n",
    "        super(NetGRU, self).__init__()\n",
    "        self.gruLayer = nn.GRU(inp, hidden_dim, batch_first=True)\n",
    "        self.outputLayer = nn.Linear(hidden_dim, 3)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.resetHidden()\n",
    "        self.inp=inp\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h0 = torch.Tensor(numpy.zeros((1, x.shape[0], self.hidden_dim)))\n",
    "        out, self.h0 = self.gruLayer(x, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "        #print(self.h0.shape)\n",
    "        out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.Tensor(numpy.zeros((1, 1, self.hidden_dim)))\n",
    "            for i in range(x.shape[1]):\n",
    "                out, h0 = self.gruLayer(x[l][i].reshape((1, 1, self.inp)), h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "                #print(out.detach().numpy().flatten().shape)\n",
    "            out = torch.tanh(out)\n",
    "            out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "            for i in range(x.shape[1]):\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "                #print(out.detach().numpy().flatten().shape)\n",
    "        return numpy.array(O),numpy.array(H)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "model=NetGRU(hidden_dim=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetGRUMWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12,inp=3):\n",
    "        super(NetGRUMWithAttention, self).__init__()\n",
    "        self.attention = SelfAttention(inp)  # Attention layer with input dimension\n",
    "        self.gruLayer = nn.GRU(inp, hidden_dim, batch_first=True)\n",
    "        self.outputLayer = nn.Linear(hidden_dim, 3)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.resetHidden()\n",
    "        self.inp = inp\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applying attention first\n",
    "        attn_out = self.attention(x)\n",
    "\n",
    "        self.h0 = torch.zeros(1, attn_out.shape[0], self.hidden_dim)\n",
    "        out, self.h0 = self.gruLayer(attn_out, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "        out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                out, h0 = self.gruLayer(attn_out, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "                \n",
    "                out = torch.tanh(out)\n",
    "                out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return numpy.array(O), numpy.array(H)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "model=NetGRUMWithAttention(hidden_dim=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:23.990361100Z",
     "start_time": "2023-12-06T18:19:23.969358500Z"
    }
   },
   "outputs": [],
   "source": [
    "def generateTrainData(num_samples, params):\n",
    "    s = []  # Sequences\n",
    "    t = []  # Labels\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        common_length = np.random.randint(params[\"min_length\"], params[\"max_length\"] + 1)\n",
    "\n",
    "        array_A = np.full(common_length, params[\"fill\"])\n",
    "        array_B = np.full(common_length, params[\"fill\"])\n",
    "        array_C = np.full(common_length, params[\"fill\"])\n",
    "\n",
    "        # Exclude the last two indices\n",
    "        possible_indices = np.arange(common_length - 2)\n",
    "\n",
    "        index_A = np.random.choice(possible_indices)\n",
    "        value_A = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_A[index_A] = value_A\n",
    "\n",
    "        # Update possible indices for array B to also exclude index_A\n",
    "        possible_indices_B = np.delete(possible_indices, np.where(possible_indices == index_A))\n",
    "        index_B = np.random.choice(possible_indices_B)\n",
    "        value_B = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_B[index_B] = value_B\n",
    "\n",
    "        value_C = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_C[-1] = value_C\n",
    "        array_C[-2] = value_C\n",
    "\n",
    "        mapped_value_A = 1 if value_A == params[\"value_2\"] else 0\n",
    "        mapped_value_B = 1 if value_B == params[\"value_2\"] else 0\n",
    "        result = int((mapped_value_A != mapped_value_B) if value_C == params[\"value_1\"] else (mapped_value_A == mapped_value_B))\n",
    "\n",
    "        # Mapping back to original value_1 and value_2 for the label\n",
    "        label_value_A = params[\"value_2\"] if mapped_value_A == 1 else params[\"value_1\"]\n",
    "        label_value_B = params[\"value_2\"] if mapped_value_B == 1 else params[\"value_1\"]\n",
    "        label_value_C = params[\"value_2\"] if result == 1 else params[\"value_1\"]\n",
    "\n",
    "        label_arr = [mapped_value_A, mapped_value_B, result]  # Label array with value_A, value_B, and result\n",
    "\n",
    "        combined_array = np.vstack([array_A, array_B, array_C]).T\n",
    "        s.append(combined_array)\n",
    "        t.append(label_arr)\n",
    "\n",
    "    return s, np.array(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:26.878858500Z",
     "start_time": "2023-12-06T18:19:26.864361100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0,  0,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0, -1,  0],\n",
      "       [-1,  0,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0, -1],\n",
      "       [ 0,  0, -1]]), array([[ 0,  0,  0],\n",
      "       [ 1,  0,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  1,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0, -1],\n",
      "       [ 0,  0, -1]])]\n",
      "[[0 0 0]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "num_seq = 2\n",
    "# Example dictionary with parameters\n",
    "parameters = {\"min_length\": 5, \"max_length\": 10, \"fill\": 0, \"value_1\": -1, \"value_2\": 1}\n",
    "\n",
    "sequences, labels = generateTrainData(num_seq, parameters)\n",
    "print(sequences)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:28.905358900Z",
     "start_time": "2023-12-06T18:19:28.879361600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "-1\n",
      "0\n",
      "1\n",
      "1\n",
      "-1\n",
      "0\n",
      "tensor([[[ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.],\n",
      "         [ 0.,  0.,  1.],\n",
      "         [ 0.,  0.,  1.]],\n",
      "\n",
      "        [[-1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0., -1.],\n",
      "         [ 0.,  0., -1.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [-1.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  1.],\n",
      "         [ 0.,  0.,  1.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0., -1.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 1.,  0.,  0.],\n",
      "         [ 0.,  0.,  1.],\n",
      "         [ 0.,  0.,  1.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [-1.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  1.],\n",
      "         [ 0.,  0.,  1.]]])\n",
      "torch.Size([5, 3])\n",
      "NetRNN(\n",
      "  (rnnLayer): RNN(3, 12, batch_first=True)\n",
      "  (outputLayer): Linear(in_features=12, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"min_length\": 10,\n",
    "    \"max_length\": 10,\n",
    "    \"fill\": 0,\n",
    "    \"value_1\": -1,\n",
    "    \"value_2\": 1,\n",
    "}\n",
    "sequences, labels = generateTrainData(5, parameters)\n",
    "model = NetRNN()\n",
    "output=model(torch.Tensor(sequences))\n",
    "print(torch.Tensor(sequences))\n",
    "print(output.shape)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:03:38.917270600Z",
     "start_time": "2023-12-06T18:03:36.540272600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_25736\\1362812023.py:40: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0, 0.9, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"min_length\": 10,\n",
    "    \"max_length\": 15,\n",
    "    \"fill\": 0,\n",
    "    \"value_1\": -1,\n",
    "    \"value_2\": 1,\n",
    "}\n",
    "sequences, labels = generateTrainData(5, parameters)\n",
    "\n",
    "def plot_sequences(sequences, labels):\n",
    "    num_samples = len(sequences)  # Number of samples to display\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(3 * num_samples, 15))\n",
    "\n",
    "    for i, (seq, label) in enumerate(zip(sequences, labels)):\n",
    "        reshaped_sequence = seq  # Use the sequence as it is\n",
    "\n",
    "        ax = plt.subplot(1, num_samples, i + 1)\n",
    "        img = ax.imshow(\n",
    "            reshaped_sequence, cmap=\"gray\", vmin=-1.0, vmax=1.0\n",
    "        )  # Adjusted vmin and vmax\n",
    "\n",
    "        operation_title = \"XOR\" if label[2] == 0 else \"XNOR\"\n",
    "        ax.set_title(f\"Operation: {operation_title}, {label}\")\n",
    "\n",
    "        ax.set_xlabel(\"Arrays (A, B, C)\")\n",
    "        ax.set_ylabel(\"Time Points\")\n",
    "        ax.set_xticks(range(3))\n",
    "        ax.set_xticklabels([\"A\", \"B\", \"C\"])\n",
    "        ax.set_yticks(range(reshaped_sequence.shape[0]))\n",
    "        ax.set_yticklabels([f\"{j+1}\" for j in range(reshaped_sequence.shape[0])])\n",
    "\n",
    "    # Adjusted positioning of colorbar\n",
    "    cbar_ax = plt.gcf().add_axes([0.93, 0.15, 0.02, 0.7])\n",
    "    cbar = plt.colorbar(img, cax=cbar_ax)\n",
    "    cbar.set_ticks([-1, 0, 1])\n",
    "    cbar.set_ticklabels([\"-1\", \"0\", \"1\"])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming sequences and labels are already generated using generateTrainData\n",
    "plot_sequences(sequences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:01:02.144838900Z",
     "start_time": "2023-12-06T17:01:02.130460400Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters_list = []\n",
    "\n",
    "min_lengths = [5, 10, 10, 20, 20, 40, 40]\n",
    "max_lengths = [5, 10, 15, 20, 25, 40, 45]\n",
    "\n",
    "for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "    parameters_list.append(\n",
    "        {\n",
    "            \"min_length\": min_len,\n",
    "            \"max_length\": max_len,\n",
    "            \"fill\": 0,\n",
    "            \"value_1\": -1,\n",
    "            \"value_2\": 1,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:44.759873Z",
     "start_time": "2023-12-06T18:19:42.281374100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN, rep: 0, epoch: 1, acc: 0.503180809020996\n",
      "RNN, rep: 0, epoch: 2, acc: 0.5065082842111588\n",
      "RNN, rep: 0, epoch: 3, acc: 0.5084999871253967\n",
      "RNN, rep: 0, epoch: 4, acc: 0.5308729633688927\n",
      "RNN, rep: 0, epoch: 5, acc: 0.588187882900238\n",
      "RNN, rep: 0, epoch: 6, acc: 0.6232257491350174\n",
      "RNN, rep: 0, epoch: 7, acc: 0.6554333120584488\n",
      "RNN, rep: 0, epoch: 8, acc: 0.6952180881798268\n",
      "RNN, rep: 0, epoch: 9, acc: 0.7304376837611198\n",
      "RNN, rep: 0, epoch: 10, acc: 0.747788907289505\n",
      "RNN, rep: 0, epoch: 11, acc: 0.76658497184515\n",
      "RNN, rep: 0, epoch: 12, acc: 0.7718523472547532\n",
      "RNN, rep: 0, epoch: 13, acc: 0.7806057235598565\n",
      "RNN, rep: 0, epoch: 14, acc: 0.784082232862711\n",
      "RNN, rep: 0, epoch: 15, acc: 0.7888004972040653\n",
      "RNN, rep: 0, epoch: 16, acc: 0.7933720628917217\n",
      "RNN, rep: 0, epoch: 17, acc: 0.8009723310172557\n",
      "RNN, rep: 0, epoch: 18, acc: 0.8032749511301518\n",
      "RNN, rep: 0, epoch: 19, acc: 0.8088079704344273\n",
      "RNN, rep: 0, epoch: 20, acc: 0.8105133919417858\n",
      "RNN, rep: 0, epoch: 21, acc: 0.8168782261013985\n",
      "RNN, rep: 0, epoch: 22, acc: 0.8203486460447311\n",
      "RNN, rep: 0, epoch: 23, acc: 0.8276168555021286\n",
      "RNN, rep: 0, epoch: 24, acc: 0.8324326127767563\n",
      "RNN, rep: 0, epoch: 25, acc: 0.8344928394258022\n",
      "RNN, rep: 0, epoch: 26, acc: 0.843521098792553\n",
      "RNN, rep: 0, epoch: 27, acc: 0.83985197275877\n",
      "RNN, rep: 0, epoch: 28, acc: 0.8473588314652443\n",
      "RNN, rep: 0, epoch: 29, acc: 0.847655917853117\n",
      "RNN, rep: 0, epoch: 30, acc: 0.8557376995682716\n",
      "RNN, rep: 0, epoch: 31, acc: 0.8619339445978403\n",
      "RNN, rep: 0, epoch: 32, acc: 0.8662962409853935\n",
      "RNN, rep: 0, epoch: 33, acc: 0.867113112732768\n",
      "RNN, rep: 0, epoch: 34, acc: 0.8752617682516575\n",
      "RNN, rep: 0, epoch: 35, acc: 0.8798989792913199\n",
      "RNN, rep: 0, epoch: 36, acc: 0.8858685922622681\n",
      "RNN, rep: 0, epoch: 37, acc: 0.8899026957154273\n",
      "RNN, rep: 0, epoch: 38, acc: 0.8932274753600359\n",
      "RNN, rep: 0, epoch: 39, acc: 0.8978384897857904\n",
      "RNN, rep: 0, epoch: 40, acc: 0.8974372297525406\n",
      "RNN, rep: 0, epoch: 41, acc: 0.9022883795946837\n",
      "RNN, rep: 0, epoch: 42, acc: 0.9074741882085801\n",
      "RNN, rep: 0, epoch: 43, acc: 0.9108135069906712\n",
      "RNN, rep: 0, epoch: 44, acc: 0.9134836605191231\n",
      "RNN, rep: 0, epoch: 45, acc: 0.9175953605026007\n",
      "RNN, rep: 0, epoch: 46, acc: 0.9198972466960549\n",
      "RNN, rep: 0, epoch: 47, acc: 0.9236766368895769\n",
      "RNN, rep: 0, epoch: 48, acc: 0.9265017116069794\n",
      "RNN, rep: 0, epoch: 49, acc: 0.9274995346739888\n",
      "RNN, rep: 0, epoch: 50, acc: 0.9301469260826707\n",
      "RNN, rep: 0, epoch: 51, acc: 0.9329471341148019\n",
      "RNN, rep: 0, epoch: 52, acc: 0.9338537470251321\n",
      "RNN, rep: 0, epoch: 53, acc: 0.936448057629168\n",
      "RNN, rep: 0, epoch: 54, acc: 0.9394386487454176\n",
      "RNN, rep: 0, epoch: 55, acc: 0.9394988592341542\n",
      "RNN, rep: 0, epoch: 56, acc: 0.9414111694693565\n",
      "RNN, rep: 0, epoch: 57, acc: 0.9453997817635537\n",
      "RNN, rep: 0, epoch: 58, acc: 0.9452228132635355\n",
      "RNN, rep: 0, epoch: 59, acc: 0.9479310734197497\n",
      "RNN, rep: 0, epoch: 60, acc: 0.9504715367406606\n",
      "RNN, rep: 0, epoch: 61, acc: 0.9508604531362653\n",
      "RNN, rep: 0, epoch: 62, acc: 0.9518951144814491\n",
      "RNN, rep: 0, epoch: 63, acc: 0.9550159139558673\n",
      "RNN, rep: 0, epoch: 64, acc: 0.9562743007019162\n",
      "RNN, rep: 0, epoch: 65, acc: 0.9578768450766801\n",
      "RNN, rep: 0, epoch: 66, acc: 0.9593749818578362\n",
      "RNN, rep: 0, epoch: 67, acc: 0.9613646125048398\n",
      "RNN, rep: 0, epoch: 68, acc: 0.962131157014519\n",
      "RNN, rep: 0, epoch: 69, acc: 0.964059018753469\n",
      "RNN, rep: 0, epoch: 70, acc: 0.9655886971391737\n",
      "RNN, rep: 0, epoch: 71, acc: 0.9660819043405354\n",
      "RNN, rep: 0, epoch: 72, acc: 0.9672819131426513\n",
      "RNN, rep: 0, epoch: 73, acc: 0.9680556359887124\n",
      "RNN, rep: 0, epoch: 74, acc: 0.970068261101842\n",
      "RNN                  Rep: 0   Epoch: 74    Acc: 0.9701 Params: min_length: 5, max_length: 5, fill: 0, value_1: -1, value_2: 1 Time: 12.89 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 3])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.5005306303501129\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.5015003344416619\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5009187537431717\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5121772193908691\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.5248491096496583\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.5573169723153114\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.5886885371804237\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.6162151029706001\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.6307820463180542\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.6444100394845009\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.6522547274827957\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.666816184669733\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.6850617635250091\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.6949652384221554\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.7163514028489589\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.7243616093695163\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.7310868082940578\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.7575875125825405\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.7640805676579475\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.7723158998787403\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.7835855265706777\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.7810210276395082\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.8066571616381407\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.8067457453906536\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.8103011630475521\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.8212228524684906\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.8281526611745358\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.8264735201746225\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.8331498125195503\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.84162338770926\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.8452405273169279\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.8472861608862877\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.8547201581299305\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.8512392023950816\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.8515049397945404\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.852039006575942\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.8600630658119917\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.8648767764866352\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.8692666909843684\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.8590426772087812\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.8584938763827086\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.8736286623030901\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.879205762706697\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.8755018423497677\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.8629367489740253\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.8820011411979795\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.8735370364412666\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.8745454140752554\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.8815505591779947\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.8801480563357472\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.8802241231501102\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.8792485897243023\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.8865883561596274\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.8837523253262043\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.8965994000807405\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.8797158898413181\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.8830811563879252\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.8861845936626196\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.897755883820355\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.8937936409562827\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.8785313792526722\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.8899547715857625\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.8811228927969933\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.8928535590693355\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.8879812451824546\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.8791550311818719\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.8789617793634534\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.8916641733050347\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.889583436474204\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.8911688295006752\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.9014180175960064\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.8993845377862454\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.8939765461534261\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.8866589262709021\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.8901062914729119\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.8925675409287215\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.8953400606662035\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.8958399972692132\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.8981823592633009\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.9062211962789297\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.9058122515305876\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.9252729745022953\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.932635897360742\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.9376044418103993\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.9427359609492123\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.9513974411599339\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.9508308029174805\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.9558020974695682\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.9554905351810157\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.9605401182547212\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.9629011448100209\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.9643199260532856\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.9677743900753558\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.9669848418049515\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.9699813743308187\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.9693460140936077\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.9713374871294945\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 97    Acc: 0.9713 Params: min_length: 5, max_length: 5, fill: 0, value_1: -1, value_2: 1 Time: 23.86 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.49803338885307313\n",
      "RNN, rep: 0, epoch: 2, acc: 0.503658972978592\n",
      "RNN, rep: 0, epoch: 3, acc: 0.5036845079064369\n",
      "RNN, rep: 0, epoch: 4, acc: 0.5010328009724617\n",
      "RNN, rep: 0, epoch: 5, acc: 0.513105026781559\n",
      "RNN, rep: 0, epoch: 6, acc: 0.5627352347970009\n",
      "RNN, rep: 0, epoch: 7, acc: 0.6005557355284691\n",
      "RNN, rep: 0, epoch: 8, acc: 0.6176805847883224\n",
      "RNN, rep: 0, epoch: 9, acc: 0.6311809763312339\n",
      "RNN, rep: 0, epoch: 10, acc: 0.6365366458892823\n",
      "RNN, rep: 0, epoch: 11, acc: 0.6445594525337219\n",
      "RNN, rep: 0, epoch: 12, acc: 0.6385790008306503\n",
      "RNN, rep: 0, epoch: 13, acc: 0.6455300796031952\n",
      "RNN, rep: 0, epoch: 14, acc: 0.6453956857323646\n",
      "RNN, rep: 0, epoch: 15, acc: 0.6478460296988487\n",
      "RNN, rep: 0, epoch: 16, acc: 0.6489592677354813\n",
      "RNN, rep: 0, epoch: 17, acc: 0.6467975032329559\n",
      "RNN, rep: 0, epoch: 18, acc: 0.6480350911617279\n",
      "RNN, rep: 0, epoch: 19, acc: 0.6557600092887879\n",
      "RNN, rep: 0, epoch: 20, acc: 0.6526103603839875\n",
      "RNN, rep: 0, epoch: 21, acc: 0.6572761401534081\n",
      "RNN, rep: 0, epoch: 22, acc: 0.6576785585284233\n",
      "RNN, rep: 0, epoch: 23, acc: 0.6458704680204391\n",
      "RNN, rep: 0, epoch: 24, acc: 0.6551227301359177\n",
      "RNN, rep: 0, epoch: 25, acc: 0.6568881154060364\n",
      "RNN, rep: 0, epoch: 26, acc: 0.6553836825489998\n",
      "RNN, rep: 0, epoch: 27, acc: 0.6563791006803512\n",
      "RNN, rep: 0, epoch: 28, acc: 0.6589897421002388\n",
      "RNN, rep: 0, epoch: 29, acc: 0.6581023815274238\n",
      "RNN, rep: 0, epoch: 30, acc: 0.657950239777565\n",
      "RNN, rep: 0, epoch: 31, acc: 0.660251475572586\n",
      "RNN, rep: 0, epoch: 32, acc: 0.6553956604003907\n",
      "RNN, rep: 0, epoch: 33, acc: 0.6619235354661942\n",
      "RNN, rep: 0, epoch: 34, acc: 0.6660488346219062\n",
      "RNN, rep: 0, epoch: 35, acc: 0.659539215862751\n",
      "RNN, rep: 0, epoch: 36, acc: 0.6696982565522194\n",
      "RNN, rep: 0, epoch: 37, acc: 0.6611333963274956\n",
      "RNN, rep: 0, epoch: 38, acc: 0.6692570734024048\n",
      "RNN, rep: 0, epoch: 39, acc: 0.677541012763977\n",
      "RNN, rep: 0, epoch: 40, acc: 0.676654748916626\n",
      "RNN, rep: 0, epoch: 41, acc: 0.6850853675603866\n",
      "RNN, rep: 0, epoch: 42, acc: 0.6873031839728355\n",
      "RNN, rep: 0, epoch: 43, acc: 0.6828071638941765\n",
      "RNN, rep: 0, epoch: 44, acc: 0.7018231356143951\n",
      "RNN, rep: 0, epoch: 45, acc: 0.7049047201871872\n",
      "RNN, rep: 0, epoch: 46, acc: 0.7106913673877716\n",
      "RNN, rep: 0, epoch: 47, acc: 0.7163908021152019\n",
      "RNN, rep: 0, epoch: 48, acc: 0.7191126927733421\n",
      "RNN, rep: 0, epoch: 49, acc: 0.7312204846739769\n",
      "RNN, rep: 0, epoch: 50, acc: 0.7396772621572018\n",
      "RNN, rep: 0, epoch: 51, acc: 0.7448932482302189\n",
      "RNN, rep: 0, epoch: 52, acc: 0.755549633204937\n",
      "RNN, rep: 0, epoch: 53, acc: 0.7640686552226543\n",
      "RNN, rep: 0, epoch: 54, acc: 0.7716410353779792\n",
      "RNN, rep: 0, epoch: 55, acc: 0.7844825419783592\n",
      "RNN, rep: 0, epoch: 56, acc: 0.79559719607234\n",
      "RNN, rep: 0, epoch: 57, acc: 0.7974903154373169\n",
      "RNN, rep: 0, epoch: 58, acc: 0.8120453986525535\n",
      "RNN, rep: 0, epoch: 59, acc: 0.8135002267360687\n",
      "RNN, rep: 0, epoch: 60, acc: 0.818348902463913\n",
      "RNN, rep: 0, epoch: 61, acc: 0.8287929791212082\n",
      "RNN, rep: 0, epoch: 62, acc: 0.8337374958395958\n",
      "RNN, rep: 0, epoch: 63, acc: 0.8353408966213465\n",
      "RNN, rep: 0, epoch: 64, acc: 0.8394559157639742\n",
      "RNN, rep: 0, epoch: 65, acc: 0.8433871202170848\n",
      "RNN, rep: 0, epoch: 66, acc: 0.853820476680994\n",
      "RNN, rep: 0, epoch: 67, acc: 0.8451493126153946\n",
      "RNN, rep: 0, epoch: 68, acc: 0.8579206675291061\n",
      "RNN, rep: 0, epoch: 69, acc: 0.8617716301977635\n",
      "RNN, rep: 0, epoch: 70, acc: 0.8631401837617159\n",
      "RNN, rep: 0, epoch: 71, acc: 0.868554912507534\n",
      "RNN, rep: 0, epoch: 72, acc: 0.8722716276347637\n",
      "RNN, rep: 0, epoch: 73, acc: 0.876032124683261\n",
      "RNN, rep: 0, epoch: 74, acc: 0.8797340204566717\n",
      "RNN, rep: 0, epoch: 75, acc: 0.8819815866649151\n",
      "RNN, rep: 0, epoch: 76, acc: 0.8867506273090839\n",
      "RNN, rep: 0, epoch: 77, acc: 0.8919663763046265\n",
      "RNN, rep: 0, epoch: 78, acc: 0.8953431398421526\n",
      "RNN, rep: 0, epoch: 79, acc: 0.8996670228987932\n",
      "RNN, rep: 0, epoch: 80, acc: 0.9078186525404454\n",
      "RNN, rep: 0, epoch: 81, acc: 0.9083721015974879\n",
      "RNN, rep: 0, epoch: 82, acc: 0.9168717897310853\n",
      "RNN, rep: 0, epoch: 83, acc: 0.9177164103090764\n",
      "RNN, rep: 0, epoch: 84, acc: 0.919970340617001\n",
      "RNN, rep: 0, epoch: 85, acc: 0.9234702380746603\n",
      "RNN, rep: 0, epoch: 86, acc: 0.9269969428703189\n",
      "RNN, rep: 0, epoch: 87, acc: 0.9315726451575757\n",
      "RNN, rep: 0, epoch: 88, acc: 0.9345230686664582\n",
      "RNN, rep: 0, epoch: 89, acc: 0.9389784028753638\n",
      "RNN, rep: 0, epoch: 90, acc: 0.9371980008855462\n",
      "RNN, rep: 0, epoch: 91, acc: 0.9426555711776018\n",
      "RNN, rep: 0, epoch: 92, acc: 0.9452354737371206\n",
      "RNN, rep: 0, epoch: 93, acc: 0.9476092656515539\n",
      "RNN, rep: 0, epoch: 94, acc: 0.9479174147360027\n",
      "RNN, rep: 0, epoch: 95, acc: 0.9516337386891246\n",
      "RNN, rep: 0, epoch: 96, acc: 0.9518750297650694\n",
      "RNN, rep: 0, epoch: 97, acc: 0.9551414843089878\n",
      "RNN, rep: 0, epoch: 98, acc: 0.9570429526828229\n",
      "RNN, rep: 0, epoch: 99, acc: 0.9584572027064859\n",
      "RNN, rep: 0, epoch: 100, acc: 0.9601406086049974\n",
      "RNN, rep: 0, epoch: 101, acc: 0.9620438845828175\n",
      "RNN, rep: 0, epoch: 102, acc: 0.9624267164058984\n",
      "RNN, rep: 0, epoch: 103, acc: 0.9651110792160034\n",
      "RNN, rep: 0, epoch: 104, acc: 0.9656766315922141\n",
      "RNN, rep: 0, epoch: 105, acc: 0.965665314681828\n",
      "RNN, rep: 0, epoch: 106, acc: 0.9682807379215955\n",
      "RNN, rep: 0, epoch: 107, acc: 0.9693027576245368\n",
      "RNN, rep: 0, epoch: 108, acc: 0.9686520679853856\n",
      "RNN, rep: 0, epoch: 109, acc: 0.9716380393318832\n",
      "RNN                  Rep: 0   Epoch: 109   Acc: 0.9716 Params: min_length: 10, max_length: 10, fill: 0, value_1: -1, value_2: 1 Time: 22.52 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.5020456886291504\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.5045629474520683\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5319650027155877\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5839257082343101\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.5977899116277695\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.613918409049511\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.6209039500355721\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.6424469301104545\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.635503543317318\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.6402914649248124\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.6435714545845985\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.6517280101776123\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.6409020651876927\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.6385885670781135\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.6354013672471046\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.5967523114383221\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.6377891927957535\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.6242127807438373\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.617729689925909\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.6056670343875885\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.61824902176857\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.6530425679683686\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.6628241375088691\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.6926104837656021\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.7132513283193112\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.7312666967511177\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.7453576318919659\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.7609212864935399\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.7678912852704525\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.7761643296480178\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.779503530561924\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.7849992276728153\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.7894111220538617\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.7937904866039753\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.7272519175708294\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.7841073347628117\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.7903861877322197\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.7972227960824967\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.7968242467939853\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.7989802019298077\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.8009959708154202\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.8026156997680665\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.8006233654916286\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.8035590256750583\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.8040594917535782\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.803493857383728\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.8062840697169303\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.8038980622589588\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.7946854110062123\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.8106895135343075\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.8121711218357086\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.8086238498985767\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.8171640430390835\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.8169136011600494\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.8190819464623929\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.8238498730957509\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.8280471982061863\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.8183735667914153\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.767904684394598\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.6197212760150432\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.6468971054255962\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.6067776881158352\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.6108908894658088\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.5883882373571396\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.5905989184975624\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.5994396176934242\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.6098062226176262\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.5838920041918755\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.5860778015851974\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.6103463593125343\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.6228493742644787\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.5930192272365093\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.6129447036981582\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.6107907146215439\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.6137874653935432\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.600030103623867\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.6382840146124363\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.6967408718168735\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.7258016522228717\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.7799828133732081\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.8063663494586945\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.8363009134680033\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.8307705304771662\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.8287414145469666\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.8316043236851692\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.8364744468778372\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.8464290841668844\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.8459063387662172\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.8594722281396389\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.8487208399176598\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.8583143216371536\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.8574203516542912\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.8632730840891599\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.8411434894055128\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.874003065302968\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.864327400252223\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.8597189539670944\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.8551532910764217\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.8585897376388312\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.8452365136891603\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.8668698056787253\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.8805120673775673\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.8714954008534551\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.8751991736143827\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.8664811111614108\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.8723855614289642\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.8821429568529129\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.8836451622471213\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.8798933766037226\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.8767997701093555\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.8806600692868233\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.8760530726611614\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.8809453526511788\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.8757208130136133\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.886599464416504\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.8852677193656564\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.8811901269108057\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.880825813934207\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.8869275664165616\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.8821116762980818\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.8888307658582926\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.8917781384661794\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.8847252275794745\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.8873959312215447\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.8808584832400084\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.7787388203665614\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.7579138853400945\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.7556190814822912\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.8739113653451205\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.8817816412448883\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.8831673803552985\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.8795449899882078\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.8799353140965104\n",
      "NetRNNWithAttention, rep: 0, epoch: 134, acc: 0.8832810908555985\n",
      "NetRNNWithAttention, rep: 0, epoch: 135, acc: 0.8757726691663266\n",
      "NetRNNWithAttention, rep: 0, epoch: 136, acc: 0.8996366790309548\n",
      "NetRNNWithAttention, rep: 0, epoch: 137, acc: 0.8757656566798687\n",
      "NetRNNWithAttention, rep: 0, epoch: 138, acc: 0.8907546697929501\n",
      "NetRNNWithAttention, rep: 0, epoch: 139, acc: 0.8970072596892714\n",
      "NetRNNWithAttention, rep: 0, epoch: 140, acc: 0.8995931100845337\n",
      "NetRNNWithAttention, rep: 0, epoch: 141, acc: 0.8883369418233633\n",
      "NetRNNWithAttention, rep: 0, epoch: 142, acc: 0.8894139027968049\n",
      "NetRNNWithAttention, rep: 0, epoch: 143, acc: 0.9054902507923543\n",
      "NetRNNWithAttention, rep: 0, epoch: 144, acc: 0.8955293374694884\n",
      "NetRNNWithAttention, rep: 0, epoch: 145, acc: 0.9000687207840383\n",
      "NetRNNWithAttention, rep: 0, epoch: 146, acc: 0.8938157965429128\n",
      "NetRNNWithAttention, rep: 0, epoch: 147, acc: 0.8897753327339888\n",
      "NetRNNWithAttention, rep: 0, epoch: 148, acc: 0.9001720825955272\n",
      "NetRNNWithAttention, rep: 0, epoch: 149, acc: 0.8983232549205422\n",
      "NetRNNWithAttention, rep: 0, epoch: 150, acc: 0.9136871033720673\n",
      "NetRNNWithAttention, rep: 0, epoch: 151, acc: 0.9147077395021915\n",
      "NetRNNWithAttention, rep: 0, epoch: 152, acc: 0.901359888613224\n",
      "NetRNNWithAttention, rep: 0, epoch: 153, acc: 0.9129791385494173\n",
      "NetRNNWithAttention, rep: 0, epoch: 154, acc: 0.9119583120569587\n",
      "NetRNNWithAttention, rep: 0, epoch: 155, acc: 0.9227150424197316\n",
      "NetRNNWithAttention, rep: 0, epoch: 156, acc: 0.9269289115443826\n",
      "NetRNNWithAttention, rep: 0, epoch: 157, acc: 0.9341683903150261\n",
      "NetRNNWithAttention, rep: 0, epoch: 158, acc: 0.922608207166195\n",
      "NetRNNWithAttention, rep: 0, epoch: 159, acc: 0.933130353204906\n",
      "NetRNNWithAttention, rep: 0, epoch: 160, acc: 0.9299632148630917\n",
      "NetRNNWithAttention, rep: 0, epoch: 161, acc: 0.9241058709099889\n",
      "NetRNNWithAttention, rep: 0, epoch: 162, acc: 0.9308710777014494\n",
      "NetRNNWithAttention, rep: 0, epoch: 163, acc: 0.932748173866421\n",
      "NetRNNWithAttention, rep: 0, epoch: 164, acc: 0.9388030424341559\n",
      "NetRNNWithAttention, rep: 0, epoch: 165, acc: 0.941144043970853\n",
      "NetRNNWithAttention, rep: 0, epoch: 166, acc: 0.9386048043053598\n",
      "NetRNNWithAttention, rep: 0, epoch: 167, acc: 0.9402798296976834\n",
      "NetRNNWithAttention, rep: 0, epoch: 168, acc: 0.9478485836181789\n",
      "NetRNNWithAttention, rep: 0, epoch: 169, acc: 0.9463186202663928\n",
      "NetRNNWithAttention, rep: 0, epoch: 170, acc: 0.9420590614154934\n",
      "NetRNNWithAttention, rep: 0, epoch: 171, acc: 0.9486358281690628\n",
      "NetRNNWithAttention, rep: 0, epoch: 172, acc: 0.9472605053335428\n",
      "NetRNNWithAttention, rep: 0, epoch: 173, acc: 0.9513810907490552\n",
      "NetRNNWithAttention, rep: 0, epoch: 174, acc: 0.9496510464511811\n",
      "NetRNNWithAttention, rep: 0, epoch: 175, acc: 0.9510077177546918\n",
      "NetRNNWithAttention, rep: 0, epoch: 176, acc: 0.9576036238577217\n",
      "NetRNNWithAttention, rep: 0, epoch: 177, acc: 0.9562616134993732\n",
      "NetRNNWithAttention, rep: 0, epoch: 178, acc: 0.9517988023534417\n",
      "NetRNNWithAttention, rep: 0, epoch: 179, acc: 0.9533758178446442\n",
      "NetRNNWithAttention, rep: 0, epoch: 180, acc: 0.9568424871750176\n",
      "NetRNNWithAttention, rep: 0, epoch: 181, acc: 0.9539410317409783\n",
      "NetRNNWithAttention, rep: 0, epoch: 182, acc: 0.9578765870351345\n",
      "NetRNNWithAttention, rep: 0, epoch: 183, acc: 0.9584870264399796\n",
      "NetRNNWithAttention, rep: 0, epoch: 184, acc: 0.9584711678046733\n",
      "NetRNNWithAttention, rep: 0, epoch: 185, acc: 0.9572458214126527\n",
      "NetRNNWithAttention, rep: 0, epoch: 186, acc: 0.9623053917195648\n",
      "NetRNNWithAttention, rep: 0, epoch: 187, acc: 0.9636039431486279\n",
      "NetRNNWithAttention, rep: 0, epoch: 188, acc: 0.9616277159564197\n",
      "NetRNNWithAttention, rep: 0, epoch: 189, acc: 0.9625558462366461\n",
      "NetRNNWithAttention, rep: 0, epoch: 190, acc: 0.9661545177269727\n",
      "NetRNNWithAttention, rep: 0, epoch: 191, acc: 0.9669828047975898\n",
      "NetRNNWithAttention, rep: 0, epoch: 192, acc: 0.9688519765995443\n",
      "NetRNNWithAttention, rep: 0, epoch: 193, acc: 0.9670746732736006\n",
      "NetRNNWithAttention, rep: 0, epoch: 194, acc: 0.9714438375085592\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 194   Acc: 0.9714 Params: min_length: 10, max_length: 10, fill: 0, value_1: -1, value_2: 1 Time: 53.56 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.4995464414358139\n",
      "RNN, rep: 0, epoch: 2, acc: 0.5000291487574577\n",
      "RNN, rep: 0, epoch: 3, acc: 0.5012980633974076\n",
      "RNN, rep: 0, epoch: 4, acc: 0.5037454569339752\n",
      "RNN, rep: 0, epoch: 5, acc: 0.5354476389288902\n",
      "RNN, rep: 0, epoch: 6, acc: 0.5681747171282768\n",
      "RNN, rep: 0, epoch: 7, acc: 0.5869902223348618\n",
      "RNN, rep: 0, epoch: 8, acc: 0.6058231869339943\n",
      "RNN, rep: 0, epoch: 9, acc: 0.6129762864112854\n",
      "RNN, rep: 0, epoch: 10, acc: 0.6264757978916168\n",
      "RNN, rep: 0, epoch: 11, acc: 0.6324890190362931\n",
      "RNN, rep: 0, epoch: 12, acc: 0.6284292483329773\n",
      "RNN, rep: 0, epoch: 13, acc: 0.6315541711449623\n",
      "RNN, rep: 0, epoch: 14, acc: 0.6381105557084084\n",
      "RNN, rep: 0, epoch: 15, acc: 0.6413703688979149\n",
      "RNN, rep: 0, epoch: 16, acc: 0.6349651771783829\n",
      "RNN, rep: 0, epoch: 17, acc: 0.6420767962932586\n",
      "RNN, rep: 0, epoch: 18, acc: 0.6422672310471534\n",
      "RNN, rep: 0, epoch: 19, acc: 0.6451295912265778\n",
      "RNN, rep: 0, epoch: 20, acc: 0.6518616870045661\n",
      "RNN, rep: 0, epoch: 21, acc: 0.6514390975236892\n",
      "RNN, rep: 0, epoch: 22, acc: 0.649008564054966\n",
      "RNN, rep: 0, epoch: 23, acc: 0.6563314345479011\n",
      "RNN, rep: 0, epoch: 24, acc: 0.6570799332857132\n",
      "RNN, rep: 0, epoch: 25, acc: 0.6757618269324303\n",
      "RNN, rep: 0, epoch: 26, acc: 0.6928263688087464\n",
      "RNN, rep: 0, epoch: 27, acc: 0.7148205590248108\n",
      "RNN, rep: 0, epoch: 28, acc: 0.7188125896453857\n",
      "RNN, rep: 0, epoch: 29, acc: 0.7337138330936432\n",
      "RNN, rep: 0, epoch: 30, acc: 0.7521403308212757\n",
      "RNN, rep: 0, epoch: 31, acc: 0.763946475982666\n",
      "RNN, rep: 0, epoch: 32, acc: 0.7774529722332955\n",
      "RNN, rep: 0, epoch: 33, acc: 0.7845389470458031\n",
      "RNN, rep: 0, epoch: 34, acc: 0.7829311619699001\n",
      "RNN, rep: 0, epoch: 35, acc: 0.7830094169080257\n",
      "RNN, rep: 0, epoch: 36, acc: 0.7806437788903713\n",
      "RNN, rep: 0, epoch: 37, acc: 0.7990434500575065\n",
      "RNN, rep: 0, epoch: 38, acc: 0.8094968074560165\n",
      "RNN, rep: 0, epoch: 39, acc: 0.8213990940153599\n",
      "RNN, rep: 0, epoch: 40, acc: 0.8066956682503224\n",
      "RNN, rep: 0, epoch: 41, acc: 0.8186308628320694\n",
      "RNN, rep: 0, epoch: 42, acc: 0.8315143922716379\n",
      "RNN, rep: 0, epoch: 43, acc: 0.8372850469499826\n",
      "RNN, rep: 0, epoch: 44, acc: 0.8372326610982418\n",
      "RNN, rep: 0, epoch: 45, acc: 0.8465359050780534\n",
      "RNN, rep: 0, epoch: 46, acc: 0.8579080621898174\n",
      "RNN, rep: 0, epoch: 47, acc: 0.8650005513429642\n",
      "RNN, rep: 0, epoch: 48, acc: 0.8722337597608566\n",
      "RNN, rep: 0, epoch: 49, acc: 0.880492915213108\n",
      "RNN, rep: 0, epoch: 50, acc: 0.880606426820159\n",
      "RNN, rep: 0, epoch: 51, acc: 0.8779907621443271\n",
      "RNN, rep: 0, epoch: 52, acc: 0.8677136304229498\n",
      "RNN, rep: 0, epoch: 53, acc: 0.8894986651092768\n",
      "RNN, rep: 0, epoch: 54, acc: 0.8982248149067164\n",
      "RNN, rep: 0, epoch: 55, acc: 0.9014735659211874\n",
      "RNN, rep: 0, epoch: 56, acc: 0.9069310434907675\n",
      "RNN, rep: 0, epoch: 57, acc: 0.9119538541138172\n",
      "RNN, rep: 0, epoch: 58, acc: 0.9147737611085176\n",
      "RNN, rep: 0, epoch: 59, acc: 0.9184449590742588\n",
      "RNN, rep: 0, epoch: 60, acc: 0.9199298537522554\n",
      "RNN, rep: 0, epoch: 61, acc: 0.9203221248835325\n",
      "RNN, rep: 0, epoch: 62, acc: 0.9278835327923298\n",
      "RNN, rep: 0, epoch: 63, acc: 0.929867483638227\n",
      "RNN, rep: 0, epoch: 64, acc: 0.9326657784730196\n",
      "RNN, rep: 0, epoch: 65, acc: 0.9344682586938142\n",
      "RNN, rep: 0, epoch: 66, acc: 0.936984968483448\n",
      "RNN, rep: 0, epoch: 67, acc: 0.9399217533692718\n",
      "RNN, rep: 0, epoch: 68, acc: 0.942071581222117\n",
      "RNN, rep: 0, epoch: 69, acc: 0.9439310149475932\n",
      "RNN, rep: 0, epoch: 70, acc: 0.9463761646673083\n",
      "RNN, rep: 0, epoch: 71, acc: 0.947654391489923\n",
      "RNN, rep: 0, epoch: 72, acc: 0.9500193442031741\n",
      "RNN, rep: 0, epoch: 73, acc: 0.9496082218363882\n",
      "RNN, rep: 0, epoch: 74, acc: 0.9359548987820745\n",
      "RNN, rep: 0, epoch: 75, acc: 0.9434057466685772\n",
      "RNN, rep: 0, epoch: 76, acc: 0.9500619284808636\n",
      "RNN, rep: 0, epoch: 77, acc: 0.9529469216242432\n",
      "RNN, rep: 0, epoch: 78, acc: 0.9554831011220812\n",
      "RNN, rep: 0, epoch: 79, acc: 0.957463801652193\n",
      "RNN, rep: 0, epoch: 80, acc: 0.9587262066826224\n",
      "RNN, rep: 0, epoch: 81, acc: 0.9590876511111855\n",
      "RNN, rep: 0, epoch: 82, acc: 0.961179535984993\n",
      "RNN, rep: 0, epoch: 83, acc: 0.9628244045935571\n",
      "RNN, rep: 0, epoch: 84, acc: 0.9635203034803271\n",
      "RNN, rep: 0, epoch: 85, acc: 0.9644867070391774\n",
      "RNN, rep: 0, epoch: 86, acc: 0.9623823995329439\n",
      "RNN, rep: 0, epoch: 87, acc: 0.9353518630564213\n",
      "RNN, rep: 0, epoch: 88, acc: 0.9605731502734125\n",
      "RNN, rep: 0, epoch: 89, acc: 0.9643186894245446\n",
      "RNN, rep: 0, epoch: 90, acc: 0.9658006737753749\n",
      "RNN, rep: 0, epoch: 91, acc: 0.9677322694659233\n",
      "RNN, rep: 0, epoch: 92, acc: 0.968685305006802\n",
      "RNN, rep: 0, epoch: 93, acc: 0.970146118272096\n",
      "RNN                  Rep: 0   Epoch: 93    Acc: 0.9701 Params: min_length: 10, max_length: 15, fill: 0, value_1: -1, value_2: 1 Time: 20.80 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.499655022919178\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.5067768839001655\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5115880706906318\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5824079468846322\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.6203702944517135\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.6363012006878853\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.5676397901773452\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.5043343126773834\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.5164388194680214\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.5422523775696755\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.5835182270407677\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.6130928200483322\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.6222738555073738\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.6347889924049377\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.6554748821258545\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.661254090666771\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.681487253010273\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.7078950910270214\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.727020183056593\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.7391352222859859\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.7488998706638813\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.757594042122364\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.7657425047457218\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.769648758172989\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.775091177970171\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.7760536752641201\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.7810255953669548\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.785057076215744\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.787144563049078\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.7923157423734665\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.7924173970520496\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.700237921923399\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.7700710828602314\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.7940427203476429\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.7962680143117905\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.7976824353635311\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.7993036937713623\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.80237061470747\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.8043223080039025\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.8071938546001911\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.805182589739561\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.8087041668593884\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.810813987404108\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.8137668639421463\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.8137989430129529\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.8140982395410538\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.6511856783926487\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.6613943572342396\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.6347747869789601\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.6606319543719291\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.6465527060627937\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.6526743513345719\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.6541475602984428\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.6583876845240593\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.6556044274568558\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.6542447036504746\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.6531640139222145\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.657011878490448\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.6561925438046455\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.662292275428772\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.649275683760643\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.664504651427269\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.6638949072360992\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.664281932413578\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.6319632512331009\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.6603544965386391\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.6621510797739029\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.6631424039602279\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.6651037079095841\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.6670699340105056\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.6689583715796471\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.6699692171812057\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.6720121589303016\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.6703762444853782\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.6585040873289109\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.6629730820655823\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.6587864220142364\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.6653752431273461\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.6652779212594032\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.6659445226192474\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.6626554173231125\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.6629790306091309\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.663580673635006\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.6641785454750061\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.6698053163290024\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.6682465720176697\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.6671817561984063\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.6714206784963608\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.6676939183473587\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.6409144240617752\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.6460423454642296\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.7141708375513554\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.7496987092494964\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.761220912784338\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.7775402447581291\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.7813090193271637\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.7953664109110832\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.8019203002750873\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.8039434619247914\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.8107944849133492\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.8145909345149994\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.8214916068315506\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.8248120513558388\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.8282099425792694\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.8386019958555698\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.8426528113335371\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.8470035719126463\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.8558978021144867\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.8573607610911131\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.8610254099965096\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.8650220748782158\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.8736647489666939\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.8813281864672899\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.8816339106857777\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.8860000164806843\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.8897557881101966\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.8929046253487468\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.8962034426257014\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.9060378218814731\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.9056561619788408\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.905876842699945\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.911728984452784\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.9115931485593319\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.9171024989336729\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.921176381148398\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.9259213815256954\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.9258382351696491\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.931126190982759\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.9344004740193487\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.9327260044403374\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.9399150069989264\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.9422808750905096\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.9437565180659294\n",
      "NetRNNWithAttention, rep: 0, epoch: 134, acc: 0.946483070962131\n",
      "NetRNNWithAttention, rep: 0, epoch: 135, acc: 0.9465235766209662\n",
      "NetRNNWithAttention, rep: 0, epoch: 136, acc: 0.9499044656194746\n",
      "NetRNNWithAttention, rep: 0, epoch: 137, acc: 0.9528525463491678\n",
      "NetRNNWithAttention, rep: 0, epoch: 138, acc: 0.9542109220661223\n",
      "NetRNNWithAttention, rep: 0, epoch: 139, acc: 0.953772389292717\n",
      "NetRNNWithAttention, rep: 0, epoch: 140, acc: 0.9584405051730573\n",
      "NetRNNWithAttention, rep: 0, epoch: 141, acc: 0.9598465836048127\n",
      "NetRNNWithAttention, rep: 0, epoch: 142, acc: 0.9602537978440523\n",
      "NetRNNWithAttention, rep: 0, epoch: 143, acc: 0.9614412228390574\n",
      "NetRNNWithAttention, rep: 0, epoch: 144, acc: 0.9628475882671773\n",
      "NetRNNWithAttention, rep: 0, epoch: 145, acc: 0.9638362709619105\n",
      "NetRNNWithAttention, rep: 0, epoch: 146, acc: 0.9654596956446767\n",
      "NetRNNWithAttention, rep: 0, epoch: 147, acc: 0.9659122951701283\n",
      "NetRNNWithAttention, rep: 0, epoch: 148, acc: 0.9667834325134754\n",
      "NetRNNWithAttention, rep: 0, epoch: 149, acc: 0.9675764570012688\n",
      "NetRNNWithAttention, rep: 0, epoch: 150, acc: 0.9683364974148572\n",
      "NetRNNWithAttention, rep: 0, epoch: 151, acc: 0.9684976747725159\n",
      "NetRNNWithAttention, rep: 0, epoch: 152, acc: 0.9712512971367687\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 152   Acc: 0.9713 Params: min_length: 10, max_length: 15, fill: 0, value_1: -1, value_2: 1 Time: 46.99 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.4992616790533066\n",
      "RNN, rep: 0, epoch: 2, acc: 0.49973344534635544\n",
      "RNN, rep: 0, epoch: 3, acc: 0.4995386099815369\n",
      "RNN, rep: 0, epoch: 4, acc: 0.4990179431438446\n",
      "RNN, rep: 0, epoch: 5, acc: 0.49890165001153947\n",
      "RNN, rep: 0, epoch: 6, acc: 0.499388764500618\n",
      "RNN, rep: 0, epoch: 7, acc: 0.5010220924019814\n",
      "RNN, rep: 0, epoch: 8, acc: 0.49996903985738755\n",
      "RNN, rep: 0, epoch: 9, acc: 0.49889968901872633\n",
      "RNN, rep: 0, epoch: 10, acc: 0.49937075436115264\n",
      "RNN, rep: 0, epoch: 11, acc: 0.5006179514527321\n",
      "RNN, rep: 0, epoch: 12, acc: 0.500266360938549\n",
      "RNN, rep: 0, epoch: 13, acc: 0.5026684346795082\n",
      "RNN, rep: 0, epoch: 14, acc: 0.5042572739720345\n",
      "RNN, rep: 0, epoch: 15, acc: 0.5059853059053421\n",
      "RNN, rep: 0, epoch: 16, acc: 0.5126039430499076\n",
      "RNN, rep: 0, epoch: 17, acc: 0.5552697092294693\n",
      "RNN, rep: 0, epoch: 18, acc: 0.5805184331536293\n",
      "RNN, rep: 0, epoch: 19, acc: 0.5960052585601807\n",
      "RNN, rep: 0, epoch: 20, acc: 0.6163703313469887\n",
      "RNN, rep: 0, epoch: 21, acc: 0.6274761939048767\n",
      "RNN, rep: 0, epoch: 22, acc: 0.6295766204595565\n",
      "RNN, rep: 0, epoch: 23, acc: 0.6375216433405876\n",
      "RNN, rep: 0, epoch: 24, acc: 0.6426149469614029\n",
      "RNN, rep: 0, epoch: 25, acc: 0.6426264080405235\n",
      "RNN, rep: 0, epoch: 26, acc: 0.6400989198684692\n",
      "RNN, rep: 0, epoch: 27, acc: 0.6412196445465088\n",
      "RNN, rep: 0, epoch: 28, acc: 0.6506300202012062\n",
      "RNN, rep: 0, epoch: 29, acc: 0.6547688528895378\n",
      "RNN, rep: 0, epoch: 30, acc: 0.6515829205513001\n",
      "RNN, rep: 0, epoch: 31, acc: 0.6488505992293357\n",
      "RNN, rep: 0, epoch: 32, acc: 0.653298311829567\n",
      "RNN, rep: 0, epoch: 33, acc: 0.655109530389309\n",
      "RNN, rep: 0, epoch: 34, acc: 0.6445563960075379\n",
      "RNN, rep: 0, epoch: 35, acc: 0.6534493100643158\n",
      "RNN, rep: 0, epoch: 36, acc: 0.6553996160626412\n",
      "RNN, rep: 0, epoch: 37, acc: 0.654801858663559\n",
      "RNN, rep: 0, epoch: 38, acc: 0.6436691224575043\n",
      "RNN, rep: 0, epoch: 39, acc: 0.6543818634748458\n",
      "RNN, rep: 0, epoch: 40, acc: 0.6449913203716278\n",
      "RNN, rep: 0, epoch: 41, acc: 0.6533546188473701\n",
      "RNN, rep: 0, epoch: 42, acc: 0.6551985636353492\n",
      "RNN, rep: 0, epoch: 43, acc: 0.6555459013581276\n",
      "RNN, rep: 0, epoch: 44, acc: 0.6558279797434807\n",
      "RNN, rep: 0, epoch: 45, acc: 0.6552810433506966\n",
      "RNN, rep: 0, epoch: 46, acc: 0.6577856341004371\n",
      "RNN, rep: 0, epoch: 47, acc: 0.6598315152525902\n",
      "RNN, rep: 0, epoch: 48, acc: 0.6602041515707969\n",
      "RNN, rep: 0, epoch: 49, acc: 0.6607732173800468\n",
      "RNN, rep: 0, epoch: 50, acc: 0.6639153823256493\n",
      "RNN, rep: 0, epoch: 51, acc: 0.6557489785552025\n",
      "RNN, rep: 0, epoch: 52, acc: 0.6615079659223556\n",
      "RNN, rep: 0, epoch: 53, acc: 0.6655798065662384\n",
      "RNN, rep: 0, epoch: 54, acc: 0.662440941631794\n",
      "RNN, rep: 0, epoch: 55, acc: 0.6508787921071053\n",
      "RNN, rep: 0, epoch: 56, acc: 0.6626313343644142\n",
      "RNN, rep: 0, epoch: 57, acc: 0.6500557011365891\n",
      "RNN, rep: 0, epoch: 58, acc: 0.6565210053324699\n",
      "RNN, rep: 0, epoch: 59, acc: 0.6590817868709564\n",
      "RNN, rep: 0, epoch: 60, acc: 0.6615783450007439\n",
      "RNN, rep: 0, epoch: 61, acc: 0.6594136860966683\n",
      "RNN, rep: 0, epoch: 62, acc: 0.6643458810448647\n",
      "RNN, rep: 0, epoch: 63, acc: 0.6649742510914802\n",
      "RNN, rep: 0, epoch: 64, acc: 0.6639127606153488\n",
      "RNN, rep: 0, epoch: 65, acc: 0.6610335549712181\n",
      "RNN, rep: 0, epoch: 66, acc: 0.6602907827496529\n",
      "RNN, rep: 0, epoch: 67, acc: 0.6658150684833527\n",
      "RNN, rep: 0, epoch: 68, acc: 0.6640672338008881\n",
      "RNN, rep: 0, epoch: 69, acc: 0.6612008261680603\n",
      "RNN, rep: 0, epoch: 70, acc: 0.6665009579062462\n",
      "RNN, rep: 0, epoch: 71, acc: 0.6612110963463783\n",
      "RNN, rep: 0, epoch: 72, acc: 0.6597410526871681\n",
      "RNN, rep: 0, epoch: 73, acc: 0.6618831980228425\n",
      "RNN, rep: 0, epoch: 74, acc: 0.6657546752691269\n",
      "RNN, rep: 0, epoch: 75, acc: 0.6705055677890778\n",
      "RNN, rep: 0, epoch: 76, acc: 0.6652761629223823\n",
      "RNN, rep: 0, epoch: 77, acc: 0.6648578646779061\n",
      "RNN, rep: 0, epoch: 78, acc: 0.6632182425260544\n",
      "RNN, rep: 0, epoch: 79, acc: 0.6688176333904267\n",
      "RNN, rep: 0, epoch: 80, acc: 0.667052728831768\n",
      "RNN, rep: 0, epoch: 81, acc: 0.6649533599615097\n",
      "RNN, rep: 0, epoch: 82, acc: 0.6695112156867981\n",
      "RNN, rep: 0, epoch: 83, acc: 0.6669321998953819\n",
      "RNN, rep: 0, epoch: 84, acc: 0.6675854107737541\n",
      "RNN, rep: 0, epoch: 85, acc: 0.6666229382157326\n",
      "RNN, rep: 0, epoch: 86, acc: 0.6724197801947593\n",
      "RNN, rep: 0, epoch: 87, acc: 0.6682218715548516\n",
      "RNN, rep: 0, epoch: 88, acc: 0.6678639793395996\n",
      "RNN, rep: 0, epoch: 89, acc: 0.6252456140518189\n",
      "RNN, rep: 0, epoch: 90, acc: 0.6740863814949989\n",
      "RNN, rep: 0, epoch: 91, acc: 0.6654838901758194\n",
      "RNN, rep: 0, epoch: 92, acc: 0.669346579015255\n",
      "RNN, rep: 0, epoch: 93, acc: 0.6711407122015953\n",
      "RNN, rep: 0, epoch: 94, acc: 0.6693084093928338\n",
      "RNN, rep: 0, epoch: 95, acc: 0.67196776419878\n",
      "RNN, rep: 0, epoch: 96, acc: 0.6732387408614159\n",
      "RNN, rep: 0, epoch: 97, acc: 0.6753538727760315\n",
      "RNN, rep: 0, epoch: 98, acc: 0.6855386793613434\n",
      "RNN, rep: 0, epoch: 99, acc: 0.6803919059038163\n",
      "RNN, rep: 0, epoch: 100, acc: 0.6261513583362103\n",
      "RNN, rep: 0, epoch: 101, acc: 0.7027799223363399\n",
      "RNN, rep: 0, epoch: 102, acc: 0.7257166573405266\n",
      "RNN, rep: 0, epoch: 103, acc: 0.7457370646297932\n",
      "RNN, rep: 0, epoch: 104, acc: 0.7414367173612118\n",
      "RNN, rep: 0, epoch: 105, acc: 0.7635527744889259\n",
      "RNN, rep: 0, epoch: 106, acc: 0.7696885229647159\n",
      "RNN, rep: 0, epoch: 107, acc: 0.778790024369955\n",
      "RNN, rep: 0, epoch: 108, acc: 0.7880938442051411\n",
      "RNN, rep: 0, epoch: 109, acc: 0.8017998979985714\n",
      "RNN, rep: 0, epoch: 110, acc: 0.776277638822794\n",
      "RNN, rep: 0, epoch: 111, acc: 0.8078485198318959\n",
      "RNN, rep: 0, epoch: 112, acc: 0.817584827542305\n",
      "RNN, rep: 0, epoch: 113, acc: 0.8033725051581859\n",
      "RNN, rep: 0, epoch: 114, acc: 0.8075074936449528\n",
      "RNN, rep: 0, epoch: 115, acc: 0.8212964764982462\n",
      "RNN, rep: 0, epoch: 116, acc: 0.8182830765843392\n",
      "RNN, rep: 0, epoch: 117, acc: 0.8366669371724129\n",
      "RNN, rep: 0, epoch: 118, acc: 0.8439795423299075\n",
      "RNN, rep: 0, epoch: 119, acc: 0.8508597691357136\n",
      "RNN, rep: 0, epoch: 120, acc: 0.8435574787855148\n",
      "RNN, rep: 0, epoch: 121, acc: 0.8601076410710812\n",
      "RNN, rep: 0, epoch: 122, acc: 0.8669557488709688\n",
      "RNN, rep: 0, epoch: 123, acc: 0.8703761721402407\n",
      "RNN, rep: 0, epoch: 124, acc: 0.8732325138896704\n",
      "RNN, rep: 0, epoch: 125, acc: 0.8702813173085451\n",
      "RNN, rep: 0, epoch: 126, acc: 0.8830616296082735\n",
      "RNN, rep: 0, epoch: 127, acc: 0.8883449027687311\n",
      "RNN, rep: 0, epoch: 128, acc: 0.8877804783731699\n",
      "RNN, rep: 0, epoch: 129, acc: 0.8948288485407829\n",
      "RNN, rep: 0, epoch: 130, acc: 0.8978921138495207\n",
      "RNN, rep: 0, epoch: 131, acc: 0.9029719618707895\n",
      "RNN, rep: 0, epoch: 132, acc: 0.9055236785113812\n",
      "RNN, rep: 0, epoch: 133, acc: 0.9091365329921246\n",
      "RNN, rep: 0, epoch: 134, acc: 0.9149026395380497\n",
      "RNN, rep: 0, epoch: 135, acc: 0.9138501009345055\n",
      "RNN, rep: 0, epoch: 136, acc: 0.9113693857938051\n",
      "RNN, rep: 0, epoch: 137, acc: 0.9200218115374446\n",
      "RNN, rep: 0, epoch: 138, acc: 0.9255124302953481\n",
      "RNN, rep: 0, epoch: 139, acc: 0.930091461315751\n",
      "RNN, rep: 0, epoch: 140, acc: 0.9331765708327293\n",
      "RNN, rep: 0, epoch: 141, acc: 0.9302954144403338\n",
      "RNN, rep: 0, epoch: 142, acc: 0.9338045451045036\n",
      "RNN, rep: 0, epoch: 143, acc: 0.9403418534994126\n",
      "RNN, rep: 0, epoch: 144, acc: 0.9428309364616871\n",
      "RNN, rep: 0, epoch: 145, acc: 0.9458307560533286\n",
      "RNN, rep: 0, epoch: 146, acc: 0.9480272301658988\n",
      "RNN, rep: 0, epoch: 147, acc: 0.9468573373183609\n",
      "RNN, rep: 0, epoch: 148, acc: 0.9528533677011728\n",
      "RNN, rep: 0, epoch: 149, acc: 0.9530535422265529\n",
      "RNN, rep: 0, epoch: 150, acc: 0.9391847633756697\n",
      "RNN, rep: 0, epoch: 151, acc: 0.9509729512222111\n",
      "RNN, rep: 0, epoch: 152, acc: 0.9538747055269777\n",
      "RNN, rep: 0, epoch: 153, acc: 0.9552606490813196\n",
      "RNN, rep: 0, epoch: 154, acc: 0.947692439518869\n",
      "RNN, rep: 0, epoch: 155, acc: 0.9536048847809434\n",
      "RNN, rep: 0, epoch: 156, acc: 0.9573117908276617\n",
      "RNN, rep: 0, epoch: 157, acc: 0.9615014225803316\n",
      "RNN, rep: 0, epoch: 158, acc: 0.9646329863555729\n",
      "RNN, rep: 0, epoch: 159, acc: 0.9626902789063752\n",
      "RNN, rep: 0, epoch: 160, acc: 0.9632107241265476\n",
      "RNN, rep: 0, epoch: 161, acc: 0.9643779731355607\n",
      "RNN, rep: 0, epoch: 162, acc: 0.9672871970012784\n",
      "RNN, rep: 0, epoch: 163, acc: 0.9644446069560945\n",
      "RNN, rep: 0, epoch: 164, acc: 0.9691170127317309\n",
      "RNN, rep: 0, epoch: 165, acc: 0.9725012089312076\n",
      "RNN                  Rep: 0   Epoch: 165   Acc: 0.9725 Params: min_length: 20, max_length: 20, fill: 0, value_1: -1, value_2: 1 Time: 45.35 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.5011499857902527\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.498548099398613\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5009145537018775\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5055115923285485\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.5036117121577263\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.5171655207872391\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.503159818649292\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.5060735595226288\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.536222227215767\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.5907738772034645\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.6148438715934753\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.629592696428299\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.6346245735883713\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.6384447541832924\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.6380489325523376\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.6420388698577881\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.6436012476682663\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.6496968194842339\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.6532531177997589\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.6477755174040795\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.6514661908149719\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.6566737368702888\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.6523567226529121\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.6520570796728135\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.6551701471209526\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.6534143468737602\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.6533856251835823\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.6555683001875877\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.655028080046177\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.6558051693439484\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.6549208721518517\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.6567724084854126\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.6565312668681145\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.656631374657154\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.6578393632173538\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.6615025520324707\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.6587799036502838\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.6581908041238784\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.6604486486315727\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.6614287981390953\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.6620081421732903\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.6632019555568696\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.6622983762621879\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.6606496676802636\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.6600463879108429\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.6602775821089745\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.662038478255272\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.6590702012181282\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.6626976037025452\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.6598044148087502\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.6609586235880852\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.6617978990077973\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.6625164747238159\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.6147809323668479\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.49766550213098526\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.5146267154812812\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.5199662396311759\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.5228317895531654\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.5354082328081131\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.5415814501047135\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.5390680262446403\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.562120672762394\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.5638961279392243\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.5846966281533241\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.5822434014081955\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.5949498006701469\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.6180640514194965\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.6052145387232304\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.6296908433735371\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.6307248245179653\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.6540873935818672\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.6528057888150215\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.6412001812458038\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.6493982964754105\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.6624370668828488\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.6617538620531559\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.6736363123357296\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.6668276190757751\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.6577837659418583\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.6633873291313648\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.6650658921897411\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.6828760369122029\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.6686604943871498\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.6913994142413139\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.7102355419099331\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.7212579450011254\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.7378559087216854\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.7373900049924851\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.7440293200314045\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.7596906150877476\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.7683694592118263\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.7768205949664115\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.7750489398837089\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.7781217700242996\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.7879004554450512\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.7825676786899567\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.7917450205981731\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.7892087283730507\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.7945593529939652\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.7965171855688095\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.8023728904128075\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.7991879282891751\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.8048094433546066\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.8056673654913902\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.8130103123188018\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.8185634487867355\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.8149144016206264\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.8240921570360661\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.8233199106156825\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.8268399687856436\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.8289625567942858\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.8356784076988697\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.842012119665742\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.849634516313672\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.8593566198647022\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.8587414205074311\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.8316567140817642\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.8622214359045028\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.8653255317360162\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.8712156196683645\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.8725056809186935\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.8764692908525467\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.8849487564712762\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.8812318504601717\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.8914459280669689\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.8932326552644372\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.8847308004274964\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.8925594558939338\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.8887717342749238\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.8991176095604897\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.8947548432275653\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.9018913777172566\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.8955400827154517\n",
      "NetRNNWithAttention, rep: 0, epoch: 134, acc: 0.9022275739535689\n",
      "NetRNNWithAttention, rep: 0, epoch: 135, acc: 0.9076997344568372\n",
      "NetRNNWithAttention, rep: 0, epoch: 136, acc: 0.8752210927382111\n",
      "NetRNNWithAttention, rep: 0, epoch: 137, acc: 0.8976804043352604\n",
      "NetRNNWithAttention, rep: 0, epoch: 138, acc: 0.897853192165494\n",
      "NetRNNWithAttention, rep: 0, epoch: 139, acc: 0.9080716429278255\n",
      "NetRNNWithAttention, rep: 0, epoch: 140, acc: 0.898210342079401\n",
      "NetRNNWithAttention, rep: 0, epoch: 141, acc: 0.901266359500587\n",
      "NetRNNWithAttention, rep: 0, epoch: 142, acc: 0.8982560382038355\n",
      "NetRNNWithAttention, rep: 0, epoch: 143, acc: 0.9024081022851169\n",
      "NetRNNWithAttention, rep: 0, epoch: 144, acc: 0.9101515629701317\n",
      "NetRNNWithAttention, rep: 0, epoch: 145, acc: 0.9023961909301579\n",
      "NetRNNWithAttention, rep: 0, epoch: 146, acc: 0.9107499750517308\n",
      "NetRNNWithAttention, rep: 0, epoch: 147, acc: 0.9087428891658783\n",
      "NetRNNWithAttention, rep: 0, epoch: 148, acc: 0.913880762886256\n",
      "NetRNNWithAttention, rep: 0, epoch: 149, acc: 0.8950412488728762\n",
      "NetRNNWithAttention, rep: 0, epoch: 150, acc: 0.9103988278657198\n",
      "NetRNNWithAttention, rep: 0, epoch: 151, acc: 0.9142613999545575\n",
      "NetRNNWithAttention, rep: 0, epoch: 152, acc: 0.92758739490062\n",
      "NetRNNWithAttention, rep: 0, epoch: 153, acc: 0.9269823809154332\n",
      "NetRNNWithAttention, rep: 0, epoch: 154, acc: 0.9264275883324444\n",
      "NetRNNWithAttention, rep: 0, epoch: 155, acc: 0.9349026892893016\n",
      "NetRNNWithAttention, rep: 0, epoch: 156, acc: 0.9345799858681858\n",
      "NetRNNWithAttention, rep: 0, epoch: 157, acc: 0.9355121334642171\n",
      "NetRNNWithAttention, rep: 0, epoch: 158, acc: 0.9336408245936036\n",
      "NetRNNWithAttention, rep: 0, epoch: 159, acc: 0.9401567253656685\n",
      "NetRNNWithAttention, rep: 0, epoch: 160, acc: 0.938197818081826\n",
      "NetRNNWithAttention, rep: 0, epoch: 161, acc: 0.9378110671043396\n",
      "NetRNNWithAttention, rep: 0, epoch: 162, acc: 0.9420351419597864\n",
      "NetRNNWithAttention, rep: 0, epoch: 163, acc: 0.9490650503709912\n",
      "NetRNNWithAttention, rep: 0, epoch: 164, acc: 0.9455275929532945\n",
      "NetRNNWithAttention, rep: 0, epoch: 165, acc: 0.9504197607934475\n",
      "NetRNNWithAttention, rep: 0, epoch: 166, acc: 0.9484655402693898\n",
      "NetRNNWithAttention, rep: 0, epoch: 167, acc: 0.9526121727749706\n",
      "NetRNNWithAttention, rep: 0, epoch: 168, acc: 0.9501822618767619\n",
      "NetRNNWithAttention, rep: 0, epoch: 169, acc: 0.9474119203723967\n",
      "NetRNNWithAttention, rep: 0, epoch: 170, acc: 0.9490967273339629\n",
      "NetRNNWithAttention, rep: 0, epoch: 171, acc: 0.9508032754808664\n",
      "NetRNNWithAttention, rep: 0, epoch: 172, acc: 0.9565543155744671\n",
      "NetRNNWithAttention, rep: 0, epoch: 173, acc: 0.9538365584146231\n",
      "NetRNNWithAttention, rep: 0, epoch: 174, acc: 0.9486260568257421\n",
      "NetRNNWithAttention, rep: 0, epoch: 175, acc: 0.9527883352991193\n",
      "NetRNNWithAttention, rep: 0, epoch: 176, acc: 0.9530142838601023\n",
      "NetRNNWithAttention, rep: 0, epoch: 177, acc: 0.960444074086845\n",
      "NetRNNWithAttention, rep: 0, epoch: 178, acc: 0.9604210244305432\n",
      "NetRNNWithAttention, rep: 0, epoch: 179, acc: 0.9600486131571233\n",
      "NetRNNWithAttention, rep: 0, epoch: 180, acc: 0.9614079032279551\n",
      "NetRNNWithAttention, rep: 0, epoch: 181, acc: 0.9609056326374411\n",
      "NetRNNWithAttention, rep: 0, epoch: 182, acc: 0.963498508175835\n",
      "NetRNNWithAttention, rep: 0, epoch: 183, acc: 0.9585561468638479\n",
      "NetRNNWithAttention, rep: 0, epoch: 184, acc: 0.9673218070436269\n",
      "NetRNNWithAttention, rep: 0, epoch: 185, acc: 0.9655136139411479\n",
      "NetRNNWithAttention, rep: 0, epoch: 186, acc: 0.9662900682259351\n",
      "NetRNNWithAttention, rep: 0, epoch: 187, acc: 0.9662176837725565\n",
      "NetRNNWithAttention, rep: 0, epoch: 188, acc: 0.9696021356666461\n",
      "NetRNNWithAttention, rep: 0, epoch: 189, acc: 0.9678098428435624\n",
      "NetRNNWithAttention, rep: 0, epoch: 190, acc: 0.9662405597791076\n",
      "NetRNNWithAttention, rep: 0, epoch: 191, acc: 0.971115996572189\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 191   Acc: 0.9711 Params: min_length: 20, max_length: 20, fill: 0, value_1: -1, value_2: 1 Time: 66.04 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.5002744713425636\n",
      "RNN, rep: 0, epoch: 2, acc: 0.5022939124703407\n",
      "RNN, rep: 0, epoch: 3, acc: 0.49870285838842393\n",
      "RNN, rep: 0, epoch: 4, acc: 0.4976062050461769\n",
      "RNN, rep: 0, epoch: 5, acc: 0.5005916371941567\n",
      "RNN, rep: 0, epoch: 6, acc: 0.5027583968639374\n",
      "RNN, rep: 0, epoch: 7, acc: 0.500046453177929\n",
      "RNN, rep: 0, epoch: 8, acc: 0.4994653743505478\n",
      "RNN, rep: 0, epoch: 9, acc: 0.49993479639291766\n",
      "RNN, rep: 0, epoch: 10, acc: 0.5015658289194107\n",
      "RNN, rep: 0, epoch: 11, acc: 0.500773330628872\n",
      "RNN, rep: 0, epoch: 12, acc: 0.5004284280538559\n",
      "RNN, rep: 0, epoch: 13, acc: 0.4990772596001625\n",
      "RNN, rep: 0, epoch: 14, acc: 0.49955330431461337\n",
      "RNN, rep: 0, epoch: 15, acc: 0.5008637368679046\n",
      "RNN, rep: 0, epoch: 16, acc: 0.49923527866601947\n",
      "RNN, rep: 0, epoch: 17, acc: 0.5006448763608933\n",
      "RNN, rep: 0, epoch: 18, acc: 0.501349735558033\n",
      "RNN, rep: 0, epoch: 19, acc: 0.4999836447834969\n",
      "RNN, rep: 0, epoch: 20, acc: 0.5008867502212524\n",
      "RNN, rep: 0, epoch: 21, acc: 0.5034712183475495\n",
      "RNN, rep: 0, epoch: 22, acc: 0.5034747821092606\n",
      "RNN, rep: 0, epoch: 23, acc: 0.5159771677851677\n",
      "RNN, rep: 0, epoch: 24, acc: 0.5436111503839492\n",
      "RNN, rep: 0, epoch: 25, acc: 0.5781939285993576\n",
      "RNN, rep: 0, epoch: 26, acc: 0.6088845270872116\n",
      "RNN, rep: 0, epoch: 27, acc: 0.6141738441586494\n",
      "RNN, rep: 0, epoch: 28, acc: 0.6221492686867713\n",
      "RNN, rep: 0, epoch: 29, acc: 0.6308737942576408\n",
      "RNN, rep: 0, epoch: 30, acc: 0.6335791686177253\n",
      "RNN, rep: 0, epoch: 31, acc: 0.6410508280992508\n",
      "RNN, rep: 0, epoch: 32, acc: 0.6494834378361702\n",
      "RNN, rep: 0, epoch: 33, acc: 0.6447384956479073\n",
      "RNN, rep: 0, epoch: 34, acc: 0.6410216745734215\n",
      "RNN, rep: 0, epoch: 35, acc: 0.6511989519000053\n",
      "RNN, rep: 0, epoch: 36, acc: 0.6542169705033303\n",
      "RNN, rep: 0, epoch: 37, acc: 0.6712395170331001\n",
      "RNN, rep: 0, epoch: 38, acc: 0.6874468043446541\n",
      "RNN, rep: 0, epoch: 39, acc: 0.7057132914662361\n",
      "RNN, rep: 0, epoch: 40, acc: 0.7109764698147774\n",
      "RNN, rep: 0, epoch: 41, acc: 0.7235364833474159\n",
      "RNN, rep: 0, epoch: 42, acc: 0.7378151711821556\n",
      "RNN, rep: 0, epoch: 43, acc: 0.740107992887497\n",
      "RNN, rep: 0, epoch: 44, acc: 0.7426384332776069\n",
      "RNN, rep: 0, epoch: 45, acc: 0.7596889553964138\n",
      "RNN, rep: 0, epoch: 46, acc: 0.7649387614428997\n",
      "RNN, rep: 0, epoch: 47, acc: 0.7743821120262147\n",
      "RNN, rep: 0, epoch: 48, acc: 0.7754728202521801\n",
      "RNN, rep: 0, epoch: 49, acc: 0.7810839802026749\n",
      "RNN, rep: 0, epoch: 50, acc: 0.7927182693779469\n",
      "RNN, rep: 0, epoch: 51, acc: 0.7870581659674645\n",
      "RNN, rep: 0, epoch: 52, acc: 0.7982008761167526\n",
      "RNN, rep: 0, epoch: 53, acc: 0.7949839769303799\n",
      "RNN, rep: 0, epoch: 54, acc: 0.7886514472961426\n",
      "RNN, rep: 0, epoch: 55, acc: 0.8032850590348244\n",
      "RNN, rep: 0, epoch: 56, acc: 0.8003555808961391\n",
      "RNN, rep: 0, epoch: 57, acc: 0.8099330985546112\n",
      "RNN, rep: 0, epoch: 58, acc: 0.8111130644381046\n",
      "RNN, rep: 0, epoch: 59, acc: 0.8184620693325997\n",
      "RNN, rep: 0, epoch: 60, acc: 0.8172992147505284\n",
      "RNN, rep: 0, epoch: 61, acc: 0.8283311082422733\n",
      "RNN, rep: 0, epoch: 62, acc: 0.8349703246355057\n",
      "RNN, rep: 0, epoch: 63, acc: 0.80089688539505\n",
      "RNN, rep: 0, epoch: 64, acc: 0.822661893516779\n",
      "RNN, rep: 0, epoch: 65, acc: 0.8396084805577994\n",
      "RNN, rep: 0, epoch: 66, acc: 0.8470277885347605\n",
      "RNN, rep: 0, epoch: 67, acc: 0.8453549952805042\n",
      "RNN, rep: 0, epoch: 68, acc: 0.8530126169323922\n",
      "RNN, rep: 0, epoch: 69, acc: 0.8614519181847572\n",
      "RNN, rep: 0, epoch: 70, acc: 0.8683685456961393\n",
      "RNN, rep: 0, epoch: 71, acc: 0.8778997153788805\n",
      "RNN, rep: 0, epoch: 72, acc: 0.8825848292559385\n",
      "RNN, rep: 0, epoch: 73, acc: 0.8801965612918139\n",
      "RNN, rep: 0, epoch: 74, acc: 0.8902291125059127\n",
      "RNN, rep: 0, epoch: 75, acc: 0.8952623285353184\n",
      "RNN, rep: 0, epoch: 76, acc: 0.8974750454723835\n",
      "RNN, rep: 0, epoch: 77, acc: 0.9025802608579397\n",
      "RNN, rep: 0, epoch: 78, acc: 0.9013409095257521\n",
      "RNN, rep: 0, epoch: 79, acc: 0.9031034216284752\n",
      "RNN, rep: 0, epoch: 80, acc: 0.9100925217941404\n",
      "RNN, rep: 0, epoch: 81, acc: 0.9179099909961224\n",
      "RNN, rep: 0, epoch: 82, acc: 0.9123072269931436\n",
      "RNN, rep: 0, epoch: 83, acc: 0.9196487686410546\n",
      "RNN, rep: 0, epoch: 84, acc: 0.8676758679002523\n",
      "RNN, rep: 0, epoch: 85, acc: 0.9239420571550727\n",
      "RNN, rep: 0, epoch: 86, acc: 0.9257972131669522\n",
      "RNN, rep: 0, epoch: 87, acc: 0.9337691223621368\n",
      "RNN, rep: 0, epoch: 88, acc: 0.9112363553792239\n",
      "RNN, rep: 0, epoch: 89, acc: 0.7246800884231925\n",
      "RNN, rep: 0, epoch: 90, acc: 0.8164041812345385\n",
      "RNN, rep: 0, epoch: 91, acc: 0.9158350441977382\n",
      "RNN, rep: 0, epoch: 92, acc: 0.9246622014790773\n",
      "RNN, rep: 0, epoch: 93, acc: 0.9282815395668149\n",
      "RNN, rep: 0, epoch: 94, acc: 0.9347283488139511\n",
      "RNN, rep: 0, epoch: 95, acc: 0.9367151051014662\n",
      "RNN, rep: 0, epoch: 96, acc: 0.9401436555385589\n",
      "RNN, rep: 0, epoch: 97, acc: 0.9420575971528887\n",
      "RNN, rep: 0, epoch: 98, acc: 0.9448694777488709\n",
      "RNN, rep: 0, epoch: 99, acc: 0.9459039196372032\n",
      "RNN, rep: 0, epoch: 100, acc: 0.9509360364452004\n",
      "RNN, rep: 0, epoch: 101, acc: 0.9510221203789115\n",
      "RNN, rep: 0, epoch: 102, acc: 0.9499929486960172\n",
      "RNN, rep: 0, epoch: 103, acc: 0.9524393508210778\n",
      "RNN, rep: 0, epoch: 104, acc: 0.9564248063787818\n",
      "RNN, rep: 0, epoch: 105, acc: 0.9273455661162734\n",
      "RNN, rep: 0, epoch: 106, acc: 0.9567944866977632\n",
      "RNN, rep: 0, epoch: 107, acc: 0.9576388422586024\n",
      "RNN, rep: 0, epoch: 108, acc: 0.9588138088583946\n",
      "RNN, rep: 0, epoch: 109, acc: 0.9597953338548542\n",
      "RNN, rep: 0, epoch: 110, acc: 0.9610318657569588\n",
      "RNN, rep: 0, epoch: 111, acc: 0.9644477178342641\n",
      "RNN, rep: 0, epoch: 112, acc: 0.9630782960169018\n",
      "RNN, rep: 0, epoch: 113, acc: 0.9661192934028804\n",
      "RNN, rep: 0, epoch: 114, acc: 0.9670012119784951\n",
      "RNN, rep: 0, epoch: 115, acc: 0.966676107365638\n",
      "RNN, rep: 0, epoch: 116, acc: 0.9705089267157018\n",
      "RNN                  Rep: 0   Epoch: 116   Acc: 0.9705 Params: min_length: 20, max_length: 25, fill: 0, value_1: -1, value_2: 1 Time: 33.43 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.4990851223468781\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.49867344230413435\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5028025734424592\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5013648340106011\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.5009078419208527\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.5029377830028534\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.5080391117930412\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.5348141235113144\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.5994339936971664\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.6288682374358178\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.6783167234063149\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.7160033077001572\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.7373887540400028\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.7544441515207291\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.7642152954638004\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.7683399085700512\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.7784093365073204\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.7818867550790309\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.7858299109339714\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.7902402894198894\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.7950014606118202\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.7955566608905792\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.7990261982381344\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.8028959590196609\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.8036152918636799\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.809032516181469\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.8119058512151242\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.8155471527576447\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.817196619361639\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.8184890995919705\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.8296027828752994\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.8333536092936993\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.8384639070928097\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.8438460354506969\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.847061862796545\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.8548113565891982\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.8551642280817032\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.8597283385694027\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.8675924079865217\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.8625949159264564\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.8660099545866251\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.8696415484696627\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.8756014450639487\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.8693463423848152\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.8754512482136488\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.874386514723301\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.8798874061182141\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.877810508236289\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.8760669378936291\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.881258566826582\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.8854020984098315\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.8848221602290869\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.8881492449343205\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.8901719046756625\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.8165204123780131\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.7800866113603115\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.8091701332107186\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.7980301509797573\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.8343603303283453\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.8170159048959613\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.8373165699467063\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.8245051854103803\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.8380024046078325\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.8157227806001902\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.8344636169448495\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.7702283631265163\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.7164257690310478\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.6902847007289529\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.6720492165908217\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.7144443549960852\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.7479099094495177\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.7300305180624127\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.7732775595784187\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.8061091520264745\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.8124027112498879\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.8153668718412519\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.8394818121939898\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.8349317945167423\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.8453309446200729\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.8425657644495368\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.8459962319582701\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.8702889600023628\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.8690445802733302\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.8746326732635498\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.8651807075738907\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.8830751929804683\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.8691102457791566\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.8661186246946454\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.8854334897920489\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.8814061176031828\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.8894717794284225\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.8772572733461856\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.8910671906545758\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.8961032285168767\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.805291792601347\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.7511937732994557\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.8301984807476401\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.8736933454498649\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.879152138158679\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.8832912643998861\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.8920809483155608\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.8912377183884382\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.8979573093168437\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.9085364743322134\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.9030106965824962\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.9100834222510457\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.9151218781620264\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.9143375737965107\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.8934518518298864\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.9072117050178349\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.9128470168821514\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.9105046316608787\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.9134629214182496\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.9250306965224445\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.9248059110343456\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.9270996252261102\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.9279693179018795\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.9341546942293644\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.9313283863663674\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.9324890168942511\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.9339931095205247\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.9311453224532307\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.9349080905504524\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.9346801140345633\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.9365660371817648\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.9400069083273411\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.9392011278122664\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.947261170707643\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.950233096703887\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.95558912537992\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.9475037069432437\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.9383145796135068\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.9462951905280351\n",
      "NetRNNWithAttention, rep: 0, epoch: 134, acc: 0.9462907655909657\n",
      "NetRNNWithAttention, rep: 0, epoch: 135, acc: 0.9477838859148323\n",
      "NetRNNWithAttention, rep: 0, epoch: 136, acc: 0.9520792120229453\n",
      "NetRNNWithAttention, rep: 0, epoch: 137, acc: 0.9500052824988962\n",
      "NetRNNWithAttention, rep: 0, epoch: 138, acc: 0.9509477593749761\n",
      "NetRNNWithAttention, rep: 0, epoch: 139, acc: 0.953291270583868\n",
      "NetRNNWithAttention, rep: 0, epoch: 140, acc: 0.9571104953903705\n",
      "NetRNNWithAttention, rep: 0, epoch: 141, acc: 0.9553679189365357\n",
      "NetRNNWithAttention, rep: 0, epoch: 142, acc: 0.9589075103402138\n",
      "NetRNNWithAttention, rep: 0, epoch: 143, acc: 0.9564500194694847\n",
      "NetRNNWithAttention, rep: 0, epoch: 144, acc: 0.9552965964004397\n",
      "NetRNNWithAttention, rep: 0, epoch: 145, acc: 0.960224232133478\n",
      "NetRNNWithAttention, rep: 0, epoch: 146, acc: 0.9586840546689928\n",
      "NetRNNWithAttention, rep: 0, epoch: 147, acc: 0.9625025192648172\n",
      "NetRNNWithAttention, rep: 0, epoch: 148, acc: 0.9619049722421914\n",
      "NetRNNWithAttention, rep: 0, epoch: 149, acc: 0.9627728844434023\n",
      "NetRNNWithAttention, rep: 0, epoch: 150, acc: 0.9599543147906661\n",
      "NetRNNWithAttention, rep: 0, epoch: 151, acc: 0.9634566068835556\n",
      "NetRNNWithAttention, rep: 0, epoch: 152, acc: 0.9650235659908504\n",
      "NetRNNWithAttention, rep: 0, epoch: 153, acc: 0.9636100328620523\n",
      "NetRNNWithAttention, rep: 0, epoch: 154, acc: 0.9663833283446729\n",
      "NetRNNWithAttention, rep: 0, epoch: 155, acc: 0.9669242665916681\n",
      "NetRNNWithAttention, rep: 0, epoch: 156, acc: 0.9660390255972743\n",
      "NetRNNWithAttention, rep: 0, epoch: 157, acc: 0.9697102563269436\n",
      "NetRNNWithAttention, rep: 0, epoch: 158, acc: 0.970109069654718\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 158   Acc: 0.9701 Params: min_length: 20, max_length: 25, fill: 0, value_1: -1, value_2: 1 Time: 57.29 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.5017749226093292\n",
      "RNN, rep: 0, epoch: 2, acc: 0.49910952657461166\n",
      "RNN, rep: 0, epoch: 3, acc: 0.5030312052369118\n",
      "RNN, rep: 0, epoch: 4, acc: 0.4986340856552124\n",
      "RNN, rep: 0, epoch: 5, acc: 0.5008370414376259\n",
      "RNN, rep: 0, epoch: 6, acc: 0.5015659448504448\n",
      "RNN, rep: 0, epoch: 7, acc: 0.5012052887678147\n",
      "RNN, rep: 0, epoch: 8, acc: 0.5094753775000572\n",
      "RNN, rep: 0, epoch: 9, acc: 0.49917309045791625\n",
      "RNN, rep: 0, epoch: 10, acc: 0.4999641051888466\n",
      "RNN, rep: 0, epoch: 11, acc: 0.5122809886932373\n",
      "RNN, rep: 0, epoch: 12, acc: 0.5155018067359924\n",
      "RNN, rep: 0, epoch: 13, acc: 0.5133991539478302\n",
      "RNN, rep: 0, epoch: 14, acc: 0.5333459633588791\n",
      "RNN, rep: 0, epoch: 15, acc: 0.5061813068389892\n",
      "RNN, rep: 0, epoch: 16, acc: 0.5435883110761642\n",
      "RNN, rep: 0, epoch: 17, acc: 0.5770545390248298\n",
      "RNN, rep: 0, epoch: 18, acc: 0.5947854951024055\n",
      "RNN, rep: 0, epoch: 19, acc: 0.6139366582036019\n",
      "RNN, rep: 0, epoch: 20, acc: 0.6239811298251152\n",
      "RNN, rep: 0, epoch: 21, acc: 0.6281899121403695\n",
      "RNN, rep: 0, epoch: 22, acc: 0.6351102790236474\n",
      "RNN, rep: 0, epoch: 23, acc: 0.6348635452985764\n",
      "RNN, rep: 0, epoch: 24, acc: 0.6398024266958237\n",
      "RNN, rep: 0, epoch: 25, acc: 0.6414629596471787\n",
      "RNN, rep: 0, epoch: 26, acc: 0.6228837397694588\n",
      "RNN, rep: 0, epoch: 27, acc: 0.5001686254143715\n",
      "RNN, rep: 0, epoch: 28, acc: 0.49052044451236726\n",
      "RNN, rep: 0, epoch: 29, acc: 0.5055426186323166\n",
      "RNN, rep: 0, epoch: 30, acc: 0.5039465582370758\n",
      "RNN, rep: 0, epoch: 31, acc: 0.5033775001764298\n",
      "RNN, rep: 0, epoch: 32, acc: 0.5070920377969742\n",
      "RNN, rep: 0, epoch: 33, acc: 0.5061150446534157\n",
      "RNN, rep: 0, epoch: 34, acc: 0.5109899634122849\n",
      "RNN, rep: 0, epoch: 35, acc: 0.5339765620231628\n",
      "RNN, rep: 0, epoch: 36, acc: 0.5593415433168412\n",
      "RNN, rep: 0, epoch: 37, acc: 0.5972819915413856\n",
      "RNN, rep: 0, epoch: 38, acc: 0.6105061969161034\n",
      "RNN, rep: 0, epoch: 39, acc: 0.6231844088435173\n",
      "RNN, rep: 0, epoch: 40, acc: 0.6280266731977463\n",
      "RNN, rep: 0, epoch: 41, acc: 0.6308653268218041\n",
      "RNN, rep: 0, epoch: 42, acc: 0.6330786156654358\n",
      "RNN, rep: 0, epoch: 43, acc: 0.6388954409956932\n",
      "RNN, rep: 0, epoch: 44, acc: 0.6411595445871353\n",
      "RNN, rep: 0, epoch: 45, acc: 0.6415399158000946\n",
      "RNN, rep: 0, epoch: 46, acc: 0.6433197012543679\n",
      "RNN, rep: 0, epoch: 47, acc: 0.6476952701807022\n",
      "RNN, rep: 0, epoch: 48, acc: 0.643951047360897\n",
      "RNN, rep: 0, epoch: 49, acc: 0.6444961163401604\n",
      "RNN, rep: 0, epoch: 50, acc: 0.64756571829319\n",
      "RNN, rep: 0, epoch: 51, acc: 0.6487062212824821\n",
      "RNN, rep: 0, epoch: 52, acc: 0.6517692840099335\n",
      "RNN, rep: 0, epoch: 53, acc: 0.649168761074543\n",
      "RNN, rep: 0, epoch: 54, acc: 0.6508814671635628\n",
      "RNN, rep: 0, epoch: 55, acc: 0.6503039327263832\n",
      "RNN, rep: 0, epoch: 56, acc: 0.6529923096299172\n",
      "RNN, rep: 0, epoch: 57, acc: 0.6526463922858238\n",
      "RNN, rep: 0, epoch: 58, acc: 0.6534766665101052\n",
      "RNN, rep: 0, epoch: 59, acc: 0.6554898032546044\n",
      "RNN, rep: 0, epoch: 60, acc: 0.6547372302412987\n",
      "RNN, rep: 0, epoch: 61, acc: 0.6523753482103348\n",
      "RNN, rep: 0, epoch: 62, acc: 0.5411689895391464\n",
      "RNN, rep: 0, epoch: 63, acc: 0.5669886898994446\n",
      "RNN, rep: 0, epoch: 64, acc: 0.5754527547955512\n",
      "RNN, rep: 0, epoch: 65, acc: 0.6433342105150223\n",
      "RNN, rep: 0, epoch: 66, acc: 0.6490033593773842\n",
      "RNN, rep: 0, epoch: 67, acc: 0.6520833787322045\n",
      "RNN, rep: 0, epoch: 68, acc: 0.6539917829632759\n",
      "RNN, rep: 0, epoch: 69, acc: 0.6551306828856468\n",
      "RNN, rep: 0, epoch: 70, acc: 0.6547055625915528\n",
      "RNN, rep: 0, epoch: 71, acc: 0.6562189090251923\n",
      "RNN, rep: 0, epoch: 72, acc: 0.6038861206173897\n",
      "RNN, rep: 0, epoch: 73, acc: 0.5371119344234466\n",
      "RNN, rep: 0, epoch: 74, acc: 0.567601065337658\n",
      "RNN, rep: 0, epoch: 75, acc: 0.5516026365756989\n",
      "RNN, rep: 0, epoch: 76, acc: 0.5365613442659378\n",
      "RNN, rep: 0, epoch: 77, acc: 0.5302043387293816\n",
      "RNN, rep: 0, epoch: 78, acc: 0.5383150151371956\n",
      "RNN, rep: 0, epoch: 79, acc: 0.5462849798798561\n",
      "RNN, rep: 0, epoch: 80, acc: 0.5458114752173424\n",
      "RNN, rep: 0, epoch: 81, acc: 0.5533912122249603\n",
      "RNN, rep: 0, epoch: 82, acc: 0.5507046073675156\n",
      "RNN, rep: 0, epoch: 83, acc: 0.5499972152709961\n",
      "RNN, rep: 0, epoch: 84, acc: 0.5320519107580185\n",
      "RNN, rep: 0, epoch: 85, acc: 0.543136422932148\n",
      "RNN, rep: 0, epoch: 86, acc: 0.546534349322319\n",
      "RNN, rep: 0, epoch: 87, acc: 0.5422760218381881\n",
      "RNN, rep: 0, epoch: 88, acc: 0.5815327227115631\n",
      "RNN, rep: 0, epoch: 89, acc: 0.6017000621557236\n",
      "RNN, rep: 0, epoch: 90, acc: 0.6153023126721382\n",
      "RNN, rep: 0, epoch: 91, acc: 0.6220523890852928\n",
      "RNN, rep: 0, epoch: 92, acc: 0.6248698058724403\n",
      "RNN, rep: 0, epoch: 93, acc: 0.6310215544700623\n",
      "RNN, rep: 0, epoch: 94, acc: 0.6356075009703637\n",
      "RNN, rep: 0, epoch: 95, acc: 0.6401578450202942\n",
      "RNN, rep: 0, epoch: 96, acc: 0.6383281034231186\n",
      "RNN, rep: 0, epoch: 97, acc: 0.6422019621729851\n",
      "RNN, rep: 0, epoch: 98, acc: 0.6423801037669182\n",
      "RNN, rep: 0, epoch: 99, acc: 0.6448336815834046\n",
      "RNN, rep: 0, epoch: 100, acc: 0.645615399479866\n",
      "RNN, rep: 0, epoch: 101, acc: 0.6464609694480896\n",
      "RNN, rep: 0, epoch: 102, acc: 0.6506253188848495\n",
      "RNN, rep: 0, epoch: 103, acc: 0.653961321413517\n",
      "RNN, rep: 0, epoch: 104, acc: 0.655574160516262\n",
      "RNN, rep: 0, epoch: 105, acc: 0.6499565836787223\n",
      "RNN, rep: 0, epoch: 106, acc: 0.6538498818874359\n",
      "RNN, rep: 0, epoch: 107, acc: 0.6525435674190522\n",
      "RNN, rep: 0, epoch: 108, acc: 0.6513789823651314\n",
      "RNN, rep: 0, epoch: 109, acc: 0.6524533650279045\n",
      "RNN, rep: 0, epoch: 110, acc: 0.6566937312483787\n",
      "RNN, rep: 0, epoch: 111, acc: 0.6554517656564712\n",
      "RNN, rep: 0, epoch: 112, acc: 0.6533903867006302\n",
      "RNN, rep: 0, epoch: 113, acc: 0.6532335248589516\n",
      "RNN, rep: 0, epoch: 114, acc: 0.6553952303528786\n",
      "RNN, rep: 0, epoch: 115, acc: 0.6568678948283195\n",
      "RNN, rep: 0, epoch: 116, acc: 0.653581708073616\n",
      "RNN, rep: 0, epoch: 117, acc: 0.6561634528636933\n",
      "RNN, rep: 0, epoch: 118, acc: 0.6569407486915588\n",
      "RNN, rep: 0, epoch: 119, acc: 0.6598699903488159\n",
      "RNN, rep: 0, epoch: 120, acc: 0.6615779349207878\n",
      "RNN, rep: 0, epoch: 121, acc: 0.6555727919936181\n",
      "RNN, rep: 0, epoch: 122, acc: 0.6560961654782296\n",
      "RNN, rep: 0, epoch: 123, acc: 0.6584205639362335\n",
      "RNN, rep: 0, epoch: 124, acc: 0.6586109736561775\n",
      "RNN, rep: 0, epoch: 125, acc: 0.6597593295574188\n",
      "RNN, rep: 0, epoch: 126, acc: 0.6603974214196205\n",
      "RNN, rep: 0, epoch: 127, acc: 0.6592116954922677\n",
      "RNN, rep: 0, epoch: 128, acc: 0.659160032570362\n",
      "RNN, rep: 0, epoch: 129, acc: 0.6589014774560928\n",
      "RNN, rep: 0, epoch: 130, acc: 0.6615667575597763\n",
      "RNN, rep: 0, epoch: 131, acc: 0.6601381060481072\n",
      "RNN, rep: 0, epoch: 132, acc: 0.6602991080284119\n",
      "RNN, rep: 0, epoch: 133, acc: 0.6599582916498185\n",
      "RNN, rep: 0, epoch: 134, acc: 0.6601107409596443\n",
      "RNN, rep: 0, epoch: 135, acc: 0.6602354842424393\n",
      "RNN, rep: 0, epoch: 136, acc: 0.6603059154748917\n",
      "RNN, rep: 0, epoch: 137, acc: 0.6609622293710709\n",
      "RNN, rep: 0, epoch: 138, acc: 0.661190120279789\n",
      "RNN, rep: 0, epoch: 139, acc: 0.6613195702433586\n",
      "RNN, rep: 0, epoch: 140, acc: 0.6624094584584236\n",
      "RNN, rep: 0, epoch: 141, acc: 0.6607447475194931\n",
      "RNN, rep: 0, epoch: 142, acc: 0.6634706640243531\n",
      "RNN, rep: 0, epoch: 143, acc: 0.6626165297627449\n",
      "RNN, rep: 0, epoch: 144, acc: 0.6629202806949616\n",
      "RNN, rep: 0, epoch: 145, acc: 0.6617050194740295\n",
      "RNN, rep: 0, epoch: 146, acc: 0.6623324525356292\n",
      "RNN, rep: 0, epoch: 147, acc: 0.6618467870354653\n",
      "RNN, rep: 0, epoch: 148, acc: 0.663585880100727\n",
      "RNN, rep: 0, epoch: 149, acc: 0.6639482334256173\n",
      "RNN, rep: 0, epoch: 150, acc: 0.6653510275483131\n",
      "RNN, rep: 0, epoch: 151, acc: 0.6619710010290146\n",
      "RNN, rep: 0, epoch: 152, acc: 0.6646351584792137\n",
      "RNN, rep: 0, epoch: 153, acc: 0.6627799615263938\n",
      "RNN, rep: 0, epoch: 154, acc: 0.6621956449747085\n",
      "RNN, rep: 0, epoch: 155, acc: 0.6623872578144073\n",
      "RNN, rep: 0, epoch: 156, acc: 0.664707114994526\n",
      "RNN, rep: 0, epoch: 157, acc: 0.6641347971558571\n",
      "RNN, rep: 0, epoch: 158, acc: 0.6622344562411309\n",
      "RNN, rep: 0, epoch: 159, acc: 0.6636910262703896\n",
      "RNN, rep: 0, epoch: 160, acc: 0.6632362660765648\n",
      "RNN, rep: 0, epoch: 161, acc: 0.6632920810580254\n",
      "RNN, rep: 0, epoch: 162, acc: 0.6643901827931404\n",
      "RNN, rep: 0, epoch: 163, acc: 0.6638926517963409\n",
      "RNN, rep: 0, epoch: 164, acc: 0.665309508740902\n",
      "RNN, rep: 0, epoch: 165, acc: 0.6634548366069793\n",
      "RNN, rep: 0, epoch: 166, acc: 0.6632976427674293\n",
      "RNN, rep: 0, epoch: 167, acc: 0.6660044759511947\n",
      "RNN, rep: 0, epoch: 168, acc: 0.661810465157032\n",
      "RNN, rep: 0, epoch: 169, acc: 0.6652800107002258\n",
      "RNN, rep: 0, epoch: 170, acc: 0.6613977074623107\n",
      "RNN, rep: 0, epoch: 171, acc: 0.6649855515360832\n",
      "RNN, rep: 0, epoch: 172, acc: 0.6646726238727569\n",
      "RNN, rep: 0, epoch: 173, acc: 0.6649809423089027\n",
      "RNN, rep: 0, epoch: 174, acc: 0.6639204892516136\n",
      "RNN, rep: 0, epoch: 175, acc: 0.6634030708670616\n",
      "RNN, rep: 0, epoch: 176, acc: 0.6644081825017929\n",
      "RNN, rep: 0, epoch: 177, acc: 0.669052283167839\n",
      "RNN, rep: 0, epoch: 178, acc: 0.6665056920051575\n",
      "RNN, rep: 0, epoch: 179, acc: 0.6666130810976029\n",
      "RNN, rep: 0, epoch: 180, acc: 0.6643207824230194\n",
      "RNN, rep: 0, epoch: 181, acc: 0.6625440242886543\n",
      "RNN, rep: 0, epoch: 182, acc: 0.6645592376589775\n",
      "RNN, rep: 0, epoch: 183, acc: 0.6658180212974548\n",
      "RNN, rep: 0, epoch: 184, acc: 0.6659188610315323\n",
      "RNN, rep: 0, epoch: 185, acc: 0.6094756332039833\n",
      "RNN, rep: 0, epoch: 186, acc: 0.6622305768728256\n",
      "RNN, rep: 0, epoch: 187, acc: 0.6654400697350502\n",
      "RNN, rep: 0, epoch: 188, acc: 0.6597413527965545\n",
      "RNN, rep: 0, epoch: 189, acc: 0.6638397151231765\n",
      "RNN, rep: 0, epoch: 190, acc: 0.6613200634717942\n",
      "RNN, rep: 0, epoch: 191, acc: 0.6654332500696182\n",
      "RNN, rep: 0, epoch: 192, acc: 0.6317998948693275\n",
      "RNN, rep: 0, epoch: 193, acc: 0.6480210834741592\n",
      "RNN, rep: 0, epoch: 194, acc: 0.6620055863261223\n",
      "RNN, rep: 0, epoch: 195, acc: 0.6610701459646225\n",
      "RNN, rep: 0, epoch: 196, acc: 0.6622968816757202\n",
      "RNN, rep: 0, epoch: 197, acc: 0.6623101007938385\n",
      "RNN, rep: 0, epoch: 198, acc: 0.6646929213404655\n",
      "RNN, rep: 0, epoch: 199, acc: 0.6616553294658661\n",
      "RNN, rep: 0, epoch: 200, acc: 0.6630459922552109\n",
      "RNN, rep: 0, epoch: 201, acc: 0.6626839181780815\n",
      "RNN, rep: 0, epoch: 202, acc: 0.6629665920138359\n",
      "RNN, rep: 0, epoch: 203, acc: 0.6639820477366447\n",
      "RNN, rep: 0, epoch: 204, acc: 0.662966416478157\n",
      "RNN, rep: 0, epoch: 205, acc: 0.6640740609169007\n",
      "RNN, rep: 0, epoch: 206, acc: 0.6628538939356804\n",
      "RNN, rep: 0, epoch: 207, acc: 0.6629333221912384\n",
      "RNN, rep: 0, epoch: 208, acc: 0.6638165441155434\n",
      "RNN, rep: 0, epoch: 209, acc: 0.6629504564404488\n",
      "RNN, rep: 0, epoch: 210, acc: 0.663914101421833\n",
      "RNN, rep: 0, epoch: 211, acc: 0.664966451227665\n",
      "RNN, rep: 0, epoch: 212, acc: 0.6684055042266845\n",
      "RNN, rep: 0, epoch: 213, acc: 0.6633182889223099\n",
      "RNN, rep: 0, epoch: 214, acc: 0.664688848555088\n",
      "RNN, rep: 0, epoch: 215, acc: 0.6637647747993469\n",
      "RNN, rep: 0, epoch: 216, acc: 0.664285096526146\n",
      "RNN, rep: 0, epoch: 217, acc: 0.6648209270834923\n",
      "RNN, rep: 0, epoch: 218, acc: 0.6632210105657578\n",
      "RNN, rep: 0, epoch: 219, acc: 0.6643985572457314\n",
      "RNN, rep: 0, epoch: 220, acc: 0.6661703976988792\n",
      "RNN, rep: 0, epoch: 221, acc: 0.6632474389672279\n",
      "RNN, rep: 0, epoch: 222, acc: 0.6669966676831245\n",
      "RNN, rep: 0, epoch: 223, acc: 0.6667878246307373\n",
      "RNN, rep: 0, epoch: 224, acc: 0.6638539296388626\n",
      "RNN, rep: 0, epoch: 225, acc: 0.6632718712091445\n",
      "RNN, rep: 0, epoch: 226, acc: 0.6599448883533477\n",
      "RNN, rep: 0, epoch: 227, acc: 0.66437768638134\n",
      "RNN, rep: 0, epoch: 228, acc: 0.6665696406364441\n",
      "RNN, rep: 0, epoch: 229, acc: 0.6642522239685058\n",
      "RNN, rep: 0, epoch: 230, acc: 0.666944353878498\n",
      "RNN, rep: 0, epoch: 231, acc: 0.6563050156831741\n",
      "RNN, rep: 0, epoch: 232, acc: 0.6338614067435264\n",
      "RNN, rep: 0, epoch: 233, acc: 0.6628161084651947\n",
      "RNN, rep: 0, epoch: 234, acc: 0.6623569652438164\n",
      "RNN, rep: 0, epoch: 235, acc: 0.6540229570865631\n",
      "RNN, rep: 0, epoch: 236, acc: 0.662596028149128\n",
      "RNN, rep: 0, epoch: 237, acc: 0.6635119014978409\n",
      "RNN, rep: 0, epoch: 238, acc: 0.6626128649711609\n",
      "RNN, rep: 0, epoch: 239, acc: 0.6649245595932007\n",
      "RNN, rep: 0, epoch: 240, acc: 0.6653541502356529\n",
      "RNN, rep: 0, epoch: 241, acc: 0.6656347748637199\n",
      "RNN, rep: 0, epoch: 242, acc: 0.6656324630975723\n",
      "RNN, rep: 0, epoch: 243, acc: 0.6633745357394218\n",
      "RNN, rep: 0, epoch: 244, acc: 0.6630855724215508\n",
      "RNN, rep: 0, epoch: 245, acc: 0.664288323521614\n",
      "RNN, rep: 0, epoch: 246, acc: 0.6641518488526345\n",
      "RNN, rep: 0, epoch: 247, acc: 0.6642740282416344\n",
      "RNN, rep: 0, epoch: 248, acc: 0.6640595200657845\n",
      "RNN, rep: 0, epoch: 249, acc: 0.6662522131204605\n",
      "RNN, rep: 0, epoch: 250, acc: 0.6619546368718148\n",
      "RNN, rep: 0, epoch: 251, acc: 0.6627468284964562\n",
      "RNN, rep: 0, epoch: 252, acc: 0.6669188529253006\n",
      "RNN, rep: 0, epoch: 253, acc: 0.6618414998054505\n",
      "RNN, rep: 0, epoch: 254, acc: 0.6633564558625221\n",
      "RNN, rep: 0, epoch: 255, acc: 0.6659564745426177\n",
      "RNN, rep: 0, epoch: 256, acc: 0.6639750891923905\n",
      "RNN, rep: 0, epoch: 257, acc: 0.6663700070977211\n",
      "RNN, rep: 0, epoch: 258, acc: 0.6647732371091842\n",
      "RNN, rep: 0, epoch: 259, acc: 0.6633388653397561\n",
      "RNN, rep: 0, epoch: 260, acc: 0.6665635025501251\n",
      "RNN, rep: 0, epoch: 261, acc: 0.6657707211375237\n",
      "RNN, rep: 0, epoch: 262, acc: 0.6631749173998833\n",
      "RNN, rep: 0, epoch: 263, acc: 0.665005368590355\n",
      "RNN, rep: 0, epoch: 264, acc: 0.6663395720720291\n",
      "RNN, rep: 0, epoch: 265, acc: 0.6647072196006775\n",
      "RNN, rep: 0, epoch: 266, acc: 0.6679650095105171\n",
      "RNN, rep: 0, epoch: 267, acc: 0.6657146647572517\n",
      "RNN, rep: 0, epoch: 268, acc: 0.6688108000159264\n",
      "RNN, rep: 0, epoch: 269, acc: 0.667769564986229\n",
      "RNN, rep: 0, epoch: 270, acc: 0.6649434465169907\n",
      "RNN, rep: 0, epoch: 271, acc: 0.6648306435346604\n",
      "RNN, rep: 0, epoch: 272, acc: 0.6636431124806405\n",
      "RNN, rep: 0, epoch: 273, acc: 0.6653867945075035\n",
      "RNN, rep: 0, epoch: 274, acc: 0.667250047326088\n",
      "RNN, rep: 0, epoch: 275, acc: 0.6674545267224312\n",
      "RNN, rep: 0, epoch: 276, acc: 0.6656723529100418\n",
      "RNN, rep: 0, epoch: 277, acc: 0.6714338010549545\n",
      "RNN, rep: 0, epoch: 278, acc: 0.6674496680498123\n",
      "RNN, rep: 0, epoch: 279, acc: 0.6698938521742821\n",
      "RNN, rep: 0, epoch: 280, acc: 0.66104940533638\n",
      "RNN, rep: 0, epoch: 281, acc: 0.6710745120048522\n",
      "RNN, rep: 0, epoch: 282, acc: 0.6686161965131759\n",
      "RNN, rep: 0, epoch: 283, acc: 0.6684245032072067\n",
      "RNN, rep: 0, epoch: 284, acc: 0.6640605971217155\n",
      "RNN, rep: 0, epoch: 285, acc: 0.6650726607441902\n",
      "RNN, rep: 0, epoch: 286, acc: 0.6660308107733727\n",
      "RNN, rep: 0, epoch: 287, acc: 0.6677024719119072\n",
      "RNN, rep: 0, epoch: 288, acc: 0.666660028398037\n",
      "RNN, rep: 0, epoch: 289, acc: 0.6638606768846512\n",
      "RNN, rep: 0, epoch: 290, acc: 0.6676939460635185\n",
      "RNN, rep: 0, epoch: 291, acc: 0.6674552083015441\n",
      "RNN, rep: 0, epoch: 292, acc: 0.6668544319272042\n",
      "RNN, rep: 0, epoch: 293, acc: 0.6678468069434166\n",
      "RNN, rep: 0, epoch: 294, acc: 0.6697542625665664\n",
      "RNN, rep: 0, epoch: 295, acc: 0.6675496900081634\n",
      "RNN, rep: 0, epoch: 296, acc: 0.6677748915553093\n",
      "RNN, rep: 0, epoch: 297, acc: 0.6692067596316338\n",
      "RNN, rep: 0, epoch: 298, acc: 0.6663946431875228\n",
      "RNN, rep: 0, epoch: 299, acc: 0.6684603714942932\n",
      "RNN, rep: 0, epoch: 300, acc: 0.671174248456955\n",
      "RNN, rep: 0, epoch: 301, acc: 0.6722517436742783\n",
      "RNN, rep: 0, epoch: 302, acc: 0.67977728754282\n",
      "RNN, rep: 0, epoch: 303, acc: 0.6721470065414905\n",
      "RNN, rep: 0, epoch: 304, acc: 0.5889081779122353\n",
      "RNN, rep: 0, epoch: 305, acc: 0.6971261455118656\n",
      "RNN, rep: 0, epoch: 306, acc: 0.697509800195694\n",
      "RNN, rep: 0, epoch: 307, acc: 0.692910622805357\n",
      "RNN, rep: 0, epoch: 308, acc: 0.6911959518492222\n",
      "RNN, rep: 0, epoch: 309, acc: 0.688553617745638\n",
      "RNN, rep: 0, epoch: 310, acc: 0.683011663556099\n",
      "RNN, rep: 0, epoch: 311, acc: 0.6864972157776356\n",
      "RNN, rep: 0, epoch: 312, acc: 0.6836925615370274\n",
      "RNN, rep: 0, epoch: 313, acc: 0.690048623085022\n",
      "RNN, rep: 0, epoch: 314, acc: 0.7045509006083012\n",
      "RNN, rep: 0, epoch: 315, acc: 0.6957993388175965\n",
      "RNN, rep: 0, epoch: 316, acc: 0.688360376060009\n",
      "RNN, rep: 0, epoch: 317, acc: 0.6906872078776359\n",
      "RNN, rep: 0, epoch: 318, acc: 0.6906662841141223\n",
      "RNN, rep: 0, epoch: 319, acc: 0.6959525299072266\n",
      "RNN, rep: 0, epoch: 320, acc: 0.6921579915285111\n",
      "RNN, rep: 0, epoch: 321, acc: 0.7000566707551479\n",
      "RNN, rep: 0, epoch: 322, acc: 0.6807913281023502\n",
      "RNN, rep: 0, epoch: 323, acc: 0.7193532864749431\n",
      "RNN, rep: 0, epoch: 324, acc: 0.7497192105650902\n",
      "RNN, rep: 0, epoch: 325, acc: 0.7671365106105804\n",
      "RNN, rep: 0, epoch: 326, acc: 0.7774510152637959\n",
      "RNN, rep: 0, epoch: 327, acc: 0.7889302361011505\n",
      "RNN, rep: 0, epoch: 328, acc: 0.797808790653944\n",
      "RNN, rep: 0, epoch: 329, acc: 0.8084769108891487\n",
      "RNN, rep: 0, epoch: 330, acc: 0.8126732966303826\n",
      "RNN, rep: 0, epoch: 331, acc: 0.8182122388482094\n",
      "RNN, rep: 0, epoch: 332, acc: 0.8230816254019737\n",
      "RNN, rep: 0, epoch: 333, acc: 0.8306895545125008\n",
      "RNN, rep: 0, epoch: 334, acc: 0.8347557316720485\n",
      "RNN, rep: 0, epoch: 335, acc: 0.8387498427927494\n",
      "RNN, rep: 0, epoch: 336, acc: 0.8507371068000793\n",
      "RNN, rep: 0, epoch: 337, acc: 0.8459381783753633\n",
      "RNN, rep: 0, epoch: 338, acc: 0.8539295304566622\n",
      "RNN, rep: 0, epoch: 339, acc: 0.8563320219516755\n",
      "RNN, rep: 0, epoch: 340, acc: 0.8611820520460606\n",
      "RNN, rep: 0, epoch: 341, acc: 0.8668609252572059\n",
      "RNN, rep: 0, epoch: 342, acc: 0.8705055461823941\n",
      "RNN, rep: 0, epoch: 343, acc: 0.8704969783872366\n",
      "RNN, rep: 0, epoch: 344, acc: 0.8786250951141119\n",
      "RNN, rep: 0, epoch: 345, acc: 0.877372223213315\n",
      "RNN, rep: 0, epoch: 346, acc: 0.8329821456223726\n",
      "RNN, rep: 0, epoch: 347, acc: 0.8812314072251319\n",
      "RNN, rep: 0, epoch: 348, acc: 0.889560526534915\n",
      "RNN, rep: 0, epoch: 349, acc: 0.8943174076080322\n",
      "RNN, rep: 0, epoch: 350, acc: 0.8872047835588455\n",
      "RNN, rep: 0, epoch: 351, acc: 0.8933536606281995\n",
      "RNN, rep: 0, epoch: 352, acc: 0.8975021082907915\n",
      "RNN, rep: 0, epoch: 353, acc: 0.9029909930378198\n",
      "RNN, rep: 0, epoch: 354, acc: 0.9052329067140817\n",
      "RNN, rep: 0, epoch: 355, acc: 0.903919977247715\n",
      "RNN, rep: 0, epoch: 356, acc: 0.9116186269372701\n",
      "RNN, rep: 0, epoch: 357, acc: 0.9131859783828259\n",
      "RNN, rep: 0, epoch: 358, acc: 0.9165733844786882\n",
      "RNN, rep: 0, epoch: 359, acc: 0.9194864276051521\n",
      "RNN, rep: 0, epoch: 360, acc: 0.9219558401405812\n",
      "RNN, rep: 0, epoch: 361, acc: 0.926062436401844\n",
      "RNN, rep: 0, epoch: 362, acc: 0.9156447960808873\n",
      "RNN, rep: 0, epoch: 363, acc: 0.926058048941195\n",
      "RNN, rep: 0, epoch: 364, acc: 0.9220430551469326\n",
      "RNN, rep: 0, epoch: 365, acc: 0.9308067945018411\n",
      "RNN, rep: 0, epoch: 366, acc: 0.9287702603638173\n",
      "RNN, rep: 0, epoch: 367, acc: 0.9342112378403544\n",
      "RNN, rep: 0, epoch: 368, acc: 0.936919258236885\n",
      "RNN, rep: 0, epoch: 369, acc: 0.9380064728483558\n",
      "RNN, rep: 0, epoch: 370, acc: 0.9402441798895598\n",
      "RNN, rep: 0, epoch: 371, acc: 0.9441767711937428\n",
      "RNN, rep: 0, epoch: 372, acc: 0.9453338675945997\n",
      "RNN, rep: 0, epoch: 373, acc: 0.945696260407567\n",
      "RNN, rep: 0, epoch: 374, acc: 0.9479900070279836\n",
      "RNN, rep: 0, epoch: 375, acc: 0.9492146223038435\n",
      "RNN, rep: 0, epoch: 376, acc: 0.9513414206728339\n",
      "RNN, rep: 0, epoch: 377, acc: 0.9516470719873905\n",
      "RNN, rep: 0, epoch: 378, acc: 0.9543409666791558\n",
      "RNN, rep: 0, epoch: 379, acc: 0.9552137455344201\n",
      "RNN, rep: 0, epoch: 380, acc: 0.9576303018629551\n",
      "RNN, rep: 0, epoch: 381, acc: 0.9562369014695287\n",
      "RNN, rep: 0, epoch: 382, acc: 0.9595480305701494\n",
      "RNN, rep: 0, epoch: 383, acc: 0.9607510194927454\n",
      "RNN, rep: 0, epoch: 384, acc: 0.9595620562881231\n",
      "RNN, rep: 0, epoch: 385, acc: 0.9632940794155002\n",
      "RNN, rep: 0, epoch: 386, acc: 0.9642237177491189\n",
      "RNN, rep: 0, epoch: 387, acc: 0.964203217048198\n",
      "RNN, rep: 0, epoch: 388, acc: 0.965636587664485\n",
      "RNN, rep: 0, epoch: 389, acc: 0.9668013799190521\n",
      "RNN, rep: 0, epoch: 390, acc: 0.9669038033485413\n",
      "RNN, rep: 0, epoch: 391, acc: 0.9673486807569861\n",
      "RNN, rep: 0, epoch: 392, acc: 0.9686610165797174\n",
      "RNN, rep: 0, epoch: 393, acc: 0.9703078394383192\n",
      "RNN                  Rep: 0   Epoch: 393   Acc: 0.9703 Params: min_length: 40, max_length: 40, fill: 0, value_1: -1, value_2: 1 Time: 160.07 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.496453133225441\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.4991662389039993\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.49974868386983873\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.4991649055480957\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.5020005229115486\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.4996316087245941\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.5000153079628944\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.5025078052282334\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.507090273797512\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.5528081408143044\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.6092083159089089\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.6054544985294342\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.6302848160266876\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.5466312572360039\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.5072481730580329\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.5007649526000023\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.5467323958873749\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.579028409421444\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.6135677614808083\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.6277953159809112\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.6357980540394783\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.6285224947333335\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.6423838499188423\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.642063675224781\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.6427278950810432\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.6471694704890251\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.6530293416976929\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.6504728034138679\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.6499910780787468\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.6462942951917648\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.6459717550873756\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.6523294427990913\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.6574546906352043\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.6558867865800857\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.6570988911390304\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.6346329626441002\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.6238074293732643\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.6531251966953278\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.6486177888512611\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.6546615380048751\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.6552612990140915\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.6545669972896576\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.6543759769201278\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.6562355506420136\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.6568085673451424\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.6564463394880294\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.6567726224660874\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.6577720686793327\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.659834906756878\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.6577575451135635\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.659169131219387\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.6611293521523476\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.6581927165389061\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.6594944855570793\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.6574666202068329\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.6589362666010856\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.6600535854697227\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.6499989286065102\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.5674966642260552\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.5951580995321274\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.6510893845558167\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.6553908628225327\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.6676005440950393\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.6202773001790046\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.629519273340702\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.6162515294551849\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.61095434948802\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.5352511900663376\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.6274018800258636\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.6422849434614182\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.6429992580413818\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.6373005267977715\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.6595125544071198\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.6603902480006218\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.648106344640255\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.6687922725081443\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.6631445705890655\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.6575235521793366\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.6760278108716011\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.6710629418492318\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.6794912910461426\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.687875714302063\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.7042930789291859\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.6957906433939933\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.7087213134765625\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.7237824712693691\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.737482195198536\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.753274749815464\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.7131664343178272\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.6593259631097317\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.7522531048953534\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.7558256204426289\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.7740621587634087\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.7736682485044003\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.7844243286550046\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.7805828437209129\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.7674442121386528\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.7920050996541977\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.7965365287661552\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.7929880970716476\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.7205001482367516\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.6337524704635144\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.7771519441902638\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.7943681482970715\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.7939134468138218\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.7966514053940773\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.7994795997440814\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.7999716447293759\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.8012730184197426\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.806647891998291\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.8027486081421376\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.8077737854421139\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.8112762561440467\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.8122005292773247\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.8088192796707153\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.8140024785697461\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.8161166325211525\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.8163496223092079\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.8171767555177212\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.8209835587441922\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.6693048760294914\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.6835275559127331\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.6776609200239182\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.7028364896774292\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.6728437235951423\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.6992068393528461\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.644300300180912\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.649488076120615\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.6786203925311566\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.6965636353194714\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.651913094818592\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.6747685547173023\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.6702392165362835\n",
      "NetRNNWithAttention, rep: 0, epoch: 134, acc: 0.6588903823494912\n",
      "NetRNNWithAttention, rep: 0, epoch: 135, acc: 0.6746455179154873\n",
      "NetRNNWithAttention, rep: 0, epoch: 136, acc: 0.6755922804772854\n",
      "NetRNNWithAttention, rep: 0, epoch: 137, acc: 0.6821679911762476\n",
      "NetRNNWithAttention, rep: 0, epoch: 138, acc: 0.7032986410707235\n",
      "NetRNNWithAttention, rep: 0, epoch: 139, acc: 0.6971067553013564\n",
      "NetRNNWithAttention, rep: 0, epoch: 140, acc: 0.7118828217685222\n",
      "NetRNNWithAttention, rep: 0, epoch: 141, acc: 0.6744138738512993\n",
      "NetRNNWithAttention, rep: 0, epoch: 142, acc: 0.7066798452287912\n",
      "NetRNNWithAttention, rep: 0, epoch: 143, acc: 0.7178352935612202\n",
      "NetRNNWithAttention, rep: 0, epoch: 144, acc: 0.7108985048532486\n",
      "NetRNNWithAttention, rep: 0, epoch: 145, acc: 0.7080320061743259\n",
      "NetRNNWithAttention, rep: 0, epoch: 146, acc: 0.7252072113752365\n",
      "NetRNNWithAttention, rep: 0, epoch: 147, acc: 0.7146290171891451\n",
      "NetRNNWithAttention, rep: 0, epoch: 148, acc: 0.723645088672638\n",
      "NetRNNWithAttention, rep: 0, epoch: 149, acc: 0.7064765024185181\n",
      "NetRNNWithAttention, rep: 0, epoch: 150, acc: 0.7211578300595284\n",
      "NetRNNWithAttention, rep: 0, epoch: 151, acc: 0.7114520542323589\n",
      "NetRNNWithAttention, rep: 0, epoch: 152, acc: 0.7292803616821766\n",
      "NetRNNWithAttention, rep: 0, epoch: 153, acc: 0.7213809771835804\n",
      "NetRNNWithAttention, rep: 0, epoch: 154, acc: 0.7364133286476136\n",
      "NetRNNWithAttention, rep: 0, epoch: 155, acc: 0.7340961672365666\n",
      "NetRNNWithAttention, rep: 0, epoch: 156, acc: 0.720173394382\n",
      "NetRNNWithAttention, rep: 0, epoch: 157, acc: 0.7317041428387165\n",
      "NetRNNWithAttention, rep: 0, epoch: 158, acc: 0.7185890655219555\n",
      "NetRNNWithAttention, rep: 0, epoch: 159, acc: 0.7347250555455684\n",
      "NetRNNWithAttention, rep: 0, epoch: 160, acc: 0.7280544885993003\n",
      "NetRNNWithAttention, rep: 0, epoch: 161, acc: 0.752572753727436\n",
      "NetRNNWithAttention, rep: 0, epoch: 162, acc: 0.7479024685919284\n",
      "NetRNNWithAttention, rep: 0, epoch: 163, acc: 0.7449262553453445\n",
      "NetRNNWithAttention, rep: 0, epoch: 164, acc: 0.7705733219534159\n",
      "NetRNNWithAttention, rep: 0, epoch: 165, acc: 0.749815685749054\n",
      "NetRNNWithAttention, rep: 0, epoch: 166, acc: 0.7595311399549246\n",
      "NetRNNWithAttention, rep: 0, epoch: 167, acc: 0.7629515707492829\n",
      "NetRNNWithAttention, rep: 0, epoch: 168, acc: 0.646354620307684\n",
      "NetRNNWithAttention, rep: 0, epoch: 169, acc: 0.7023428022861481\n",
      "NetRNNWithAttention, rep: 0, epoch: 170, acc: 0.6961593709141016\n",
      "NetRNNWithAttention, rep: 0, epoch: 171, acc: 0.7166968850046396\n",
      "NetRNNWithAttention, rep: 0, epoch: 172, acc: 0.7274874438345432\n",
      "NetRNNWithAttention, rep: 0, epoch: 173, acc: 0.7297419554740191\n",
      "NetRNNWithAttention, rep: 0, epoch: 174, acc: 0.7543374449014664\n",
      "NetRNNWithAttention, rep: 0, epoch: 175, acc: 0.7440025176852941\n",
      "NetRNNWithAttention, rep: 0, epoch: 176, acc: 0.7364273319393396\n",
      "NetRNNWithAttention, rep: 0, epoch: 177, acc: 0.7403231524676085\n",
      "NetRNNWithAttention, rep: 0, epoch: 178, acc: 0.728375175446272\n",
      "NetRNNWithAttention, rep: 0, epoch: 179, acc: 0.7549227234721184\n",
      "NetRNNWithAttention, rep: 0, epoch: 180, acc: 0.8225458067655563\n",
      "NetRNNWithAttention, rep: 0, epoch: 181, acc: 0.8091502530127763\n",
      "NetRNNWithAttention, rep: 0, epoch: 182, acc: 0.7915420914441347\n",
      "NetRNNWithAttention, rep: 0, epoch: 183, acc: 0.7911303806304931\n",
      "NetRNNWithAttention, rep: 0, epoch: 184, acc: 0.7957273673266172\n",
      "NetRNNWithAttention, rep: 0, epoch: 185, acc: 0.8004587315022945\n",
      "NetRNNWithAttention, rep: 0, epoch: 186, acc: 0.7866558265686036\n",
      "NetRNNWithAttention, rep: 0, epoch: 187, acc: 0.79639240026474\n",
      "NetRNNWithAttention, rep: 0, epoch: 188, acc: 0.7882598046958447\n",
      "NetRNNWithAttention, rep: 0, epoch: 189, acc: 0.8052518376708031\n",
      "NetRNNWithAttention, rep: 0, epoch: 190, acc: 0.803664060011506\n",
      "NetRNNWithAttention, rep: 0, epoch: 191, acc: 0.775794088691473\n",
      "NetRNNWithAttention, rep: 0, epoch: 192, acc: 0.6678569650650025\n",
      "NetRNNWithAttention, rep: 0, epoch: 193, acc: 0.7591338697075843\n",
      "NetRNNWithAttention, rep: 0, epoch: 194, acc: 0.802631753012538\n",
      "NetRNNWithAttention, rep: 0, epoch: 195, acc: 0.771951749548316\n",
      "NetRNNWithAttention, rep: 0, epoch: 196, acc: 0.779050658121705\n",
      "NetRNNWithAttention, rep: 0, epoch: 197, acc: 0.8159833136200905\n",
      "NetRNNWithAttention, rep: 0, epoch: 198, acc: 0.8266701506823302\n",
      "NetRNNWithAttention, rep: 0, epoch: 199, acc: 0.8448328026384115\n",
      "NetRNNWithAttention, rep: 0, epoch: 200, acc: 0.838085094988346\n",
      "NetRNNWithAttention, rep: 0, epoch: 201, acc: 0.8102596773952245\n",
      "NetRNNWithAttention, rep: 0, epoch: 202, acc: 0.8509874562174082\n",
      "NetRNNWithAttention, rep: 0, epoch: 203, acc: 0.8700754693895578\n",
      "NetRNNWithAttention, rep: 0, epoch: 204, acc: 0.8615210063010454\n",
      "NetRNNWithAttention, rep: 0, epoch: 205, acc: 0.8728226458281279\n",
      "NetRNNWithAttention, rep: 0, epoch: 206, acc: 0.865590525791049\n",
      "NetRNNWithAttention, rep: 0, epoch: 207, acc: 0.8372708988189698\n",
      "NetRNNWithAttention, rep: 0, epoch: 208, acc: 0.8466882327944041\n",
      "NetRNNWithAttention, rep: 0, epoch: 209, acc: 0.8372265706956387\n",
      "NetRNNWithAttention, rep: 0, epoch: 210, acc: 0.8364377197250724\n",
      "NetRNNWithAttention, rep: 0, epoch: 211, acc: 0.8425804968178272\n",
      "NetRNNWithAttention, rep: 0, epoch: 212, acc: 0.8496068424358963\n",
      "NetRNNWithAttention, rep: 0, epoch: 213, acc: 0.8316622722521424\n",
      "NetRNNWithAttention, rep: 0, epoch: 214, acc: 0.8549775965511799\n",
      "NetRNNWithAttention, rep: 0, epoch: 215, acc: 0.8760061502829194\n",
      "NetRNNWithAttention, rep: 0, epoch: 216, acc: 0.8649736019968987\n",
      "NetRNNWithAttention, rep: 0, epoch: 217, acc: 0.867810800075531\n",
      "NetRNNWithAttention, rep: 0, epoch: 218, acc: 0.8958850137516856\n",
      "NetRNNWithAttention, rep: 0, epoch: 219, acc: 0.8830551133304835\n",
      "NetRNNWithAttention, rep: 0, epoch: 220, acc: 0.8803803497180342\n",
      "NetRNNWithAttention, rep: 0, epoch: 221, acc: 0.8953761802241206\n",
      "NetRNNWithAttention, rep: 0, epoch: 222, acc: 0.8969810717180371\n",
      "NetRNNWithAttention, rep: 0, epoch: 223, acc: 0.872925905175507\n",
      "NetRNNWithAttention, rep: 0, epoch: 224, acc: 0.9045678523741663\n",
      "NetRNNWithAttention, rep: 0, epoch: 225, acc: 0.9141439404338598\n",
      "NetRNNWithAttention, rep: 0, epoch: 226, acc: 0.913568374607712\n",
      "NetRNNWithAttention, rep: 0, epoch: 227, acc: 0.9123264643922449\n",
      "NetRNNWithAttention, rep: 0, epoch: 228, acc: 0.9138952518068254\n",
      "NetRNNWithAttention, rep: 0, epoch: 229, acc: 0.900260930750519\n",
      "NetRNNWithAttention, rep: 0, epoch: 230, acc: 0.9109024690464139\n",
      "NetRNNWithAttention, rep: 0, epoch: 231, acc: 0.9097312377765775\n",
      "NetRNNWithAttention, rep: 0, epoch: 232, acc: 0.9229818718321622\n",
      "NetRNNWithAttention, rep: 0, epoch: 233, acc: 0.9097154131904245\n",
      "NetRNNWithAttention, rep: 0, epoch: 234, acc: 0.8932325757853686\n",
      "NetRNNWithAttention, rep: 0, epoch: 235, acc: 0.9105837815068663\n",
      "NetRNNWithAttention, rep: 0, epoch: 236, acc: 0.8854143241420388\n",
      "NetRNNWithAttention, rep: 0, epoch: 237, acc: 0.8233596087247134\n",
      "NetRNNWithAttention, rep: 0, epoch: 238, acc: 0.7645728204399347\n",
      "NetRNNWithAttention, rep: 0, epoch: 239, acc: 0.8048407374322415\n",
      "NetRNNWithAttention, rep: 0, epoch: 240, acc: 0.783713507913053\n",
      "NetRNNWithAttention, rep: 0, epoch: 241, acc: 0.8043109016120433\n",
      "NetRNNWithAttention, rep: 0, epoch: 242, acc: 0.7933292722702027\n",
      "NetRNNWithAttention, rep: 0, epoch: 243, acc: 0.813330027833581\n",
      "NetRNNWithAttention, rep: 0, epoch: 244, acc: 0.8239822905883193\n",
      "NetRNNWithAttention, rep: 0, epoch: 245, acc: 0.822360760346055\n",
      "NetRNNWithAttention, rep: 0, epoch: 246, acc: 0.8169683143869042\n",
      "NetRNNWithAttention, rep: 0, epoch: 247, acc: 0.8344588736817241\n",
      "NetRNNWithAttention, rep: 0, epoch: 248, acc: 0.8420779120922088\n",
      "NetRNNWithAttention, rep: 0, epoch: 249, acc: 0.8388671229779721\n",
      "NetRNNWithAttention, rep: 0, epoch: 250, acc: 0.8652614956349134\n",
      "NetRNNWithAttention, rep: 0, epoch: 251, acc: 0.8625819448381662\n",
      "NetRNNWithAttention, rep: 0, epoch: 252, acc: 0.8752402364462614\n",
      "NetRNNWithAttention, rep: 0, epoch: 253, acc: 0.8840546518191695\n",
      "NetRNNWithAttention, rep: 0, epoch: 254, acc: 0.8859022317454219\n",
      "NetRNNWithAttention, rep: 0, epoch: 255, acc: 0.883231345936656\n",
      "NetRNNWithAttention, rep: 0, epoch: 256, acc: 0.8967418795078993\n",
      "NetRNNWithAttention, rep: 0, epoch: 257, acc: 0.8805750340595841\n",
      "NetRNNWithAttention, rep: 0, epoch: 258, acc: 0.8916573854535819\n",
      "NetRNNWithAttention, rep: 0, epoch: 259, acc: 0.8774557421728969\n",
      "NetRNNWithAttention, rep: 0, epoch: 260, acc: 0.8821089991182088\n",
      "NetRNNWithAttention, rep: 0, epoch: 261, acc: 0.8730250640213489\n",
      "NetRNNWithAttention, rep: 0, epoch: 262, acc: 0.8501689770445228\n",
      "NetRNNWithAttention, rep: 0, epoch: 263, acc: 0.8600814709998668\n",
      "NetRNNWithAttention, rep: 0, epoch: 264, acc: 0.8992081375420093\n",
      "NetRNNWithAttention, rep: 0, epoch: 265, acc: 0.8869480118341744\n",
      "NetRNNWithAttention, rep: 0, epoch: 266, acc: 0.9156029431149364\n",
      "NetRNNWithAttention, rep: 0, epoch: 267, acc: 0.8897104317322373\n",
      "NetRNNWithAttention, rep: 0, epoch: 268, acc: 0.8665828060172498\n",
      "NetRNNWithAttention, rep: 0, epoch: 269, acc: 0.9044060382433236\n",
      "NetRNNWithAttention, rep: 0, epoch: 270, acc: 0.9026909304223955\n",
      "NetRNNWithAttention, rep: 0, epoch: 271, acc: 0.8948917434550822\n",
      "NetRNNWithAttention, rep: 0, epoch: 272, acc: 0.907931058332324\n",
      "NetRNNWithAttention, rep: 0, epoch: 273, acc: 0.9083876767009497\n",
      "NetRNNWithAttention, rep: 0, epoch: 274, acc: 0.9068822187744081\n",
      "NetRNNWithAttention, rep: 0, epoch: 275, acc: 0.9066403436288237\n",
      "NetRNNWithAttention, rep: 0, epoch: 276, acc: 0.8663648436404765\n",
      "NetRNNWithAttention, rep: 0, epoch: 277, acc: 0.8974565578997136\n",
      "NetRNNWithAttention, rep: 0, epoch: 278, acc: 0.9071148833818734\n",
      "NetRNNWithAttention, rep: 0, epoch: 279, acc: 0.8979181168228387\n",
      "NetRNNWithAttention, rep: 0, epoch: 280, acc: 0.8997453268431127\n",
      "NetRNNWithAttention, rep: 0, epoch: 281, acc: 0.9054931003972888\n",
      "NetRNNWithAttention, rep: 0, epoch: 282, acc: 0.9160613382421434\n",
      "NetRNNWithAttention, rep: 0, epoch: 283, acc: 0.9125002234056592\n",
      "NetRNNWithAttention, rep: 0, epoch: 284, acc: 0.9078732502274215\n",
      "NetRNNWithAttention, rep: 0, epoch: 285, acc: 0.8996732027456165\n",
      "NetRNNWithAttention, rep: 0, epoch: 286, acc: 0.903018638137728\n",
      "NetRNNWithAttention, rep: 0, epoch: 287, acc: 0.9212480133026838\n",
      "NetRNNWithAttention, rep: 0, epoch: 288, acc: 0.9152164760045707\n",
      "NetRNNWithAttention, rep: 0, epoch: 289, acc: 0.9270756838843226\n",
      "NetRNNWithAttention, rep: 0, epoch: 290, acc: 0.9075689632445574\n",
      "NetRNNWithAttention, rep: 0, epoch: 291, acc: 0.914928008876741\n",
      "NetRNNWithAttention, rep: 0, epoch: 292, acc: 0.9251021427009255\n",
      "NetRNNWithAttention, rep: 0, epoch: 293, acc: 0.9248541313316673\n",
      "NetRNNWithAttention, rep: 0, epoch: 294, acc: 0.9079754785820842\n",
      "NetRNNWithAttention, rep: 0, epoch: 295, acc: 0.9169902225863189\n",
      "NetRNNWithAttention, rep: 0, epoch: 296, acc: 0.9285277642682195\n",
      "NetRNNWithAttention, rep: 0, epoch: 297, acc: 0.9326385179162026\n",
      "NetRNNWithAttention, rep: 0, epoch: 298, acc: 0.9293096478655934\n",
      "NetRNNWithAttention, rep: 0, epoch: 299, acc: 0.9289988051913679\n",
      "NetRNNWithAttention, rep: 0, epoch: 300, acc: 0.934174896189943\n",
      "NetRNNWithAttention, rep: 0, epoch: 301, acc: 0.937565436353907\n",
      "NetRNNWithAttention, rep: 0, epoch: 302, acc: 0.9324785740859807\n",
      "NetRNNWithAttention, rep: 0, epoch: 303, acc: 0.8358806168101728\n",
      "NetRNNWithAttention, rep: 0, epoch: 304, acc: 0.9257133994158357\n",
      "NetRNNWithAttention, rep: 0, epoch: 305, acc: 0.9367458064667881\n",
      "NetRNNWithAttention, rep: 0, epoch: 306, acc: 0.9370486113522202\n",
      "NetRNNWithAttention, rep: 0, epoch: 307, acc: 0.9306209850497544\n",
      "NetRNNWithAttention, rep: 0, epoch: 308, acc: 0.9412342618405819\n",
      "NetRNNWithAttention, rep: 0, epoch: 309, acc: 0.9361638662591577\n",
      "NetRNNWithAttention, rep: 0, epoch: 310, acc: 0.9390061492472888\n",
      "NetRNNWithAttention, rep: 0, epoch: 311, acc: 0.9367139092646539\n",
      "NetRNNWithAttention, rep: 0, epoch: 312, acc: 0.938269790187478\n",
      "NetRNNWithAttention, rep: 0, epoch: 313, acc: 0.9285236044693739\n",
      "NetRNNWithAttention, rep: 0, epoch: 314, acc: 0.9422883750125766\n",
      "NetRNNWithAttention, rep: 0, epoch: 315, acc: 0.9447826319001615\n",
      "NetRNNWithAttention, rep: 0, epoch: 316, acc: 0.9408971470873803\n",
      "NetRNNWithAttention, rep: 0, epoch: 317, acc: 0.9426034379098565\n",
      "NetRNNWithAttention, rep: 0, epoch: 318, acc: 0.9427112704422325\n",
      "NetRNNWithAttention, rep: 0, epoch: 319, acc: 0.9441513438336551\n",
      "NetRNNWithAttention, rep: 0, epoch: 320, acc: 0.9288244560733437\n",
      "NetRNNWithAttention, rep: 0, epoch: 321, acc: 0.9443933932157234\n",
      "NetRNNWithAttention, rep: 0, epoch: 322, acc: 0.9472144522704184\n",
      "NetRNNWithAttention, rep: 0, epoch: 323, acc: 0.9496761078247801\n",
      "NetRNNWithAttention, rep: 0, epoch: 324, acc: 0.949065852900967\n",
      "NetRNNWithAttention, rep: 0, epoch: 325, acc: 0.951638842835091\n",
      "NetRNNWithAttention, rep: 0, epoch: 326, acc: 0.950879752621986\n",
      "NetRNNWithAttention, rep: 0, epoch: 327, acc: 0.9479535171110183\n",
      "NetRNNWithAttention, rep: 0, epoch: 328, acc: 0.9490477803070099\n",
      "NetRNNWithAttention, rep: 0, epoch: 329, acc: 0.9425696611776948\n",
      "NetRNNWithAttention, rep: 0, epoch: 330, acc: 0.9581237134058028\n",
      "NetRNNWithAttention, rep: 0, epoch: 331, acc: 0.9595244319643825\n",
      "NetRNNWithAttention, rep: 0, epoch: 332, acc: 0.9581538982596248\n",
      "NetRNNWithAttention, rep: 0, epoch: 333, acc: 0.9604418207053095\n",
      "NetRNNWithAttention, rep: 0, epoch: 334, acc: 0.9595658673159778\n",
      "NetRNNWithAttention, rep: 0, epoch: 335, acc: 0.9614586212392896\n",
      "NetRNNWithAttention, rep: 0, epoch: 336, acc: 0.9625966911530122\n",
      "NetRNNWithAttention, rep: 0, epoch: 337, acc: 0.9417823611898348\n",
      "NetRNNWithAttention, rep: 0, epoch: 338, acc: 0.7990587316639721\n",
      "NetRNNWithAttention, rep: 0, epoch: 339, acc: 0.8932300090044737\n",
      "NetRNNWithAttention, rep: 0, epoch: 340, acc: 0.9090074740489945\n",
      "NetRNNWithAttention, rep: 0, epoch: 341, acc: 0.8853669747198001\n",
      "NetRNNWithAttention, rep: 0, epoch: 342, acc: 0.8892333432566375\n",
      "NetRNNWithAttention, rep: 0, epoch: 343, acc: 0.9507840548548847\n",
      "NetRNNWithAttention, rep: 0, epoch: 344, acc: 0.9360648668557405\n",
      "NetRNNWithAttention, rep: 0, epoch: 345, acc: 0.9487708156649023\n",
      "NetRNNWithAttention, rep: 0, epoch: 346, acc: 0.9537541869934648\n",
      "NetRNNWithAttention, rep: 0, epoch: 347, acc: 0.9538755507254973\n",
      "NetRNNWithAttention, rep: 0, epoch: 348, acc: 0.9545249128574506\n",
      "NetRNNWithAttention, rep: 0, epoch: 349, acc: 0.9542408518306911\n",
      "NetRNNWithAttention, rep: 0, epoch: 350, acc: 0.9532629862241447\n",
      "NetRNNWithAttention, rep: 0, epoch: 351, acc: 0.942422346570529\n",
      "NetRNNWithAttention, rep: 0, epoch: 352, acc: 0.9569531796406955\n",
      "NetRNNWithAttention, rep: 0, epoch: 353, acc: 0.9620234555425122\n",
      "NetRNNWithAttention, rep: 0, epoch: 354, acc: 0.9597540022386238\n",
      "NetRNNWithAttention, rep: 0, epoch: 355, acc: 0.9618204718269407\n",
      "NetRNNWithAttention, rep: 0, epoch: 356, acc: 0.962281514192\n",
      "NetRNNWithAttention, rep: 0, epoch: 357, acc: 0.9590407716901973\n",
      "NetRNNWithAttention, rep: 0, epoch: 358, acc: 0.9647023512469605\n",
      "NetRNNWithAttention, rep: 0, epoch: 359, acc: 0.9647826855024323\n",
      "NetRNNWithAttention, rep: 0, epoch: 360, acc: 0.9628836068650708\n",
      "NetRNNWithAttention, rep: 0, epoch: 361, acc: 0.9655176497623325\n",
      "NetRNNWithAttention, rep: 0, epoch: 362, acc: 0.9675351234478876\n",
      "NetRNNWithAttention, rep: 0, epoch: 363, acc: 0.9693579823104664\n",
      "NetRNNWithAttention, rep: 0, epoch: 364, acc: 0.9660375680215657\n",
      "NetRNNWithAttention, rep: 0, epoch: 365, acc: 0.9689358509285375\n",
      "NetRNNWithAttention, rep: 0, epoch: 366, acc: 0.9714223780855537\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 366   Acc: 0.9714 Params: min_length: 40, max_length: 40, fill: 0, value_1: -1, value_2: 1 Time: 175.82 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.49954628348350527\n",
      "RNN, rep: 0, epoch: 2, acc: 0.49920103907585145\n",
      "RNN, rep: 0, epoch: 3, acc: 0.4990070700645447\n",
      "RNN, rep: 0, epoch: 4, acc: 0.5037135517597199\n",
      "RNN, rep: 0, epoch: 5, acc: 0.49998405784368516\n",
      "RNN, rep: 0, epoch: 6, acc: 0.4986700910329819\n",
      "RNN, rep: 0, epoch: 7, acc: 0.4993318152427673\n",
      "RNN, rep: 0, epoch: 8, acc: 0.500931139588356\n",
      "RNN, rep: 0, epoch: 9, acc: 0.5046389281749726\n",
      "RNN, rep: 0, epoch: 10, acc: 0.5009442156553269\n",
      "RNN, rep: 0, epoch: 11, acc: 0.4977236333489418\n",
      "RNN, rep: 0, epoch: 12, acc: 0.49807765036821366\n",
      "RNN, rep: 0, epoch: 13, acc: 0.4996061435341835\n",
      "RNN, rep: 0, epoch: 14, acc: 0.501282112300396\n",
      "RNN, rep: 0, epoch: 15, acc: 0.49880609959363936\n",
      "RNN, rep: 0, epoch: 16, acc: 0.5013381946086883\n",
      "RNN, rep: 0, epoch: 17, acc: 0.5037977442145347\n",
      "RNN, rep: 0, epoch: 18, acc: 0.4991266432404518\n",
      "RNN, rep: 0, epoch: 19, acc: 0.49960500091314314\n",
      "RNN, rep: 0, epoch: 20, acc: 0.5034435260295868\n",
      "RNN, rep: 0, epoch: 21, acc: 0.496981635093689\n",
      "RNN, rep: 0, epoch: 22, acc: 0.49895481556653976\n",
      "RNN, rep: 0, epoch: 23, acc: 0.4988211718201637\n",
      "RNN, rep: 0, epoch: 24, acc: 0.5028491267561912\n",
      "RNN, rep: 0, epoch: 25, acc: 0.4999560329318047\n",
      "RNN, rep: 0, epoch: 26, acc: 0.4990354189276695\n",
      "RNN, rep: 0, epoch: 27, acc: 0.5015352064371109\n",
      "RNN, rep: 0, epoch: 28, acc: 0.5020346522331238\n",
      "RNN, rep: 0, epoch: 29, acc: 0.5009217193722725\n",
      "RNN, rep: 0, epoch: 30, acc: 0.5016705146431923\n",
      "RNN, rep: 0, epoch: 31, acc: 0.49798814743757247\n",
      "RNN, rep: 0, epoch: 32, acc: 0.4996605974435806\n",
      "RNN, rep: 0, epoch: 33, acc: 0.5006823858618736\n",
      "RNN, rep: 0, epoch: 34, acc: 0.5015501156449318\n",
      "RNN, rep: 0, epoch: 35, acc: 0.49878867596387866\n",
      "RNN, rep: 0, epoch: 36, acc: 0.5006977275013924\n",
      "RNN, rep: 0, epoch: 37, acc: 0.49954606682062147\n",
      "RNN, rep: 0, epoch: 38, acc: 0.5008123680949211\n",
      "RNN, rep: 0, epoch: 39, acc: 0.498881573677063\n",
      "RNN, rep: 0, epoch: 40, acc: 0.5005206471681595\n",
      "RNN, rep: 0, epoch: 41, acc: 0.5001510137319565\n",
      "RNN, rep: 0, epoch: 42, acc: 0.5002786281704903\n",
      "RNN, rep: 0, epoch: 43, acc: 0.49999491423368453\n",
      "RNN, rep: 0, epoch: 44, acc: 0.5006917694211006\n",
      "RNN, rep: 0, epoch: 45, acc: 0.5006796160340309\n",
      "RNN, rep: 0, epoch: 46, acc: 0.49966912150382997\n",
      "RNN, rep: 0, epoch: 47, acc: 0.5007338646054268\n",
      "RNN, rep: 0, epoch: 48, acc: 0.501831850707531\n",
      "RNN, rep: 0, epoch: 49, acc: 0.5022033980488777\n",
      "RNN, rep: 0, epoch: 50, acc: 0.5000174874067307\n",
      "RNN, rep: 0, epoch: 51, acc: 0.4991976100206375\n",
      "RNN, rep: 0, epoch: 52, acc: 0.5031042966246605\n",
      "RNN, rep: 0, epoch: 53, acc: 0.5018040800094604\n",
      "RNN, rep: 0, epoch: 54, acc: 0.5033052110671997\n",
      "RNN, rep: 0, epoch: 55, acc: 0.5005460843443871\n",
      "RNN, rep: 0, epoch: 56, acc: 0.4994336885213852\n",
      "RNN, rep: 0, epoch: 57, acc: 0.5007972264289856\n",
      "RNN, rep: 0, epoch: 58, acc: 0.4997501116991043\n",
      "RNN, rep: 0, epoch: 59, acc: 0.49937735348939893\n",
      "RNN, rep: 0, epoch: 60, acc: 0.5014389106631278\n",
      "RNN, rep: 0, epoch: 61, acc: 0.5007341742515564\n",
      "RNN, rep: 0, epoch: 62, acc: 0.49950856626033785\n",
      "RNN, rep: 0, epoch: 63, acc: 0.4999746748805046\n",
      "RNN, rep: 0, epoch: 64, acc: 0.5001161623001099\n",
      "RNN, rep: 0, epoch: 65, acc: 0.5021171897649765\n",
      "RNN, rep: 0, epoch: 66, acc: 0.49981124460697174\n",
      "RNN, rep: 0, epoch: 67, acc: 0.4997041437029839\n",
      "RNN, rep: 0, epoch: 68, acc: 0.5013015741109847\n",
      "RNN, rep: 0, epoch: 69, acc: 0.502447517812252\n",
      "RNN, rep: 0, epoch: 70, acc: 0.5099944984912872\n",
      "RNN, rep: 0, epoch: 71, acc: 0.5192621928453446\n",
      "RNN, rep: 0, epoch: 72, acc: 0.5613245421648025\n",
      "RNN, rep: 0, epoch: 73, acc: 0.5214190077781677\n",
      "RNN, rep: 0, epoch: 74, acc: 0.5790326833724976\n",
      "RNN, rep: 0, epoch: 75, acc: 0.6064423674345016\n",
      "RNN, rep: 0, epoch: 76, acc: 0.6216217029094696\n",
      "RNN, rep: 0, epoch: 77, acc: 0.6279201939702034\n",
      "RNN, rep: 0, epoch: 78, acc: 0.6315227809548378\n",
      "RNN, rep: 0, epoch: 79, acc: 0.6372680589556694\n",
      "RNN, rep: 0, epoch: 80, acc: 0.5943206089735031\n",
      "RNN, rep: 0, epoch: 81, acc: 0.6365582323074341\n",
      "RNN, rep: 0, epoch: 82, acc: 0.6419921877980233\n",
      "RNN, rep: 0, epoch: 83, acc: 0.6501486855745315\n",
      "RNN, rep: 0, epoch: 84, acc: 0.6435303246974945\n",
      "RNN, rep: 0, epoch: 85, acc: 0.6469765841960907\n",
      "RNN, rep: 0, epoch: 86, acc: 0.649971817433834\n",
      "RNN, rep: 0, epoch: 87, acc: 0.6483181822299957\n",
      "RNN, rep: 0, epoch: 88, acc: 0.6496096450090408\n",
      "RNN, rep: 0, epoch: 89, acc: 0.6498326662182808\n",
      "RNN, rep: 0, epoch: 90, acc: 0.6512575268745422\n",
      "RNN, rep: 0, epoch: 91, acc: 0.6514079597592354\n",
      "RNN, rep: 0, epoch: 92, acc: 0.6539221876859664\n",
      "RNN, rep: 0, epoch: 93, acc: 0.6531057325005531\n",
      "RNN, rep: 0, epoch: 94, acc: 0.6541436767578125\n",
      "RNN, rep: 0, epoch: 95, acc: 0.6562895998358727\n",
      "RNN, rep: 0, epoch: 96, acc: 0.6568229478597641\n",
      "RNN, rep: 0, epoch: 97, acc: 0.6560526722669602\n",
      "RNN, rep: 0, epoch: 98, acc: 0.6579775360226631\n",
      "RNN, rep: 0, epoch: 99, acc: 0.6577270141243935\n",
      "RNN, rep: 0, epoch: 100, acc: 0.6590504232048988\n",
      "RNN, rep: 0, epoch: 101, acc: 0.6581112995743752\n",
      "RNN, rep: 0, epoch: 102, acc: 0.657223818898201\n",
      "RNN, rep: 0, epoch: 103, acc: 0.5573018521070481\n",
      "RNN, rep: 0, epoch: 104, acc: 0.5488893276453018\n",
      "RNN, rep: 0, epoch: 105, acc: 0.5333545613288879\n",
      "RNN, rep: 0, epoch: 106, acc: 0.6051914197206497\n",
      "RNN, rep: 0, epoch: 107, acc: 0.6341117143630981\n",
      "RNN, rep: 0, epoch: 108, acc: 0.6400827151536942\n",
      "RNN, rep: 0, epoch: 109, acc: 0.6427792000770569\n",
      "RNN, rep: 0, epoch: 110, acc: 0.6457770305871964\n",
      "RNN, rep: 0, epoch: 111, acc: 0.6459029629826546\n",
      "RNN, rep: 0, epoch: 112, acc: 0.6504332318902015\n",
      "RNN, rep: 0, epoch: 113, acc: 0.6517514330148697\n",
      "RNN, rep: 0, epoch: 114, acc: 0.6538733685016632\n",
      "RNN, rep: 0, epoch: 115, acc: 0.649508541226387\n",
      "RNN, rep: 0, epoch: 116, acc: 0.6542452102899552\n",
      "RNN, rep: 0, epoch: 117, acc: 0.6542076712846756\n",
      "RNN, rep: 0, epoch: 118, acc: 0.6555232208967209\n",
      "RNN, rep: 0, epoch: 119, acc: 0.6532177618145942\n",
      "RNN, rep: 0, epoch: 120, acc: 0.6568049409985542\n",
      "RNN, rep: 0, epoch: 121, acc: 0.6560525333881378\n",
      "RNN, rep: 0, epoch: 122, acc: 0.6560617622733116\n",
      "RNN, rep: 0, epoch: 123, acc: 0.6561101448535919\n",
      "RNN, rep: 0, epoch: 124, acc: 0.6561556053161621\n",
      "RNN, rep: 0, epoch: 125, acc: 0.6562333911657333\n",
      "RNN, rep: 0, epoch: 126, acc: 0.6581192162632942\n",
      "RNN, rep: 0, epoch: 127, acc: 0.6575446262955665\n",
      "RNN, rep: 0, epoch: 128, acc: 0.6583737328648567\n",
      "RNN, rep: 0, epoch: 129, acc: 0.6599675783514977\n",
      "RNN, rep: 0, epoch: 130, acc: 0.659591659605503\n",
      "RNN, rep: 0, epoch: 131, acc: 0.6603876912593841\n",
      "RNN, rep: 0, epoch: 132, acc: 0.6612045380473137\n",
      "RNN, rep: 0, epoch: 133, acc: 0.6603009593486786\n",
      "RNN, rep: 0, epoch: 134, acc: 0.6610098561644554\n",
      "RNN, rep: 0, epoch: 135, acc: 0.6616567462682724\n",
      "RNN, rep: 0, epoch: 136, acc: 0.661744669675827\n",
      "RNN, rep: 0, epoch: 137, acc: 0.6639915603399277\n",
      "RNN, rep: 0, epoch: 138, acc: 0.6615806537866592\n",
      "RNN, rep: 0, epoch: 139, acc: 0.6619901770353317\n",
      "RNN, rep: 0, epoch: 140, acc: 0.660385408103466\n",
      "RNN, rep: 0, epoch: 141, acc: 0.6616431209445\n",
      "RNN, rep: 0, epoch: 142, acc: 0.6625978076457977\n",
      "RNN, rep: 0, epoch: 143, acc: 0.6624100464582443\n",
      "RNN, rep: 0, epoch: 144, acc: 0.6612335142493247\n",
      "RNN, rep: 0, epoch: 145, acc: 0.659131502211094\n",
      "RNN, rep: 0, epoch: 146, acc: 0.6623383542895317\n",
      "RNN, rep: 0, epoch: 147, acc: 0.6638898396492005\n",
      "RNN, rep: 0, epoch: 148, acc: 0.6624526441097259\n",
      "RNN, rep: 0, epoch: 149, acc: 0.6629108107089996\n",
      "RNN, rep: 0, epoch: 150, acc: 0.6606255042552948\n",
      "RNN, rep: 0, epoch: 151, acc: 0.6619521233439446\n",
      "RNN, rep: 0, epoch: 152, acc: 0.6634846964478492\n",
      "RNN, rep: 0, epoch: 153, acc: 0.6631322649121284\n",
      "RNN, rep: 0, epoch: 154, acc: 0.6627475309371949\n",
      "RNN, rep: 0, epoch: 155, acc: 0.662865543961525\n",
      "RNN, rep: 0, epoch: 156, acc: 0.6632380798459053\n",
      "RNN, rep: 0, epoch: 157, acc: 0.6646563175320626\n",
      "RNN, rep: 0, epoch: 158, acc: 0.6628071197867393\n",
      "RNN, rep: 0, epoch: 159, acc: 0.6656973877549172\n",
      "RNN, rep: 0, epoch: 160, acc: 0.662541811466217\n",
      "RNN, rep: 0, epoch: 161, acc: 0.6626998671889305\n",
      "RNN, rep: 0, epoch: 162, acc: 0.6639505338668823\n",
      "RNN, rep: 0, epoch: 163, acc: 0.663897587954998\n",
      "RNN, rep: 0, epoch: 164, acc: 0.6636137217283249\n",
      "RNN, rep: 0, epoch: 165, acc: 0.662210765182972\n",
      "RNN, rep: 0, epoch: 166, acc: 0.6629871565103531\n",
      "RNN, rep: 0, epoch: 167, acc: 0.6678238245844841\n",
      "RNN, rep: 0, epoch: 168, acc: 0.6671502676606178\n",
      "RNN, rep: 0, epoch: 169, acc: 0.6695350849628449\n",
      "RNN, rep: 0, epoch: 170, acc: 0.6691630885004998\n",
      "RNN, rep: 0, epoch: 171, acc: 0.664062095284462\n",
      "RNN, rep: 0, epoch: 172, acc: 0.6627658742666245\n",
      "RNN, rep: 0, epoch: 173, acc: 0.6640354132652283\n",
      "RNN, rep: 0, epoch: 174, acc: 0.6681903213262558\n",
      "RNN, rep: 0, epoch: 175, acc: 0.6663469392061233\n",
      "RNN, rep: 0, epoch: 176, acc: 0.6666816821694375\n",
      "RNN, rep: 0, epoch: 177, acc: 0.6637646788358689\n",
      "RNN, rep: 0, epoch: 178, acc: 0.6652910777926445\n",
      "RNN, rep: 0, epoch: 179, acc: 0.6607657328248024\n",
      "RNN, rep: 0, epoch: 180, acc: 0.6643310663104057\n",
      "RNN, rep: 0, epoch: 181, acc: 0.6669189295172692\n",
      "RNN, rep: 0, epoch: 182, acc: 0.6634824472665787\n",
      "RNN, rep: 0, epoch: 183, acc: 0.6658211243152619\n",
      "RNN, rep: 0, epoch: 184, acc: 0.6647678443789482\n",
      "RNN, rep: 0, epoch: 185, acc: 0.6648793497681618\n",
      "RNN, rep: 0, epoch: 186, acc: 0.6644202914834022\n",
      "RNN, rep: 0, epoch: 187, acc: 0.6658468151092529\n",
      "RNN, rep: 0, epoch: 188, acc: 0.659053663611412\n",
      "RNN, rep: 0, epoch: 189, acc: 0.6653694093227387\n",
      "RNN, rep: 0, epoch: 190, acc: 0.6629449611902237\n",
      "RNN, rep: 0, epoch: 191, acc: 0.6677697932720185\n",
      "RNN, rep: 0, epoch: 192, acc: 0.6673374959826469\n",
      "RNN, rep: 0, epoch: 193, acc: 0.6665163663029671\n",
      "RNN, rep: 0, epoch: 194, acc: 0.6650511875748635\n",
      "RNN, rep: 0, epoch: 195, acc: 0.667515658736229\n",
      "RNN, rep: 0, epoch: 196, acc: 0.6664002838730813\n",
      "RNN, rep: 0, epoch: 197, acc: 0.6675311243534088\n",
      "RNN, rep: 0, epoch: 198, acc: 0.6658401307463646\n",
      "RNN, rep: 0, epoch: 199, acc: 0.667021258175373\n",
      "RNN, rep: 0, epoch: 200, acc: 0.6645151114463806\n",
      "RNN, rep: 0, epoch: 201, acc: 0.666619111597538\n",
      "RNN, rep: 0, epoch: 202, acc: 0.664872477054596\n",
      "RNN, rep: 0, epoch: 203, acc: 0.6425516280531883\n",
      "RNN, rep: 0, epoch: 204, acc: 0.6626890307664871\n",
      "RNN, rep: 0, epoch: 205, acc: 0.6670434042811394\n",
      "RNN, rep: 0, epoch: 206, acc: 0.6649091830849647\n",
      "RNN, rep: 0, epoch: 207, acc: 0.667002614736557\n",
      "RNN, rep: 0, epoch: 208, acc: 0.6647145077586174\n",
      "RNN, rep: 0, epoch: 209, acc: 0.6661168643832207\n",
      "RNN, rep: 0, epoch: 210, acc: 0.6661318632960319\n",
      "RNN, rep: 0, epoch: 211, acc: 0.665653301179409\n",
      "RNN, rep: 0, epoch: 212, acc: 0.6655030080676079\n",
      "RNN, rep: 0, epoch: 213, acc: 0.6656737479567528\n",
      "RNN, rep: 0, epoch: 214, acc: 0.665597605407238\n",
      "RNN, rep: 0, epoch: 215, acc: 0.6675136747956276\n",
      "RNN, rep: 0, epoch: 216, acc: 0.6670784199237824\n",
      "RNN, rep: 0, epoch: 217, acc: 0.6650976505875588\n",
      "RNN, rep: 0, epoch: 218, acc: 0.6648264425992966\n",
      "RNN, rep: 0, epoch: 219, acc: 0.6674200969934464\n",
      "RNN, rep: 0, epoch: 220, acc: 0.6637787073850632\n",
      "RNN, rep: 0, epoch: 221, acc: 0.6640436673164367\n",
      "RNN, rep: 0, epoch: 222, acc: 0.6662641993165016\n",
      "RNN, rep: 0, epoch: 223, acc: 0.6656756287813187\n",
      "RNN, rep: 0, epoch: 224, acc: 0.6648794391751289\n",
      "RNN, rep: 0, epoch: 225, acc: 0.6658241847157478\n",
      "RNN, rep: 0, epoch: 226, acc: 0.6666248181462288\n",
      "RNN, rep: 0, epoch: 227, acc: 0.663673291504383\n",
      "RNN, rep: 0, epoch: 228, acc: 0.6663332307338714\n",
      "RNN, rep: 0, epoch: 229, acc: 0.6651876851916313\n",
      "RNN, rep: 0, epoch: 230, acc: 0.6652130025625229\n",
      "RNN, rep: 0, epoch: 231, acc: 0.6652155363559723\n",
      "RNN, rep: 0, epoch: 232, acc: 0.6672897359728813\n",
      "RNN, rep: 0, epoch: 233, acc: 0.665993258357048\n",
      "RNN, rep: 0, epoch: 234, acc: 0.6682529392838478\n",
      "RNN, rep: 0, epoch: 235, acc: 0.6532435488700866\n",
      "RNN, rep: 0, epoch: 236, acc: 0.6674234107136726\n",
      "RNN, rep: 0, epoch: 237, acc: 0.6671347627043724\n",
      "RNN, rep: 0, epoch: 238, acc: 0.6642906621098519\n",
      "RNN, rep: 0, epoch: 239, acc: 0.6678952518105506\n",
      "RNN, rep: 0, epoch: 240, acc: 0.6665876817703247\n",
      "RNN, rep: 0, epoch: 241, acc: 0.6702441155910492\n",
      "RNN, rep: 0, epoch: 242, acc: 0.6663653892278671\n",
      "RNN, rep: 0, epoch: 243, acc: 0.6659260275959968\n",
      "RNN, rep: 0, epoch: 244, acc: 0.6646233612298965\n",
      "RNN, rep: 0, epoch: 245, acc: 0.6664253640174865\n",
      "RNN, rep: 0, epoch: 246, acc: 0.6665597978234291\n",
      "RNN, rep: 0, epoch: 247, acc: 0.6650855436921119\n",
      "RNN, rep: 0, epoch: 248, acc: 0.6670784661173821\n",
      "RNN, rep: 0, epoch: 249, acc: 0.6651405146718026\n",
      "RNN, rep: 0, epoch: 250, acc: 0.6616226986050606\n",
      "RNN, rep: 0, epoch: 251, acc: 0.6664825835824013\n",
      "RNN, rep: 0, epoch: 252, acc: 0.6659915235638618\n",
      "RNN, rep: 0, epoch: 253, acc: 0.6688894566893577\n",
      "RNN, rep: 0, epoch: 254, acc: 0.6710206151008606\n",
      "RNN, rep: 0, epoch: 255, acc: 0.6670787200331688\n",
      "RNN, rep: 0, epoch: 256, acc: 0.6679762101173401\n",
      "RNN, rep: 0, epoch: 257, acc: 0.6686471775174141\n",
      "RNN, rep: 0, epoch: 258, acc: 0.6672350877523422\n",
      "RNN, rep: 0, epoch: 259, acc: 0.6630448845028877\n",
      "RNN, rep: 0, epoch: 260, acc: 0.6707423302531242\n",
      "RNN, rep: 0, epoch: 261, acc: 0.6737052497267723\n",
      "RNN, rep: 0, epoch: 262, acc: 0.6701856353878974\n",
      "RNN, rep: 0, epoch: 263, acc: 0.6762051673233509\n",
      "RNN, rep: 0, epoch: 264, acc: 0.6618404716253281\n",
      "RNN, rep: 0, epoch: 265, acc: 0.6757376706600189\n",
      "RNN, rep: 0, epoch: 266, acc: 0.6689017930626869\n",
      "RNN, rep: 0, epoch: 267, acc: 0.675336044728756\n",
      "RNN, rep: 0, epoch: 268, acc: 0.6753694048523903\n",
      "RNN, rep: 0, epoch: 269, acc: 0.6761068466305733\n",
      "RNN, rep: 0, epoch: 270, acc: 0.6842243087291717\n",
      "RNN, rep: 0, epoch: 271, acc: 0.7243583241105079\n",
      "RNN, rep: 0, epoch: 272, acc: 0.7497197137773037\n",
      "RNN, rep: 0, epoch: 273, acc: 0.7372355955839157\n",
      "RNN, rep: 0, epoch: 274, acc: 0.6744790785014629\n",
      "RNN, rep: 0, epoch: 275, acc: 0.7146684464812278\n",
      "RNN, rep: 0, epoch: 276, acc: 0.7277872188389302\n",
      "RNN, rep: 0, epoch: 277, acc: 0.7365688641369342\n",
      "RNN, rep: 0, epoch: 278, acc: 0.7340059208869935\n",
      "RNN, rep: 0, epoch: 279, acc: 0.7223917376995087\n",
      "RNN, rep: 0, epoch: 280, acc: 0.7195303012430668\n",
      "RNN, rep: 0, epoch: 281, acc: 0.7430087268352509\n",
      "RNN, rep: 0, epoch: 282, acc: 0.7492072302103042\n",
      "RNN, rep: 0, epoch: 283, acc: 0.7573118329048156\n",
      "RNN, rep: 0, epoch: 284, acc: 0.768100840896368\n",
      "RNN, rep: 0, epoch: 285, acc: 0.7702345672249794\n",
      "RNN, rep: 0, epoch: 286, acc: 0.7894030953943729\n",
      "RNN, rep: 0, epoch: 287, acc: 0.7996089930832386\n",
      "RNN, rep: 0, epoch: 288, acc: 0.7778607040643692\n",
      "RNN, rep: 0, epoch: 289, acc: 0.7992103971540928\n",
      "RNN, rep: 0, epoch: 290, acc: 0.8105549328029156\n",
      "RNN, rep: 0, epoch: 291, acc: 0.8169246409833432\n",
      "RNN, rep: 0, epoch: 292, acc: 0.8200237528979778\n",
      "RNN, rep: 0, epoch: 293, acc: 0.8245506335794925\n",
      "RNN, rep: 0, epoch: 294, acc: 0.8270576621592045\n",
      "RNN, rep: 0, epoch: 295, acc: 0.8224939674139022\n",
      "RNN, rep: 0, epoch: 296, acc: 0.8300873413681984\n",
      "RNN, rep: 0, epoch: 297, acc: 0.8351273506879806\n",
      "RNN, rep: 0, epoch: 298, acc: 0.8406736552715302\n",
      "RNN, rep: 0, epoch: 299, acc: 0.8397354310750962\n",
      "RNN, rep: 0, epoch: 300, acc: 0.8398090600967407\n",
      "RNN, rep: 0, epoch: 301, acc: 0.843066180050373\n",
      "RNN, rep: 0, epoch: 302, acc: 0.8486128696799278\n",
      "RNN, rep: 0, epoch: 303, acc: 0.8586339325457811\n",
      "RNN, rep: 0, epoch: 304, acc: 0.85885428160429\n",
      "RNN, rep: 0, epoch: 305, acc: 0.8590799807012082\n",
      "RNN, rep: 0, epoch: 306, acc: 0.861345746293664\n",
      "RNN, rep: 0, epoch: 307, acc: 0.8711158512532711\n",
      "RNN, rep: 0, epoch: 308, acc: 0.8700740154087544\n",
      "RNN, rep: 0, epoch: 309, acc: 0.8756883543729782\n",
      "RNN, rep: 0, epoch: 310, acc: 0.8771673416346312\n",
      "RNN, rep: 0, epoch: 311, acc: 0.8755818931013346\n",
      "RNN, rep: 0, epoch: 312, acc: 0.883274593576789\n",
      "RNN, rep: 0, epoch: 313, acc: 0.8857492356747388\n",
      "RNN, rep: 0, epoch: 314, acc: 0.8912426602095366\n",
      "RNN, rep: 0, epoch: 315, acc: 0.8644349618256092\n",
      "RNN, rep: 0, epoch: 316, acc: 0.816234835088253\n",
      "RNN, rep: 0, epoch: 317, acc: 0.8390630802512169\n",
      "RNN, rep: 0, epoch: 318, acc: 0.8907673205435276\n",
      "RNN, rep: 0, epoch: 319, acc: 0.8936074053496122\n",
      "RNN, rep: 0, epoch: 320, acc: 0.8968857696652413\n",
      "RNN, rep: 0, epoch: 321, acc: 0.9028734082728624\n",
      "RNN, rep: 0, epoch: 322, acc: 0.9022142273187638\n",
      "RNN, rep: 0, epoch: 323, acc: 0.9030121161043644\n",
      "RNN, rep: 0, epoch: 324, acc: 0.9080247769504786\n",
      "RNN, rep: 0, epoch: 325, acc: 0.9092026430368424\n",
      "RNN, rep: 0, epoch: 326, acc: 0.9111890882253647\n",
      "RNN, rep: 0, epoch: 327, acc: 0.9141053010895849\n",
      "RNN, rep: 0, epoch: 328, acc: 0.9177354304119945\n",
      "RNN, rep: 0, epoch: 329, acc: 0.9168548400327563\n",
      "RNN, rep: 0, epoch: 330, acc: 0.9229007415473461\n",
      "RNN, rep: 0, epoch: 331, acc: 0.9260085308924317\n",
      "RNN, rep: 0, epoch: 332, acc: 0.928346615768969\n",
      "RNN, rep: 0, epoch: 333, acc: 0.9305363181605935\n",
      "RNN, rep: 0, epoch: 334, acc: 0.9305666434392333\n",
      "RNN, rep: 0, epoch: 335, acc: 0.9341421405225993\n",
      "RNN, rep: 0, epoch: 336, acc: 0.9328090188279748\n",
      "RNN, rep: 0, epoch: 337, acc: 0.9352387952432036\n",
      "RNN, rep: 0, epoch: 338, acc: 0.9382499795034528\n",
      "RNN, rep: 0, epoch: 339, acc: 0.9430298858508468\n",
      "RNN, rep: 0, epoch: 340, acc: 0.9417526188492775\n",
      "RNN, rep: 0, epoch: 341, acc: 0.9443650292232633\n",
      "RNN, rep: 0, epoch: 342, acc: 0.9481914453580975\n",
      "RNN, rep: 0, epoch: 343, acc: 0.9468341550044715\n",
      "RNN, rep: 0, epoch: 344, acc: 0.9484181717783212\n",
      "RNN, rep: 0, epoch: 345, acc: 0.951940210480243\n",
      "RNN, rep: 0, epoch: 346, acc: 0.9532265034876763\n",
      "RNN, rep: 0, epoch: 347, acc: 0.9554342459514737\n",
      "RNN, rep: 0, epoch: 348, acc: 0.9561441146023572\n",
      "RNN, rep: 0, epoch: 349, acc: 0.958952367901802\n",
      "RNN, rep: 0, epoch: 350, acc: 0.9602298265881837\n",
      "RNN, rep: 0, epoch: 351, acc: 0.9608583161234856\n",
      "RNN, rep: 0, epoch: 352, acc: 0.9614371139369905\n",
      "RNN, rep: 0, epoch: 353, acc: 0.9641375637799502\n",
      "RNN, rep: 0, epoch: 354, acc: 0.9655836475640536\n",
      "RNN, rep: 0, epoch: 355, acc: 0.963886602949351\n",
      "RNN, rep: 0, epoch: 356, acc: 0.965925498791039\n",
      "RNN, rep: 0, epoch: 357, acc: 0.9676280329190194\n",
      "RNN, rep: 0, epoch: 358, acc: 0.9674650429561734\n",
      "RNN, rep: 0, epoch: 359, acc: 0.9683362199552357\n",
      "RNN, rep: 0, epoch: 360, acc: 0.9711861707270145\n",
      "RNN                  Rep: 0   Epoch: 360   Acc: 0.9712 Params: min_length: 40, max_length: 45, fill: 0, value_1: -1, value_2: 1 Time: 152.58 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.49899990260601046\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.5024422782659531\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5008130034804344\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5023353749513626\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.4977352759242058\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.5032163807749748\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.4977256774902344\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.5008684176206589\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.49896735697984695\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.5009483754634857\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.5001843744516372\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.5002623224258422\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.5018734583258628\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.5053912806510925\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.5131307062506676\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.5782296949625015\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.6082836112380028\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.6072872871160507\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.6207420417666435\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.6256693440675736\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.6331688389182091\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.6589975926280022\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.7063240399956703\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.7309630660712719\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.746455372273922\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.7553745594620704\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.7668761737644673\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.7698883083462715\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.7733122588694096\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.7811156685650349\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.782222638130188\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.7857259938120842\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.7904821889102459\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.7944307704269886\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.7903559592366218\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.7972142370045185\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.7992717912793159\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.8004620614647865\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.8042112445831299\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.7742572198808193\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.7869864593446255\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.8153435190021991\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.8157282653450966\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.8233302707970143\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.8363325123488903\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.8251398758590222\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.8390620478987694\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.8420104676485062\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.8468289773911237\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.8464460384100676\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.8491824293136596\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.853799816519022\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.8562165396660566\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.8589546722918748\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.8562761336565018\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.8669161786139011\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.8668175844848156\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.8668513082712889\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.8690438183397055\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.8759567610174418\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.8643377400934696\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.871505921408534\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.8723546697944403\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.8823014618828893\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.8805220982059836\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.885145956017077\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.8752926805615425\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.8529874312132597\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.8875646265968681\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.8739220487698912\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.8954525814577937\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.8873382100462913\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.8795569424703717\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.8801919276639819\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.8858862871676684\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.886573373824358\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.8935946459323167\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.8950789013504982\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.884999586418271\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.8998910891637206\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.809830229319632\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.7473994842544198\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.7856506348401308\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.7988164748623967\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.7460894811153412\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.7753139160946012\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.765683185607195\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.8041710016876459\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.8560054346174002\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.8661343264579773\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.8757038725167513\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.8827254941314459\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.8812233149260282\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.8878886636719108\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.8982174724340439\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.8983154626935721\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.8958509005978703\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.9036333149671555\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.9119612320885062\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.9159594750031829\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.9113128892704845\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.9187338788062334\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.9199660920724273\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.9215060902386903\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.9231093366071582\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.925008382089436\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.9302845526859165\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.9320884690433741\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.9359252644330263\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.9396352906525135\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.9399953234568238\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.9398339519277215\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.9386424440145492\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.9346289430931211\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.9428985414840281\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.9438532631844282\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.9476063302718103\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.9495677566714584\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.9464206047914923\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.9506650280766189\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.9537802746146917\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.9559334718622268\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.9559919714741408\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.9575467003136873\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.9556391174346208\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.9588982970640063\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.9607333996333182\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.9628511891700328\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.9576979598775506\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.9623759750276804\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.965286343600601\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.9639352423883975\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.960843660812825\n",
      "NetRNNWithAttention, rep: 0, epoch: 134, acc: 0.9658405641466379\n",
      "NetRNNWithAttention, rep: 0, epoch: 135, acc: 0.9658464901708066\n",
      "NetRNNWithAttention, rep: 0, epoch: 136, acc: 0.9668359766155481\n",
      "NetRNNWithAttention, rep: 0, epoch: 137, acc: 0.9689443396776914\n",
      "NetRNNWithAttention, rep: 0, epoch: 138, acc: 0.9698667722195387\n",
      "NetRNNWithAttention, rep: 0, epoch: 139, acc: 0.9707293065451086\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 139   Acc: 0.9707 Params: min_length: 40, max_length: 45, fill: 0, value_1: -1, value_2: 1 Time: 68.90 sec\n"
     ]
    }
   ],
   "source": [
    "collectorA = dict()\n",
    "num_samples = 100\n",
    "for rep in range(1):\n",
    "    for params in parameters_list:\n",
    "        params_str = \", \".join([f\"{key}: {value}\" for key, value in params.items()])\n",
    "        for kind in [\"RNN\",\"LSTM\",\"GRU\",\"NetRNNWithAttention\",\"NetLSTMWithAttention\", \"NetGRUWithAttention\" ]:\n",
    "            if kind == \"RNN\":\n",
    "                model = NetRNN(hidden_dim=12, inp=3)\n",
    "            if kind == \"NetRNNWithAttention\":\n",
    "                model = NetRNNWithAttention(hidden_dim=12, inp=3)\n",
    "            optimizer = optim.Adam(model.parameters())\n",
    "            error = nn.MSELoss()\n",
    "            acc = 0.0\n",
    "            W = []\n",
    "            A = []\n",
    "            start_time = time.time()  # Start time of the epoch\n",
    "            while acc < 0.97:\n",
    "                model.resetHidden()\n",
    "                sequences, targets = generateTrainData(num_samples, params)\n",
    "\n",
    "                divs = []\n",
    "                for seq, target in zip(sequences, targets):\n",
    "                #for i, seq in enumerate(sequences):\n",
    "                    optimizer.zero_grad()\n",
    "                    seq_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "                    target_tensor = torch.tensor(target, dtype=torch.float32).unsqueeze(0)  # Add batch dimension to target as well\n",
    "                    \n",
    "                    \n",
    "                    output = model(seq_tensor)\n",
    "                    #if output.shape != target_tensor.shape:\n",
    "                    #    output = output.squeeze()  # Adjusting the output shape\n",
    "                    loss = error(output, target_tensor)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    div = output.detach().numpy() - target_tensor.numpy()\n",
    "                    divs.append(1.0 - abs(div).mean())\n",
    "\n",
    "                W.append(loss.item())\n",
    "                acc = mean(divs)\n",
    "                A.append(acc)\n",
    "                print(f\"{kind}, rep: {rep}, epoch: {len(A) }, acc: {acc}\")\n",
    "\n",
    "                # Restart training if not converging\n",
    "                # if acc < 0.97 and len(A) > 2000:\n",
    "                #     if kind == \"RNN\":\n",
    "                #         model = NetRNN(hidden_dim=12, inp=3)\n",
    "                #     if kind == \"NetRNNWithAttention\":\n",
    "                #         model = NetRNNWithAttention(hidden_dim=12, inp=3)\n",
    "                #     optimizer = optim.Adam(model.parameters())\n",
    "                #     acc = 0.0\n",
    "                #     W = []\n",
    "                #     A = []\n",
    "                #     print(\"repeat\")\n",
    "            end_time = time.time()  # End time of the epoch\n",
    "            epoch_duration = end_time - start_time  # Calculate duration\n",
    "            collectorA[\"{0} {1}\".format(kind, rep)] = A\n",
    "            params_save_str = \" \".join([f\"{key}_{value}\" for key, value in params.items()])\n",
    "            torch.save(model, f'models/model_{kind}_{params_save_str}.model')\n",
    "            print(f\"{kind:<20} Rep: {rep:<3} Epoch: {len(A):<5} Acc: {acc:.4f} \" f\"Params: {params_str:<40} Time: {epoch_duration:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:01:12.582743400Z",
     "start_time": "2023-12-06T17:01:12.545745700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./models/model_RNN_min_length_5 max_length_5 fill_0 value_1_-1 value_2_1.model\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "params = {\n",
    "            'min_length': 5, \n",
    "            'max_length': 5, \n",
    "            'fill': 0, \n",
    "            'value_1': -1, \n",
    "            'value_2': 1\n",
    "        }\n",
    "\n",
    "    \n",
    "def calculate_accuracy(predictions, targets):\n",
    "    # Ensure predictions and targets are the same shape\n",
    "    predictions = predictions.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    # Round predictions to the nearest integer (0 or 1)\n",
    "    predictions = predictions.round()\n",
    "\n",
    "    # Calculate the number of correct predictions\n",
    "    correct = (predictions == targets).float()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = correct.sum() / correct.numel()  # Use numel() instead of len()\n",
    "    return acc.item()\n",
    "\n",
    "\n",
    "# Generate test data\n",
    "sequences, targets = generateTrainData(100, params)  # You can use a different function for test data\n",
    "\n",
    "# Convert sequences and targets to tensors and pad sequences\n",
    "seq_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "padded_seq_tensors = pad_sequence(seq_tensors, batch_first=True)\n",
    "target_tensors = torch.tensor(targets, dtype=torch.float32).squeeze()\n",
    "\n",
    "# Evaluate the model on test data\n",
    "with torch.no_grad():\n",
    "    total_acc = 0.0\n",
    "    for seq_tensor, target_tensor in zip(padded_seq_tensors, target_tensors):\n",
    "        output = model(seq_tensor.unsqueeze(0))  # Add batch dimension\n",
    "        acc = calculate_accuracy(output, target_tensor)\n",
    "        total_acc += acc\n",
    "\n",
    "    # Calculate average accuracy\n",
    "    avg_acc = total_acc / len(padded_seq_tensors)\n",
    "    print(f\"Average Test Accuracy: {avg_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:50:29.630985800Z",
     "start_time": "2023-12-06T17:50:29.440984500Z"
    }
   },
   "outputs": [],
   "source": [
    "def symbolEntropy(D,base=2):\n",
    "    value,counts = numpy.unique(D, return_counts=True)\n",
    "    return entropy(counts,base=base)\n",
    "\n",
    "def computeTransmissionHfast(I,H,O,maskC,maskNC,iMult=2,oMult=2):\n",
    "    #print(\"I H O\",I.shape,H.shape,O.shape)\n",
    "    B=numpy.bitwise_and(H,maskNC)\n",
    "    IB=(B*iMult)+I\n",
    "    AB=H#numpy.bitwise_and(H,maskC+maskNC)\n",
    "    BO=(B*oMult)+O\n",
    "    IAB=(AB*iMult)+I\n",
    "    IBO=(B*(iMult*oMult))+(I*oMult)+O\n",
    "    ABO=(AB*oMult)+O\n",
    "    IABO=(AB*(iMult*oMult))+(I*oMult)+O\n",
    "    hB=symbolEntropy(B, base=2)\n",
    "    hIB=symbolEntropy(IB, base=2)\n",
    "    hAB=symbolEntropy(AB, base=2)\n",
    "    hBO=symbolEntropy(BO, base=2)\n",
    "    hIAB=symbolEntropy(IAB, base=2)\n",
    "    hIBO=symbolEntropy(IBO, base=2)\n",
    "    hABO=symbolEntropy(ABO, base=2)\n",
    "    hIABO=symbolEntropy(IABO, base=2)\n",
    "    #-H(B)+H(IB)+H(AB)+H(BO)-H(IAB)-H(IBO)-H(ABO)+H(IABO)\n",
    "    #print(hB,hIB,hAB,hBO,hIAB,hIBO,hABO,hIABO)\n",
    "    return-hB+hIB+hAB+hBO-hIAB-hIBO-hABO+hIABO\n",
    "\n",
    "def singleShrinkingDecompositionInformation(I,H,O,width,iMult=2,oMult=2):\n",
    "    nodes=list(range(width))\n",
    "    cols=[]\n",
    "    colh=[]\n",
    "    while len(nodes)>0:\n",
    "        infos=[]\n",
    "        for node in nodes:\n",
    "            subset=copy.deepcopy(nodes)\n",
    "            subset.remove(node)\n",
    "            maskA=0\n",
    "            for s in subset:\n",
    "                maskA+=1*(2**s)\n",
    "            maskA=int(maskA)\n",
    "            maskB=numpy.bitwise_and(numpy.bitwise_not(maskA),((2**width)-1))\n",
    "            h=computeTransmissionHfast(I,H,O,maskA,maskB,iMult=iMult,oMult=oMult)\n",
    "            infos.append(h)\n",
    "        nodeToDrop=nodes[infos.index(max(infos))]\n",
    "        nodes.remove(nodeToDrop)\n",
    "        cols.append(copy.deepcopy(nodes))\n",
    "        colh.append(max(infos))\n",
    "    return cols,colh\n",
    "\n",
    "def getOutTaH(model,dataSet):\n",
    "    O,H=model.step(torch.Tensor(dataSet))\n",
    "    #print(H.shape,H.min(),H.max())\n",
    "    #figure()\n",
    "    #hist(H.flatten())\n",
    "    H=H.transpose()\n",
    "    O=O.transpose()\n",
    "    B=numpy.zeros(H.shape)\n",
    "    clusterNr=2\n",
    "    for i in range(B.shape[0]):\n",
    "        a=H[i].reshape(-1, 1)\n",
    "        if len(numpy.unique(a))==1:\n",
    "            who=numpy.random.randint(len(a))\n",
    "            a[who]=1-a[who]\n",
    "        kmeans = KMeans(n_clusters=clusterNr).fit(a)\n",
    "        B[i]=kmeans.labels_\n",
    "        #B[i]=1.0*(H[i]>numpy.median(H[i]))\n",
    "\n",
    "\n",
    "    H=numpy.zeros((H.shape))\n",
    "    for i in range(12):\n",
    "        H+=B[i]*(clusterNr**i)\n",
    "    H=H.astype((int))\n",
    "    return O,H\n",
    "\n",
    "def shrinkingDecompositionInformation(model,width,dataSet,target,numbers=[0,1,2],whichTS=5,dsLength=8):\n",
    "    output,H=getOutTaH(model,dataSet)\n",
    "    output=output.transpose()[whichTS::dsLength].transpose()\n",
    "    #print(\"target.shape\",target.shape,\"output.shape\",output.shape,\"H.shape\",H.shape,\"dataset.shape\",dataSet.shape)\n",
    "    H=H.transpose()[whichTS::dsLength].transpose()\n",
    "    #target=target.transpose()[whichTS::dsLength].transpose()\n",
    "    #print(H.shape,target.shape,numpy.array(range(512))[whichTS::dsLength])\n",
    "    collectorSet=dict()\n",
    "    collectorH=dict()\n",
    "    for number in numbers:\n",
    "        I=target[number].astype(int)\n",
    "        O=(1.0*(output[number]>0.5)).astype(int)\n",
    "        #print(\"O\",O,\"T\",target[number])\n",
    "        #print(number,\"I.shape\",I.shape,\"O.shape\",O.shape,\"H.shape\",H.shape)\n",
    "        s,h=singleShrinkingDecompositionInformation(I,H,O,width)\n",
    "        collectorSet[number]=s\n",
    "        collectorH[number]=h\n",
    "    return collectorSet,collectorH\n",
    "\n",
    "def removalIntoVec(res,width,H):\n",
    "    V=numpy.zeros(width)\n",
    "    #for i,r in enumerate(res):\n",
    "    #    for e in r:\n",
    "    #        V[e]+=H[0]-H[i]\n",
    "    fullSet=list(range(width))\n",
    "    nRes=copy.deepcopy(res)\n",
    "    nRes.insert(0,fullSet)\n",
    "    nodeList=[]\n",
    "    for i in range(width):\n",
    "        removedNode=list(set(nRes[i])-set(nRes[i+1]))[0]\n",
    "        nodeList.append(removedNode)\n",
    "    for i,node in enumerate(nodeList):\n",
    "        V[node]=H[0]-H[i]\n",
    "    #V=sqrt(V)\n",
    "    if V.sum()==0:\n",
    "        return V\n",
    "    return V#/V.max()\n",
    "\n",
    "def removalIntoMatrix(res,width,H):\n",
    "    M=[]\n",
    "    for i in range(len(res)):\n",
    "        M.append(removalIntoVec(res[i],width,H[i]))\n",
    "    return numpy.array(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:20:00.458639400Z",
     "start_time": "2023-12-06T18:19:58.361140500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.340117468876707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.7126643734656324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.9754957138064104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.9795669569232222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.9878848269371867\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./Models/model_RNN_min_length_5 max_length_5 fill_0 value_1_-1 value_2_1.model\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "params = {\n",
    "            'min_length': 5, \n",
    "            'max_length': 5, \n",
    "            'fill': 0, \n",
    "            'value_1': -1, \n",
    "            'value_2': 1\n",
    "        }\n",
    "\n",
    "# Run the analysis\n",
    "for i in range(5):\n",
    "    s, t = generateTrainData(100, params)  # Use your generateTrainData function\n",
    "    # Visualize the results\n",
    "    S,H=shrinkingDecompositionInformation(model,12,s,t.transpose(),whichTS=i,dsLength=5)\n",
    "    subplot(6,2,i+1)\n",
    "    M=removalIntoMatrix(S,12,H)\n",
    "    imshow(M)\n",
    "    print(M.min(),M.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.006404620456178289 0.2681953410059519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.06397692423332924 0.3513068361813434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.16434386671074064 0.979208511469825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.13196566760620243 0.9697344247051172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.11588844995402692 0.9927481269466885\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./Models/model_NetRNNWithAttention_min_length_5 max_length_5 fill_0 value_1_-1 value_2_1.model\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "params = {\n",
    "            'min_length': 5, \n",
    "            'max_length': 5, \n",
    "            'fill': 0, \n",
    "            'value_1': -1, \n",
    "            'value_2': 1\n",
    "        }\n",
    "\n",
    "# Run the analysis\n",
    "for i in range(5):\n",
    "    s, t = generateTrainData(100, params)  # Use your generateTrainData function\n",
    "    # Visualize the results\n",
    "    S,H=shrinkingDecompositionInformation(model,12,s,t.transpose(),whichTS=i,dsLength=5)\n",
    "    subplot(6,2,i+1)\n",
    "    M=removalIntoMatrix(S,12,H)\n",
    "    imshow(M)\n",
    "    print(M.min(),M.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2657c45ad10>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "modelRNN=torch.load(\"models/model_NetRNNWithAttention_min_length_5 max_length_5 fill_0 value_1_-1 value_2_1.model\")\n",
    "modelRNNWithAttention=torch.load(\"models/model_RNN_min_length_5 max_length_5 fill_0 value_1_-1 value_2_1.model\")\n",
    "params = {\n",
    "            'min_length': 5, \n",
    "            'max_length': 5, \n",
    "            'fill': 0, \n",
    "            'value_1': -1, \n",
    "            'value_2': 1\n",
    "        }\n",
    "s, t = generateTrainData(100, params)  \n",
    "S,H=shrinkingDecompositionInformation(modelRNN,12,s,t.transpose(),whichTS=i,dsLength=5)\n",
    "figure()\n",
    "imshow(removalIntoMatrix(S,12,H))\n",
    "S,H=shrinkingDecompositionInformation(modelRNNWithAttention,12,s,t.transpose(),whichTS=i,dsLength=5)\n",
    "figure()\n",
    "imshow(removalIntoMatrix(S,12,H))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
