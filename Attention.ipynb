{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import numpy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib.pyplot import figure, subplots, imshow, xticks, yticks, title\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.cluster import mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "from statistics import mean\n",
    "from scipy.stats import entropy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim**0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted\n",
    "\n",
    "\n",
    "class NetRNNWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetRNNWithAttention, self).__init__()\n",
    "        self.attention = SelfAttention(inp)  # Attention layer with input dimension\n",
    "        self.rnnLayer = nn.RNN(inp, hidden_dim, batch_first=True)\n",
    "        self.outputLayer = nn.Linear(hidden_dim, 1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.resetHidden()\n",
    "        self.inp = inp\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applying attention first\n",
    "        attn_out = self.attention(x)\n",
    "        # Feeding the output of the attention layer into the RNN\n",
    "        h0 = torch.zeros(1, x.shape[0], self.hidden_dim)\n",
    "        rnn_out, _ = self.rnnLayer(attn_out, h0)\n",
    "        rnn_out = torch.tanh(rnn_out)\n",
    "\n",
    "        # Applying the final output layer\n",
    "        out = torch.sigmoid(self.outputLayer(rnn_out[:, -1, :])).squeeze()\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                # Feeding the output of the attention layer into the RNN\n",
    "                out, h0 = self.rnnLayer(attn_out, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)\n",
    "                out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "\n",
    "\n",
    "model = NetRNNWithAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRNN(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetRNN, self).__init__()\n",
    "        self.rnnLayer = nn.RNN(inp, hidden_dim, batch_first=True)\n",
    "        self.outputLayer = nn.Linear(hidden_dim, 1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.resetHidden()\n",
    "        self.inp = inp\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.h0 = torch.Tensor(numpy.zeros((1, x.shape[0], self.hidden_dim)))\n",
    "        out, self.h0 = self.rnnLayer(x, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "        out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.Tensor(numpy.zeros((1, 1, self.hidden_dim)))\n",
    "            for i in range(x.shape[1]):\n",
    "                out, h0 = self.rnnLayer(x[l][i].reshape((1, 1, self.inp)), h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "                # print(out.detach().numpy().flatten().shape)\n",
    "            out = torch.tanh(out)\n",
    "            out = torch.sigmoid(self.outputLayer(out[:, -1, :]))\n",
    "            for i in range(x.shape[1]):\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "                # print(out.detach().numpy().flatten().shape)\n",
    "        return numpy.array(O), numpy.array(H)\n",
    "\n",
    "\n",
    "model = NetRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTrainData(num_samples, min_length, max_length):\n",
    "    s = []  # Sequences (list of arrays)\n",
    "    t = []  # Targets (list of labels)\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Randomized common length between min_length and max_length\n",
    "        common_length = np.random.randint(min_length, max_length + 1)\n",
    "\n",
    "        # Given arrays\n",
    "        array_A = np.full(common_length, 0.5)\n",
    "        array_B = np.full(common_length, 0.5)\n",
    "        array_C = np.full(common_length, 0.5)\n",
    "\n",
    "        # Random index for array A\n",
    "        index_A = np.random.randint(common_length)\n",
    "        value_A = np.random.choice([0, 1])\n",
    "        array_A[index_A] = value_A\n",
    "\n",
    "        # Different random index for array B\n",
    "        indices_B = np.delete(\n",
    "            np.arange(common_length), index_A\n",
    "        )  # Removing the index used in Array A\n",
    "        index_B = np.random.choice(indices_B)\n",
    "        value_B = np.random.choice([0, 1])\n",
    "        array_B[index_B] = value_B\n",
    "\n",
    "        # Setting the last index of array C to either 0 or 1\n",
    "        value_C = np.random.choice([0, 1])\n",
    "        array_C[-1] = value_C\n",
    "\n",
    "        # Generating label based on value_C\n",
    "        label = int((value_A != value_B) if value_C == 0 else (value_A == value_B))\n",
    "        label_arr = [label]\n",
    "        # Combine arrays\n",
    "        combined_array = np.vstack([array_A, array_B, array_C]).T\n",
    "\n",
    "        s.append(combined_array)\n",
    "        t.append(label_arr)\n",
    "\n",
    "    return s, numpy.array(t)  # Returning as lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[0.5, 0.5, 0.5],\n",
       "         [0.5, 0. , 0.5],\n",
       "         [0.5, 0.5, 0.5],\n",
       "         [0. , 0.5, 0.5],\n",
       "         [0.5, 0.5, 1. ]]),\n",
       "  array([[0.5, 0.5, 0.5],\n",
       "         [0.5, 0.5, 0.5],\n",
       "         [0.5, 0.5, 0.5],\n",
       "         [0.5, 0.5, 0.5],\n",
       "         [1. , 0.5, 0.5],\n",
       "         [0.5, 0.5, 0.5],\n",
       "         [0.5, 0. , 0.5],\n",
       "         [0.5, 0.5, 0.5],\n",
       "         [0.5, 0.5, 0.5],\n",
       "         [0.5, 0.5, 1. ]])],\n",
       " array([[1],\n",
       "        [0]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_seq = 2\n",
    "array_min_length = 5\n",
    "array_max_length = 10\n",
    "sequences, labels = generateTrainData(num_seq, array_min_length, array_max_length)\n",
    "sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "NetRNNWithAttention(\n",
      "  (attention): SelfAttention(\n",
      "    (query): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (key): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (value): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (softmax): Softmax(dim=2)\n",
      "  )\n",
      "  (rnnLayer): RNN(3, 12, batch_first=True)\n",
      "  (outputLayer): Linear(in_features=12, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NetRNNWithAttention()\n",
    "sequences, labels = generateTrainData(100, 10, 10)\n",
    "output = model(torch.Tensor(sequences))\n",
    "print(output.shape)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x1500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming sequences and labels are already generated using generateTrainData\n",
    "num_samples = len(sequences)  # Number of samples to display\n",
    "\n",
    "# Plotting\n",
    "# Adjust the figure size to accommodate horizontal layout\n",
    "plt.figure(figsize=(3 * num_samples, 15))\n",
    "\n",
    "for i, (seq, label) in enumerate(zip(sequences, labels)):\n",
    "    # Reshape each sequence for visualization\n",
    "    reshaped_sequence = seq  # Use the sequence as it is\n",
    "\n",
    "    # Add a subplot for each sequence in a horizontal layout\n",
    "    ax = plt.subplot(1, num_samples, i + 1)\n",
    "    img = ax.imshow(reshaped_sequence, cmap=\"gray\", vmin=0.0, vmax=1.0)\n",
    "\n",
    "    # Determine the operation title based on the label\n",
    "    operation_title = \"XOR\" if label == 0 else \"XNOR\"\n",
    "\n",
    "    # Setting the title with the operation label\n",
    "    ax.set_title(f\"Sequence {i+1} - Operation: {operation_title}, {label}\")\n",
    "\n",
    "    # Setting labels for features (X-axis) and detailed time points (Y-axis)\n",
    "    ax.set_xlabel(\"Arrays (A, B, C)\")\n",
    "    ax.set_ylabel(\"Time Points\")\n",
    "\n",
    "    # Setting tick marks for each array on the X-axis\n",
    "    ax.set_xticks(range(3))\n",
    "    ax.set_xticklabels([\"A\", \"B\", \"C\"])\n",
    "\n",
    "    # Setting tick marks for each time point on the Y-axis\n",
    "    # Here, the number of ticks should be equal to the length of the sequence (number of rows)\n",
    "    ax.set_yticks(range(reshaped_sequence.shape[0]))\n",
    "    ax.set_yticklabels([f\"{j+1}\" for j in range(reshaped_sequence.shape[0])])\n",
    "\n",
    "# Adding a colorbar as the key, placed at the side\n",
    "# cbar_ax = plt.gcf().add_axes([0.93, 0.15, 0.02, 0.7])  # Adjust these values as needed for positioning\n",
    "# cbar = plt.colorbar(img, cax=cbar_ax)\n",
    "# cbar.set_ticks([0, 1])\n",
    "# cbar.set_ticklabels(['0 (Black)', '1 (White)'])\n",
    "\n",
    "# Adjust the main figure to make room for the colorbar\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, target in test_data:\n",
    "            seq_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0)\n",
    "            target_tensor = torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "            output = model(seq_tensor)\n",
    "            loss = criterion(output, target_tensor)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            predicted = (output >= 0.5).float()\n",
    "            correct += (predicted == target_tensor).sum().item()\n",
    "            total += target_tensor.size(0)\n",
    "\n",
    "    test_loss /= len(test_data)\n",
    "    test_accuracy = correct / total\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/87/pnb9p7_n30s29gprj1gbqbbm0000gn/T/ipykernel_69101/1135384333.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sequences = numpy.array(sequences)  # Convert to numpy array\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.49971693754196167\n",
      "acc: 0.5007854348421097\n",
      "acc: 0.5015515345335007\n",
      "acc: 0.49740520179271697\n",
      "acc: 0.4982327884435654\n",
      "acc: 0.4992725411057472\n",
      "acc: 0.49874116986989975\n",
      "acc: 0.49947957694530487\n",
      "acc: 0.498787496984005\n",
      "acc: 0.49888986587524414\n",
      "acc: 0.49834255754947665\n",
      "acc: 0.4994177010655403\n",
      "acc: 0.4976574179530144\n",
      "acc: 0.4996391648054123\n",
      "acc: 0.500681585073471\n",
      "acc: 0.49972247779369355\n",
      "acc: 0.5057710179686546\n",
      "acc: 0.5006762027740479\n",
      "acc: 0.5005372440814972\n",
      "acc: 0.49935747861862184\n",
      "acc: 0.5001687520742416\n",
      "acc: 0.5009673288464547\n",
      "acc: 0.49956880897283557\n",
      "acc: 0.5013512861728668\n",
      "acc: 0.5013872814178467\n",
      "acc: 0.5025703948736191\n",
      "acc: 0.4974604317545891\n",
      "acc: 0.49889081418514253\n",
      "acc: 0.49963635474443435\n",
      "acc: 0.49891698330640794\n",
      "acc: 0.49976319938898084\n",
      "acc: 0.49914450228214263\n",
      "acc: 0.49954500257968903\n",
      "acc: 0.49823315620422365\n",
      "acc: 0.4996211451292038\n",
      "acc: 0.49902799069881437\n",
      "acc: 0.4994273254275322\n",
      "acc: 0.5002330327033997\n",
      "acc: 0.5003901249170304\n",
      "acc: 0.4980689010024071\n",
      "acc: 0.5006771382689476\n",
      "acc: 0.49973357051610945\n",
      "acc: 0.49876832485198974\n",
      "acc: 0.5002306133508683\n",
      "acc: 0.4999002540111542\n",
      "acc: 0.49872507840394975\n",
      "acc: 0.5003201657533646\n",
      "acc: 0.4989013344049454\n",
      "acc: 0.5000660994648933\n",
      "acc: 0.5026394200325012\n",
      "acc: 0.5048276886343956\n",
      "acc: 0.4993940141797066\n",
      "acc: 0.4986082401871681\n",
      "acc: 0.5004100212454796\n",
      "acc: 0.4986354821920395\n",
      "acc: 0.5008270910382271\n",
      "acc: 0.4994488695263863\n",
      "acc: 0.499166040122509\n",
      "acc: 0.5015068247914314\n",
      "acc: 0.4980566835403442\n",
      "acc: 0.4990740445256233\n",
      "acc: 0.5008573439717293\n",
      "acc: 0.5002467283606529\n",
      "acc: 0.5015752002596855\n",
      "acc: 0.5012439748644829\n",
      "acc: 0.49899879068136216\n",
      "acc: 0.4994602540135384\n",
      "acc: 0.4994462925195694\n",
      "acc: 0.5002948850393295\n",
      "acc: 0.4991357833147049\n",
      "acc: 0.49995660185813906\n",
      "acc: 0.49903436601161955\n",
      "acc: 0.5005602943897247\n",
      "acc: 0.4985346060991287\n",
      "acc: 0.5000663098692893\n",
      "acc: 0.4990506961941719\n",
      "acc: 0.4998481523990631\n",
      "acc: 0.5014527100324631\n",
      "acc: 0.49875415027141573\n",
      "acc: 0.5009879553318024\n",
      "acc: 0.5009085792303085\n",
      "acc: 0.5012478494644165\n",
      "acc: 0.5013015395402909\n",
      "acc: 0.4983043974637985\n",
      "acc: 0.4988283121585846\n",
      "acc: 0.4990724587440491\n",
      "acc: 0.49950272172689436\n",
      "acc: 0.4997736129164696\n",
      "acc: 0.49966966301202775\n",
      "acc: 0.5040348279476166\n",
      "acc: 0.496516615152359\n",
      "acc: 0.5009061056375503\n",
      "acc: 0.5015436071157455\n",
      "acc: 0.49835564017295836\n",
      "acc: 0.49905431032180786\n",
      "acc: 0.5016799640655517\n",
      "acc: 0.5039676141738891\n",
      "acc: 0.5022865080833435\n",
      "acc: 0.5017535990476608\n",
      "acc: 0.49807279467582705\n",
      "acc: 0.5008783972263336\n",
      "acc: 0.49941169798374174\n",
      "acc: 0.5019021046161651\n",
      "acc: 0.5021516454219818\n",
      "acc: 0.4970434337854385\n",
      "acc: 0.5002070188522338\n",
      "acc: 0.4989789718389511\n",
      "acc: 0.4993949955701828\n",
      "acc: 0.5012070596218109\n",
      "acc: 0.49924448192119597\n",
      "acc: 0.4984483951330185\n",
      "acc: 0.5002413779497147\n",
      "acc: 0.5003278744220734\n",
      "acc: 0.5003999042510986\n",
      "acc: 0.4990007537603378\n",
      "acc: 0.49989654421806334\n",
      "acc: 0.5004168647527695\n",
      "acc: 0.5020643091201782\n",
      "acc: 0.49790511786937713\n",
      "acc: 0.4995122003555298\n",
      "acc: 0.4994420862197876\n",
      "acc: 0.4996234661340713\n",
      "acc: 0.4989354205131531\n",
      "acc: 0.4996940168738365\n",
      "acc: 0.500723406970501\n",
      "acc: 0.5027846294641495\n",
      "acc: 0.4984058570861816\n",
      "acc: 0.4980124083161354\n",
      "acc: 0.4995928859710693\n",
      "acc: 0.501345947086811\n",
      "acc: 0.5007713636755944\n",
      "acc: 0.5016873523592948\n",
      "acc: 0.5019772443175315\n",
      "acc: 0.49836781442165373\n",
      "acc: 0.5027442082762719\n",
      "acc: 0.5090345832705497\n",
      "acc: 0.4932311049103737\n",
      "acc: 0.4973474943637848\n",
      "acc: 0.5003930515050888\n",
      "acc: 0.5005100032687188\n",
      "acc: 0.49865547329187393\n",
      "acc: 0.4993205910921097\n",
      "acc: 0.49961632907390596\n",
      "acc: 0.4996333807706833\n",
      "acc: 0.5000881254673004\n",
      "acc: 0.4995932760834694\n",
      "acc: 0.4991111862659454\n",
      "acc: 0.4997632837295532\n",
      "acc: 0.50070880651474\n",
      "acc: 0.5034090667963028\n",
      "acc: 0.501462749838829\n",
      "acc: 0.49928576946258546\n",
      "acc: 0.5005180829763413\n",
      "acc: 0.49780816584825516\n",
      "acc: 0.5016320106387139\n",
      "acc: 0.49854526996612547\n",
      "acc: 0.5007489857077598\n",
      "acc: 0.5052885603904724\n",
      "acc: 0.4990341725945473\n",
      "acc: 0.4961545032262802\n",
      "acc: 0.4990012526512146\n",
      "acc: 0.5011441645026207\n",
      "acc: 0.49785899341106415\n",
      "acc: 0.49924145042896273\n",
      "acc: 0.4987772101163864\n",
      "acc: 0.49953035831451414\n",
      "acc: 0.500002290904522\n",
      "acc: 0.4998408478498459\n",
      "acc: 0.500359328687191\n",
      "acc: 0.5032673087716103\n",
      "acc: 0.4983145269751549\n",
      "acc: 0.5003520974516869\n",
      "acc: 0.4984170725941658\n",
      "acc: 0.49927185505628585\n",
      "acc: 0.5004755967855453\n",
      "acc: 0.498844168484211\n",
      "acc: 0.5003545385599136\n",
      "acc: 0.5008886912465096\n",
      "acc: 0.5019697210192681\n",
      "acc: 0.500095933675766\n",
      "acc: 0.4970181366801262\n",
      "acc: 0.4993686431646347\n",
      "acc: 0.4999368068575859\n",
      "acc: 0.4995136910676956\n",
      "acc: 0.5026673021912574\n",
      "acc: 0.4988203635811806\n",
      "acc: 0.5029065811634064\n",
      "acc: 0.5003610721230507\n",
      "acc: 0.4996961140632629\n",
      "acc: 0.5013661554455757\n",
      "acc: 0.49979422301054\n",
      "acc: 0.5033467155694962\n",
      "acc: 0.5002805414795876\n",
      "acc: 0.4976894515752792\n",
      "acc: 0.5022248372435569\n",
      "acc: 0.5074767830967903\n",
      "acc: 0.5020991283655166\n",
      "acc: 0.501109194457531\n",
      "acc: 0.49802628576755525\n",
      "acc: 0.5002459514141083\n",
      "acc: 0.4969827845692635\n",
      "acc: 0.5004647529125213\n",
      "acc: 0.5002034720778465\n",
      "acc: 0.49849469780921934\n",
      "acc: 0.4997188547253609\n",
      "acc: 0.49971066147089005\n",
      "acc: 0.4999672746658325\n",
      "acc: 0.5011102256178855\n",
      "acc: 0.501087791621685\n",
      "acc: 0.4996687880158424\n",
      "acc: 0.49909854114055635\n",
      "acc: 0.5021767872571945\n",
      "acc: 0.5048933663964271\n",
      "acc: 0.5045291030406952\n",
      "acc: 0.5051221668720245\n",
      "acc: 0.5183504387736321\n",
      "acc: 0.5116253405809402\n",
      "acc: 0.5170220622420311\n",
      "acc: 0.5298758727312088\n",
      "acc: 0.5412569865584373\n",
      "acc: 0.5526279067993164\n",
      "acc: 0.5458229291439056\n",
      "acc: 0.53945869743824\n",
      "acc: 0.5502116307616234\n",
      "acc: 0.5820619228482247\n",
      "acc: 0.5694635510444641\n",
      "acc: 0.5801685696840286\n",
      "acc: 0.5769520111382007\n",
      "acc: 0.5916505782306194\n",
      "acc: 0.614565259963274\n",
      "acc: 0.6273803307116032\n",
      "acc: 0.6180302965641021\n",
      "acc: 0.6322897703945637\n",
      "acc: 0.6382265222817659\n",
      "acc: 0.650117475092411\n",
      "acc: 0.6521068751811981\n",
      "acc: 0.6308406514674425\n",
      "acc: 0.6350252091884613\n",
      "acc: 0.6699809481203556\n",
      "acc: 0.6503974370658397\n",
      "acc: 0.689837587699294\n",
      "acc: 0.6871661770343781\n",
      "acc: 0.6761738436296583\n",
      "acc: 0.6967078534513712\n",
      "acc: 0.7098044291511179\n",
      "acc: 0.7380798906832934\n",
      "acc: 0.7611438919138164\n",
      "acc: 0.7408338938653469\n",
      "acc: 0.7698215328902006\n",
      "acc: 0.7477726988168434\n",
      "acc: 0.7857109084445983\n",
      "acc: 0.7763708230108023\n",
      "acc: 0.8055204955488443\n",
      "acc: 0.8212065567448735\n",
      "acc: 0.8319701571948827\n",
      "acc: 0.8435144306905568\n",
      "acc: 0.8416778374335263\n",
      "acc: 0.8484483568032738\n",
      "acc: 0.864366992812138\n",
      "acc: 0.863880108013982\n",
      "acc: 0.8806683341099415\n",
      "acc: 0.8776238987746183\n",
      "acc: 0.8955688915750943\n",
      "acc: 0.8938636734476313\n",
      "acc: 0.8691776930796914\n",
      "acc: 0.871861379124457\n",
      "acc: 0.8916263458598405\n",
      "acc: 0.8939312602329301\n",
      "acc: 0.8863172584213317\n",
      "acc: 0.9012454864781466\n",
      "acc: 0.916310283341445\n",
      "acc: 0.9285864310557372\n",
      "acc: 0.9087822503352072\n",
      "acc: 0.9303703652141848\n",
      "acc: 0.927361964462325\n",
      "acc: 0.9345253222156316\n",
      "acc: 0.9258041851490271\n",
      "acc: 0.9434623311937321\n",
      "acc: 0.9304674393337337\n",
      "acc: 0.9434260503941914\n",
      "acc: 0.9135076146374922\n",
      "acc: 0.9347164867061656\n",
      "acc: 0.9242932686244604\n",
      "acc: 0.9428874318208545\n",
      "acc: 0.9202925031245104\n",
      "acc: 0.923445244484974\n",
      "acc: 0.927370747293462\n",
      "acc: 0.926530029841233\n",
      "acc: 0.9451319633892854\n",
      "acc: 0.9414927484845975\n",
      "acc: 0.944188743182749\n",
      "acc: 0.9469291100092232\n",
      "acc: 0.9472594503497385\n",
      "acc: 0.9561549246974755\n",
      "acc: 0.934431406791482\n",
      "acc: 0.9461385490280372\n",
      "acc: 0.9434659287933027\n",
      "acc: 0.9312649055545625\n",
      "acc: 0.9538287443117588\n",
      "acc: 0.9557051423750818\n",
      "acc: 0.9538310927504426\n",
      "acc: 0.9569451882046996\n",
      "acc: 0.9322124559171789\n",
      "acc: 0.965192127198934\n",
      "acc: 0.9605095517456357\n",
      "acc: 0.957518312336324\n",
      "acc: 0.9651304572461231\n",
      "acc: 0.9654460093262969\n",
      "acc: 0.9651890605719018\n",
      "acc: 0.9583879424657789\n",
      "acc: 0.9520683579982869\n",
      "acc: 0.9568763794283587\n",
      "acc: 0.9261103906128119\n",
      "acc: 0.9639381609559314\n",
      "acc: 0.956065737239387\n",
      "acc: 0.9665316728131802\n",
      "acc: 0.9391371873162643\n",
      "acc: 0.9512637561777956\n",
      "acc: 0.9604736196514568\n",
      "acc: 0.9669655655807401\n",
      "acc: 0.9692471006981941\n",
      "acc: 0.9620391186628876\n",
      "acc: 0.9554569222914324\n",
      "acc: 0.9661745494408933\n",
      "acc: 0.9751141146764712\n",
      "NetRNNWithAttention 0 325\n",
      "acc: 0.5004364669322967\n",
      "acc: 0.4992841619253159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.5001458263397217\n",
      "acc: 0.504493522644043\n",
      "acc: 0.494122157394886\n",
      "acc: 0.4990120452642441\n",
      "acc: 0.49908666729927065\n",
      "acc: 0.4999968785047531\n",
      "acc: 0.5010291194915771\n",
      "acc: 0.4980049750208855\n",
      "acc: 0.49994400292634966\n",
      "acc: 0.502105659544468\n",
      "acc: 0.4944231927394867\n",
      "acc: 0.4997034439444542\n",
      "acc: 0.4995520776510239\n",
      "acc: 0.5014867067337037\n",
      "acc: 0.4966037139296532\n",
      "acc: 0.49874521493911744\n",
      "acc: 0.5004216134548187\n",
      "acc: 0.4982471165060997\n",
      "acc: 0.49855332463979724\n",
      "acc: 0.4975636053085327\n",
      "acc: 0.4991313925385475\n",
      "acc: 0.5003482106328011\n",
      "acc: 0.49860035479068754\n",
      "acc: 0.4991968297958374\n",
      "acc: 0.5008794111013413\n",
      "acc: 0.4985036903619766\n",
      "acc: 0.49912919849157333\n",
      "acc: 0.49959662556648254\n",
      "acc: 0.4994980603456497\n",
      "acc: 0.49869352489709856\n",
      "acc: 0.499599267244339\n",
      "acc: 0.5006216526031494\n",
      "acc: 0.4970171946287155\n",
      "acc: 0.498617856502533\n",
      "acc: 0.5012636414170265\n",
      "acc: 0.4965627670288086\n",
      "acc: 0.5002935457229615\n",
      "acc: 0.4994040471315384\n",
      "acc: 0.5041680884361267\n",
      "acc: 0.5031114995479584\n",
      "acc: 0.4990206980705261\n",
      "acc: 0.4995888802409172\n",
      "acc: 0.4996491751074791\n",
      "acc: 0.49566895455121995\n",
      "acc: 0.49885274827480314\n",
      "acc: 0.4987622720003128\n",
      "acc: 0.4987919461727142\n",
      "acc: 0.49933722347021103\n",
      "acc: 0.49881473541259763\n",
      "acc: 0.49886261105537416\n",
      "acc: 0.49838567823171614\n",
      "acc: 0.49953627556562424\n",
      "acc: 0.4993325546383858\n",
      "acc: 0.4998228406906128\n",
      "acc: 0.5019143584370613\n",
      "acc: 0.5003112858533859\n",
      "acc: 0.497649287879467\n",
      "acc: 0.5001650956273079\n",
      "acc: 0.5001621395349503\n",
      "acc: 0.5026365351676941\n",
      "acc: 0.497419596016407\n",
      "acc: 0.4983509451150894\n",
      "acc: 0.49988600343465805\n",
      "acc: 0.50103884100914\n",
      "acc: 0.5009460330009461\n",
      "acc: 0.500073467195034\n",
      "acc: 0.4996930694580078\n",
      "acc: 0.5018361443281174\n",
      "acc: 0.5033342733979225\n",
      "acc: 0.4988479268550873\n",
      "acc: 0.5057547008991241\n",
      "acc: 0.4968839693069458\n",
      "acc: 0.49819874167442324\n",
      "acc: 0.4995394110679626\n",
      "acc: 0.4974163341522217\n",
      "acc: 0.5001248157024384\n",
      "acc: 0.4989765548706055\n",
      "acc: 0.49935960739851\n",
      "acc: 0.5002881643176079\n",
      "acc: 0.4986141070723534\n",
      "acc: 0.5033550602197647\n",
      "acc: 0.4993360531330109\n",
      "acc: 0.5038494175672531\n",
      "acc: 0.49662765979766843\n",
      "acc: 0.5011337780952454\n",
      "acc: 0.4975831586122513\n",
      "acc: 0.4992111960053444\n",
      "acc: 0.49927505761384966\n",
      "acc: 0.4991386550664902\n",
      "acc: 0.49997524082660677\n",
      "acc: 0.500827602148056\n",
      "acc: 0.5001461058855057\n",
      "acc: 0.5003219413757324\n",
      "acc: 0.49847320139408113\n",
      "acc: 0.4999230885505676\n",
      "acc: 0.49983772546052935\n",
      "acc: 0.49970425099134447\n",
      "acc: 0.5019135609269142\n",
      "acc: 0.4978337809443474\n",
      "acc: 0.4979410755634308\n",
      "acc: 0.49990096360445024\n",
      "acc: 0.4995368087291718\n",
      "acc: 0.4990838348865509\n",
      "acc: 0.49948773950338365\n",
      "acc: 0.49893183797597884\n",
      "acc: 0.4987574577331543\n",
      "acc: 0.4985183733701706\n",
      "acc: 0.5001295572519302\n",
      "acc: 0.49899715453386306\n",
      "acc: 0.4996397966146469\n",
      "acc: 0.4991693383455276\n",
      "acc: 0.4980356603860855\n",
      "acc: 0.4993091356754303\n",
      "acc: 0.50115696310997\n",
      "acc: 0.5008893716335296\n",
      "acc: 0.5023504301905632\n",
      "acc: 0.49993230402469635\n",
      "acc: 0.49932793974876405\n",
      "acc: 0.4995653867721558\n",
      "acc: 0.5007771429419517\n",
      "acc: 0.49777035385370255\n",
      "acc: 0.49763264030218124\n",
      "acc: 0.5001074939966201\n",
      "acc: 0.4986803215742111\n",
      "acc: 0.5000164020061493\n",
      "acc: 0.4998800021409988\n",
      "acc: 0.5008126366138458\n",
      "acc: 0.5010872113704682\n",
      "acc: 0.5008212673664093\n",
      "acc: 0.497397837638855\n",
      "acc: 0.49993624091148375\n",
      "acc: 0.500697380900383\n",
      "acc: 0.49871022403240206\n",
      "acc: 0.49933100014925\n",
      "acc: 0.49973931670188904\n",
      "acc: 0.4995171931385994\n",
      "acc: 0.49932057827711107\n",
      "acc: 0.4997050443291664\n",
      "acc: 0.49961839348077774\n",
      "acc: 0.5000195893645286\n",
      "acc: 0.4993400079011917\n",
      "acc: 0.5003885006904603\n",
      "acc: 0.5029322648048401\n",
      "acc: 0.498465279340744\n",
      "acc: 0.5017560964822769\n",
      "acc: 0.49880205929279325\n",
      "acc: 0.4990028214454651\n",
      "acc: 0.500603594481945\n",
      "acc: 0.5019734826683998\n",
      "acc: 0.5024036481976509\n",
      "acc: 0.4980305016040802\n",
      "acc: 0.5004132187366486\n",
      "acc: 0.49922697007656097\n",
      "acc: 0.5022948384284973\n",
      "acc: 0.5002990704774857\n",
      "acc: 0.4976841568946838\n",
      "acc: 0.4978823181986809\n",
      "acc: 0.49790799140930175\n",
      "acc: 0.49920556753873824\n",
      "acc: 0.49979807674884796\n",
      "acc: 0.5001183503866196\n",
      "acc: 0.4989057710766792\n",
      "acc: 0.4994595518708229\n",
      "acc: 0.5028306719660759\n",
      "acc: 0.5031739777326584\n",
      "acc: 0.5057169619202614\n",
      "acc: 0.4975608724355698\n",
      "acc: 0.5004629901051522\n",
      "acc: 0.5073050644993782\n",
      "acc: 0.49930529087781905\n",
      "acc: 0.4975234919786453\n",
      "acc: 0.4976214987039566\n",
      "acc: 0.4997216010093689\n",
      "acc: 0.5002039614319801\n",
      "acc: 0.49960851430892944\n",
      "acc: 0.5022071784734726\n",
      "acc: 0.5006113329529762\n",
      "acc: 0.5028607207536697\n",
      "acc: 0.49934955149888993\n",
      "acc: 0.49981495678424837\n",
      "acc: 0.5046445089578628\n",
      "acc: 0.5027637434005737\n",
      "acc: 0.49745305329561235\n",
      "acc: 0.5028367745876312\n",
      "acc: 0.49880111128091814\n",
      "acc: 0.4979230272769928\n",
      "acc: 0.49901366531848906\n",
      "acc: 0.5045153871178627\n",
      "acc: 0.4973553058505058\n",
      "acc: 0.49950084209442136\n",
      "acc: 0.49932446479797366\n",
      "acc: 0.49920923680067064\n",
      "acc: 0.49976617872714996\n",
      "acc: 0.499749356508255\n",
      "acc: 0.4988323774933815\n",
      "acc: 0.49983517289161683\n",
      "acc: 0.49927811801433564\n",
      "acc: 0.4994501692056656\n",
      "acc: 0.49991613149642944\n",
      "acc: 0.4992485460639\n",
      "acc: 0.5001053357124329\n",
      "acc: 0.4995884871482849\n",
      "acc: 0.5009406322240829\n",
      "acc: 0.4992637062072754\n",
      "acc: 0.4996593910455704\n",
      "acc: 0.49904285967350004\n",
      "acc: 0.5002877080440521\n",
      "acc: 0.4993570789694786\n",
      "acc: 0.49976011395454406\n",
      "acc: 0.5008228945732117\n",
      "acc: 0.5008023437857628\n",
      "acc: 0.49934912025928496\n",
      "acc: 0.5017480424046517\n",
      "acc: 0.49792480617761614\n",
      "acc: 0.4986948877573013\n",
      "acc: 0.5002243599295616\n",
      "acc: 0.4985362109541893\n",
      "acc: 0.5001177337765693\n",
      "acc: 0.495246140062809\n",
      "acc: 0.4982373896241188\n",
      "acc: 0.49920091420412066\n",
      "acc: 0.4989982041716576\n",
      "acc: 0.5001912996172905\n",
      "acc: 0.5007517623901367\n",
      "acc: 0.49503362596035005\n",
      "acc: 0.49980262577533724\n",
      "acc: 0.49825119465589524\n",
      "acc: 0.4995323723554611\n",
      "acc: 0.49962684720754624\n",
      "acc: 0.49851867228746416\n",
      "acc: 0.4985323712229729\n",
      "acc: 0.504473987519741\n",
      "acc: 0.5021617412567139\n",
      "acc: 0.4968397730588913\n",
      "acc: 0.49818712085485456\n",
      "acc: 0.4986655423045158\n",
      "acc: 0.5011059325933457\n",
      "acc: 0.5007289072871208\n",
      "acc: 0.500645074248314\n",
      "acc: 0.4980707463622093\n",
      "acc: 0.5009089139103889\n",
      "acc: 0.5039005890488625\n",
      "acc: 0.5031499603390693\n",
      "acc: 0.500868801176548\n",
      "acc: 0.4967144602537155\n",
      "acc: 0.4987143078446388\n",
      "acc: 0.4990593522787094\n",
      "acc: 0.5006009364128112\n",
      "acc: 0.5021171802282334\n",
      "acc: 0.49973843574523924\n",
      "acc: 0.49831659227609637\n",
      "acc: 0.5014486521482467\n",
      "acc: 0.4996698445081711\n",
      "acc: 0.5005191546678543\n",
      "acc: 0.4998160535097122\n",
      "acc: 0.5002957293391228\n",
      "acc: 0.4986369961500168\n",
      "acc: 0.4978961196541786\n",
      "acc: 0.4999524822831154\n",
      "acc: 0.5038195529580116\n",
      "acc: 0.5025074851512908\n",
      "acc: 0.49706451207399366\n",
      "acc: 0.5009966778755188\n",
      "acc: 0.4973652449250221\n",
      "acc: 0.5031500980257988\n",
      "acc: 0.49908518731594087\n",
      "acc: 0.5016922202706336\n",
      "acc: 0.5018130519986153\n",
      "acc: 0.4985364407300949\n",
      "acc: 0.4995456817746162\n",
      "acc: 0.5019866967201233\n",
      "acc: 0.5044193848967552\n",
      "acc: 0.502914128601551\n",
      "acc: 0.49180914044380186\n",
      "acc: 0.49988167941570283\n",
      "acc: 0.4996203336119652\n",
      "acc: 0.4978431102633476\n",
      "acc: 0.500598840713501\n",
      "acc: 0.4992572423815727\n",
      "acc: 0.5006687399744988\n",
      "acc: 0.4976931828260422\n",
      "acc: 0.497969745695591\n",
      "acc: 0.499610333442688\n",
      "acc: 0.49967267125844955\n",
      "acc: 0.49979457050561904\n",
      "acc: 0.500495497584343\n",
      "acc: 0.49779480934143067\n",
      "acc: 0.49888138949871064\n",
      "acc: 0.49933160543441774\n",
      "acc: 0.4990095639228821\n",
      "acc: 0.5000513255596161\n",
      "acc: 0.5001478800177575\n",
      "acc: 0.4997166708111763\n",
      "acc: 0.5009282177686691\n",
      "acc: 0.5034937235713005\n",
      "acc: 0.5005759081244469\n",
      "acc: 0.49711250931024553\n",
      "acc: 0.4981858566403389\n",
      "acc: 0.49991234332323076\n",
      "acc: 0.5001512405276298\n",
      "acc: 0.4986849939823151\n",
      "acc: 0.4997028714418411\n",
      "acc: 0.49942893385887144\n",
      "acc: 0.49971172779798506\n",
      "acc: 0.5020294860005379\n",
      "acc: 0.4988246536254883\n",
      "acc: 0.49881167978048324\n",
      "acc: 0.5020914736390114\n",
      "acc: 0.4999150523543358\n",
      "acc: 0.4996932554244995\n",
      "acc: 0.5002673161029816\n",
      "acc: 0.4978401571512222\n",
      "acc: 0.4997924089431763\n",
      "acc: 0.49913551807403567\n",
      "acc: 0.4996472439169884\n",
      "acc: 0.4997734996676445\n",
      "acc: 0.5069579726457596\n",
      "acc: 0.5007560500502586\n",
      "acc: 0.4979627749323845\n",
      "acc: 0.4989833989739418\n",
      "acc: 0.4994426915049553\n",
      "acc: 0.4998541697859764\n",
      "acc: 0.5000474530458451\n",
      "acc: 0.498951076567173\n",
      "acc: 0.4998269191384315\n",
      "acc: 0.5001895594596862\n",
      "acc: 0.5019296690821647\n",
      "acc: 0.4974955335259438\n",
      "acc: 0.4997353234887123\n",
      "acc: 0.49941023588180544\n",
      "acc: 0.5000618049502372\n",
      "acc: 0.4990699595212936\n",
      "acc: 0.5000615057349205\n",
      "acc: 0.49919302493333817\n",
      "acc: 0.5001737761497498\n",
      "acc: 0.5003530210256577\n",
      "acc: 0.5015010613203049\n",
      "acc: 0.49749499320983886\n",
      "acc: 0.4995522633194923\n",
      "acc: 0.49954429775476455\n",
      "acc: 0.5005050763487816\n",
      "acc: 0.5033031541109085\n",
      "acc: 0.5003828009963036\n",
      "acc: 0.5004677078127862\n",
      "acc: 0.5009740757942199\n",
      "acc: 0.5023057734966279\n",
      "acc: 0.5002585476636887\n",
      "acc: 0.4967949452996254\n",
      "acc: 0.49687315076589583\n",
      "acc: 0.4985746815800667\n",
      "acc: 0.5014607441425324\n",
      "acc: 0.4973196467757225\n",
      "acc: 0.5000157651305198\n",
      "acc: 0.5041563326120376\n",
      "acc: 0.4995502534508705\n",
      "acc: 0.4975434422492981\n",
      "acc: 0.5001340663433075\n",
      "acc: 0.500327481329441\n",
      "acc: 0.49948293894529344\n",
      "acc: 0.5001067739725112\n",
      "acc: 0.5042395719885826\n",
      "acc: 0.49981737077236177\n",
      "acc: 0.4986302024126053\n",
      "acc: 0.4982141768932343\n",
      "acc: 0.49945894837379456\n",
      "acc: 0.49912285566329956\n",
      "acc: 0.5004878959059715\n",
      "acc: 0.49927015483379367\n",
      "acc: 0.499879866540432\n",
      "acc: 0.5000702556967735\n",
      "acc: 0.4999733638763428\n",
      "acc: 0.5029167947173119\n",
      "acc: 0.4998534008860588\n",
      "acc: 0.5005588012933732\n",
      "acc: 0.5018189352750778\n",
      "acc: 0.49656210154294966\n",
      "acc: 0.49958159774541855\n",
      "acc: 0.4994503417611122\n",
      "acc: 0.4998346325755119\n",
      "acc: 0.5000469905138015\n",
      "acc: 0.4992321500182152\n",
      "acc: 0.4999726819992065\n",
      "acc: 0.5003124618530274\n",
      "acc: 0.49982445418834687\n",
      "acc: 0.5008104169368743\n",
      "acc: 0.5002114880084991\n",
      "acc: 0.49869895219802857\n",
      "acc: 0.5001254403591155\n",
      "acc: 0.49923712730407716\n",
      "acc: 0.4989371311664581\n",
      "acc: 0.49958628982305525\n",
      "acc: 0.5006313940882683\n",
      "acc: 0.49974080741405486\n",
      "acc: 0.49953572571277616\n",
      "acc: 0.49932925671339035\n",
      "acc: 0.5000161230564117\n",
      "acc: 0.4996219700574875\n",
      "acc: 0.49977620005607604\n",
      "acc: 0.5009141957759857\n",
      "acc: 0.4998039489984512\n",
      "acc: 0.49938766062259676\n",
      "acc: 0.49918011486530306\n",
      "acc: 0.49988896429538726\n",
      "acc: 0.5001867842674256\n",
      "acc: 0.5000162124633789\n",
      "acc: 0.4995506343245506\n",
      "acc: 0.4991862532496452\n",
      "acc: 0.4992643415927887\n",
      "acc: 0.4993721088767052\n",
      "acc: 0.5002211314439774\n",
      "acc: 0.5000530934333801\n",
      "acc: 0.5005335628986358\n",
      "acc: 0.49972023993730547\n",
      "acc: 0.49860129088163374\n",
      "acc: 0.499554443359375\n",
      "acc: 0.5005962985754013\n",
      "acc: 0.5008405297994614\n",
      "acc: 0.49858070611953736\n",
      "acc: 0.5005569583177567\n",
      "acc: 0.5012858939170838\n",
      "acc: 0.5029218161106109\n",
      "acc: 0.4970517963171005\n",
      "acc: 0.5013478058576584\n",
      "acc: 0.49964185178279874\n",
      "acc: 0.49844366431236264\n",
      "acc: 0.4988663598895073\n",
      "acc: 0.5003184261918068\n",
      "acc: 0.5000176578760147\n",
      "acc: 0.4992821264266968\n",
      "acc: 0.49941961735486984\n",
      "acc: 0.4996219804883003\n",
      "acc: 0.4999491447210312\n",
      "acc: 0.4998115262389183\n",
      "acc: 0.4998470628261566\n",
      "acc: 0.4994546377658844\n",
      "acc: 0.49963371455669403\n",
      "acc: 0.5001802384853363\n",
      "acc: 0.4991126099228859\n",
      "acc: 0.4991729247570038\n",
      "acc: 0.49957904785871504\n",
      "acc: 0.4995675176382065\n",
      "acc: 0.4998534369468689\n",
      "acc: 0.5000167652964592\n",
      "acc: 0.4997907531261444\n",
      "acc: 0.49967989027500154\n",
      "acc: 0.49979885697364806\n",
      "acc: 0.5000929391384125\n",
      "acc: 0.5001012900471687\n",
      "acc: 0.5004812610149384\n",
      "acc: 0.49942852795124054\n",
      "acc: 0.4995918893814087\n",
      "acc: 0.4994824430346489\n",
      "acc: 0.5015280011296273\n",
      "acc: 0.49959249317646026\n",
      "acc: 0.49979372262954713\n",
      "acc: 0.49936084270477293\n",
      "acc: 0.5005967262387275\n",
      "acc: 0.5014550223946571\n",
      "acc: 0.4983713585138321\n",
      "acc: 0.499536789059639\n",
      "acc: 0.49892335027456286\n",
      "acc: 0.49946947127580643\n",
      "acc: 0.49981649190187455\n",
      "acc: 0.49992073357105254\n",
      "acc: 0.49963523060083387\n",
      "acc: 0.49919554382562636\n",
      "acc: 0.499810950756073\n",
      "acc: 0.4996992999315262\n",
      "acc: 0.5000558716058731\n",
      "acc: 0.5003540030121804\n",
      "acc: 0.5028629517555236\n",
      "acc: 0.4982642793655396\n",
      "acc: 0.49798772364854815\n",
      "acc: 0.49964767128229143\n",
      "acc: 0.49977271288633346\n",
      "acc: 0.4996198427677154\n",
      "acc: 0.49938027024269105\n",
      "acc: 0.4997034305334091\n",
      "acc: 0.5004175090789795\n",
      "acc: 0.4987618836760521\n",
      "acc: 0.49956399738788604\n",
      "acc: 0.5027311414480209\n",
      "acc: 0.49836880624294283\n",
      "acc: 0.49928355276584624\n",
      "acc: 0.49999113380908966\n",
      "acc: 0.49914956867694854\n",
      "acc: 0.5002895295619965\n",
      "acc: 0.4994607189297676\n",
      "acc: 0.5012395918369293\n",
      "acc: 0.4995489889383316\n",
      "acc: 0.49879266440868375\n",
      "acc: 0.49940367370843886\n",
      "acc: 0.5001131129264832\n",
      "acc: 0.499970069527626\n",
      "acc: 0.50082848072052\n",
      "acc: 0.4994845449924469\n",
      "acc: 0.4990953016281128\n",
      "acc: 0.499644413292408\n",
      "acc: 0.49989898890256884\n",
      "acc: 0.49997510939836504\n",
      "acc: 0.49901659429073336\n",
      "acc: 0.5001218068599701\n",
      "acc: 0.5005366757512093\n",
      "acc: 0.5005049887299537\n",
      "acc: 0.5005680620670319\n",
      "acc: 0.49945702761411664\n",
      "acc: 0.5002035915851593\n",
      "acc: 0.49864471465349197\n",
      "acc: 0.5012780490517617\n",
      "acc: 0.5002607968449593\n",
      "acc: 0.5011806493997574\n",
      "acc: 0.4993883368372917\n",
      "acc: 0.5037515041232109\n",
      "acc: 0.5012831711769103\n",
      "acc: 0.4959941756725311\n",
      "acc: 0.4981199210882187\n",
      "acc: 0.50225037753582\n",
      "acc: 0.5001420783996582\n",
      "acc: 0.4962711387872696\n",
      "acc: 0.4986574926972389\n",
      "acc: 0.5003883588314056\n",
      "acc: 0.49929635614156725\n",
      "acc: 0.4986444479227066\n",
      "acc: 0.498766662478447\n",
      "acc: 0.5014840149879456\n",
      "acc: 0.5000237458944321\n",
      "acc: 0.49704847544431685\n",
      "acc: 0.4992404708266258\n",
      "acc: 0.49948792815208437\n",
      "acc: 0.5007091715931893\n",
      "acc: 0.49854991912841795\n",
      "acc: 0.49889917314052584\n",
      "acc: 0.5004495784640313\n",
      "acc: 0.5005977848172187\n",
      "acc: 0.4998872590065002\n",
      "acc: 0.49753419250249864\n",
      "acc: 0.5016650366783142\n",
      "acc: 0.5000118663907052\n",
      "acc: 0.503488945364952\n",
      "acc: 0.5036082604527473\n",
      "acc: 0.5021270823478698\n",
      "acc: 0.5010225144028664\n",
      "acc: 0.5008542931079865\n",
      "acc: 0.4969159898161888\n",
      "acc: 0.5032161968946457\n",
      "acc: 0.49809660881757734\n",
      "acc: 0.5025353628396988\n",
      "acc: 0.499375556409359\n",
      "acc: 0.5004133757948875\n",
      "acc: 0.5069503402709961\n",
      "acc: 0.49927928030490876\n",
      "acc: 0.5019264206290245\n",
      "acc: 0.49656735092401505\n",
      "acc: 0.49811267852783203\n",
      "acc: 0.5000260987877846\n",
      "acc: 0.4992479053139687\n",
      "acc: 0.4976602286100388\n",
      "acc: 0.5005283218622207\n",
      "acc: 0.5007279413938522\n",
      "acc: 0.5011482459306716\n",
      "acc: 0.501004589498043\n",
      "acc: 0.4979196813702583\n",
      "acc: 0.498845696747303\n",
      "acc: 0.4987182143330574\n",
      "acc: 0.502865434885025\n",
      "acc: 0.49999842077493667\n",
      "acc: 0.5003460690379142\n",
      "acc: 0.4965825328230858\n",
      "acc: 0.49802306056022644\n",
      "acc: 0.4992899462580681\n",
      "acc: 0.5012644743919372\n",
      "acc: 0.500674566924572\n",
      "acc: 0.5012605112791061\n",
      "acc: 0.5042933270335197\n",
      "acc: 0.5052596679329873\n",
      "acc: 0.500620704293251\n",
      "acc: 0.49668763190507886\n",
      "acc: 0.5023828551173211\n",
      "acc: 0.501189424097538\n",
      "acc: 0.5063188731670379\n",
      "acc: 0.49800850689411164\n",
      "acc: 0.49422611892223356\n",
      "acc: 0.4977435463666916\n",
      "acc: 0.5003967368602753\n",
      "acc: 0.4984149619936943\n",
      "acc: 0.5007202389836312\n",
      "acc: 0.49876089841127397\n",
      "acc: 0.49981487542390823\n",
      "acc: 0.49974109113216403\n",
      "acc: 0.5010624307394028\n",
      "acc: 0.5007120123505593\n",
      "acc: 0.5004986181855202\n",
      "acc: 0.5117359820008278\n",
      "acc: 0.5080729880928994\n",
      "acc: 0.49966010719537735\n",
      "acc: 0.49754253834486006\n",
      "acc: 0.4953605043888092\n",
      "acc: 0.49976672261953353\n",
      "acc: 0.4985038283467293\n",
      "acc: 0.4993695768713951\n",
      "acc: 0.49768793940544126\n",
      "acc: 0.5016478052735329\n",
      "acc: 0.4998472034931183\n",
      "acc: 0.49729863941669467\n",
      "acc: 0.49914068371057513\n",
      "acc: 0.49925589382648466\n",
      "acc: 0.5002772098779679\n",
      "acc: 0.49791079938411714\n",
      "acc: 0.49876333117485044\n",
      "acc: 0.49955920368433\n",
      "acc: 0.5020943987369537\n",
      "acc: 0.4986651912331581\n",
      "acc: 0.4992789953947067\n",
      "acc: 0.4988356545567513\n",
      "acc: 0.5013424527645111\n",
      "acc: 0.5001785120368004\n",
      "acc: 0.4991062277555466\n",
      "acc: 0.501080470085144\n",
      "acc: 0.5008282455801963\n",
      "acc: 0.5028048780560493\n",
      "acc: 0.4999507662653923\n",
      "acc: 0.506828425526619\n",
      "acc: 0.49707643896341325\n",
      "acc: 0.49820240676403044\n",
      "acc: 0.49910045862197877\n",
      "acc: 0.499115035533905\n",
      "acc: 0.5007423532009124\n",
      "acc: 0.50267782330513\n",
      "acc: 0.4976861268281937\n",
      "acc: 0.4980302095413208\n",
      "acc: 0.5017042279243469\n",
      "acc: 0.4985117098689079\n",
      "acc: 0.4999071916937828\n",
      "acc: 0.49891485035419464\n",
      "acc: 0.49854085326194764\n",
      "acc: 0.5008025738596916\n",
      "acc: 0.4991206642985344\n",
      "acc: 0.5012197503447533\n",
      "acc: 0.5005899450182915\n",
      "acc: 0.5005051904916763\n",
      "acc: 0.49638171672821046\n",
      "acc: 0.49915778458118437\n",
      "acc: 0.4992249810695648\n",
      "acc: 0.5004358124732972\n",
      "acc: 0.5001342648267746\n",
      "acc: 0.5027215594053268\n",
      "acc: 0.49662961274385453\n",
      "acc: 0.5024724066257477\n",
      "acc: 0.5013372561335564\n",
      "acc: 0.49791407138109206\n",
      "acc: 0.49708139181137084\n",
      "acc: 0.500009987950325\n",
      "acc: 0.5015948048233986\n",
      "acc: 0.4984725096821785\n",
      "acc: 0.500722513794899\n",
      "acc: 0.49884124636650085\n",
      "acc: 0.49867943227291106\n",
      "acc: 0.4991444745659828\n",
      "acc: 0.5015907654166222\n",
      "acc: 0.5059441536664963\n",
      "acc: 0.5019818592071533\n",
      "acc: 0.5039190557599068\n",
      "acc: 0.5061306273937225\n",
      "acc: 0.5039661288261413\n",
      "acc: 0.506578406393528\n",
      "acc: 0.5026074522733688\n",
      "acc: 0.4945046231150627\n",
      "acc: 0.49641183972358705\n",
      "acc: 0.5024637228250504\n",
      "acc: 0.49860130339860914\n",
      "acc: 0.4990196794271469\n",
      "acc: 0.49960605084896087\n",
      "acc: 0.5009247073531151\n",
      "acc: 0.502565521299839\n",
      "acc: 0.4990159398317337\n",
      "acc: 0.5020239317417144\n",
      "acc: 0.5026862496137618\n",
      "acc: 0.4976465407013893\n",
      "acc: 0.4966004267334938\n",
      "acc: 0.5018891891837121\n",
      "acc: 0.4992512360215187\n",
      "acc: 0.49927355736494067\n",
      "acc: 0.49822111874818803\n",
      "acc: 0.5013769459724426\n",
      "acc: 0.4981356406211853\n",
      "acc: 0.5004207742214203\n",
      "acc: 0.4996743482351303\n",
      "acc: 0.5003771159052849\n",
      "acc: 0.502927757203579\n",
      "acc: 0.49931849628686903\n",
      "acc: 0.500076767206192\n",
      "acc: 0.4988377931714058\n",
      "acc: 0.49890672594308855\n",
      "acc: 0.5023877418041229\n",
      "acc: 0.495328281223774\n",
      "acc: 0.5011513558030128\n",
      "acc: 0.5000599703192711\n",
      "acc: 0.5008617043495178\n",
      "acc: 0.4995118537545204\n",
      "acc: 0.4979841765761375\n",
      "acc: 0.4981733897328377\n",
      "acc: 0.5009913226962089\n",
      "acc: 0.49780090630054474\n",
      "acc: 0.5002347028255463\n",
      "acc: 0.5002541607618332\n",
      "acc: 0.5008222180604934\n",
      "acc: 0.4992934665083885\n",
      "acc: 0.49873306423425673\n",
      "acc: 0.5003012773394585\n",
      "acc: 0.4984320837259293\n",
      "acc: 0.5003605961799622\n",
      "acc: 0.5023210674524308\n",
      "acc: 0.5005279642343521\n",
      "acc: 0.4982451996207237\n",
      "acc: 0.5003232055902481\n",
      "acc: 0.5007117915153504\n",
      "acc: 0.4979062667489052\n",
      "acc: 0.5015986946225166\n",
      "acc: 0.5002398744225502\n",
      "acc: 0.49773564785718916\n",
      "acc: 0.49982219070196154\n",
      "acc: 0.4991969493031502\n",
      "acc: 0.500847886800766\n",
      "acc: 0.4999275079369545\n",
      "acc: 0.4991224333643913\n",
      "acc: 0.49962472409009934\n",
      "acc: 0.4986553877592087\n",
      "acc: 0.49827208191156386\n",
      "acc: 0.4996373388171196\n",
      "acc: 0.49969384968280794\n",
      "acc: 0.5006723266839981\n",
      "acc: 0.498329883813858\n",
      "acc: 0.5004375192523003\n",
      "acc: 0.5024800100922584\n",
      "acc: 0.5029914596676827\n",
      "acc: 0.49869371205568314\n",
      "acc: 0.5026906901597976\n",
      "acc: 0.4947343733906746\n",
      "acc: 0.5028551816940308\n",
      "acc: 0.4966627162694931\n",
      "acc: 0.5000082039833069\n",
      "acc: 0.4978522738814354\n",
      "acc: 0.49898541629314425\n",
      "acc: 0.5001620092988014\n",
      "acc: 0.49806368589401245\n",
      "acc: 0.4992805233597755\n",
      "acc: 0.5014887547492981\n",
      "acc: 0.503093752861023\n",
      "acc: 0.5004592877626419\n",
      "acc: 0.4989319384098053\n",
      "acc: 0.503480136692524\n",
      "acc: 0.49598111808300016\n",
      "acc: 0.5040875235199929\n",
      "acc: 0.5018542265892029\n",
      "acc: 0.5017491507530213\n",
      "acc: 0.4951401114463806\n",
      "acc: 0.5016652724146843\n",
      "acc: 0.4982860293984413\n",
      "acc: 0.49777393162250516\n",
      "acc: 0.5002597987651825\n",
      "acc: 0.5044176876544952\n",
      "acc: 0.5061946219205856\n",
      "acc: 0.497940217256546\n",
      "acc: 0.4995583489537239\n",
      "acc: 0.5046863493323326\n",
      "acc: 0.5004132264852523\n",
      "acc: 0.5012081360816956\n",
      "acc: 0.5056405851244926\n",
      "acc: 0.4992836472392082\n",
      "acc: 0.49903838723897936\n",
      "acc: 0.4968220371007919\n",
      "acc: 0.4993610927462578\n",
      "acc: 0.5004214018583297\n",
      "acc: 0.4995343899726868\n",
      "acc: 0.49905847638845446\n",
      "acc: 0.49880222022533416\n",
      "acc: 0.4988760554790497\n",
      "acc: 0.49979961305856707\n",
      "acc: 0.5004088145494461\n",
      "acc: 0.4995400556921959\n",
      "acc: 0.4992241084575653\n",
      "acc: 0.5025678551197053\n",
      "acc: 0.5003817895054817\n",
      "acc: 0.49864968448877334\n",
      "acc: 0.4984668588638306\n",
      "acc: 0.5007686004042625\n",
      "acc: 0.500464363694191\n",
      "acc: 0.4999567657709122\n",
      "acc: 0.502002403140068\n",
      "acc: 0.4992036727070808\n",
      "acc: 0.498759092092514\n",
      "acc: 0.5002534094452858\n",
      "acc: 0.5008473062515258\n",
      "acc: 0.4985098150372505\n",
      "acc: 0.5013973546028138\n",
      "acc: 0.4982543629407883\n",
      "acc: 0.5015600499510765\n",
      "acc: 0.5005763176083565\n",
      "acc: 0.4961265861988068\n",
      "acc: 0.499551422894001\n",
      "acc: 0.501659263074398\n",
      "acc: 0.5038450491428376\n",
      "acc: 0.49893981277942656\n",
      "acc: 0.5012263453006744\n",
      "acc: 0.4987924736738205\n",
      "acc: 0.4978382578492165\n",
      "acc: 0.49827093780040743\n",
      "acc: 0.49900820016860964\n",
      "acc: 0.5039590549468994\n",
      "acc: 0.5050314342975617\n",
      "acc: 0.49955070167779925\n",
      "acc: 0.5007841283082962\n",
      "acc: 0.49895209789276124\n",
      "acc: 0.49807128340005874\n",
      "acc: 0.4997877275943756\n",
      "acc: 0.5021477484703064\n",
      "acc: 0.5025645798444748\n",
      "acc: 0.5107216024398804\n",
      "acc: 0.49720950961112975\n",
      "acc: 0.49781710624694825\n",
      "acc: 0.5005582165718079\n",
      "acc: 0.4987635767459869\n",
      "acc: 0.5006005921959877\n",
      "acc: 0.49758109629154207\n",
      "acc: 0.49956061094999316\n",
      "acc: 0.4993061363697052\n",
      "acc: 0.49899626910686495\n",
      "acc: 0.4996239686012268\n",
      "acc: 0.5008340844511986\n",
      "acc: 0.49998248249292376\n",
      "acc: 0.4999089190363884\n",
      "acc: 0.5011918795108795\n",
      "acc: 0.49939132392406466\n",
      "acc: 0.5017278480529785\n",
      "acc: 0.5005472627282143\n",
      "acc: 0.4951337110996246\n",
      "acc: 0.5016833314299584\n",
      "acc: 0.5003431969881058\n",
      "acc: 0.503815812766552\n",
      "acc: 0.49762597113847734\n",
      "acc: 0.49839926689863207\n",
      "acc: 0.4996438932418823\n",
      "acc: 0.4987580782175064\n",
      "acc: 0.49949019730091093\n",
      "acc: 0.5006069803237915\n",
      "acc: 0.498070253431797\n",
      "acc: 0.5011130028963089\n",
      "acc: 0.5010356742143631\n",
      "acc: 0.5009294486045838\n",
      "acc: 0.49658120155334473\n",
      "acc: 0.5000003546476364\n",
      "acc: 0.498241947889328\n",
      "acc: 0.49789774090051653\n",
      "acc: 0.4975352150201797\n",
      "acc: 0.500317964553833\n",
      "acc: 0.49808590531349184\n",
      "acc: 0.49950450837612154\n",
      "acc: 0.49907062023878096\n",
      "acc: 0.49838783472776416\n",
      "acc: 0.49820430934429166\n",
      "acc: 0.499294647872448\n",
      "acc: 0.4990805503726006\n",
      "acc: 0.5000092357397079\n",
      "acc: 0.5001508179306984\n",
      "acc: 0.5000956815481186\n",
      "acc: 0.5002479559183121\n",
      "acc: 0.4965124651789665\n",
      "acc: 0.5028589490056038\n",
      "acc: 0.4976072061061859\n",
      "acc: 0.5022806090116501\n",
      "acc: 0.4970776546001434\n",
      "acc: 0.5021044179797173\n",
      "acc: 0.5001459386944771\n",
      "acc: 0.500693179666996\n",
      "acc: 0.4997249957919121\n",
      "acc: 0.5026911306381225\n",
      "acc: 0.5011616069078445\n",
      "acc: 0.4978454175591469\n",
      "acc: 0.5014430356025695\n",
      "acc: 0.4951786208152771\n",
      "acc: 0.5015092179179191\n",
      "acc: 0.4993492439389229\n",
      "acc: 0.49739848732948305\n",
      "acc: 0.4977836492657661\n",
      "acc: 0.5005063304305076\n",
      "acc: 0.4980206087231636\n",
      "acc: 0.5003962567448617\n",
      "acc: 0.4992440205812454\n",
      "acc: 0.500011313855648\n",
      "acc: 0.4976294600963593\n",
      "acc: 0.49804081857204435\n",
      "acc: 0.49829052269458773\n",
      "acc: 0.49979100048542024\n",
      "acc: 0.49976964622735975\n",
      "acc: 0.4995423719286919\n",
      "acc: 0.49792463183403013\n",
      "acc: 0.5000912269949913\n",
      "acc: 0.5001223906874657\n",
      "acc: 0.5001265105605125\n",
      "acc: 0.5016704806685448\n",
      "acc: 0.49960986226797105\n",
      "acc: 0.4961870416998863\n",
      "acc: 0.4989356461167336\n",
      "acc: 0.4974968510866165\n",
      "acc: 0.4992923241853714\n",
      "acc: 0.4989678993821144\n",
      "acc: 0.49834685802459716\n",
      "acc: 0.5003311988711358\n",
      "acc: 0.4987539479136467\n",
      "acc: 0.5004768607020378\n",
      "acc: 0.4995633870363235\n",
      "acc: 0.500495922267437\n",
      "acc: 0.5011593788862229\n",
      "acc: 0.49848882883787154\n",
      "acc: 0.4999158799648285\n",
      "acc: 0.5000350502133369\n",
      "acc: 0.49953475803136826\n",
      "acc: 0.49943627953529357\n",
      "acc: 0.49908853143453596\n",
      "acc: 0.49968567728996277\n",
      "acc: 0.5001764333248139\n",
      "acc: 0.4983402851223946\n",
      "acc: 0.49988857835531236\n",
      "acc: 0.4993123531341553\n",
      "acc: 0.49927948504686354\n",
      "acc: 0.4996005392074585\n",
      "acc: 0.49872907549142836\n",
      "acc: 0.49917226046323776\n",
      "acc: 0.49875316590070723\n",
      "acc: 0.5002871686220169\n",
      "acc: 0.4991025394201279\n",
      "acc: 0.4984931308031082\n",
      "acc: 0.49897486984729766\n",
      "acc: 0.5002113825082779\n",
      "acc: 0.5004982024431228\n",
      "acc: 0.4982566916942596\n",
      "acc: 0.4988647320866585\n",
      "acc: 0.501838561296463\n",
      "acc: 0.5054275920987129\n",
      "acc: 0.4998210945725441\n",
      "acc: 0.49640884160995485\n",
      "acc: 0.49997268736362455\n",
      "acc: 0.49787097066640856\n",
      "acc: 0.501439259648323\n",
      "acc: 0.5031519842147827\n",
      "acc: 0.4997928228974342\n",
      "acc: 0.49843950420618055\n",
      "acc: 0.49800138503313063\n",
      "acc: 0.49901209145784375\n",
      "acc: 0.49921068966388704\n",
      "acc: 0.49891024470329287\n",
      "acc: 0.4997924703359604\n",
      "acc: 0.4996935972571373\n",
      "acc: 0.4994798845052719\n",
      "acc: 0.498139391541481\n",
      "acc: 0.49860658138990405\n",
      "acc: 0.4990668249130249\n",
      "acc: 0.5000152584910392\n",
      "acc: 0.49873562663793564\n",
      "acc: 0.4990393218398094\n",
      "acc: 0.503084027171135\n",
      "acc: 0.49951413422822954\n",
      "acc: 0.5022337093949318\n",
      "acc: 0.4958282005786896\n",
      "acc: 0.5018015778064728\n",
      "acc: 0.5019895321130753\n",
      "acc: 0.49780465483665465\n",
      "acc: 0.4973403063416481\n",
      "acc: 0.500377536714077\n",
      "acc: 0.49955068200826647\n",
      "acc: 0.5001877585053444\n",
      "acc: 0.49729168534278867\n",
      "acc: 0.5003712370991706\n",
      "acc: 0.49949518859386444\n",
      "acc: 0.49926727056503295\n",
      "acc: 0.4982007896900177\n",
      "acc: 0.49972606867551805\n",
      "acc: 0.5099197578430176\n",
      "acc: 0.5037725967168808\n",
      "acc: 0.5027102273702622\n",
      "acc: 0.5037958341836929\n",
      "acc: 0.5022881281375885\n",
      "acc: 0.49902144730091097\n",
      "acc: 0.5006517684459686\n",
      "acc: 0.49900095254182814\n",
      "acc: 0.4995863062143326\n",
      "acc: 0.4992477551102638\n",
      "acc: 0.4993995121121407\n",
      "acc: 0.5003217422962188\n",
      "acc: 0.5038317450881005\n",
      "acc: 0.49915716916322705\n",
      "acc: 0.5014753156900406\n",
      "acc: 0.49987447559833526\n",
      "acc: 0.498411545753479\n",
      "acc: 0.49952348828315735\n",
      "acc: 0.49839759290218355\n",
      "acc: 0.5006696009635925\n",
      "acc: 0.4990835845470428\n",
      "acc: 0.49876521855592726\n",
      "acc: 0.4990232878923416\n",
      "acc: 0.5013043341040612\n",
      "acc: 0.5015896365046502\n",
      "acc: 0.49849214106798173\n",
      "acc: 0.4991571816802025\n",
      "acc: 0.49936092138290405\n",
      "acc: 0.4995081606507301\n",
      "acc: 0.5029729983210564\n",
      "acc: 0.503013336956501\n",
      "acc: 0.4960587140917778\n",
      "acc: 0.4995479372143745\n",
      "acc: 0.4990122690796852\n",
      "acc: 0.49945045650005343\n",
      "acc: 0.5002798512578011\n",
      "acc: 0.4986069136857986\n",
      "acc: 0.5025046330690384\n",
      "acc: 0.4973014724254608\n",
      "acc: 0.49895311921834945\n",
      "acc: 0.5008665403723717\n",
      "acc: 0.5053386819362641\n",
      "acc: 0.49839754313230517\n",
      "acc: 0.5003749176859855\n",
      "acc: 0.5000845867395401\n",
      "acc: 0.4986968952417374\n",
      "acc: 0.5005520179867744\n",
      "acc: 0.5025749740004539\n",
      "acc: 0.49733460813760755\n",
      "acc: 0.5025791803002357\n",
      "acc: 0.5042141950130463\n",
      "acc: 0.4974554964900017\n",
      "acc: 0.49849614679813387\n",
      "acc: 0.5004926234483719\n",
      "acc: 0.4967144140601158\n",
      "acc: 0.4989089581370354\n",
      "acc: 0.5014151936769485\n",
      "acc: 0.4959719005227089\n",
      "acc: 0.49891084522008894\n",
      "acc: 0.4995465499162674\n",
      "acc: 0.4992870697379112\n",
      "acc: 0.49880129367113113\n",
      "acc: 0.4982292914390564\n",
      "acc: 0.4990723305940628\n",
      "acc: 0.5025729256868362\n",
      "acc: 0.4996090149879456\n",
      "acc: 0.49701632142066954\n",
      "acc: 0.4992912459373474\n",
      "acc: 0.4992348670959473\n",
      "acc: 0.4996070736646652\n",
      "acc: 0.498954798579216\n",
      "acc: 0.5003135704994202\n",
      "acc: 0.5011582225561142\n",
      "acc: 0.4997615987062454\n",
      "acc: 0.49932427823543546\n",
      "acc: 0.5060118836164474\n",
      "acc: 0.5004488742351532\n",
      "acc: 0.4994111275672913\n",
      "acc: 0.4981016844511032\n",
      "acc: 0.49955039530992507\n",
      "acc: 0.5009150487184525\n",
      "acc: 0.4971551588177681\n",
      "acc: 0.4994749283790588\n",
      "acc: 0.5023108142614364\n",
      "acc: 0.4995858243107796\n",
      "acc: 0.49866550832986833\n",
      "acc: 0.4988510060310364\n",
      "acc: 0.49817993611097333\n",
      "acc: 0.49988045275211335\n",
      "acc: 0.5054432529211045\n",
      "acc: 0.498339050412178\n",
      "acc: 0.4982744470238686\n",
      "acc: 0.5007102236151695\n",
      "acc: 0.5019093891978263\n",
      "acc: 0.4976572051644325\n",
      "acc: 0.5027839469909668\n",
      "acc: 0.4998259377479553\n",
      "acc: 0.5007150858640671\n",
      "acc: 0.500477939248085\n",
      "acc: 0.4995194470882416\n",
      "acc: 0.4974265542626381\n",
      "acc: 0.499247100353241\n",
      "acc: 0.4991391259431839\n",
      "acc: 0.5005196690559387\n",
      "acc: 0.4985082823038101\n",
      "acc: 0.5030543029308319\n",
      "acc: 0.4963675624132156\n",
      "acc: 0.49937506169080736\n",
      "acc: 0.49937263906002044\n",
      "acc: 0.5023478078842163\n",
      "acc: 0.5029413184523582\n",
      "acc: 0.49907258331775667\n",
      "acc: 0.49875793993473055\n",
      "acc: 0.499186672270298\n",
      "acc: 0.5000616028904915\n",
      "acc: 0.5011791148781777\n",
      "acc: 0.5004790854454041\n",
      "acc: 0.4984829586744308\n",
      "acc: 0.49899117231369017\n",
      "acc: 0.4996465343236923\n",
      "acc: 0.4999612393975258\n",
      "acc: 0.5019881755113602\n",
      "acc: 0.49975248366594316\n",
      "acc: 0.5004143530130386\n",
      "acc: 0.4996531343460083\n",
      "acc: 0.4997483292222023\n",
      "acc: 0.4981030395627022\n",
      "acc: 0.4990196567773819\n",
      "acc: 0.5007851684093475\n",
      "acc: 0.50038399040699\n",
      "acc: 0.49832514762878416\n",
      "acc: 0.5013694787025451\n",
      "acc: 0.5007549434900284\n",
      "acc: 0.4991006702184677\n",
      "acc: 0.49940884351730347\n",
      "acc: 0.4991986697912216\n",
      "acc: 0.5005716115236283\n",
      "acc: 0.4996732300519943\n",
      "acc: 0.49876504600048066\n",
      "acc: 0.4993849667906761\n",
      "acc: 0.49977149844169616\n",
      "acc: 0.5000254273414612\n",
      "acc: 0.4994886264204979\n",
      "acc: 0.49996949404478075\n",
      "acc: 0.5010463458299637\n",
      "acc: 0.5000380831956863\n",
      "acc: 0.49919372975826265\n",
      "acc: 0.49921498239040374\n",
      "acc: 0.5002748703956604\n",
      "acc: 0.5028025388717652\n",
      "acc: 0.4980603045225143\n",
      "acc: 0.5019987195730209\n",
      "acc: 0.4974402743577957\n",
      "acc: 0.501179364323616\n",
      "acc: 0.5019269299507141\n",
      "acc: 0.5041697251796723\n",
      "acc: 0.5037436074018479\n",
      "acc: 0.4993680387735367\n",
      "acc: 0.49850846350193023\n",
      "acc: 0.499202584028244\n",
      "acc: 0.5000217109918594\n",
      "acc: 0.4991420841217041\n",
      "acc: 0.4993353658914566\n",
      "acc: 0.4993737173080444\n",
      "acc: 0.49963845789432526\n",
      "acc: 0.5026972988247871\n",
      "acc: 0.5006589516997337\n",
      "acc: 0.49774996608495714\n",
      "acc: 0.4982105058431625\n",
      "acc: 0.49961135238409043\n",
      "acc: 0.4995187947154045\n",
      "acc: 0.49925095528364183\n",
      "acc: 0.4998851728439331\n",
      "acc: 0.5009558862447738\n",
      "acc: 0.49811030030250547\n",
      "acc: 0.5000656342506409\n",
      "acc: 0.49979587495326994\n",
      "acc: 0.49897495299577715\n",
      "acc: 0.49972784459590913\n",
      "acc: 0.49963032215833664\n",
      "acc: 0.5000226429104805\n",
      "acc: 0.49985111236572266\n",
      "acc: 0.49911217212677\n",
      "acc: 0.4995132699608803\n",
      "acc: 0.4995838952064514\n",
      "acc: 0.49937172144651415\n",
      "acc: 0.4994510531425476\n",
      "acc: 0.50027428150177\n",
      "acc: 0.49988085865974424\n",
      "acc: 0.5003009450435638\n",
      "acc: 0.49959954649209976\n",
      "acc: 0.4990395766496658\n",
      "acc: 0.5008325803279877\n",
      "acc: 0.5011773562431335\n",
      "acc: 0.49825661242008207\n",
      "acc: 0.5031383502483368\n",
      "acc: 0.502204738855362\n",
      "acc: 0.49723056495189666\n",
      "acc: 0.498592971265316\n",
      "acc: 0.4992369568347931\n",
      "acc: 0.49948262065649035\n",
      "acc: 0.5025198218226433\n",
      "acc: 0.5004591289162635\n",
      "acc: 0.49788971602916715\n",
      "acc: 0.49885878622531893\n",
      "acc: 0.500008005797863\n",
      "acc: 0.49980273127555847\n",
      "acc: 0.4994157695770264\n",
      "acc: 0.4995525169372559\n",
      "acc: 0.4997558295726776\n",
      "acc: 0.500973190665245\n",
      "acc: 0.4978311225771904\n",
      "acc: 0.4997179463505745\n",
      "acc: 0.5003901165723801\n",
      "acc: 0.49964415848255156\n",
      "acc: 0.5021760377287865\n",
      "acc: 0.49784831553697584\n",
      "acc: 0.500100875198841\n",
      "acc: 0.5012912169098854\n",
      "acc: 0.5011350798606873\n",
      "acc: 0.4982443708181381\n",
      "acc: 0.49920264422893523\n",
      "acc: 0.5013788333535194\n",
      "acc: 0.5009493482112884\n",
      "acc: 0.4995149475336075\n",
      "acc: 0.4995772629976273\n",
      "acc: 0.5011283904314041\n",
      "acc: 0.5058869385719299\n",
      "acc: 0.5044670867919921\n",
      "acc: 0.49986036837100983\n",
      "acc: 0.5165536659955978\n",
      "acc: 0.49585374355316164\n",
      "acc: 0.49643078923225403\n",
      "acc: 0.49873967975378036\n",
      "acc: 0.4999688950181007\n",
      "acc: 0.4986462485790253\n",
      "acc: 0.4993207064270973\n",
      "acc: 0.5000710427761078\n",
      "acc: 0.5028312331438065\n",
      "acc: 0.5015406316518783\n",
      "acc: 0.4960072606801987\n",
      "acc: 0.5007723730802536\n",
      "acc: 0.5017130121588707\n",
      "acc: 0.4988791772723198\n",
      "acc: 0.4997523212432861\n",
      "acc: 0.4986185970902443\n",
      "acc: 0.49941085070371627\n",
      "acc: 0.4987669458985329\n",
      "acc: 0.500326178073883\n",
      "acc: 0.5000706428289413\n",
      "acc: 0.499175478219986\n",
      "acc: 0.4992370402812958\n",
      "acc: 0.49877637028694155\n",
      "acc: 0.500434176325798\n",
      "acc: 0.4999823123216629\n",
      "acc: 0.49961892902851107\n",
      "acc: 0.5000629693269729\n",
      "acc: 0.5011293429136277\n",
      "acc: 0.5002722948789596\n",
      "acc: 0.5013336333632469\n",
      "acc: 0.4994743201136589\n",
      "acc: 0.5007204362750053\n",
      "acc: 0.4988944444060326\n",
      "acc: 0.4981968802213669\n",
      "acc: 0.49947072714567187\n",
      "acc: 0.5017410489916801\n",
      "acc: 0.5015008306503296\n",
      "acc: 0.5024935048818588\n",
      "acc: 0.49728978395462037\n",
      "acc: 0.49960296392440795\n",
      "acc: 0.4997151890397072\n",
      "acc: 0.5015783601999283\n",
      "acc: 0.49779730588197707\n",
      "acc: 0.5004040747880936\n",
      "acc: 0.4991162595152855\n",
      "acc: 0.499791197180748\n",
      "acc: 0.5000377231836319\n",
      "acc: 0.49982322096824644\n",
      "acc: 0.5012684848904609\n",
      "acc: 0.5005296048521996\n",
      "acc: 0.498412726521492\n",
      "acc: 0.49619062542915343\n",
      "acc: 0.4979188945889473\n",
      "acc: 0.5012700587511063\n",
      "acc: 0.49760643124580384\n",
      "acc: 0.49897975742816925\n",
      "acc: 0.4999775344133377\n",
      "acc: 0.5000250047445297\n",
      "acc: 0.5015964049100876\n",
      "acc: 0.49802976071834565\n",
      "acc: 0.4986011379957199\n",
      "acc: 0.499730504155159\n",
      "acc: 0.4997703102231026\n",
      "acc: 0.5037040138244628\n",
      "acc: 0.49876892536878586\n",
      "acc: 0.49687993466854097\n",
      "acc: 0.49879640579223633\n",
      "acc: 0.4997618439793587\n",
      "acc: 0.49981579661369324\n",
      "acc: 0.5000819429755211\n",
      "acc: 0.5006258773803711\n",
      "acc: 0.5004199826717377\n",
      "acc: 0.4986992138624191\n",
      "acc: 0.5018685787916184\n",
      "acc: 0.499401770234108\n",
      "acc: 0.5009131699800491\n",
      "acc: 0.4987031066417694\n",
      "acc: 0.49909802913665774\n",
      "acc: 0.49971833467483523\n",
      "acc: 0.49979444801807404\n",
      "acc: 0.5000566822290421\n",
      "acc: 0.5001011544466019\n",
      "acc: 0.5005481564998626\n",
      "acc: 0.5044341248273849\n",
      "acc: 0.4960011702775955\n",
      "acc: 0.5006676799058914\n",
      "acc: 0.4990468695759773\n",
      "acc: 0.49916675716638564\n",
      "acc: 0.5007889875769616\n",
      "acc: 0.4992859584093094\n",
      "acc: 0.5003463709354401\n",
      "acc: 0.4999284136295319\n",
      "acc: 0.49956963866949083\n",
      "acc: 0.4995839801430702\n",
      "acc: 0.49905831605196\n",
      "acc: 0.4997104546427727\n",
      "acc: 0.500130620598793\n",
      "acc: 0.5011774265766143\n",
      "acc: 0.49880071103572843\n",
      "acc: 0.49905965745449066\n",
      "acc: 0.49920852839946744\n",
      "acc: 0.500261989235878\n",
      "acc: 0.4996186286211014\n",
      "acc: 0.502330847978592\n",
      "acc: 0.49911403059959414\n",
      "acc: 0.5013620343804359\n",
      "acc: 0.5006142669916153\n",
      "acc: 0.5039309394359589\n",
      "acc: 0.49763086676597595\n",
      "acc: 0.5050742077827454\n",
      "acc: 0.49535193502902986\n",
      "acc: 0.49971498787403107\n",
      "acc: 0.5015564620494842\n",
      "acc: 0.5007524365186691\n",
      "acc: 0.5037735342979431\n",
      "acc: 0.49731901109218596\n",
      "acc: 0.49904079020023345\n",
      "acc: 0.4987654772400856\n",
      "acc: 0.500187278687954\n",
      "acc: 0.49698599219322204\n",
      "acc: 0.49969944268465044\n",
      "acc: 0.4992481142282486\n",
      "acc: 0.5008579581975937\n",
      "acc: 0.5020120784640312\n",
      "acc: 0.4972546929121017\n",
      "acc: 0.5006854051351547\n",
      "acc: 0.4989662030339241\n",
      "acc: 0.4993332499265671\n",
      "acc: 0.49933905333280565\n",
      "acc: 0.4994432756304741\n",
      "acc: 0.4986870461702347\n",
      "acc: 0.5013292950391769\n",
      "acc: 0.49858305633068084\n",
      "acc: 0.49884120166301726\n",
      "acc: 0.4990714591741562\n",
      "acc: 0.49964616298675535\n",
      "acc: 0.5009996217489242\n",
      "acc: 0.49861727058887484\n",
      "acc: 0.4992816174030304\n",
      "acc: 0.5004371559619903\n",
      "acc: 0.4985458335280418\n",
      "acc: 0.5006635400652886\n",
      "acc: 0.49948341071605684\n",
      "acc: 0.5001386821269989\n",
      "acc: 0.49861397832632065\n",
      "acc: 0.49870585203170775\n",
      "acc: 0.49925149708986283\n",
      "acc: 0.4996814063191414\n",
      "acc: 0.4995215645432472\n",
      "acc: 0.5004698404669762\n",
      "acc: 0.4982778719067574\n",
      "acc: 0.5009015506505966\n",
      "acc: 0.5008095234632493\n",
      "acc: 0.5037465679645539\n",
      "acc: 0.5008536332845688\n",
      "acc: 0.5095141541957855\n",
      "acc: 0.49175806701183317\n",
      "acc: 0.4994922095537186\n",
      "acc: 0.49962478697299956\n",
      "acc: 0.4993194979429245\n",
      "acc: 0.49946060866117475\n",
      "acc: 0.4994557344913483\n",
      "acc: 0.5005492326617241\n",
      "acc: 0.4992095324397087\n",
      "acc: 0.49963744074106214\n",
      "acc: 0.4996347838640213\n",
      "acc: 0.4999546229839325\n",
      "acc: 0.5002911275625229\n",
      "acc: 0.5008173125982285\n",
      "acc: 0.4985750013589859\n",
      "acc: 0.500173014998436\n",
      "acc: 0.5004647901654243\n",
      "acc: 0.5007522681355476\n",
      "acc: 0.5019198608398437\n",
      "acc: 0.4984496557712555\n",
      "acc: 0.4982216128706932\n",
      "acc: 0.4984482508897781\n",
      "acc: 0.49980647712945936\n",
      "acc: 0.4984620061516762\n",
      "acc: 0.49964729338884356\n",
      "acc: 0.5023128765821457\n",
      "acc: 0.4991651606559753\n",
      "acc: 0.5011255019903182\n",
      "acc: 0.5003503483533859\n",
      "acc: 0.5022538101673126\n",
      "acc: 0.49528257369995116\n",
      "acc: 0.4995604658126831\n",
      "acc: 0.49873322129249575\n",
      "acc: 0.49971599817276\n",
      "acc: 0.5005142039060593\n",
      "acc: 0.4983019286394119\n",
      "acc: 0.49985226154327395\n",
      "acc: 0.4997116738557816\n",
      "acc: 0.49938048094511034\n",
      "acc: 0.49938850045204164\n",
      "acc: 0.499499523639679\n",
      "acc: 0.5031888443231582\n",
      "acc: 0.4976178115606308\n",
      "acc: 0.5002263689041138\n",
      "acc: 0.4997623664140701\n",
      "acc: 0.4998264181613922\n",
      "acc: 0.49913483411073684\n",
      "acc: 0.4996360057592392\n",
      "acc: 0.49986501604318617\n",
      "acc: 0.49944190442562103\n",
      "acc: 0.5013670232892037\n",
      "acc: 0.4997314015030861\n",
      "acc: 0.4988438430428505\n",
      "acc: 0.5009116432070733\n",
      "acc: 0.4988133752346039\n",
      "acc: 0.49933447152376176\n",
      "acc: 0.500470050573349\n",
      "acc: 0.5007280310988427\n",
      "acc: 0.4995105719566345\n",
      "acc: 0.5010763216018677\n",
      "acc: 0.5009153839945794\n",
      "acc: 0.497468798160553\n",
      "acc: 0.4999366092681885\n",
      "acc: 0.4986696994304657\n",
      "acc: 0.4996853357553482\n",
      "acc: 0.49996212929487227\n",
      "acc: 0.4991569659113884\n",
      "acc: 0.49963488399982453\n",
      "acc: 0.50004718542099\n",
      "acc: 0.4996291187405586\n",
      "acc: 0.49983852475881574\n",
      "acc: 0.4991928344964981\n",
      "acc: 0.49992687165737154\n",
      "acc: 0.4995497515797615\n",
      "acc: 0.4997693407535553\n",
      "acc: 0.499683176279068\n",
      "acc: 0.4999487566947937\n",
      "acc: 0.499457303583622\n",
      "acc: 0.4993455272912979\n",
      "acc: 0.4999816170334816\n",
      "acc: 0.5010110750794411\n",
      "acc: 0.5010000887513161\n",
      "acc: 0.49834970831871034\n",
      "acc: 0.49928725689649583\n",
      "acc: 0.5003171253204346\n",
      "acc: 0.5011285021901131\n",
      "acc: 0.4990097963809967\n",
      "acc: 0.49959072828292844\n",
      "acc: 0.5002993902564049\n",
      "acc: 0.49948545813560485\n",
      "acc: 0.4994460791349411\n",
      "acc: 0.49929022520780564\n",
      "acc: 0.4998734611272812\n",
      "acc: 0.5002730891108513\n",
      "acc: 0.4981774738430977\n",
      "acc: 0.4997618433833122\n",
      "acc: 0.5007938474416733\n",
      "acc: 0.5003436577320098\n",
      "acc: 0.4989790108799934\n",
      "acc: 0.500292629301548\n",
      "acc: 0.49980983972549436\n",
      "acc: 0.49928117364645\n",
      "acc: 0.4997749337553978\n",
      "acc: 0.4993037122488022\n",
      "acc: 0.5000383394956589\n",
      "acc: 0.4999471068382263\n",
      "acc: 0.5003636738657952\n",
      "acc: 0.49971067070961\n",
      "acc: 0.5013064113259316\n",
      "acc: 0.49768568396568297\n",
      "acc: 0.5024388232827186\n",
      "acc: 0.5001517418026924\n",
      "acc: 0.4963784310221672\n",
      "acc: 0.4986941781640053\n",
      "acc: 0.5017206910252571\n",
      "acc: 0.5021467679738998\n",
      "acc: 0.5003005793690681\n",
      "acc: 0.4980268529057503\n",
      "acc: 0.4992902538180351\n",
      "acc: 0.5002732375264167\n",
      "acc: 0.49927855491638184\n",
      "acc: 0.5007924553751946\n",
      "acc: 0.49968602538108825\n",
      "acc: 0.4987727105617523\n",
      "acc: 0.4999095645546913\n",
      "acc: 0.5004091197252274\n",
      "acc: 0.5016923668980598\n",
      "acc: 0.5015567648410797\n",
      "acc: 0.4978389108181\n",
      "acc: 0.49993025630712506\n",
      "acc: 0.5000273567438126\n",
      "acc: 0.5003604313731194\n",
      "acc: 0.5004961016774178\n",
      "acc: 0.5000921142101288\n",
      "acc: 0.5039598140120506\n",
      "acc: 0.5041416391730309\n",
      "acc: 0.49647375017404555\n",
      "acc: 0.501580630838871\n",
      "acc: 0.49765336632728574\n",
      "acc: 0.5003743231296539\n",
      "acc: 0.5050642663240432\n",
      "acc: 0.4991550958156586\n",
      "acc: 0.4985510203242302\n",
      "acc: 0.4997355490922928\n",
      "acc: 0.5010541626811027\n",
      "acc: 0.5007296401262283\n",
      "acc: 0.5032339689135551\n",
      "acc: 0.49922769755125046\n",
      "acc: 0.4977754917740822\n",
      "acc: 0.4997503745555878\n",
      "acc: 0.5014907184243202\n",
      "acc: 0.501492828130722\n",
      "acc: 0.4994096854329109\n",
      "acc: 0.5023765677213669\n",
      "acc: 0.4972169288992882\n",
      "acc: 0.5010134822130203\n",
      "acc: 0.5019412672519684\n",
      "acc: 0.5020874762535095\n",
      "acc: 0.49948126256465913\n",
      "acc: 0.499005968272686\n",
      "acc: 0.5005352348089218\n",
      "acc: 0.49922447055578234\n",
      "acc: 0.5001395958662033\n",
      "acc: 0.4982753610610962\n",
      "acc: 0.4999863240122795\n",
      "acc: 0.5000190764665604\n",
      "acc: 0.4993446969985962\n",
      "acc: 0.5035063341259957\n",
      "acc: 0.49968403041362763\n",
      "acc: 0.49962507009506224\n",
      "acc: 0.49933062344789503\n",
      "acc: 0.4982979336380959\n",
      "acc: 0.5002604895830154\n",
      "acc: 0.4993772315979004\n",
      "acc: 0.49919602394104\n",
      "acc: 0.5025025168061257\n",
      "acc: 0.5063741868734359\n",
      "acc: 0.5026852077245713\n",
      "acc: 0.5007347369194031\n",
      "acc: 0.5012816554307937\n",
      "acc: 0.5004303985834122\n",
      "acc: 0.5042789369821549\n",
      "acc: 0.4951050415635109\n",
      "acc: 0.49948352068662644\n",
      "acc: 0.5000656947493554\n",
      "acc: 0.5006674998998641\n",
      "acc: 0.4979582634568214\n",
      "acc: 0.49856834948062895\n",
      "acc: 0.4993353307247162\n",
      "acc: 0.5002390769124031\n",
      "acc: 0.5007232180237771\n",
      "acc: 0.4983941775560379\n",
      "acc: 0.49976033002138137\n",
      "acc: 0.5001177316904069\n",
      "acc: 0.4992208653688431\n",
      "acc: 0.499696831703186\n",
      "acc: 0.49985611140728\n",
      "acc: 0.5000630843639374\n",
      "acc: 0.4992758005857468\n",
      "acc: 0.5000603634119034\n",
      "acc: 0.4999666166305542\n",
      "acc: 0.5006167715787888\n",
      "acc: 0.4994027864933014\n",
      "acc: 0.49950662136077884\n",
      "acc: 0.49878027468919756\n",
      "acc: 0.5019561460614205\n",
      "acc: 0.5026457315683365\n",
      "acc: 0.49698433220386506\n",
      "acc: 0.5005675208568573\n",
      "acc: 0.49939052641391757\n",
      "acc: 0.49934289902448653\n",
      "acc: 0.49981376677751543\n",
      "acc: 0.5001504045724868\n",
      "acc: 0.49967334389686585\n",
      "acc: 0.5005008280277252\n",
      "acc: 0.4986777725815773\n",
      "acc: 0.4993373683094978\n",
      "acc: 0.5007690450549126\n",
      "acc: 0.4982004028558731\n",
      "acc: 0.499514639377594\n",
      "acc: 0.4999218100309372\n",
      "acc: 0.4996412432193756\n",
      "acc: 0.499562115073204\n",
      "acc: 0.49960290163755416\n",
      "acc: 0.5002330330014229\n",
      "acc: 0.500343705713749\n",
      "acc: 0.5003811886906624\n",
      "acc: 0.5004217824339867\n",
      "acc: 0.5012630832195282\n",
      "acc: 0.5021990478038788\n",
      "acc: 0.5016236421465874\n",
      "acc: 0.4999991661310196\n",
      "acc: 0.5006081214547158\n",
      "acc: 0.5005122366547584\n",
      "acc: 0.495569748878479\n",
      "acc: 0.5009317934513092\n",
      "acc: 0.5010177817940712\n",
      "acc: 0.4960946151614189\n",
      "acc: 0.4987874498963356\n",
      "acc: 0.4994707876443863\n",
      "acc: 0.49956328302621844\n",
      "acc: 0.5002761074900627\n",
      "acc: 0.5007116949558258\n",
      "acc: 0.4989330688118935\n",
      "acc: 0.4993747165799141\n",
      "acc: 0.5003396368026733\n",
      "acc: 0.5019754266738892\n",
      "acc: 0.5072435113787651\n",
      "acc: 0.5009005990624428\n",
      "acc: 0.5009556195139885\n",
      "acc: 0.5035108903050423\n",
      "acc: 0.49821000337600707\n",
      "acc: 0.5014627286791802\n",
      "acc: 0.5001310855150223\n",
      "acc: 0.4998044618964195\n",
      "acc: 0.5066713869571686\n",
      "acc: 0.5000286895036697\n",
      "acc: 0.49855263859033583\n",
      "acc: 0.5004549399018288\n",
      "acc: 0.4972529312968254\n",
      "acc: 0.4978943800926208\n",
      "acc: 0.49993872523307803\n",
      "acc: 0.5004018089175224\n",
      "acc: 0.49883325457572936\n",
      "acc: 0.50067776709795\n",
      "acc: 0.4988420212268829\n",
      "acc: 0.5004691636562347\n",
      "acc: 0.5004863637685776\n",
      "acc: 0.4985765689611435\n",
      "acc: 0.49973814249038695\n",
      "acc: 0.5030803874135017\n",
      "acc: 0.49934982508420944\n",
      "acc: 0.5031549936532974\n",
      "acc: 0.4974621221423149\n",
      "acc: 0.5004094284772873\n",
      "acc: 0.4993885090947151\n",
      "acc: 0.5031655052304268\n",
      "acc: 0.49582088470458985\n",
      "acc: 0.4990321344137192\n",
      "acc: 0.49956157863140105\n",
      "acc: 0.4991428151726723\n",
      "acc: 0.4986697825789452\n",
      "acc: 0.4994600427150726\n",
      "acc: 0.4991237670183182\n",
      "acc: 0.499826600253582\n",
      "acc: 0.5006586009263992\n",
      "acc: 0.4996575462818146\n",
      "acc: 0.502712876200676\n",
      "acc: 0.49996719300746917\n",
      "acc: 0.4977265602350235\n",
      "acc: 0.49884057402610776\n",
      "acc: 0.501351049542427\n",
      "acc: 0.5023059624433518\n",
      "acc: 0.49932609260082245\n",
      "acc: 0.4991381680965424\n",
      "acc: 0.49987818986177446\n",
      "acc: 0.49901666164398195\n",
      "acc: 0.4995304983854294\n",
      "acc: 0.5022316211462021\n",
      "acc: 0.4988236153125763\n",
      "acc: 0.49879242837429044\n",
      "acc: 0.498633131980896\n",
      "acc: 0.4998867404460907\n",
      "acc: 0.49995933622121813\n",
      "acc: 0.5029484701156616\n",
      "acc: 0.49956178545951846\n",
      "acc: 0.5009572494029999\n",
      "acc: 0.49948086082935333\n",
      "acc: 0.49888273239135744\n",
      "acc: 0.5027815079689026\n",
      "acc: 0.49739740312099456\n",
      "acc: 0.50176882147789\n",
      "acc: 0.5007182419300079\n",
      "acc: 0.4983049619197846\n",
      "acc: 0.5008039826154709\n",
      "acc: 0.49993225157260895\n",
      "acc: 0.49983433485031126\n",
      "acc: 0.5016853600740433\n",
      "acc: 0.5017122626304626\n",
      "acc: 0.4965403854846954\n",
      "acc: 0.4991756165027618\n",
      "acc: 0.5023003679513931\n",
      "acc: 0.4992411285638809\n",
      "acc: 0.5009960767626762\n",
      "acc: 0.4966031211614609\n",
      "acc: 0.5003232669830322\n",
      "acc: 0.4992824864387512\n",
      "acc: 0.49941855162382126\n",
      "acc: 0.4987019842863083\n",
      "acc: 0.5002370843291283\n",
      "acc: 0.5000755172967911\n",
      "acc: 0.5009005710482597\n",
      "acc: 0.5006908723711967\n",
      "acc: 0.5005003583431243\n",
      "acc: 0.4979492121934891\n",
      "acc: 0.499079287648201\n",
      "acc: 0.49992991238832474\n",
      "acc: 0.5007017016410827\n",
      "acc: 0.5012024092674255\n",
      "acc: 0.5032416188716888\n",
      "acc: 0.4994857358932495\n",
      "acc: 0.498925678730011\n",
      "acc: 0.5012796121835709\n",
      "acc: 0.49957722902297974\n",
      "acc: 0.4989093989133835\n",
      "acc: 0.49944891095161437\n",
      "acc: 0.4989874997735024\n",
      "acc: 0.5040159440040588\n",
      "acc: 0.4973529976606369\n",
      "acc: 0.49996044009923934\n",
      "acc: 0.5039125123620033\n",
      "acc: 0.5025411066412926\n",
      "acc: 0.4972290706634521\n",
      "acc: 0.49871080487966535\n",
      "acc: 0.49899225413799286\n",
      "acc: 0.5005925112962722\n",
      "acc: 0.49933934330940244\n",
      "acc: 0.4996401935815811\n",
      "acc: 0.49945577919483186\n",
      "acc: 0.4999716407060623\n",
      "acc: 0.49894799292087555\n",
      "acc: 0.4995822975039482\n",
      "acc: 0.4996189233660698\n",
      "acc: 0.500238242149353\n",
      "acc: 0.5009948968887329\n",
      "acc: 0.5002657812833786\n",
      "acc: 0.49826406836509707\n",
      "acc: 0.49922647535800935\n",
      "acc: 0.499942025244236\n",
      "acc: 0.5006830441951752\n",
      "acc: 0.4998501765727997\n",
      "acc: 0.5020382302999497\n",
      "acc: 0.5014368063211441\n",
      "acc: 0.49998133301734926\n",
      "acc: 0.49975683391094206\n",
      "acc: 0.5006056165695191\n",
      "acc: 0.4986843812465668\n",
      "acc: 0.49965670585632327\n",
      "acc: 0.5003903979063034\n",
      "acc: 0.49925491988658904\n",
      "acc: 0.4996777784824371\n",
      "acc: 0.4987164822220802\n",
      "acc: 0.5001922199130058\n",
      "acc: 0.4993314817547798\n",
      "acc: 0.4999288776516914\n",
      "acc: 0.50229975014925\n",
      "acc: 0.5046256560087204\n",
      "acc: 0.5074666452407837\n",
      "acc: 0.5042834958434105\n",
      "acc: 0.4946215242147446\n",
      "acc: 0.49879731476306916\n",
      "acc: 0.501423671245575\n",
      "acc: 0.5036042562127113\n",
      "acc: 0.49679572105407716\n",
      "acc: 0.4983940175175667\n",
      "acc: 0.5006592771410943\n",
      "acc: 0.5014340955018998\n",
      "acc: 0.5012848922610282\n",
      "acc: 0.49888681173324584\n",
      "acc: 0.5011102890968323\n",
      "acc: 0.5027070653438568\n",
      "acc: 0.49901122838258744\n",
      "acc: 0.5009480607509613\n",
      "acc: 0.4986685010790825\n",
      "acc: 0.49942602157592775\n",
      "acc: 0.4999625840783119\n",
      "acc: 0.5000270819664001\n",
      "acc: 0.5015641596913337\n",
      "acc: 0.4981221067905426\n",
      "acc: 0.5002566140890121\n",
      "acc: 0.5009730985760689\n",
      "acc: 0.5006268942356109\n",
      "acc: 0.4995818099379539\n",
      "acc: 0.49929367065429686\n",
      "acc: 0.4995038247108459\n",
      "acc: 0.5004476374387741\n",
      "acc: 0.4998423013091087\n",
      "acc: 0.4998978754878044\n",
      "acc: 0.49933275997638704\n",
      "acc: 0.49994650453329087\n",
      "acc: 0.5001570549607277\n",
      "acc: 0.49915785014629366\n",
      "acc: 0.4995429235696793\n",
      "acc: 0.5011092454195023\n",
      "acc: 0.500573958158493\n",
      "acc: 0.4989181917905807\n",
      "acc: 0.5001418614387512\n",
      "acc: 0.4991395515203476\n",
      "acc: 0.49922284841537473\n",
      "acc: 0.4997224363684654\n",
      "acc: 0.4991603976488113\n",
      "acc: 0.4993949103355408\n",
      "acc: 0.49994734227657317\n",
      "acc: 0.49979772806167605\n",
      "acc: 0.4996748128533363\n",
      "acc: 0.4996763476729393\n",
      "acc: 0.4999680009484291\n",
      "acc: 0.5002386310696602\n",
      "acc: 0.4994413095712662\n",
      "acc: 0.4992936861515045\n",
      "acc: 0.5008693838119507\n",
      "acc: 0.5003040009737014\n",
      "acc: 0.4987824630737305\n",
      "acc: 0.5013249564170837\n",
      "acc: 0.5010723730921746\n",
      "acc: 0.49768502175807955\n",
      "acc: 0.5015012043714523\n",
      "acc: 0.5015590372681618\n",
      "acc: 0.49983934104442596\n",
      "acc: 0.49913391053676603\n",
      "acc: 0.5043385767936707\n",
      "acc: 0.49740541517734527\n",
      "acc: 0.4987249308824539\n",
      "acc: 0.5004185372591019\n",
      "acc: 0.4997976911067963\n",
      "acc: 0.5016373008489609\n",
      "acc: 0.4981787818670273\n",
      "acc: 0.4997685331106186\n",
      "acc: 0.4991277974843979\n",
      "acc: 0.49995025008916855\n",
      "acc: 0.5012944394350052\n",
      "acc: 0.4981300261616707\n",
      "acc: 0.49997750282287595\n",
      "acc: 0.49792461931705473\n",
      "acc: 0.49976545870304107\n",
      "acc: 0.5008666089177132\n",
      "acc: 0.5012736541032791\n",
      "acc: 0.5006189134716987\n",
      "acc: 0.5013417580723762\n",
      "acc: 0.4983497649431229\n",
      "acc: 0.498389735519886\n",
      "acc: 0.5001706045866012\n",
      "acc: 0.5017308399081231\n",
      "acc: 0.5002663427591324\n",
      "acc: 0.5000495377182961\n",
      "acc: 0.49862286567687986\n",
      "acc: 0.5020866876840592\n",
      "acc: 0.5002012145519257\n",
      "acc: 0.4988687843084335\n",
      "acc: 0.4990158861875534\n",
      "acc: 0.49877079159021376\n",
      "acc: 0.49926070153713226\n",
      "acc: 0.4995833820104599\n",
      "acc: 0.5012142291665077\n",
      "acc: 0.5010230416059493\n",
      "acc: 0.4980580946803093\n",
      "acc: 0.49997229009866717\n",
      "acc: 0.49986482471227645\n",
      "acc: 0.49973431378602984\n",
      "acc: 0.498346329331398\n",
      "acc: 0.49999642044305803\n",
      "acc: 0.499483100771904\n",
      "acc: 0.5009583356976509\n",
      "acc: 0.5002978122234345\n",
      "acc: 0.5010427409410476\n",
      "acc: 0.5023368871212006\n",
      "acc: 0.49860904276371004\n",
      "acc: 0.5018146234750748\n",
      "acc: 0.5003690844774247\n",
      "acc: 0.5012178021669388\n",
      "acc: 0.5004693132638931\n",
      "acc: 0.5030862474441529\n",
      "acc: 0.5001261174678803\n",
      "acc: 0.4979814457893372\n",
      "acc: 0.5005564922094345\n",
      "acc: 0.5012500870227814\n",
      "acc: 0.4989452511072159\n",
      "acc: 0.5003721386194229\n",
      "acc: 0.5029481363296509\n",
      "acc: 0.4987121719121933\n",
      "acc: 0.5000806963443756\n",
      "acc: 0.5035402607917786\n",
      "acc: 0.4969337993860245\n",
      "acc: 0.4999920707941055\n",
      "acc: 0.5004867678880691\n",
      "acc: 0.5009963512420654\n",
      "acc: 0.4988292664289474\n",
      "acc: 0.5006586384773254\n",
      "acc: 0.5006989908218383\n",
      "acc: 0.4977072697877884\n",
      "acc: 0.5037475901842118\n",
      "acc: 0.5024678140878678\n",
      "acc: 0.5002958166599274\n",
      "acc: 0.4988403880596161\n",
      "acc: 0.4972041302919388\n",
      "acc: 0.5012859308719635\n",
      "acc: 0.5005920922756195\n",
      "acc: 0.5010066652297973\n",
      "acc: 0.4997137516736984\n",
      "acc: 0.5004121345281601\n",
      "acc: 0.4977558273077011\n",
      "acc: 0.5011916977167129\n",
      "acc: 0.5042170706391335\n",
      "acc: 0.4971184641122818\n",
      "acc: 0.4991560465097427\n",
      "acc: 0.4974696171283722\n",
      "acc: 0.49895557820796965\n",
      "acc: 0.49906199783086774\n",
      "acc: 0.5004530718922615\n",
      "acc: 0.5007689148187637\n",
      "acc: 0.5011304703354835\n",
      "acc: 0.5003428775072097\n",
      "acc: 0.5004009938240052\n",
      "acc: 0.49879594445228576\n",
      "acc: 0.5006917998194694\n",
      "acc: 0.5005353033542633\n",
      "acc: 0.5006390765309334\n",
      "acc: 0.49794969022274016\n",
      "acc: 0.4999147608876228\n",
      "acc: 0.49902421921491624\n",
      "acc: 0.501031197309494\n",
      "acc: 0.49973048955202104\n",
      "acc: 0.4998520776629448\n",
      "acc: 0.49859441071748734\n",
      "acc: 0.5002769687771798\n",
      "acc: 0.5003266483545303\n",
      "acc: 0.5000703778862953\n",
      "acc: 0.5015090596675873\n",
      "acc: 0.5005455246567726\n",
      "acc: 0.49701903939247133\n",
      "acc: 0.4986934059858322\n",
      "acc: 0.4993333554267883\n",
      "acc: 0.4995452728867531\n",
      "acc: 0.49895069539546966\n",
      "acc: 0.500229409635067\n",
      "acc: 0.4995272096991539\n",
      "acc: 0.5009558629989624\n",
      "acc: 0.49887401312589646\n",
      "acc: 0.5009470072388649\n",
      "acc: 0.5000231927633285\n",
      "acc: 0.5007076609134674\n",
      "acc: 0.4986204758286476\n",
      "acc: 0.5004037103056908\n",
      "acc: 0.4987736478447914\n",
      "acc: 0.49984838128089903\n",
      "acc: 0.5007891020178795\n",
      "acc: 0.49889845550060274\n",
      "acc: 0.49878646433353424\n",
      "acc: 0.4998261713981628\n",
      "acc: 0.5002087780833244\n",
      "acc: 0.49930783808231355\n",
      "acc: 0.49950671166181565\n",
      "acc: 0.5000752663612366\n",
      "acc: 0.5001679533720016\n",
      "acc: 0.5012494567036628\n",
      "acc: 0.49938700735569\n",
      "acc: 0.5000360122323037\n",
      "acc: 0.5001643472909927\n",
      "acc: 0.49930163025856017\n",
      "acc: 0.5016862663626671\n",
      "acc: 0.49838888555765154\n",
      "acc: 0.5004962331056595\n",
      "acc: 0.5011868023872376\n",
      "acc: 0.4973752573132515\n",
      "acc: 0.4992760768532753\n",
      "acc: 0.5021576869487763\n",
      "acc: 0.5017704778909683\n",
      "acc: 0.5060998696088791\n",
      "acc: 0.49891880273818967\n",
      "acc: 0.5018947833776474\n",
      "acc: 0.49788876354694367\n",
      "acc: 0.4994520837068558\n",
      "acc: 0.4994043219089508\n",
      "acc: 0.5007934173941613\n",
      "acc: 0.4998320332169533\n",
      "acc: 0.4996965941786766\n",
      "acc: 0.49823091983795165\n",
      "acc: 0.4993549108505249\n",
      "acc: 0.5009392592310905\n",
      "acc: 0.4983711451292038\n",
      "acc: 0.4992907610535622\n",
      "acc: 0.5019755673408508\n",
      "acc: 0.5029906350374221\n",
      "acc: 0.5024281439185142\n",
      "acc: 0.501915548145771\n",
      "acc: 0.49615746855735776\n",
      "acc: 0.4990265455842018\n",
      "acc: 0.5018901044130325\n",
      "acc: 0.5052487659454346\n",
      "acc: 0.4961912351846695\n",
      "acc: 0.5013078987598419\n",
      "acc: 0.49935525745153425\n",
      "acc: 0.49881617218255997\n",
      "acc: 0.4999845752120018\n",
      "acc: 0.4978709536790848\n",
      "acc: 0.5022852209210396\n",
      "acc: 0.501051045358181\n",
      "acc: 0.5006701895594596\n",
      "acc: 0.49947902709245684\n",
      "acc: 0.5026842325925827\n",
      "acc: 0.5044975882768631\n",
      "acc: 0.49876222759485245\n",
      "acc: 0.4995971792936325\n",
      "acc: 0.4984833550453186\n",
      "acc: 0.4969156548380852\n",
      "acc: 0.49919977486133577\n",
      "acc: 0.5007497209310532\n",
      "repeat\n",
      "acc: 0.5032667854428291\n",
      "acc: 0.4976829192042351\n",
      "acc: 0.5023756378889084\n",
      "acc: 0.4976897433400154\n",
      "acc: 0.4994387596845627\n",
      "acc: 0.498984649181366\n",
      "acc: 0.4983560651540756\n",
      "acc: 0.5007888662815094\n",
      "acc: 0.5001319727301597\n",
      "acc: 0.4987509778141975\n",
      "acc: 0.5009385693073273\n",
      "acc: 0.4992848587036133\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ahmadar/Downloads/Master-sThesis/Attention.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/Master-sThesis/Attention.ipynb#X32sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m output \u001b[39m=\u001b[39m model(seq_tensor)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/Master-sThesis/Attention.ipynb#X32sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m error(output, target_tensor)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/Master-sThesis/Attention.ipynb#X32sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/Master-sThesis/Attention.ipynb#X32sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/Master-sThesis/Attention.ipynb#X32sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m div \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \u001b[39m-\u001b[39m target_tensor\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "collectorA = dict()\n",
    "array_min_length = 17  # Define the min length of Arrays A, B, C\n",
    "array_max_length = 25  # Define the max length of Arrays A, B, C\n",
    "num_samples = 100\n",
    "for rep in range(1):\n",
    "    for kind in [\"NetRNNWithAttention\", \"RNN\"]:\n",
    "        if kind == \"RNN\":\n",
    "            model = NetRNN(hidden_dim=12, inp=3)\n",
    "        if kind == \"NetRNNWithAttention\":\n",
    "            model = NetRNNWithAttention(hidden_dim=12, inp=3)\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        error = nn.MSELoss()\n",
    "        acc = 0.0\n",
    "        W = []\n",
    "        A = []\n",
    "\n",
    "        while acc < 0.97:\n",
    "            model.resetHidden()\n",
    "            sequences, targets = generateTrainData(\n",
    "                num_samples, array_min_length, array_max_length\n",
    "            )\n",
    "            sequences = numpy.array(sequences)  # Convert to numpy array\n",
    "\n",
    "            divs = []\n",
    "            for seq, target in zip(sequences, targets):\n",
    "                optimizer.zero_grad()\n",
    "                # Process each sequence individually\n",
    "                seq_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(\n",
    "                    0\n",
    "                )  # Add batch dimension\n",
    "                target_tensor = torch.tensor(\n",
    "                    target, dtype=torch.float32\n",
    "                )  # Add batch dimension\n",
    "                # seq_tensor =torch.Tensor(seq.reshape(1, seq.shape[0], 3))\n",
    "                output = model(seq_tensor)\n",
    "                loss = error(output, target_tensor)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                div = output.detach().numpy() - target_tensor.numpy()\n",
    "                divs.append(1.0 - abs(div).mean())\n",
    "\n",
    "            W.append(loss.item())\n",
    "            acc = mean(divs)\n",
    "            A.append(acc)\n",
    "            print(f\"acc: {acc}\")\n",
    "\n",
    "            # Restart training if not converging\n",
    "            if acc < 0.97 and len(A) > 2000:\n",
    "                if kind == \"RNN\":\n",
    "                    model = NetRNN(hidden_dim=12, inp=3)\n",
    "                if kind == \"NetRNNWithAttention\":\n",
    "                    model = NetRNNWithAttention(hidden_dim=12, inp=3)\n",
    "                optimizer = optim.Adam(model.parameters())\n",
    "                acc = 0.0\n",
    "                W = []\n",
    "                A = []\n",
    "                print(\"repeat\")\n",
    "\n",
    "        # torch.save(model, f\"model{rep}.pth\")\n",
    "        collectorA[\"{0} {1}\".format(kind, rep)] = A\n",
    "        torch.save(model, \"Models/model_{0}_{1}.model\".format(kind, rep))\n",
    "        print(\"{0} {1}\".format(kind, rep), len(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_min_length = 5\n",
    "array_max_length = 10\n",
    "analysis_interval = 500  # Interval for performing analysis\n",
    "\n",
    "# Training Loop\n",
    "for rep in range(1):\n",
    "    model = NetRNNWithAttention(hidden_dim=12)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    error = nn.MSELoss()\n",
    "    acc = 0.0\n",
    "    W = []  # To store loss values\n",
    "    A = []  # To store accuracy values\n",
    "\n",
    "    iteration = 0\n",
    "    while acc < 0.97:\n",
    "        model.resetHidden()\n",
    "        sequences, targets = generateTrainData(2, array_min_length, array_max_length)\n",
    "\n",
    "        divs = []\n",
    "        for seq, target in zip(sequences, targets):\n",
    "            optimizer.zero_grad()\n",
    "            seq_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0)\n",
    "            target_tensor = torch.tensor([target], dtype=torch.float32)\n",
    "\n",
    "            output = model(seq_tensor)\n",
    "            loss = error(output, target_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            div = output.detach().numpy() - target_tensor.numpy()\n",
    "            divs.append(1.0 - abs(div).mean())\n",
    "\n",
    "        W.append(loss.item())\n",
    "        acc = mean(divs)\n",
    "        A.append(acc)\n",
    "        # print(f\"acc: {acc}\")\n",
    "\n",
    "        # Analysis Part\n",
    "        if iteration % analysis_interval == 0:\n",
    "            analysis_sequences, _ = generateTrainData(\n",
    "                1, array_min_length, array_max_length\n",
    "            )\n",
    "            analysis_seq_tensor = torch.tensor(\n",
    "                analysis_sequences[0], dtype=torch.float32\n",
    "            ).unsqueeze(0)\n",
    "            analysis_outputs, analysis_hidden_states = model.step(analysis_seq_tensor)\n",
    "\n",
    "            # Logging\n",
    "            # mean_outputs = analysis_outputs.mean(axis=0)\n",
    "            # std_outputs = analysis_outputs.std(axis=0)\n",
    "            # print(f\"Analysis - Step Function Outputs: Mean = {mean_outputs}, Std Dev = {std_outputs}\")\n",
    "\n",
    "            # mean_hidden_states = analysis_hidden_states.mean(axis=0)\n",
    "            # std_hidden_states = analysis_hidden_states.std(axis=0)\n",
    "            # print(f\"Analysis - Step Function Hidden States: Mean = {mean_hidden_states}, Std Dev = {std_hidden_states}\")\n",
    "\n",
    "            # Visualization\n",
    "            # plt.figure(figsize=(12, 4))\n",
    "            # plt.subplot(1, 2, 1)\n",
    "            # plt.plot(analysis_outputs)\n",
    "            # plt.title(\"Step Function Outputs Over Time\")\n",
    "            # plt.xlabel(\"Timestep\")\n",
    "            # plt.ylabel(\"Output\")\n",
    "\n",
    "            # plt.subplot(1, 2, 2)\n",
    "            # plt.plot(analysis_hidden_states)\n",
    "            # plt.title(\"Hidden States Over Time\")\n",
    "            # plt.xlabel(\"Timestep\")\n",
    "            # plt.ylabel(\"Hidden State Value\")\n",
    "\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "\n",
    "        # Restart training if not converging\n",
    "        if acc < 0.97 and len(A) > 2000:\n",
    "            model = NetRNNWithAttention(hidden_dim=12)\n",
    "            optimizer = optim.Adam(model.parameters())\n",
    "            acc = 0.0\n",
    "            W = []\n",
    "            A = []\n",
    "            print(\"repeat\")\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    print(f\"RNN {rep}\", len(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbolEntropy(D, base=2):\n",
    "    value, counts = numpy.unique(D, return_counts=True)\n",
    "    return entropy(counts, base=base)\n",
    "\n",
    "\n",
    "def computeTransmissionHfast(I, H, O, maskC, maskNC, iMult=2, oMult=2):\n",
    "    # print(\"I H O\",I.shape,H.shape,O.shape)\n",
    "    B = numpy.bitwise_and(H, maskNC)\n",
    "    IB = (B * iMult) + I\n",
    "    AB = H  # numpy.bitwise_and(H,maskC+maskNC)\n",
    "    BO = (B * oMult) + O\n",
    "    IAB = (AB * iMult) + I\n",
    "    IBO = (B * (iMult * oMult)) + (I * oMult) + O\n",
    "    ABO = (AB * oMult) + O\n",
    "    IABO = (AB * (iMult * oMult)) + (I * oMult) + O\n",
    "    hB = symbolEntropy(B, base=2)\n",
    "    hIB = symbolEntropy(IB, base=2)\n",
    "    hAB = symbolEntropy(AB, base=2)\n",
    "    hBO = symbolEntropy(BO, base=2)\n",
    "    hIAB = symbolEntropy(IAB, base=2)\n",
    "    hIBO = symbolEntropy(IBO, base=2)\n",
    "    hABO = symbolEntropy(ABO, base=2)\n",
    "    hIABO = symbolEntropy(IABO, base=2)\n",
    "    # -H(B)+H(IB)+H(AB)+H(BO)-H(IAB)-H(IBO)-H(ABO)+H(IABO)\n",
    "    # print(hB,hIB,hAB,hBO,hIAB,hIBO,hABO,hIABO)\n",
    "    return -hB + hIB + hAB + hBO - hIAB - hIBO - hABO + hIABO\n",
    "\n",
    "\n",
    "def singleShrinkingDecompositionInformation(I, H, O, width, iMult=2, oMult=2):\n",
    "    nodes = list(range(width))\n",
    "    cols = []\n",
    "    colh = []\n",
    "    while len(nodes) > 0:\n",
    "        infos = []\n",
    "        for node in nodes:\n",
    "            subset = copy.deepcopy(nodes)\n",
    "            subset.remove(node)\n",
    "            maskA = 0\n",
    "            for s in subset:\n",
    "                maskA += 1 * (2**s)\n",
    "            maskA = int(maskA)\n",
    "            maskB = numpy.bitwise_and(numpy.bitwise_not(maskA), ((2**width) - 1))\n",
    "            h = computeTransmissionHfast(\n",
    "                I, H, O, maskA, maskB, iMult=iMult, oMult=oMult\n",
    "            )\n",
    "            infos.append(h)\n",
    "        nodeToDrop = nodes[infos.index(max(infos))]\n",
    "        nodes.remove(nodeToDrop)\n",
    "        cols.append(copy.deepcopy(nodes))\n",
    "        colh.append(max(infos))\n",
    "    return cols, colh\n",
    "\n",
    "\n",
    "def getOutTaH(model, dataSet):\n",
    "    O, H = model.step(torch.Tensor(dataSet))\n",
    "    # print(H.shape,H.min(),H.max())\n",
    "    # figure()\n",
    "    # hist(H.flatten())\n",
    "    H = H.transpose()\n",
    "    O = O.transpose()\n",
    "    B = numpy.zeros(H.shape)\n",
    "    clusterNr = 2\n",
    "    for i in range(B.shape[0]):\n",
    "        a = H[i].reshape(-1, 1)\n",
    "        if len(numpy.unique(a)) == 1:\n",
    "            who = numpy.random.randint(len(a))\n",
    "            a[who] = 1 - a[who]\n",
    "        kmeans = KMeans(n_clusters=clusterNr).fit(a)\n",
    "        B[i] = kmeans.labels_\n",
    "        # B[i]=1.0*(H[i]>numpy.median(H[i]))\n",
    "\n",
    "    H = numpy.zeros((H.shape))\n",
    "    for i in range(12):\n",
    "        H += B[i] * (clusterNr**i)\n",
    "    H = H.astype((int))\n",
    "    return O, H\n",
    "\n",
    "\n",
    "def shrinkingDecompositionInformation(\n",
    "    model, width, dataSet, target, numbers=[0, 1, 2], whichTS=5, dsLength=8\n",
    "):\n",
    "    output, H = getOutTaH(model, dataSet)\n",
    "    output = output.transpose()[whichTS::dsLength].transpose()\n",
    "    # print(\"target.shape\",target.shape,\"output.shape\",output.shape,\"H.shape\",H.shape,\"dataset.shape\",dataSet.shape)\n",
    "    H = H.transpose()[whichTS::dsLength].transpose()\n",
    "    # target=target.transpose()[whichTS::dsLength].transpose()\n",
    "    # print(H.shape,target.shape,numpy.array(range(512))[whichTS::dsLength])\n",
    "    collectorSet = dict()\n",
    "    collectorH = dict()\n",
    "    for number in numbers:\n",
    "        I = target[number].astype(int)\n",
    "        O = (1.0 * (output[number] > 0.5)).astype(int)\n",
    "        # print(\"O\",O,\"T\",target[number])\n",
    "        # print(number,\"I.shape\",I.shape,\"O.shape\",O.shape,\"H.shape\",H.shape)\n",
    "        s, h = singleShrinkingDecompositionInformation(I, H, O, width)\n",
    "        collectorSet[number] = s\n",
    "        collectorH[number] = h\n",
    "    return collectorSet, collectorH\n",
    "\n",
    "\n",
    "def removalIntoVec(res, width, H):\n",
    "    V = numpy.zeros(width)\n",
    "    # for i,r in enumerate(res):\n",
    "    #    for e in r:\n",
    "    #        V[e]+=H[0]-H[i]\n",
    "    fullSet = list(range(width))\n",
    "    nRes = copy.deepcopy(res)\n",
    "    nRes.insert(0, fullSet)\n",
    "    nodeList = []\n",
    "    for i in range(width):\n",
    "        removedNode = list(set(nRes[i]) - set(nRes[i + 1]))[0]\n",
    "        nodeList.append(removedNode)\n",
    "    for i, node in enumerate(nodeList):\n",
    "        V[node] = H[0] - H[i]\n",
    "    # V=sqrt(V)\n",
    "    if V.sum() == 0:\n",
    "        return V\n",
    "    return V  # /V.max()\n",
    "\n",
    "\n",
    "def removalIntoMatrix(res, width, H):\n",
    "    M = []\n",
    "    for i in range(len(res)):\n",
    "        M.append(removalIntoVec(res[i], width, H[i]))\n",
    "    return numpy.array(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy: 52.00%\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./Models/model_NetRNNWithAttention_0.model\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    # Ensure predictions and targets are the same shape\n",
    "    predictions = predictions.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    # Round predictions to the nearest integer (0 or 1)\n",
    "    predictions = predictions.round()\n",
    "\n",
    "    # Calculate the number of correct predictions\n",
    "    correct = (predictions == targets).float()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = correct.sum() / correct.numel()  # Use numel() instead of len()\n",
    "    return acc.item()\n",
    "\n",
    "\n",
    "# Generate test data\n",
    "sequences, targets = generateTrainData(\n",
    "    100, array_min_length, array_max_length\n",
    ")  # You can use a different function for test data\n",
    "\n",
    "# Convert sequences and targets to tensors and pad sequences\n",
    "seq_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "padded_seq_tensors = pad_sequence(seq_tensors, batch_first=True)\n",
    "target_tensors = torch.tensor(targets, dtype=torch.float32).squeeze()\n",
    "\n",
    "# Evaluate the model on test data\n",
    "with torch.no_grad():\n",
    "    total_acc = 0.0\n",
    "    for seq_tensor, target_tensor in zip(padded_seq_tensors, target_tensors):\n",
    "        output = model(seq_tensor.unsqueeze(0))  # Add batch dimension\n",
    "        acc = calculate_accuracy(output, target_tensor)\n",
    "        total_acc += acc\n",
    "\n",
    "    # Calculate average accuracy\n",
    "    avg_acc = total_acc / len(padded_seq_tensors)\n",
    "    print(f\"Average Test Accuracy: {avg_acc * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
