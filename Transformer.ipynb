{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f0f018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b94190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from torch import nn  # nn module contains CrossEntropyLoss\n",
    "from torch import optim  # optim module contains SGD and lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8cb628f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'C', 'D', 'E', 'F', 'G']\n",
      "[[0], [1], [2], [3], [4], [5]]\n",
      "[[1], [1], [5], [5], [0], [0], [5], [4], [4], [3], [3], [2], [2], [1], [5], [5], [4], [4], [3], [3], [2], [5], [5], [4], [4], [3], [3], [2], [1], [1], [5], [5], [0], [0], [5], [4], [4], [3], [3], [2], [2], [1]]\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "songString = \"CCGGAAGFFEEDDCGGFFEEDGGFFEEDCCGGAAGFFEEDDC\"\n",
    "notes=list(unique(list(songString)))\n",
    "print(notes)\n",
    "chord=[[0],\n",
    "       [1],\n",
    "       [2],\n",
    "       [3],\n",
    "       [4],\n",
    "       [5]]\n",
    "print(chord)\n",
    "DaisyBell=list()\n",
    "DaisyBellMatrix=[]\n",
    "for note in list(songString):\n",
    "    row=[0]*6\n",
    "    row[notes.index(note)]=1\n",
    "    DaisyBellMatrix.append(row)\n",
    "    DaisyBell.append(chord[notes.index(note)])\n",
    "print(DaisyBell)\n",
    "print(len(DaisyBell))\n",
    "DaisyBellMatrix=numpy.array(DaisyBellMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b05238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [0], [1], [1], [5], [5], [5], [4], [4], [4], [4], [2], [2], [5], [1], [1], [4], [4], [2], [2], [2], [0], [0], [4], [4], [1], [1], [2], [1], [1], [5], [5], [5], [5], [5], [4], [4], [3], [3], [2], [2], [1]]\n"
     ]
    }
   ],
   "source": [
    "HotDogsongString = \"AACCGGGFFFFDDGCCFFDDDAAFFCCDCCGGGGGFFEEDDC\"\n",
    "notes=list(unique(list(HotDogsongString)))\n",
    "chord=[[0],\n",
    "       [1],\n",
    "       [2],\n",
    "       [3],\n",
    "       [4],\n",
    "       [5]]\n",
    "HotDog=list()\n",
    "HotDogMatrix=[]\n",
    "for note in list(HotDogsongString):\n",
    "    row=[0]*6\n",
    "    row[notes.index(note)]=1\n",
    "    HotDogMatrix.append(row)\n",
    "    HotDog.append(chord[notes.index(note)])\n",
    "print(HotDog)\n",
    "HotDogMatrix=numpy.array(HotDogMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f34b4f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0],\n",
       " [0],\n",
       " [1],\n",
       " [1],\n",
       " [5],\n",
       " [5],\n",
       " [5],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [2],\n",
       " [5],\n",
       " [1],\n",
       " [1],\n",
       " [4],\n",
       " [4],\n",
       " [2],\n",
       " [2],\n",
       " [2],\n",
       " [0],\n",
       " [0],\n",
       " [4],\n",
       " [4],\n",
       " [1],\n",
       " [1],\n",
       " [2],\n",
       " [1],\n",
       " [1],\n",
       " [5],\n",
       " [5],\n",
       " [5],\n",
       " [5],\n",
       " [5],\n",
       " [4],\n",
       " [4],\n",
       " [3],\n",
       " [3],\n",
       " [2],\n",
       " [2],\n",
       " [1]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HotDog#DaisyBell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a54b2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d282f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2], [3], [4], [0], [4], [3], [2], [1], [2], [3], [4], [0], [4], [3], [2], [1], [2], [3], [4], [0], [4], [3], [2], [1], [2], [3], [4], [0], [4], [3], [2], [1], [2], [3], [4], [0], [4]]\n"
     ]
    }
   ],
   "source": [
    "song3 = \"CDEGAGEDCDEGAGEDCDEGAGEDCDEGAGEDCDEGAG\"\n",
    "notes=list(unique(list(song3)))\n",
    "chord=[[0],\n",
    "       [1],\n",
    "       [2],\n",
    "       [3],\n",
    "       [4],\n",
    "       [5]]\n",
    "song=list()\n",
    "songMatrix=[]\n",
    "for note in list(song3):\n",
    "    row=[0]*6\n",
    "    row[notes.index(note)]=1\n",
    "    songMatrix.append(row)\n",
    "    song.append(chord[notes.index(note)])\n",
    "print(song)\n",
    "songMatrix=numpy.array(songMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "090a12cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the songs in a list\n",
    "songs = [DaisyBell, HotDog, song]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ac1fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, song_index, bptt=42):\n",
    "    data=[]\n",
    "    target=[]\n",
    "    for pos in range(i,i+bptt):\n",
    "        data.append(source[song_index][pos%len(source[song_index])])\n",
    "        target+=source[song_index][(pos+1)%len(source[song_index])]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt=42\n",
    "for i in range(42):\n",
    "    d,t=get_batch(songs,i,1)\n",
    "    print(f\"input {d}\")\n",
    "    print(f\"output {t}\")\n",
    "    print(\"\\n\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d38c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, d_model, nhead, d_hid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout), nlayers)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor, verbose=False) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        if verbose:\n",
    "            print(src.shape)\n",
    "            figure()\n",
    "            imshow(src.detach().numpy().reshape((42,20)))\n",
    "        src = self.pos_encoder(src)\n",
    "        if verbose:\n",
    "            print(src.shape)\n",
    "            figure()\n",
    "            imshow(src.detach().numpy().reshape((42,20)))\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        #print(output.shape)\n",
    "        #self.retainer=output[-1].copy()\n",
    "        if verbose:\n",
    "            print(output.shape)\n",
    "            figure()\n",
    "            imshow(output.detach().numpy().reshape((42,20)))\n",
    "        output = self.decoder(output)\n",
    "        if verbose:\n",
    "            print(output.shape)\n",
    "            figure()\n",
    "            imshow(output.detach().numpy().reshape((42,6)))\n",
    "        return output\n",
    "\n",
    "        \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I generated this part not sure if its right\n",
    "model = TransformerModel(ntoken=6, d_model=64, nhead=4, d_hid=256, nlayers=2, dropout=0.2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5344ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64cd21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50eafb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.95 | Training Perplexity: 7.03\n",
      "| End of epoch   1 | Validation Loss: 1.83 | Validation Perplexity:     6.22\n",
      "Training Loss: 1.75 | Training Perplexity: 5.76\n",
      "| End of epoch   2 | Validation Loss: 1.72 | Validation Perplexity:     5.60\n",
      "Training Loss: 1.66 | Training Perplexity: 5.25\n",
      "| End of epoch   3 | Validation Loss: 1.64 | Validation Perplexity:     5.16\n",
      "Training Loss: 1.57 | Training Perplexity: 4.82\n",
      "| End of epoch   4 | Validation Loss: 1.52 | Validation Perplexity:     4.59\n",
      "Training Loss: 1.44 | Training Perplexity: 4.21\n",
      "| End of epoch   5 | Validation Loss: 1.39 | Validation Perplexity:     4.03\n",
      "Training Loss: 1.32 | Training Perplexity: 3.76\n",
      "| End of epoch   6 | Validation Loss: 1.29 | Validation Perplexity:     3.63\n",
      "Training Loss: 1.23 | Training Perplexity: 3.43\n",
      "| End of epoch   7 | Validation Loss: 1.21 | Validation Perplexity:     3.36\n",
      "Training Loss: 1.15 | Training Perplexity: 3.16\n",
      "| End of epoch   8 | Validation Loss: 1.13 | Validation Perplexity:     3.08\n",
      "Training Loss: 1.06 | Training Perplexity: 2.90\n",
      "| End of epoch   9 | Validation Loss: 1.05 | Validation Perplexity:     2.85\n",
      "Training Loss: 0.99 | Training Perplexity: 2.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/87/pnb9p7_n30s29gprj1gbqbbm0000gn/T/ipykernel_62662/2798756065.py:41: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  avg_loss = total_loss / sum(len(song) for song in songs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| End of epoch  10 | Validation Loss: 0.98 | Validation Perplexity:     2.66\n",
      "Training Loss: 0.93 | Training Perplexity: 2.53\n",
      "| End of epoch  11 | Validation Loss: 0.90 | Validation Perplexity:     2.46\n",
      "Training Loss: 0.86 | Training Perplexity: 2.35\n",
      "| End of epoch  12 | Validation Loss: 0.83 | Validation Perplexity:     2.29\n",
      "Training Loss: 0.78 | Training Perplexity: 2.18\n",
      "| End of epoch  13 | Validation Loss: 0.76 | Validation Perplexity:     2.13\n",
      "Training Loss: 0.71 | Training Perplexity: 2.04\n",
      "| End of epoch  14 | Validation Loss: 0.69 | Validation Perplexity:     1.99\n",
      "Training Loss: 0.65 | Training Perplexity: 1.91\n",
      "| End of epoch  15 | Validation Loss: 0.62 | Validation Perplexity:     1.86\n",
      "Training Loss: 0.58 | Training Perplexity: 1.79\n",
      "| End of epoch  16 | Validation Loss: 0.56 | Validation Perplexity:     1.76\n",
      "Training Loss: 0.53 | Training Perplexity: 1.70\n",
      "| End of epoch  17 | Validation Loss: 0.51 | Validation Perplexity:     1.67\n",
      "Training Loss: 0.51 | Training Perplexity: 1.66\n",
      "| End of epoch  18 | Validation Loss: 0.46 | Validation Perplexity:     1.58\n",
      "Training Loss: 0.46 | Training Perplexity: 1.59\n",
      "| End of epoch  19 | Validation Loss: 0.45 | Validation Perplexity:     1.57\n",
      "Training Loss: 0.43 | Training Perplexity: 1.54\n",
      "| End of epoch  20 | Validation Loss: 0.45 | Validation Perplexity:     1.58\n",
      "Training Loss: 0.42 | Training Perplexity: 1.52\n",
      "| End of epoch  21 | Validation Loss: 0.37 | Validation Perplexity:     1.44\n",
      "Training Loss: 0.36 | Training Perplexity: 1.44\n",
      "| End of epoch  22 | Validation Loss: 0.32 | Validation Perplexity:     1.38\n",
      "Training Loss: 0.32 | Training Perplexity: 1.38\n",
      "| End of epoch  23 | Validation Loss: 0.29 | Validation Perplexity:     1.33\n",
      "Training Loss: 0.27 | Training Perplexity: 1.31\n",
      "| End of epoch  24 | Validation Loss: 0.28 | Validation Perplexity:     1.32\n",
      "Training Loss: 0.26 | Training Perplexity: 1.30\n",
      "| End of epoch  25 | Validation Loss: 0.23 | Validation Perplexity:     1.26\n",
      "Training Loss: 0.22 | Training Perplexity: 1.25\n",
      "| End of epoch  26 | Validation Loss: 0.21 | Validation Perplexity:     1.23\n",
      "Training Loss: 0.20 | Training Perplexity: 1.22\n",
      "| End of epoch  27 | Validation Loss: 0.18 | Validation Perplexity:     1.20\n",
      "Training Loss: 0.18 | Training Perplexity: 1.20\n",
      "| End of epoch  28 | Validation Loss: 0.17 | Validation Perplexity:     1.18\n",
      "Training Loss: 0.16 | Training Perplexity: 1.17\n",
      "| End of epoch  29 | Validation Loss: 0.18 | Validation Perplexity:     1.19\n",
      "Training Loss: 0.17 | Training Perplexity: 1.18\n",
      "| End of epoch  30 | Validation Loss: 0.20 | Validation Perplexity:     1.22\n",
      "Training Loss: 0.18 | Training Perplexity: 1.20\n",
      "| End of epoch  31 | Validation Loss: 0.15 | Validation Perplexity:     1.16\n",
      "Training Loss: 0.14 | Training Perplexity: 1.15\n",
      "| End of epoch  32 | Validation Loss: 0.12 | Validation Perplexity:     1.13\n",
      "Training Loss: 0.12 | Training Perplexity: 1.13\n",
      "| End of epoch  33 | Validation Loss: 0.12 | Validation Perplexity:     1.13\n",
      "Training Loss: 0.12 | Training Perplexity: 1.13\n",
      "| End of epoch  34 | Validation Loss: 0.11 | Validation Perplexity:     1.11\n",
      "Training Loss: 0.10 | Training Perplexity: 1.11\n",
      "| End of epoch  35 | Validation Loss: 0.09 | Validation Perplexity:     1.10\n",
      "Training Loss: 0.09 | Training Perplexity: 1.09\n",
      "| End of epoch  36 | Validation Loss: 0.09 | Validation Perplexity:     1.09\n",
      "Training Loss: 0.09 | Training Perplexity: 1.09\n",
      "| End of epoch  37 | Validation Loss: 0.08 | Validation Perplexity:     1.08\n",
      "Training Loss: 0.08 | Training Perplexity: 1.08\n",
      "| End of epoch  38 | Validation Loss: 0.07 | Validation Perplexity:     1.07\n",
      "Training Loss: 0.07 | Training Perplexity: 1.07\n",
      "| End of epoch  39 | Validation Loss: 0.06 | Validation Perplexity:     1.06\n",
      "Training Loss: 0.06 | Training Perplexity: 1.06\n",
      "| End of epoch  40 | Validation Loss: 0.06 | Validation Perplexity:     1.06\n",
      "Training Loss: 0.06 | Training Perplexity: 1.06\n",
      "| End of epoch  41 | Validation Loss: 0.05 | Validation Perplexity:     1.06\n",
      "Training Loss: 0.05 | Training Perplexity: 1.06\n",
      "| End of epoch  42 | Validation Loss: 0.05 | Validation Perplexity:     1.05\n",
      "Training Loss: 0.05 | Training Perplexity: 1.05\n",
      "| End of epoch  43 | Validation Loss: 0.05 | Validation Perplexity:     1.05\n",
      "Training Loss: 0.05 | Training Perplexity: 1.05\n",
      "| End of epoch  44 | Validation Loss: 0.04 | Validation Perplexity:     1.05\n",
      "Training Loss: 0.04 | Training Perplexity: 1.05\n",
      "| End of epoch  45 | Validation Loss: 0.04 | Validation Perplexity:     1.04\n",
      "Training Loss: 0.04 | Training Perplexity: 1.04\n",
      "| End of epoch  46 | Validation Loss: 0.04 | Validation Perplexity:     1.04\n",
      "Training Loss: 0.04 | Training Perplexity: 1.04\n",
      "| End of epoch  47 | Validation Loss: 0.04 | Validation Perplexity:     1.04\n",
      "Training Loss: 0.04 | Training Perplexity: 1.04\n",
      "| End of epoch  48 | Validation Loss: 0.04 | Validation Perplexity:     1.04\n",
      "Training Loss: 0.04 | Training Perplexity: 1.04\n",
      "| End of epoch  49 | Validation Loss: 0.03 | Validation Perplexity:     1.04\n",
      "Training Loss: 0.04 | Training Perplexity: 1.04\n",
      "| End of epoch  50 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  51 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  52 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  53 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  54 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  55 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  56 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  57 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  58 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  59 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  60 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  61 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  62 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  63 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  64 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch  65 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch  66 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch  67 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch  68 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch  69 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch  70 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch  71 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch  72 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch  73 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch  74 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.05 | Training Perplexity: 1.06\n",
      "| End of epoch  75 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.04 | Training Perplexity: 1.04\n",
      "| End of epoch  76 | Validation Loss: 0.04 | Validation Perplexity:     1.04\n",
      "Training Loss: 0.04 | Training Perplexity: 1.04\n",
      "| End of epoch  77 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.04 | Training Perplexity: 1.05\n",
      "| End of epoch  78 | Validation Loss: 0.17 | Validation Perplexity:     1.18\n",
      "Training Loss: 0.23 | Training Perplexity: 1.25\n",
      "| End of epoch  79 | Validation Loss: 0.79 | Validation Perplexity:     2.20\n",
      "Training Loss: 0.94 | Training Perplexity: 2.55\n",
      "| End of epoch  80 | Validation Loss: 0.72 | Validation Perplexity:     2.05\n",
      "Training Loss: 1.17 | Training Perplexity: 3.22\n",
      "| End of epoch  81 | Validation Loss: 1.29 | Validation Perplexity:     3.63\n",
      "Training Loss: 1.09 | Training Perplexity: 2.97\n",
      "| End of epoch  82 | Validation Loss: 0.71 | Validation Perplexity:     2.04\n",
      "Training Loss: 0.78 | Training Perplexity: 2.19\n",
      "| End of epoch  83 | Validation Loss: 0.57 | Validation Perplexity:     1.77\n",
      "Training Loss: 0.56 | Training Perplexity: 1.76\n",
      "| End of epoch  84 | Validation Loss: 0.45 | Validation Perplexity:     1.57\n",
      "Training Loss: 0.44 | Training Perplexity: 1.56\n",
      "| End of epoch  85 | Validation Loss: 0.35 | Validation Perplexity:     1.42\n",
      "Training Loss: 0.35 | Training Perplexity: 1.43\n",
      "| End of epoch  86 | Validation Loss: 0.29 | Validation Perplexity:     1.33\n",
      "Training Loss: 0.29 | Training Perplexity: 1.34\n",
      "| End of epoch  87 | Validation Loss: 0.26 | Validation Perplexity:     1.30\n",
      "Training Loss: 0.24 | Training Perplexity: 1.27\n",
      "| End of epoch  88 | Validation Loss: 0.21 | Validation Perplexity:     1.23\n",
      "Training Loss: 0.19 | Training Perplexity: 1.21\n",
      "| End of epoch  89 | Validation Loss: 0.17 | Validation Perplexity:     1.19\n",
      "Training Loss: 0.16 | Training Perplexity: 1.18\n",
      "| End of epoch  90 | Validation Loss: 0.15 | Validation Perplexity:     1.16\n",
      "Training Loss: 0.14 | Training Perplexity: 1.15\n",
      "| End of epoch  91 | Validation Loss: 0.14 | Validation Perplexity:     1.14\n",
      "Training Loss: 0.13 | Training Perplexity: 1.14\n",
      "| End of epoch  92 | Validation Loss: 0.11 | Validation Perplexity:     1.12\n",
      "Training Loss: 0.10 | Training Perplexity: 1.11\n",
      "| End of epoch  93 | Validation Loss: 0.09 | Validation Perplexity:     1.10\n",
      "Training Loss: 0.09 | Training Perplexity: 1.10\n",
      "| End of epoch  94 | Validation Loss: 0.08 | Validation Perplexity:     1.09\n",
      "Training Loss: 0.08 | Training Perplexity: 1.08\n",
      "| End of epoch  95 | Validation Loss: 0.07 | Validation Perplexity:     1.08\n",
      "Training Loss: 0.07 | Training Perplexity: 1.07\n",
      "| End of epoch  96 | Validation Loss: 0.06 | Validation Perplexity:     1.07\n",
      "Training Loss: 0.06 | Training Perplexity: 1.06\n",
      "| End of epoch  97 | Validation Loss: 0.06 | Validation Perplexity:     1.06\n",
      "Training Loss: 0.05 | Training Perplexity: 1.06\n",
      "| End of epoch  98 | Validation Loss: 0.05 | Validation Perplexity:     1.05\n",
      "Training Loss: 0.05 | Training Perplexity: 1.05\n",
      "| End of epoch  99 | Validation Loss: 0.05 | Validation Perplexity:     1.05\n",
      "Training Loss: 0.05 | Training Perplexity: 1.05\n",
      "| End of epoch 100 | Validation Loss: 0.04 | Validation Perplexity:     1.04\n",
      "Training Loss: 0.04 | Training Perplexity: 1.04\n",
      "| End of epoch 101 | Validation Loss: 0.04 | Validation Perplexity:     1.04\n",
      "Training Loss: 0.04 | Training Perplexity: 1.04\n",
      "| End of epoch 102 | Validation Loss: 0.04 | Validation Perplexity:     1.04\n",
      "Training Loss: 0.03 | Training Perplexity: 1.04\n",
      "| End of epoch 103 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch 104 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch 105 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch 106 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch 107 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch 108 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.03 | Training Perplexity: 1.03\n",
      "| End of epoch 109 | Validation Loss: 0.03 | Validation Perplexity:     1.03\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 110 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 111 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 112 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 113 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 114 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 115 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 116 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 117 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 118 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 119 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 120 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 121 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 122 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n",
      "Training Loss: 0.02 | Training Perplexity: 1.02\n",
      "| End of epoch 123 | Validation Loss: 0.02 | Validation Perplexity:     1.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msongs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Evaluate on the validation dataset\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, songs, criterion)  \u001b[38;5;66;03m# Replace `songs` with your validation dataset if available\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, songs, optimizer, criterion, bptt, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data_tensor, src_mask)\n\u001b[0;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, songs, optimizer, criterion, bptt=42, device='cpu'):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    total_items = 0\n",
    "\n",
    "    for song_index, song in enumerate(songs):\n",
    "        for i in range(0, len(song) - 1, bptt):\n",
    "            data, targets = get_batch(songs, i, song_index, bptt)\n",
    "            data_tensor = torch.tensor(data, dtype=torch.long).to(device)\n",
    "            targets_tensor = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "            src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data_tensor, src_mask)\n",
    "            loss = criterion(output.view(-1, ntokens), targets_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * data_tensor.size(0)\n",
    "            total_items += data_tensor.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_items\n",
    "    ppl = math.exp(avg_loss)\n",
    "    print(f'Training Loss: {avg_loss:.2f} | Training Perplexity: {ppl:.2f}')\n",
    "\n",
    "\n",
    "def evaluate(model, songs, criterion, bptt=42, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for song_index, song in enumerate(songs):\n",
    "            for i in range(0, len(song) - 1, bptt):\n",
    "                data, targets = get_batch(songs, i, song_index, bptt)\n",
    "                data_tensor = torch.tensor(data, dtype=torch.long).to(device)\n",
    "                targets_tensor = torch.tensor(targets, dtype=torch.long).to(device)\n",
    "                src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "                output = model(data_tensor, src_mask)\n",
    "                total_loss += len(data) * criterion(output.view(-1, ntokens), targets_tensor).item()\n",
    "\n",
    "    avg_loss = total_loss / sum(len(song) for song in songs)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# Parameters setup\n",
    "ntokens = 6 # number of unique notes\n",
    "emsize = 80 # embedding dimension\n",
    "nhid = 80 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 4 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4 # the number of heads in the multiheadattention models\n",
    "dropout = 0.0 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "epochs = 2000 # The number of epochs\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, songs, optimizer, criterion)\n",
    "    # Evaluate on the validation dataset\n",
    "    val_loss = evaluate(model, songs, criterion)  # Replace `songs` with your validation dataset if available\n",
    "    \n",
    "    # Calculate and print the validation perplexity\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    print(f'| End of epoch {epoch:3d} | Validation Loss: {val_loss:.2f} | Validation Perplexity: {val_ppl:8.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ecd203c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for song 0, index 0: 0.98\n",
      "Accuracy for song 0, index 1: 0.57\n",
      "Accuracy for song 0, index 2: 0.31\n",
      "Accuracy for song 0, index 3: 0.19\n",
      "Accuracy for song 0, index 4: 0.21\n",
      "Accuracy for song 0, index 5: 0.14\n",
      "Accuracy for song 0, index 6: 0.21\n",
      "Accuracy for song 0, index 7: 0.31\n",
      "Accuracy for song 0, index 8: 0.21\n",
      "Accuracy for song 0, index 9: 0.12\n",
      "Accuracy for song 0, index 10: 0.19\n",
      "Accuracy for song 0, index 11: 0.29\n",
      "Accuracy for song 0, index 12: 0.38\n",
      "Accuracy for song 0, index 13: 0.43\n",
      "Accuracy for song 0, index 14: 0.43\n",
      "Accuracy for song 0, index 15: 0.24\n",
      "Accuracy for song 0, index 16: 0.26\n",
      "Accuracy for song 0, index 17: 0.24\n",
      "Accuracy for song 0, index 18: 0.21\n",
      "Accuracy for song 0, index 19: 0.17\n",
      "Accuracy for song 0, index 20: 0.17\n",
      "Accuracy for song 0, index 21: 0.26\n",
      "Accuracy for song 0, index 22: 0.21\n",
      "Accuracy for song 0, index 23: 0.21\n",
      "Accuracy for song 0, index 24: 0.21\n",
      "Accuracy for song 0, index 25: 0.31\n",
      "Accuracy for song 0, index 26: 0.36\n",
      "Accuracy for song 0, index 27: 0.31\n",
      "Accuracy for song 0, index 28: 0.45\n",
      "Accuracy for song 0, index 29: 0.31\n",
      "Accuracy for song 0, index 30: 0.36\n",
      "Accuracy for song 0, index 31: 0.36\n",
      "Accuracy for song 0, index 32: 0.31\n",
      "Accuracy for song 0, index 33: 0.19\n",
      "Accuracy for song 0, index 34: 0.21\n",
      "Accuracy for song 0, index 35: 0.19\n",
      "Accuracy for song 0, index 36: 0.12\n",
      "Accuracy for song 0, index 37: 0.12\n",
      "Accuracy for song 0, index 38: 0.21\n",
      "Accuracy for song 0, index 39: 0.33\n",
      "Accuracy for song 0, index 40: 0.33\n",
      "Accuracy for song 0, index 41: 0.55\n",
      "Accuracy for song 1, index 0: 1.00\n",
      "Accuracy for song 1, index 1: 0.55\n",
      "Accuracy for song 1, index 2: 0.36\n",
      "Accuracy for song 1, index 3: 0.12\n",
      "Accuracy for song 1, index 4: 0.24\n",
      "Accuracy for song 1, index 5: 0.14\n",
      "Accuracy for song 1, index 6: 0.12\n",
      "Accuracy for song 1, index 7: 0.24\n",
      "Accuracy for song 1, index 8: 0.26\n",
      "Accuracy for song 1, index 9: 0.12\n",
      "Accuracy for song 1, index 10: 0.21\n",
      "Accuracy for song 1, index 11: 0.17\n",
      "Accuracy for song 1, index 12: 0.24\n",
      "Accuracy for song 1, index 13: 0.21\n",
      "Accuracy for song 1, index 14: 0.31\n",
      "Accuracy for song 1, index 15: 0.26\n",
      "Accuracy for song 1, index 16: 0.36\n",
      "Accuracy for song 1, index 17: 0.24\n",
      "Accuracy for song 1, index 18: 0.19\n",
      "Accuracy for song 1, index 19: 0.17\n",
      "Accuracy for song 1, index 20: 0.19\n",
      "Accuracy for song 1, index 21: 0.29\n",
      "Accuracy for song 1, index 22: 0.21\n",
      "Accuracy for song 1, index 23: 0.29\n",
      "Accuracy for song 1, index 24: 0.21\n",
      "Accuracy for song 1, index 25: 0.19\n",
      "Accuracy for song 1, index 26: 0.33\n",
      "Accuracy for song 1, index 27: 0.29\n",
      "Accuracy for song 1, index 28: 0.40\n",
      "Accuracy for song 1, index 29: 0.29\n",
      "Accuracy for song 1, index 30: 0.19\n",
      "Accuracy for song 1, index 31: 0.24\n",
      "Accuracy for song 1, index 32: 0.26\n",
      "Accuracy for song 1, index 33: 0.26\n",
      "Accuracy for song 1, index 34: 0.12\n",
      "Accuracy for song 1, index 35: 0.26\n",
      "Accuracy for song 1, index 36: 0.19\n",
      "Accuracy for song 1, index 37: 0.12\n",
      "Accuracy for song 1, index 38: 0.12\n",
      "Accuracy for song 1, index 39: 0.19\n",
      "Accuracy for song 1, index 40: 0.31\n",
      "Accuracy for song 1, index 41: 0.55\n",
      "Accuracy for song 2, index 0: 1.00\n",
      "Accuracy for song 2, index 1: 0.83\n",
      "Accuracy for song 2, index 2: 0.57\n",
      "Accuracy for song 2, index 3: 0.52\n",
      "Accuracy for song 2, index 4: 0.43\n",
      "Accuracy for song 2, index 5: 0.48\n",
      "Accuracy for song 2, index 6: 0.60\n",
      "Accuracy for song 2, index 7: 0.83\n",
      "Accuracy for song 2, index 8: 0.86\n",
      "Accuracy for song 2, index 9: 0.71\n",
      "Accuracy for song 2, index 10: 0.52\n",
      "Accuracy for song 2, index 11: 0.48\n",
      "Accuracy for song 2, index 12: 0.52\n",
      "Accuracy for song 2, index 13: 0.60\n",
      "Accuracy for song 2, index 14: 0.71\n",
      "Accuracy for song 2, index 15: 0.81\n",
      "Accuracy for song 2, index 16: 0.81\n",
      "Accuracy for song 2, index 17: 0.64\n",
      "Accuracy for song 2, index 18: 0.45\n",
      "Accuracy for song 2, index 19: 0.48\n",
      "Accuracy for song 2, index 20: 0.52\n",
      "Accuracy for song 2, index 21: 0.60\n",
      "Accuracy for song 2, index 22: 0.79\n",
      "Accuracy for song 2, index 23: 0.79\n",
      "Accuracy for song 2, index 24: 0.76\n",
      "Accuracy for song 2, index 25: 0.60\n",
      "Accuracy for song 2, index 26: 0.36\n",
      "Accuracy for song 2, index 27: 0.38\n",
      "Accuracy for song 2, index 28: 0.60\n",
      "Accuracy for song 2, index 29: 0.67\n",
      "Accuracy for song 2, index 30: 0.83\n",
      "Accuracy for song 2, index 31: 0.71\n",
      "Accuracy for song 2, index 32: 0.71\n",
      "Accuracy for song 2, index 33: 0.52\n",
      "Accuracy for song 2, index 34: 0.40\n",
      "Accuracy for song 2, index 35: 0.33\n",
      "Accuracy for song 2, index 36: 0.60\n",
      "Accuracy for song 2, index 37: 0.86\n",
      "Accuracy for song 2, index 38: 1.00\n",
      "Accuracy for song 2, index 39: 0.83\n",
      "Accuracy for song 2, index 40: 0.57\n",
      "Accuracy for song 2, index 41: 0.52\n"
     ]
    }
   ],
   "source": [
    "for i ,song in enumerate(songs):\n",
    "    for j in range(bptt):\n",
    "        data,target=get_batch(songs,j, i)\n",
    "        # Convert the input notes to a tensor and add an extra dimension\n",
    "\n",
    "        # Create a mask for the input\n",
    "        src_mask = generate_square_subsequent_mask(len(data)).to(device)\n",
    "\n",
    "        # Run the model on the input tensor\n",
    "        output = model(torch.tensor(data,dtype=torch.long),src_mask)\n",
    "\n",
    "        # Detach the output from the computation graph and convert to numpy array\n",
    "        output_array = output.detach().numpy()\n",
    "\n",
    "        # Get the predicted notes by finding the index of the maximum value in each output vector\n",
    "        predicted_notes = np.argmax(output_array, axis=-1)\n",
    "\n",
    "        predicted_notes_list = [note for sublist in predicted_notes for note in sublist]  # Flattening the list\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(np.array(predicted_notes_list) == np.array(target))\n",
    "        print(f'Accuracy for song {i}, index {j}: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
