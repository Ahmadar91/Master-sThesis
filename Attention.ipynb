{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:31.277897100Z",
     "start_time": "2023-12-06T17:00:30.656125100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:39.317492Z",
     "start_time": "2023-12-06T17:00:34.720993Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import numpy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib.pyplot import figure, subplots, imshow, xticks, yticks, title\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.cluster import mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "from statistics import mean\n",
    "from scipy.stats import entropy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim**0.5)\n",
    "        attention = self.softmax(scores)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:41.605650400Z",
     "start_time": "2023-12-06T17:00:41.594151100Z"
    }
   },
   "outputs": [],
   "source": [
    "class NetRNN(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Expansion layer to match CustomRNN\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        self.rnnLayer = nn.RNN(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        x = self.expand_layer(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        self.h0 = torch.Tensor(numpy.zeros((1, x.shape[0], self.hidden_dim)))\n",
    "        out, self.h0 = self.rnnLayer(x, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "        out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.Tensor(numpy.zeros((1, 1, self.hidden_dim)))\n",
    "            for i in range(x.shape[1]):\n",
    "                # Apply the expansion layer to each step\n",
    "                step_input = self.expand_layer(x[l][i].reshape((1, 1, self.inp)))\n",
    "                step_input = torch.tanh(step_input)\n",
    "\n",
    "                out, h0 = self.rnnLayer(step_input, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "            out = torch.tanh(out)\n",
    "            out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "            for i in range(x.shape[1]):\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "        return numpy.array(O), numpy.array(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:00:40.939651100Z",
     "start_time": "2023-12-06T17:00:40.913651900Z"
    }
   },
   "outputs": [],
   "source": [
    "class NetRNNWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetRNNWithAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = SelfAttention(inp)  # Assuming SelfAttention is defined elsewhere\n",
    "\n",
    "        # Expansion layer to match CustomRNN\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        self.rnnLayer = nn.RNN(self.hidden_dim, self.hidden_dim, batch_first=True, nonlinearity='tanh')\n",
    "        \n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(x)\n",
    "\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        expanded_attn_out = self.expand_layer(attn_out)\n",
    "        expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "        # RNN processing\n",
    "        h0 = torch.zeros(1, x.shape[0], self.hidden_dim)\n",
    "        rnn_out, _ = self.rnnLayer(expanded_attn_out, h0)\n",
    "        rnn_out = torch.tanh(rnn_out)\n",
    "\n",
    "        # Final output layer\n",
    "        out = torch.tanh(self.outputLayer(rnn_out[:, -1, :])).squeeze()\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                # Apply the expansion layer with tanh activation\n",
    "                expanded_attn_out = self.expand_layer(attn_out)\n",
    "                expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "                # RNN processing\n",
    "                out, h0 = self.rnnLayer(expanded_attn_out, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRNNWithAttentionExpFirst(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetRNNWithAttentionExpFirst, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Expansion layer\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # Attention layer applied after expansion\n",
    "        self.attention = SelfAttention(self.hidden_dim)  # Assuming SelfAttention is defined elsewhere\n",
    "\n",
    "        # RNN layer\n",
    "        self.rnnLayer = nn.RNN(self.hidden_dim, self.hidden_dim, batch_first=True, nonlinearity='tanh')\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        expanded_x = self.expand_layer(x)\n",
    "        expanded_x = torch.tanh(expanded_x)\n",
    "\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(expanded_x)\n",
    "\n",
    "        # RNN processing\n",
    "        h0 = torch.zeros(1, attn_out.shape[0], self.hidden_dim)\n",
    "        rnn_out, _ = self.rnnLayer(attn_out, h0)\n",
    "        rnn_out = torch.tanh(rnn_out)\n",
    "\n",
    "        # Final output layer\n",
    "        out = torch.tanh(self.outputLayer(rnn_out[:, -1, :])).squeeze()\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Apply the expansion layer with tanh activation\n",
    "                expanded_input = self.expand_layer(x[l][i].reshape((1, 1, self.inp)))\n",
    "                expanded_input = torch.tanh(expanded_input)\n",
    "\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(expanded_input)\n",
    "\n",
    "                # RNN processing\n",
    "                out, h0 = self.rnnLayer(attn_out, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "\n",
    "model = NetRNNWithAttention(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Expansion layer\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstmLayer = nn.LSTM(self.hidden_dim, int(self.hidden_dim/2), 1, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(int(self.hidden_dim/2), 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        x = self.expand_layer(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        # Initialize hidden and cell states\n",
    "        self.h0 = (torch.zeros(1, x.shape[0], int(self.hidden_dim/2)),\n",
    "                   torch.zeros(1, x.shape[0], int(self.hidden_dim/2)))\n",
    "\n",
    "        # LSTM processing\n",
    "        out, self.h0 = self.lstmLayer(x, self.h0)\n",
    "        out = torch.tanh(out)  # Apply tanh to the LSTM output if needed\n",
    "\n",
    "        # Concatenate hidden and cell states\n",
    "        hh = numpy.concatenate((self.h0[0].detach().numpy(), self.h0[1].detach().numpy()), 2)\n",
    "        self.hidden.append(hh)\n",
    "\n",
    "        # Final output layer with tanh activation\n",
    "        out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = (torch.zeros(1, 1, int(self.hidden_dim/2)),\n",
    "                  torch.zeros(1, 1, int(self.hidden_dim/2)))\n",
    "            for i in range(x.shape[1]):\n",
    "                # Apply the expansion layer to each step\n",
    "                step_input = self.expand_layer(x[l][i].reshape((1, 1, self.inp)))\n",
    "                step_input = torch.tanh(step_input)\n",
    "\n",
    "                out, h0 = self.lstmLayer(step_input, h0)\n",
    "                hh = numpy.concatenate((h0[0].detach().numpy().flatten(), h0[1].detach().numpy().flatten()))\n",
    "                H.append(hh.flatten())\n",
    "\n",
    "            out = torch.tanh(out)  # Apply tanh to the LSTM output if needed\n",
    "            out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "            for i in range(x.shape[1]):\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "        return numpy.array(O), numpy.array(H)\n",
    "    \n",
    "model = NetLSTM(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetLSTMWithAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = SelfAttention(inp)  # Assuming SelfAttention is defined elsewhere\n",
    "\n",
    "        # Expansion layer to match CustomRNN\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstmLayer = nn.LSTM(self.hidden_dim, int(self.hidden_dim/2), batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(int(self.hidden_dim/2), 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(x)\n",
    "\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        expanded_attn_out = self.expand_layer(attn_out)\n",
    "        expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "        # LSTM processing\n",
    "        h0 = (torch.zeros(1, x.shape[0], int(self.hidden_dim/2)),\n",
    "              torch.zeros(1, x.shape[0], int(self.hidden_dim/2)))\n",
    "        lstm_out, _ = self.lstmLayer(expanded_attn_out, h0)\n",
    "        lstm_out = torch.tanh(lstm_out)\n",
    "\n",
    "        # Final output layer\n",
    "        out = torch.tanh(self.outputLayer(lstm_out[:, -1, :])).squeeze()\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = (torch.zeros(1, 1, int(self.hidden_dim/2)),\n",
    "                  torch.zeros(1, 1, int(self.hidden_dim/2)))\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                # Apply the expansion layer with tanh activation\n",
    "                expanded_attn_out = self.expand_layer(attn_out)\n",
    "                expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "                # LSTM processing\n",
    "                out, h0 = self.lstmLayer(expanded_attn_out, h0)\n",
    "                H.append(torch.cat((h0[0].detach(), h0[1].detach()), 2).numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "    \n",
    "model = NetLSTMWithAttention(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetGRU(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetGRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Expansion layer\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # GRU layer\n",
    "        self.gruLayer = nn.GRU(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        x = self.expand_layer(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        # GRU processing\n",
    "        self.h0 = torch.zeros(1, x.shape[0], self.hidden_dim)\n",
    "        out, self.h0 = self.gruLayer(x, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "\n",
    "        # Final output layer with tanh activation\n",
    "        out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Apply the expansion layer to each step\n",
    "                step_input = self.expand_layer(x[l][i].reshape((1, 1, self.inp)))\n",
    "                step_input = torch.tanh(step_input)\n",
    "\n",
    "                # GRU processing\n",
    "                out, h0 = self.gruLayer(step_input, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)  # Apply tanh to the GRU output if needed\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "    \n",
    "model = NetGRU(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetGRUMWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim=12, inp=3):\n",
    "        super(NetGRUMWithAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inp = inp\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = SelfAttention(inp)  # Assuming SelfAttention is defined elsewhere\n",
    "\n",
    "        # Expansion layer\n",
    "        self.expand_layer = nn.Linear(in_features=self.inp, out_features=self.hidden_dim)\n",
    "\n",
    "        # GRU layer\n",
    "        self.gruLayer = nn.GRU(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(self.hidden_dim, 3)\n",
    "\n",
    "        self.resetHidden()\n",
    "\n",
    "    def resetHidden(self):\n",
    "        self.hidden = list()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply attention\n",
    "        attn_out = self.attention(x)\n",
    "\n",
    "        # Apply the expansion layer with tanh activation\n",
    "        expanded_attn_out = self.expand_layer(attn_out)\n",
    "        expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "        # GRU processing\n",
    "        self.h0 = torch.zeros(1, x.shape[0], self.hidden_dim)\n",
    "        out, self.h0 = self.gruLayer(expanded_attn_out, self.h0)\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        self.hidden.append(copy.deepcopy(self.h0.detach().numpy()))\n",
    "\n",
    "        # Final output layer with tanh activation\n",
    "        out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "        return out\n",
    "\n",
    "    def step(self, x):\n",
    "        O = []\n",
    "        H = []\n",
    "        for l in range(x.shape[0]):\n",
    "            h0 = torch.zeros(1, 1, self.hidden_dim)\n",
    "            for i in range(x.shape[1]):\n",
    "                # Applying attention to each timestep\n",
    "                attn_out = self.attention(x[l][i].reshape((1, 1, self.inp)))\n",
    "\n",
    "                # Apply the expansion layer with tanh activation\n",
    "                expanded_attn_out = self.expand_layer(attn_out)\n",
    "                expanded_attn_out = torch.tanh(expanded_attn_out)\n",
    "\n",
    "                # GRU processing\n",
    "                out, h0 = self.gruLayer(expanded_attn_out, h0)\n",
    "                H.append(out.detach().numpy().flatten())\n",
    "\n",
    "                out = torch.tanh(out)  # Apply tanh to the GRU output if needed\n",
    "                out = torch.tanh(self.outputLayer(out[:, -1, :]))\n",
    "                O.append(out.detach().numpy().flatten())\n",
    "\n",
    "        return np.array(O), np.array(H)\n",
    "\n",
    "model = NetGRUMWithAttention(hidden_dim=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:23.990361100Z",
     "start_time": "2023-12-06T18:19:23.969358500Z"
    }
   },
   "outputs": [],
   "source": [
    "def generateTrainData(num_samples, params):\n",
    "    s = []  # Sequences\n",
    "    t = []  # Labels\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        common_length = np.random.randint(params[\"min_length\"], params[\"max_length\"] + 1)\n",
    "\n",
    "        array_A = np.full(common_length, params[\"fill\"])\n",
    "        array_B = np.full(common_length, params[\"fill\"])\n",
    "        array_C = np.full(common_length, params[\"fill\"])\n",
    "\n",
    "        # Exclude the last two indices\n",
    "        possible_indices = np.arange(common_length - 2)\n",
    "\n",
    "        index_A = np.random.choice(possible_indices)\n",
    "        value_A = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_A[index_A] = value_A\n",
    "\n",
    "        # Update possible indices for array B to also exclude index_A\n",
    "        possible_indices_B = np.delete(possible_indices, np.where(possible_indices == index_A))\n",
    "        index_B = np.random.choice(possible_indices_B)\n",
    "        value_B = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_B[index_B] = value_B\n",
    "\n",
    "        value_C = np.random.choice([params[\"value_1\"], params[\"value_2\"]])\n",
    "        array_C[-1] = value_C\n",
    "        array_C[-2] = value_C\n",
    "\n",
    "        mapped_value_A = 1 if value_A == params[\"value_2\"] else 0\n",
    "        mapped_value_B = 1 if value_B == params[\"value_2\"] else 0\n",
    "        result = int((mapped_value_A != mapped_value_B) if value_C == params[\"value_1\"] else (mapped_value_A == mapped_value_B))\n",
    "\n",
    "        # Mapping back to original value_1 and value_2 for the label\n",
    "        label_value_A = params[\"value_2\"] if mapped_value_A == 1 else params[\"value_1\"]\n",
    "        label_value_B = params[\"value_2\"] if mapped_value_B == 1 else params[\"value_1\"]\n",
    "        label_value_C = params[\"value_2\"] if result == 1 else params[\"value_1\"]\n",
    "\n",
    "        #label_arr = [mapped_value_A, mapped_value_B, result]  # Label array with value_A, value_B, and result\n",
    "        label_arr = [label_value_A, label_value_B, label_value_C]  # Label array with value_A, value_B, and result\n",
    "\n",
    "        combined_array = np.vstack([array_A, array_B, array_C]).T\n",
    "        s.append(combined_array)\n",
    "        t.append(label_arr)\n",
    "\n",
    "    return s, np.array(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:26.878858500Z",
     "start_time": "2023-12-06T18:19:26.864361100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0,  0,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  1,  0],\n",
      "       [ 1,  0,  0],\n",
      "       [ 0,  0, -1],\n",
      "       [ 0,  0, -1]]), array([[-1,  0,  0],\n",
      "       [ 0,  1,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0, -1],\n",
      "       [ 0,  0, -1]])]\n",
      "[[ 1  1 -1]\n",
      " [-1  1  1]]\n"
     ]
    }
   ],
   "source": [
    "num_seq = 2\n",
    "# Example dictionary with parameters\n",
    "parameters = {\"min_length\": 5, \"max_length\": 10, \"fill\": 0, \"value_1\": -1, \"value_2\": 1}\n",
    "\n",
    "sequences, labels = generateTrainData(num_seq, parameters)\n",
    "print(sequences)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:28.905358900Z",
     "start_time": "2023-12-06T18:19:28.879361600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [-1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  1.],\n",
      "         [ 0.,  0.,  1.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.],\n",
      "         [ 0., -1.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  1.],\n",
      "         [ 0.,  0.,  1.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0., -1.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  1.],\n",
      "         [ 0.,  0.,  1.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [-1.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0., -1.],\n",
      "         [ 0.,  0., -1.]],\n",
      "\n",
      "        [[ 1.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  1.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.],\n",
      "         [ 0.,  0., -1.],\n",
      "         [ 0.,  0., -1.]]])\n",
      "torch.Size([5, 3])\n",
      "NetRNN(\n",
      "  (expand_layer): Linear(in_features=3, out_features=12, bias=True)\n",
      "  (rnnLayer): RNN(12, 12, batch_first=True)\n",
      "  (outputLayer): Linear(in_features=12, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"min_length\": 10,\n",
    "    \"max_length\": 10,\n",
    "    \"fill\": 0,\n",
    "    \"value_1\": -1,\n",
    "    \"value_2\": 1,\n",
    "}\n",
    "sequences, labels = generateTrainData(5, parameters)\n",
    "model = NetRNN()\n",
    "output=model(torch.Tensor(sequences))\n",
    "print(torch.Tensor(sequences))\n",
    "print(output.shape)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:03:38.917270600Z",
     "start_time": "2023-12-06T18:03:36.540272600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_25736\\1362812023.py:40: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  plt.tight_layout(rect=[0, 0, 0.9, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"min_length\": 10,\n",
    "    \"max_length\": 15,\n",
    "    \"fill\": 0,\n",
    "    \"value_1\": -1,\n",
    "    \"value_2\": 1,\n",
    "}\n",
    "sequences, labels = generateTrainData(5, parameters)\n",
    "\n",
    "def plot_sequences(sequences, labels):\n",
    "    num_samples = len(sequences)  # Number of samples to display\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(3 * num_samples, 15))\n",
    "\n",
    "    for i, (seq, label) in enumerate(zip(sequences, labels)):\n",
    "        reshaped_sequence = seq  # Use the sequence as it is\n",
    "\n",
    "        ax = plt.subplot(1, num_samples, i + 1)\n",
    "        img = ax.imshow(\n",
    "            reshaped_sequence, cmap=\"gray\", vmin=-1.0, vmax=1.0\n",
    "        )  # Adjusted vmin and vmax\n",
    "\n",
    "        operation_title = \"XOR\" if label[2] == 0 else \"XNOR\"\n",
    "        ax.set_title(f\"Operation: {operation_title}, {label}\")\n",
    "\n",
    "        ax.set_xlabel(\"Arrays (A, B, C)\")\n",
    "        ax.set_ylabel(\"Time Points\")\n",
    "        ax.set_xticks(range(3))\n",
    "        ax.set_xticklabels([\"A\", \"B\", \"C\"])\n",
    "        ax.set_yticks(range(reshaped_sequence.shape[0]))\n",
    "        ax.set_yticklabels([f\"{j+1}\" for j in range(reshaped_sequence.shape[0])])\n",
    "\n",
    "    # Adjusted positioning of colorbar\n",
    "    cbar_ax = plt.gcf().add_axes([0.93, 0.15, 0.02, 0.7])\n",
    "    cbar = plt.colorbar(img, cax=cbar_ax)\n",
    "    cbar.set_ticks([-1, 0, 1])\n",
    "    cbar.set_ticklabels([\"-1\", \"0\", \"1\"])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming sequences and labels are already generated using generateTrainData\n",
    "plot_sequences(sequences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:01:02.144838900Z",
     "start_time": "2023-12-06T17:01:02.130460400Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters_list = []\n",
    "\n",
    "min_lengths = [5, 10, 10, 20, 20, 40, 40]\n",
    "max_lengths = [5, 10, 15, 20, 25, 40, 45]\n",
    "\n",
    "for min_len, max_len in zip(min_lengths, max_lengths):\n",
    "    parameters_list.append(\n",
    "        {\n",
    "            \"min_length\": min_len,\n",
    "            \"max_length\": max_len,\n",
    "            \"fill\": 0,\n",
    "            \"value_1\": -1,\n",
    "            \"value_2\": 1,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:19:44.759873Z",
     "start_time": "2023-12-06T18:19:42.281374100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN, rep: 0, epoch: 1, acc: 0.5033334493637085, Loss 1.038053160905838\n",
      "RNN, rep: 0, epoch: 2, acc: 0.5899999737739563, Loss 0.9599295181035995\n",
      "RNN, rep: 0, epoch: 3, acc: 0.6933333873748779, Loss 0.7754093664884567\n",
      "RNN, rep: 0, epoch: 4, acc: 0.7333332896232605, Loss 0.6606944778561592\n",
      "RNN, rep: 0, epoch: 5, acc: 0.7033334374427795, Loss 0.6737125924229622\n",
      "RNN, rep: 0, epoch: 6, acc: 0.6966667175292969, Loss 0.6594766819477081\n",
      "RNN, rep: 0, epoch: 7, acc: 0.800000011920929, Loss 0.5985074463486671\n",
      "RNN, rep: 0, epoch: 8, acc: 0.8299997448921204, Loss 0.562473874092102\n",
      "RNN, rep: 0, epoch: 9, acc: 0.8366665840148926, Loss 0.49017244189977643\n",
      "RNN, rep: 0, epoch: 10, acc: 0.8999999761581421, Loss 0.3968024541437626\n",
      "RNN, rep: 0, epoch: 11, acc: 0.9333332777023315, Loss 0.3546666117012501\n",
      "RNN, rep: 0, epoch: 12, acc: 0.9399998188018799, Loss 0.3218146346509457\n",
      "RNN, rep: 0, epoch: 13, acc: 0.9800000190734863, Loss 0.25499204561114314\n",
      "RNN                  Rep: 0   Epoch: 1     Acc: 0.9800 Params: min_length: 5, max_length: 5, fill: 0, value_1: -1, value_2: 1 Time: 2.47 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 3])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.5199999213218689, Loss 1.0257568114995956\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.6366668939590454, Loss 0.9050984835624695\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.7733331918716431, Loss 0.721352756023407\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.699999988079071, Loss 0.660465299487114\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.8166667222976685, Loss 0.5322286289930344\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.8433331847190857, Loss 0.39664523750543595\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.8766664862632751, Loss 0.33793956622481347\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.9266665577888489, Loss 0.30118595480918886\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.940000057220459, Loss 0.2603044325113297\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.9699997901916504, Loss 0.19179649297147988\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.9700000286102295, Loss 0.19939239766448735\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 1     Acc: 0.9700 Params: min_length: 5, max_length: 5, fill: 0, value_1: -1, value_2: 1 Time: 2.91 sec\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 1, acc: 0.5066666603088379, Loss 1.0087509900331497\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 2, acc: 0.5766667723655701, Loss 0.9520100581645966\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 3, acc: 0.7233333587646484, Loss 0.7731778442859649\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 4, acc: 0.8033332228660583, Loss 0.601063072681427\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 5, acc: 0.8866666555404663, Loss 0.4662717132270336\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 6, acc: 0.8966666460037231, Loss 0.3275235505402088\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 7, acc: 0.93666672706604, Loss 0.2396897164732218\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 8, acc: 1.0, Loss 0.1730939272791147\n",
      "NetRNNWithAttentionExpFirst Rep: 0   Epoch: 1     Acc: 1.0000 Params: min_length: 5, max_length: 5, fill: 0, value_1: -1, value_2: 1 Time: 2.28 sec\n",
      "LSTM, rep: 0, epoch: 1, acc: 0.5133330821990967, Loss 1.0287481611967086\n",
      "LSTM, rep: 0, epoch: 2, acc: 0.5666666626930237, Loss 0.9767631155252456\n",
      "LSTM, rep: 0, epoch: 3, acc: 0.6733335256576538, Loss 0.864891996383667\n",
      "LSTM, rep: 0, epoch: 4, acc: 0.7466665506362915, Loss 0.7077643555402756\n",
      "LSTM, rep: 0, epoch: 5, acc: 0.8233332633972168, Loss 0.6439558303356171\n",
      "LSTM, rep: 0, epoch: 6, acc: 0.8433331847190857, Loss 0.5778057351708412\n",
      "LSTM, rep: 0, epoch: 7, acc: 0.9166665077209473, Loss 0.47187436133623123\n",
      "LSTM, rep: 0, epoch: 8, acc: 0.9099997878074646, Loss 0.40165397331118585\n",
      "LSTM, rep: 0, epoch: 9, acc: 0.8933331370353699, Loss 0.3619370764493942\n",
      "LSTM, rep: 0, epoch: 10, acc: 0.916666567325592, Loss 0.3179730339348316\n",
      "LSTM, rep: 0, epoch: 11, acc: 0.9299999475479126, Loss 0.29651630252599714\n",
      "LSTM, rep: 0, epoch: 12, acc: 0.929999828338623, Loss 0.26516793921589854\n",
      "LSTM, rep: 0, epoch: 13, acc: 0.9533332586288452, Loss 0.235553822517395\n",
      "LSTM, rep: 0, epoch: 14, acc: 0.986666738986969, Loss 0.19764192782342435\n",
      "LSTM                 Rep: 0   Epoch: 1     Acc: 0.9867 Params: min_length: 5, max_length: 5, fill: 0, value_1: -1, value_2: 1 Time: 3.78 sec\n",
      "NetLSTMWithAttention, rep: 0, epoch: 1, acc: 0.4766666293144226, Loss 1.0457814311981202\n",
      "NetLSTMWithAttention, rep: 0, epoch: 2, acc: 0.5700000524520874, Loss 0.9973041725158691\n",
      "NetLSTMWithAttention, rep: 0, epoch: 3, acc: 0.5600001811981201, Loss 0.9834455269575119\n",
      "NetLSTMWithAttention, rep: 0, epoch: 4, acc: 0.6733332872390747, Loss 0.8359760898351669\n",
      "NetLSTMWithAttention, rep: 0, epoch: 5, acc: 0.7033333778381348, Loss 0.6793393859267235\n",
      "NetLSTMWithAttention, rep: 0, epoch: 6, acc: 0.8066666126251221, Loss 0.5718925848603249\n",
      "NetLSTMWithAttention, rep: 0, epoch: 7, acc: 0.8966665863990784, Loss 0.4407915945351124\n",
      "NetLSTMWithAttention, rep: 0, epoch: 8, acc: 0.9266666173934937, Loss 0.3882405024766922\n",
      "NetLSTMWithAttention, rep: 0, epoch: 9, acc: 0.9233333468437195, Loss 0.3330612647533417\n",
      "NetLSTMWithAttention, rep: 0, epoch: 10, acc: 0.9633334279060364, Loss 0.297312056273222\n",
      "NetLSTMWithAttention, rep: 0, epoch: 11, acc: 0.9733332991600037, Loss 0.25572234600782395\n",
      "NetLSTMWithAttention Rep: 0   Epoch: 1     Acc: 0.9733 Params: min_length: 5, max_length: 5, fill: 0, value_1: -1, value_2: 1 Time: 3.72 sec\n",
      "GRU, rep: 0, epoch: 1, acc: 0.5233331918716431, Loss 1.0262404757738113\n",
      "GRU, rep: 0, epoch: 2, acc: 0.6266669034957886, Loss 0.9300768589973449\n",
      "GRU, rep: 0, epoch: 3, acc: 0.7700001001358032, Loss 0.8414335852861404\n",
      "GRU, rep: 0, epoch: 4, acc: 0.7833333015441895, Loss 0.6542739295959472\n",
      "GRU, rep: 0, epoch: 5, acc: 0.8333331942558289, Loss 0.543733549118042\n",
      "GRU, rep: 0, epoch: 6, acc: 0.8433333039283752, Loss 0.45447091162204745\n",
      "GRU, rep: 0, epoch: 7, acc: 0.8533332943916321, Loss 0.383549779355526\n",
      "GRU, rep: 0, epoch: 8, acc: 0.8833332061767578, Loss 0.32687260806560514\n",
      "GRU, rep: 0, epoch: 9, acc: 0.8633330464363098, Loss 0.32555687084794044\n",
      "GRU, rep: 0, epoch: 10, acc: 0.9366664886474609, Loss 0.29429664939641953\n",
      "GRU, rep: 0, epoch: 11, acc: 0.9966667294502258, Loss 0.23942283064126968\n",
      "GRU                  Rep: 0   Epoch: 1     Acc: 0.9967 Params: min_length: 5, max_length: 5, fill: 0, value_1: -1, value_2: 1 Time: 2.83 sec\n",
      "NetGRUWithAttention, rep: 0, epoch: 1, acc: 0.5066666603088379, Loss 1.0200762915611268\n",
      "NetGRUWithAttention, rep: 0, epoch: 2, acc: 0.5099999308586121, Loss 0.9961845940351486\n",
      "NetGRUWithAttention, rep: 0, epoch: 3, acc: 0.6233334541320801, Loss 0.9697831708192826\n",
      "NetGRUWithAttention, rep: 0, epoch: 4, acc: 0.7166668772697449, Loss 0.9166489851474762\n",
      "NetGRUWithAttention, rep: 0, epoch: 5, acc: 0.7166666388511658, Loss 0.7692964321374893\n",
      "NetGRUWithAttention, rep: 0, epoch: 6, acc: 0.7233332991600037, Loss 0.6333921259641647\n",
      "NetGRUWithAttention, rep: 0, epoch: 7, acc: 0.8733331561088562, Loss 0.4716807344555855\n",
      "NetGRUWithAttention, rep: 0, epoch: 8, acc: 0.8499998450279236, Loss 0.3639566496014595\n",
      "NetGRUWithAttention, rep: 0, epoch: 9, acc: 0.90666663646698, Loss 0.31224955633282664\n",
      "NetGRUWithAttention, rep: 0, epoch: 10, acc: 0.9233331084251404, Loss 0.2765172471106052\n",
      "NetGRUWithAttention, rep: 0, epoch: 11, acc: 0.9333332777023315, Loss 0.22959070295095443\n",
      "NetGRUWithAttention, rep: 0, epoch: 12, acc: 0.986666738986969, Loss 0.20549695782363414\n",
      "NetGRUWithAttention  Rep: 0   Epoch: 1     Acc: 0.9867 Params: min_length: 5, max_length: 5, fill: 0, value_1: -1, value_2: 1 Time: 4.43 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.5466668605804443, Loss 0.9990288436412811\n",
      "RNN, rep: 0, epoch: 2, acc: 0.5066664814949036, Loss 1.0072814804315566\n",
      "RNN, rep: 0, epoch: 3, acc: 0.5266665816307068, Loss 1.0075883382558823\n",
      "RNN, rep: 0, epoch: 4, acc: 0.5500003695487976, Loss 0.9967803925275802\n",
      "RNN, rep: 0, epoch: 5, acc: 0.573333203792572, Loss 0.948558418750763\n",
      "RNN, rep: 0, epoch: 6, acc: 0.6566666960716248, Loss 0.8048816603422165\n",
      "RNN, rep: 0, epoch: 7, acc: 0.6833335161209106, Loss 0.7443095642328262\n",
      "RNN, rep: 0, epoch: 8, acc: 0.6933332085609436, Loss 0.6527596825361252\n",
      "RNN, rep: 0, epoch: 9, acc: 0.7133336067199707, Loss 0.7066753754019737\n",
      "RNN, rep: 0, epoch: 10, acc: 0.7033336162567139, Loss 0.6859223711490631\n",
      "RNN, rep: 0, epoch: 11, acc: 0.7266666293144226, Loss 0.6445826631784439\n",
      "RNN, rep: 0, epoch: 12, acc: 0.7533334493637085, Loss 0.6219968345761299\n",
      "RNN, rep: 0, epoch: 13, acc: 0.7733333706855774, Loss 0.5749234169721603\n",
      "RNN, rep: 0, epoch: 14, acc: 0.8100000023841858, Loss 0.5681481562554836\n",
      "RNN, rep: 0, epoch: 15, acc: 0.8433331251144409, Loss 0.5299848589301109\n",
      "RNN, rep: 0, epoch: 16, acc: 0.8366665840148926, Loss 0.5144357632100582\n",
      "RNN, rep: 0, epoch: 17, acc: 0.8400000929832458, Loss 0.5630696588754653\n",
      "RNN, rep: 0, epoch: 18, acc: 0.8666665554046631, Loss 0.4816850361227989\n",
      "RNN, rep: 0, epoch: 19, acc: 0.8666664958000183, Loss 0.46099930003285405\n",
      "RNN, rep: 0, epoch: 20, acc: 0.856666624546051, Loss 0.4756456395983696\n",
      "RNN, rep: 0, epoch: 21, acc: 0.8733334541320801, Loss 0.4531953303515911\n",
      "RNN, rep: 0, epoch: 22, acc: 0.8500000238418579, Loss 0.46107924714684484\n",
      "RNN, rep: 0, epoch: 23, acc: 0.8633332252502441, Loss 0.4178567224740982\n",
      "RNN, rep: 0, epoch: 24, acc: 0.8633332848548889, Loss 0.4233522893488407\n",
      "RNN, rep: 0, epoch: 25, acc: 0.8699999451637268, Loss 0.38349809221923353\n",
      "RNN, rep: 0, epoch: 26, acc: 0.8799999356269836, Loss 0.3817580882459879\n",
      "RNN, rep: 0, epoch: 27, acc: 0.9333332180976868, Loss 0.33276860632002353\n",
      "RNN, rep: 0, epoch: 28, acc: 0.9399999380111694, Loss 0.3283888275921345\n",
      "RNN, rep: 0, epoch: 29, acc: 0.9299997687339783, Loss 0.32823118269443513\n",
      "RNN, rep: 0, epoch: 30, acc: 0.9733332991600037, Loss 0.28188939318060874\n",
      "RNN                  Rep: 0   Epoch: 1     Acc: 0.9733 Params: min_length: 10, max_length: 10, fill: 0, value_1: -1, value_2: 1 Time: 6.99 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.4666666090488434, Loss 1.058396565914154\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.5166667103767395, Loss 0.9929685169458389\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5100001096725464, Loss 1.0060064971446991\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5900001525878906, Loss 0.9707858687639237\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.6300000548362732, Loss 0.9330907589197159\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.7099999189376831, Loss 0.781548735499382\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.7466667294502258, Loss 0.678406754732132\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.8166665434837341, Loss 0.5668185122311116\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.8499999046325684, Loss 0.41821666330099105\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.8199997544288635, Loss 0.4033875280618668\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.8566665649414062, Loss 0.3480457362532616\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.8199999332427979, Loss 0.3734484426677227\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.8699999451637268, Loss 0.3399493882060051\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.8466666340827942, Loss 0.3535528691112995\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.8699999451637268, Loss 0.3294657252728939\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.9133332967758179, Loss 0.3133124840259552\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.8799998760223389, Loss 0.3191766135394573\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.9000000953674316, Loss 0.2924453733861446\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.9366665482521057, Loss 0.2456722918152809\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.9399998188018799, Loss 0.21208128444850444\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.9233332872390747, Loss 0.22215155359357597\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.9066663980484009, Loss 0.22664300825446845\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.8866667151451111, Loss 0.21718343250453473\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.9033331274986267, Loss 0.1887950395606458\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.8966666460037231, Loss 0.20372827408835292\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.9133332967758179, Loss 0.1715656780079007\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.9233333468437195, Loss 0.17221724696457386\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.9366664886474609, Loss 0.15565726546570657\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.9366666674613953, Loss 0.15774846101179718\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.9333332777023315, Loss 0.17731481983326375\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.9800000190734863, Loss 0.14838859484530986\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 1     Acc: 0.9800 Params: min_length: 10, max_length: 10, fill: 0, value_1: -1, value_2: 1 Time: 9.26 sec\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 1, acc: 0.5433332324028015, Loss 1.0267015898227692\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 2, acc: 0.5633333325386047, Loss 0.9836503803730011\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 3, acc: 0.6166667342185974, Loss 0.9645825642347335\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 4, acc: 0.6933335065841675, Loss 0.784741433262825\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 5, acc: 0.8033332228660583, Loss 0.5655444841086864\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 6, acc: 0.8266665935516357, Loss 0.4642800585925579\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 7, acc: 0.8466666340827942, Loss 0.36901756390929225\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 8, acc: 0.8599998354911804, Loss 0.3355876649916172\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 9, acc: 0.8899998664855957, Loss 0.32195283010602\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 10, acc: 0.8700000047683716, Loss 0.31655292749404906\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 11, acc: 0.9266665577888489, Loss 0.2828681033849716\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 12, acc: 0.9433332681655884, Loss 0.24902846947312354\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 13, acc: 0.9800000190734863, Loss 0.20569737032055854\n",
      "NetRNNWithAttentionExpFirst Rep: 0   Epoch: 1     Acc: 0.9800 Params: min_length: 10, max_length: 10, fill: 0, value_1: -1, value_2: 1 Time: 4.13 sec\n",
      "LSTM, rep: 0, epoch: 1, acc: 0.4999997317790985, Loss 1.0409326803684236\n",
      "LSTM, rep: 0, epoch: 2, acc: 0.49333325028419495, Loss 1.0060606271028518\n",
      "LSTM, rep: 0, epoch: 3, acc: 0.49000000953674316, Loss 0.9958008450269699\n",
      "LSTM, rep: 0, epoch: 4, acc: 0.5099999308586121, Loss 0.9770266664028168\n",
      "LSTM, rep: 0, epoch: 5, acc: 0.5833336114883423, Loss 0.9560911744832993\n",
      "LSTM, rep: 0, epoch: 6, acc: 0.706666886806488, Loss 0.8514107954502106\n",
      "LSTM, rep: 0, epoch: 7, acc: 0.699999988079071, Loss 0.806993137896061\n",
      "LSTM, rep: 0, epoch: 8, acc: 0.6933333873748779, Loss 0.8142485898733139\n",
      "LSTM, rep: 0, epoch: 9, acc: 0.7200000882148743, Loss 0.7750578349828721\n",
      "LSTM, rep: 0, epoch: 10, acc: 0.7400000095367432, Loss 0.7568793287873268\n",
      "LSTM, rep: 0, epoch: 11, acc: 0.7566666603088379, Loss 0.6927258574962616\n",
      "LSTM, rep: 0, epoch: 12, acc: 0.7799999713897705, Loss 0.6852173590660096\n",
      "LSTM, rep: 0, epoch: 13, acc: 0.8033332824707031, Loss 0.6381600666046142\n",
      "LSTM, rep: 0, epoch: 14, acc: 0.830000102519989, Loss 0.5576823052763938\n",
      "LSTM, rep: 0, epoch: 15, acc: 0.8433333039283752, Loss 0.5151077684760094\n",
      "LSTM, rep: 0, epoch: 16, acc: 0.8666664361953735, Loss 0.45877152889966966\n",
      "LSTM, rep: 0, epoch: 17, acc: 0.8433330655097961, Loss 0.4298862814903259\n",
      "LSTM, rep: 0, epoch: 18, acc: 0.8666665554046631, Loss 0.40492618560791016\n",
      "LSTM, rep: 0, epoch: 19, acc: 0.856666624546051, Loss 0.4030626919865608\n",
      "LSTM, rep: 0, epoch: 20, acc: 0.8933331966400146, Loss 0.36583164215087893\n",
      "LSTM, rep: 0, epoch: 21, acc: 0.8633330464363098, Loss 0.37783949315547943\n",
      "LSTM, rep: 0, epoch: 22, acc: 0.8866665363311768, Loss 0.34373987957835195\n",
      "LSTM, rep: 0, epoch: 23, acc: 0.9066665768623352, Loss 0.323975794762373\n",
      "LSTM, rep: 0, epoch: 24, acc: 0.9133331179618835, Loss 0.30975727960467336\n",
      "LSTM, rep: 0, epoch: 25, acc: 0.9133331775665283, Loss 0.2955124643445015\n",
      "LSTM, rep: 0, epoch: 26, acc: 0.9499997496604919, Loss 0.2659135568141937\n",
      "LSTM, rep: 0, epoch: 27, acc: 0.9466665387153625, Loss 0.2738222301006317\n",
      "LSTM, rep: 0, epoch: 28, acc: 0.9533333778381348, Loss 0.2559215447306633\n",
      "LSTM, rep: 0, epoch: 29, acc: 0.9900000095367432, Loss 0.250267189592123\n",
      "LSTM                 Rep: 0   Epoch: 1     Acc: 0.9900 Params: min_length: 10, max_length: 10, fill: 0, value_1: -1, value_2: 1 Time: 7.72 sec\n",
      "NetLSTMWithAttention, rep: 0, epoch: 1, acc: 0.5666667819023132, Loss 0.991790674328804\n",
      "NetLSTMWithAttention, rep: 0, epoch: 2, acc: 0.4766666293144226, Loss 1.049251554608345\n",
      "NetLSTMWithAttention, rep: 0, epoch: 3, acc: 0.5266669392585754, Loss 1.011230520606041\n",
      "NetLSTMWithAttention, rep: 0, epoch: 4, acc: 0.5, Loss 1.004599291086197\n",
      "NetLSTMWithAttention, rep: 0, epoch: 5, acc: 0.5366668105125427, Loss 0.9956046187877655\n",
      "NetLSTMWithAttention, rep: 0, epoch: 6, acc: 0.5799999833106995, Loss 0.9828931432962418\n",
      "NetLSTMWithAttention, rep: 0, epoch: 7, acc: 0.5866667628288269, Loss 0.9557148748636246\n",
      "NetLSTMWithAttention, rep: 0, epoch: 8, acc: 0.6833333373069763, Loss 0.8022650370001793\n",
      "NetLSTMWithAttention, rep: 0, epoch: 9, acc: 0.7100000977516174, Loss 0.6838713413476945\n",
      "NetLSTMWithAttention, rep: 0, epoch: 10, acc: 0.6866670250892639, Loss 0.7051305404305458\n",
      "NetLSTMWithAttention, rep: 0, epoch: 11, acc: 0.7399999499320984, Loss 0.6724128958582878\n",
      "NetLSTMWithAttention, rep: 0, epoch: 12, acc: 0.8299999833106995, Loss 0.5667659065127373\n",
      "NetLSTMWithAttention, rep: 0, epoch: 13, acc: 0.8033332824707031, Loss 0.545590068846941\n",
      "NetLSTMWithAttention, rep: 0, epoch: 14, acc: 0.8699998259544373, Loss 0.44669956907629965\n",
      "NetLSTMWithAttention, rep: 0, epoch: 15, acc: 0.9333333373069763, Loss 0.3449289512634277\n",
      "NetLSTMWithAttention, rep: 0, epoch: 16, acc: 0.9200000762939453, Loss 0.3018012537062168\n",
      "NetLSTMWithAttention, rep: 0, epoch: 17, acc: 0.9433332085609436, Loss 0.24686647430062295\n",
      "NetLSTMWithAttention, rep: 0, epoch: 18, acc: 0.9166665077209473, Loss 0.24118201389908792\n",
      "NetLSTMWithAttention, rep: 0, epoch: 19, acc: 0.9133332967758179, Loss 0.23518015660345554\n",
      "NetLSTMWithAttention, rep: 0, epoch: 20, acc: 0.9299997687339783, Loss 0.19892507657408715\n",
      "NetLSTMWithAttention, rep: 0, epoch: 21, acc: 0.903333306312561, Loss 0.22063197288662195\n",
      "NetLSTMWithAttention, rep: 0, epoch: 22, acc: 0.9166667461395264, Loss 0.21039490077644588\n",
      "NetLSTMWithAttention, rep: 0, epoch: 23, acc: 0.8800001740455627, Loss 0.30945378009229896\n",
      "NetLSTMWithAttention, rep: 0, epoch: 24, acc: 0.9066663980484009, Loss 0.22020086623728274\n",
      "NetLSTMWithAttention, rep: 0, epoch: 25, acc: 0.9333332180976868, Loss 0.18526832528412343\n",
      "NetLSTMWithAttention, rep: 0, epoch: 26, acc: 0.9466666579246521, Loss 0.17324477955698966\n",
      "NetLSTMWithAttention, rep: 0, epoch: 27, acc: 0.940000057220459, Loss 0.17434845758602024\n",
      "NetLSTMWithAttention, rep: 0, epoch: 28, acc: 0.9433332085609436, Loss 0.1922309441678226\n",
      "NetLSTMWithAttention, rep: 0, epoch: 29, acc: 0.9299999475479126, Loss 0.19443890197202562\n",
      "NetLSTMWithAttention, rep: 0, epoch: 30, acc: 0.919999897480011, Loss 0.1867985113710165\n",
      "NetLSTMWithAttention, rep: 0, epoch: 31, acc: 0.9299997687339783, Loss 0.18721964368596672\n",
      "NetLSTMWithAttention, rep: 0, epoch: 32, acc: 0.9266665577888489, Loss 0.18176979007199406\n",
      "NetLSTMWithAttention, rep: 0, epoch: 33, acc: 0.919999897480011, Loss 0.2036110917851329\n",
      "NetLSTMWithAttention, rep: 0, epoch: 34, acc: 0.9499999284744263, Loss 0.1775109407864511\n",
      "NetLSTMWithAttention, rep: 0, epoch: 35, acc: 0.9399999976158142, Loss 0.149186957385391\n",
      "NetLSTMWithAttention, rep: 0, epoch: 36, acc: 0.9233332276344299, Loss 0.16558222962543367\n",
      "NetLSTMWithAttention, rep: 0, epoch: 37, acc: 0.9333332180976868, Loss 0.17077339196577668\n",
      "NetLSTMWithAttention, rep: 0, epoch: 38, acc: 0.9566665887832642, Loss 0.15687803741544484\n",
      "NetLSTMWithAttention, rep: 0, epoch: 39, acc: 0.9433332085609436, Loss 0.15485229596495628\n",
      "NetLSTMWithAttention, rep: 0, epoch: 40, acc: 0.9533334374427795, Loss 0.17664352379739284\n",
      "NetLSTMWithAttention, rep: 0, epoch: 41, acc: 0.9833332896232605, Loss 0.13530096042901277\n",
      "NetLSTMWithAttention Rep: 0   Epoch: 1     Acc: 0.9833 Params: min_length: 10, max_length: 10, fill: 0, value_1: -1, value_2: 1 Time: 14.19 sec\n",
      "GRU, rep: 0, epoch: 1, acc: 0.5399996638298035, Loss 0.9931729131937027\n",
      "GRU, rep: 0, epoch: 2, acc: 0.49666664004325867, Loss 0.9985784691572189\n",
      "GRU, rep: 0, epoch: 3, acc: 0.49999988079071045, Loss 0.9703579485416413\n",
      "GRU, rep: 0, epoch: 4, acc: 0.6799999475479126, Loss 0.8570043390989304\n",
      "GRU, rep: 0, epoch: 5, acc: 0.660000205039978, Loss 0.725794135928154\n",
      "GRU, rep: 0, epoch: 6, acc: 0.6900001764297485, Loss 0.682601380944252\n",
      "GRU, rep: 0, epoch: 7, acc: 0.7166668176651001, Loss 0.6703945201635361\n",
      "GRU, rep: 0, epoch: 8, acc: 0.7100001573562622, Loss 0.6702862939238549\n",
      "GRU, rep: 0, epoch: 9, acc: 0.7399999499320984, Loss 0.6712918168306351\n",
      "GRU, rep: 0, epoch: 10, acc: 0.7133333683013916, Loss 0.6561428624391555\n",
      "GRU, rep: 0, epoch: 11, acc: 0.716666579246521, Loss 0.641690536737442\n",
      "GRU, rep: 0, epoch: 12, acc: 0.7700001001358032, Loss 0.6102659344673157\n",
      "GRU, rep: 0, epoch: 13, acc: 0.7666666507720947, Loss 0.6191275498270988\n",
      "GRU, rep: 0, epoch: 14, acc: 0.7833331227302551, Loss 0.588874697983265\n",
      "GRU, rep: 0, epoch: 15, acc: 0.7933332324028015, Loss 0.5168666312098503\n",
      "GRU, rep: 0, epoch: 16, acc: 0.8333331346511841, Loss 0.4507798632979393\n",
      "GRU, rep: 0, epoch: 17, acc: 0.903333306312561, Loss 0.3798331767320633\n",
      "GRU, rep: 0, epoch: 18, acc: 0.8599998354911804, Loss 0.34496467143297194\n",
      "GRU, rep: 0, epoch: 19, acc: 0.8399999141693115, Loss 0.34472610175609586\n",
      "GRU, rep: 0, epoch: 20, acc: 0.8866665363311768, Loss 0.3219817139208317\n",
      "GRU, rep: 0, epoch: 21, acc: 0.9166667461395264, Loss 0.3271337796747684\n",
      "GRU, rep: 0, epoch: 22, acc: 0.9433334469795227, Loss 0.30575704962015154\n",
      "GRU, rep: 0, epoch: 23, acc: 0.9566667079925537, Loss 0.2928985184431076\n",
      "GRU, rep: 0, epoch: 24, acc: 0.9799998998641968, Loss 0.26980059817433355\n",
      "GRU                  Rep: 0   Epoch: 1     Acc: 0.9800 Params: min_length: 10, max_length: 10, fill: 0, value_1: -1, value_2: 1 Time: 8.70 sec\n",
      "NetGRUWithAttention, rep: 0, epoch: 1, acc: 0.49999985098838806, Loss 1.02118497133255\n",
      "NetGRUWithAttention, rep: 0, epoch: 2, acc: 0.5433334112167358, Loss 0.9905655324459076\n",
      "NetGRUWithAttention, rep: 0, epoch: 3, acc: 0.539999783039093, Loss 0.9616238111257553\n",
      "NetGRUWithAttention, rep: 0, epoch: 4, acc: 0.7066669464111328, Loss 0.8458748579025268\n",
      "NetGRUWithAttention, rep: 0, epoch: 5, acc: 0.6700001358985901, Loss 0.6932004064321518\n",
      "NetGRUWithAttention, rep: 0, epoch: 6, acc: 0.7400001287460327, Loss 0.6669387173652649\n",
      "NetGRUWithAttention, rep: 0, epoch: 7, acc: 0.7266668677330017, Loss 0.6528098863363266\n",
      "NetGRUWithAttention, rep: 0, epoch: 8, acc: 0.7833331227302551, Loss 0.6157175847887992\n",
      "NetGRUWithAttention, rep: 0, epoch: 9, acc: 0.7700001001358032, Loss 0.6119730657339096\n",
      "NetGRUWithAttention, rep: 0, epoch: 10, acc: 0.8399999737739563, Loss 0.49797742664813993\n",
      "NetGRUWithAttention, rep: 0, epoch: 11, acc: 0.8333331942558289, Loss 0.4079855768382549\n",
      "NetGRUWithAttention, rep: 0, epoch: 12, acc: 0.8566665053367615, Loss 0.3353081977367401\n",
      "NetGRUWithAttention, rep: 0, epoch: 13, acc: 0.8433331847190857, Loss 0.3487359282374382\n",
      "NetGRUWithAttention, rep: 0, epoch: 14, acc: 0.8333334922790527, Loss 0.3329921928048134\n",
      "NetGRUWithAttention, rep: 0, epoch: 15, acc: 0.8366665840148926, Loss 0.3312786090373993\n",
      "NetGRUWithAttention, rep: 0, epoch: 16, acc: 0.8533331155776978, Loss 0.3262830525636673\n",
      "NetGRUWithAttention, rep: 0, epoch: 17, acc: 0.9133332967758179, Loss 0.30125580593943596\n",
      "NetGRUWithAttention, rep: 0, epoch: 18, acc: 0.8766664266586304, Loss 0.297742450684309\n",
      "NetGRUWithAttention, rep: 0, epoch: 19, acc: 0.8733332753181458, Loss 0.3407435403764248\n",
      "NetGRUWithAttention, rep: 0, epoch: 20, acc: 0.9033331871032715, Loss 0.27608947865664957\n",
      "NetGRUWithAttention, rep: 0, epoch: 21, acc: 0.9133331775665283, Loss 0.24878847740590573\n",
      "NetGRUWithAttention, rep: 0, epoch: 22, acc: 0.9033331871032715, Loss 0.22526602275669574\n",
      "NetGRUWithAttention, rep: 0, epoch: 23, acc: 0.9833333492279053, Loss 0.1873387235030532\n",
      "NetGRUWithAttention  Rep: 0   Epoch: 1     Acc: 0.9833 Params: min_length: 10, max_length: 10, fill: 0, value_1: -1, value_2: 1 Time: 10.15 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.4833332300186157, Loss 1.042176181077957\n",
      "RNN, rep: 0, epoch: 2, acc: 0.5033334493637085, Loss 1.0054301464557647\n",
      "RNN, rep: 0, epoch: 3, acc: 0.5233333110809326, Loss 0.9964926761388778\n",
      "RNN, rep: 0, epoch: 4, acc: 0.5366668105125427, Loss 0.9938419204950333\n",
      "RNN, rep: 0, epoch: 5, acc: 0.5699998736381531, Loss 0.9811787760257721\n",
      "RNN, rep: 0, epoch: 6, acc: 0.5299999713897705, Loss 0.9918962490558624\n",
      "RNN, rep: 0, epoch: 7, acc: 0.6166666746139526, Loss 0.9330821418762207\n",
      "RNN, rep: 0, epoch: 8, acc: 0.6466666460037231, Loss 0.7675700691342354\n",
      "RNN, rep: 0, epoch: 9, acc: 0.6866669654846191, Loss 0.7186867880821228\n",
      "RNN, rep: 0, epoch: 10, acc: 0.6866665482521057, Loss 0.6864732575416564\n",
      "RNN, rep: 0, epoch: 11, acc: 0.7133333683013916, Loss 0.6735508358478546\n",
      "RNN, rep: 0, epoch: 12, acc: 0.7400000095367432, Loss 0.6556004619598389\n",
      "RNN, rep: 0, epoch: 13, acc: 0.7400000095367432, Loss 0.632069463133812\n",
      "RNN, rep: 0, epoch: 14, acc: 0.7200002074241638, Loss 0.6562619334459305\n",
      "RNN, rep: 0, epoch: 15, acc: 0.7566666603088379, Loss 0.6251476848125458\n",
      "RNN, rep: 0, epoch: 16, acc: 0.7166668176651001, Loss 0.6135857006907464\n",
      "RNN, rep: 0, epoch: 17, acc: 0.7466667890548706, Loss 0.6061521208286286\n",
      "RNN, rep: 0, epoch: 18, acc: 0.746666669845581, Loss 0.621252513229847\n",
      "RNN, rep: 0, epoch: 19, acc: 0.8066666126251221, Loss 0.566545941233635\n",
      "RNN, rep: 0, epoch: 20, acc: 0.783333420753479, Loss 0.5804547071456909\n",
      "RNN, rep: 0, epoch: 21, acc: 0.7699999213218689, Loss 0.5958516097068787\n",
      "RNN, rep: 0, epoch: 22, acc: 0.8433331847190857, Loss 0.5336261430382728\n",
      "RNN, rep: 0, epoch: 23, acc: 0.8066667914390564, Loss 0.5763553214073182\n",
      "RNN, rep: 0, epoch: 24, acc: 0.8166666626930237, Loss 0.5622824975848197\n",
      "RNN, rep: 0, epoch: 25, acc: 0.8466666340827942, Loss 0.5007656329870224\n",
      "RNN, rep: 0, epoch: 26, acc: 0.8366665840148926, Loss 0.5074478666484356\n",
      "RNN, rep: 0, epoch: 27, acc: 0.8466665744781494, Loss 0.4836116334795952\n",
      "RNN, rep: 0, epoch: 28, acc: 0.823333203792572, Loss 0.5577577175199986\n",
      "RNN, rep: 0, epoch: 29, acc: 0.8100000619888306, Loss 0.49038660317659377\n",
      "RNN, rep: 0, epoch: 30, acc: 0.8833332657814026, Loss 0.3978214222192764\n",
      "RNN, rep: 0, epoch: 31, acc: 0.8766665458679199, Loss 0.3985022930800915\n",
      "RNN, rep: 0, epoch: 32, acc: 0.800000011920929, Loss 0.4505854344367981\n",
      "RNN, rep: 0, epoch: 33, acc: 0.8700000047683716, Loss 0.41388377994298936\n",
      "RNN, rep: 0, epoch: 34, acc: 0.8999999165534973, Loss 0.34442952036857605\n",
      "RNN, rep: 0, epoch: 35, acc: 0.856666624546051, Loss 0.4114528655260801\n",
      "RNN, rep: 0, epoch: 36, acc: 0.8966666460037231, Loss 0.3359838230162859\n",
      "RNN, rep: 0, epoch: 37, acc: 0.919999897480011, Loss 0.31272790424525737\n",
      "RNN, rep: 0, epoch: 38, acc: 0.8899999856948853, Loss 0.3310181004554033\n",
      "RNN, rep: 0, epoch: 39, acc: 0.9566666483879089, Loss 0.2511620904505253\n",
      "RNN, rep: 0, epoch: 40, acc: 0.9466666579246521, Loss 0.29414673425257204\n",
      "RNN, rep: 0, epoch: 41, acc: 0.9633333086967468, Loss 0.26291667722165585\n",
      "RNN, rep: 0, epoch: 42, acc: 0.9766666293144226, Loss 0.24188454672694207\n",
      "RNN                  Rep: 0   Epoch: 1     Acc: 0.9767 Params: min_length: 10, max_length: 15, fill: 0, value_1: -1, value_2: 1 Time: 10.35 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.47000011801719666, Loss 1.0457532608509064\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.5766665935516357, Loss 0.9875242924690246\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5666665434837341, Loss 0.9793302297592164\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5466668009757996, Loss 0.9632918775081635\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.6700000762939453, Loss 0.913664722442627\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.6733335256576538, Loss 0.7691489672660827\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.6500000953674316, Loss 0.7163157874345779\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.6700002551078796, Loss 0.6732697156071663\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.7033334374427795, Loss 0.654436454474926\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.6366668343544006, Loss 0.6881342822313309\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.696666955947876, Loss 0.6183123347163201\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.7000001668930054, Loss 0.658687428534031\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.7000001668930054, Loss 0.5997586488723755\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.7599999904632568, Loss 0.5486285975575447\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.8433331847190857, Loss 0.5036536347866059\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.9033331871032715, Loss 0.3538882313668728\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.9466667175292969, Loss 0.3074063079059124\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.986666738986969, Loss 0.22669404171407223\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 1     Acc: 0.9867 Params: min_length: 10, max_length: 15, fill: 0, value_1: -1, value_2: 1 Time: 5.87 sec\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 1, acc: 0.5433334112167358, Loss 1.0319356286525727\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 2, acc: 0.5500000715255737, Loss 0.99140456199646\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 3, acc: 0.5733333826065063, Loss 0.9785912346839905\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 4, acc: 0.6733332872390747, Loss 0.8508397817611695\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 5, acc: 0.68666672706604, Loss 0.6677245771884919\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 6, acc: 0.7566666603088379, Loss 0.6308768951892852\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 7, acc: 0.7799997925758362, Loss 0.57698661506176\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 8, acc: 0.823333203792572, Loss 0.44735883340239524\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 9, acc: 0.8099997639656067, Loss 0.4064784848690033\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 10, acc: 0.8199997544288635, Loss 0.3675715520977974\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 11, acc: 0.8966665863990784, Loss 0.33913848131895064\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 12, acc: 0.8799998760223389, Loss 0.333179894387722\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 13, acc: 0.9199997782707214, Loss 0.3137934927642345\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 14, acc: 0.9266666173934937, Loss 0.28162976205348966\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 15, acc: 0.9133331179618835, Loss 0.26995021902024746\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 16, acc: 0.9366667866706848, Loss 0.24938205666840077\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 17, acc: 0.9733331799507141, Loss 0.21692763812839985\n",
      "NetRNNWithAttentionExpFirst Rep: 0   Epoch: 1     Acc: 0.9733 Params: min_length: 10, max_length: 15, fill: 0, value_1: -1, value_2: 1 Time: 5.54 sec\n",
      "LSTM, rep: 0, epoch: 1, acc: 0.4433332085609436, Loss 1.1214120733737944\n",
      "LSTM, rep: 0, epoch: 2, acc: 0.49000003933906555, Loss 1.0275318396091462\n",
      "LSTM, rep: 0, epoch: 3, acc: 0.4666667878627777, Loss 1.030089104771614\n",
      "LSTM, rep: 0, epoch: 4, acc: 0.5099998712539673, Loss 1.0079174935817719\n",
      "LSTM, rep: 0, epoch: 5, acc: 0.5033333897590637, Loss 1.003510274887085\n",
      "LSTM, rep: 0, epoch: 6, acc: 0.5233333110809326, Loss 0.9939306855201722\n",
      "LSTM, rep: 0, epoch: 7, acc: 0.5100002288818359, Loss 0.9893238860368728\n",
      "LSTM, rep: 0, epoch: 8, acc: 0.54666668176651, Loss 0.9834229707717895\n",
      "LSTM, rep: 0, epoch: 9, acc: 0.5466669201850891, Loss 0.9663449364900589\n",
      "LSTM, rep: 0, epoch: 10, acc: 0.6333333253860474, Loss 0.9420533108711243\n",
      "LSTM, rep: 0, epoch: 11, acc: 0.7133333086967468, Loss 0.8103334844112396\n",
      "LSTM, rep: 0, epoch: 12, acc: 0.7200000882148743, Loss 0.7593536657094956\n",
      "LSTM, rep: 0, epoch: 13, acc: 0.6666663885116577, Loss 0.7304172760248184\n",
      "LSTM, rep: 0, epoch: 14, acc: 0.7166668176651001, Loss 0.689424929022789\n",
      "LSTM, rep: 0, epoch: 15, acc: 0.7266665697097778, Loss 0.6600239235162735\n",
      "LSTM, rep: 0, epoch: 16, acc: 0.6399999856948853, Loss 0.7129023250937462\n",
      "LSTM, rep: 0, epoch: 17, acc: 0.7133331894874573, Loss 0.670251866877079\n",
      "LSTM, rep: 0, epoch: 18, acc: 0.76666659116745, Loss 0.6434084981679916\n",
      "LSTM, rep: 0, epoch: 19, acc: 0.7766666412353516, Loss 0.6323899960517884\n",
      "LSTM, rep: 0, epoch: 20, acc: 0.8366665840148926, Loss 0.5953881612420082\n",
      "LSTM, rep: 0, epoch: 21, acc: 0.8233331441879272, Loss 0.5809877172112465\n",
      "LSTM, rep: 0, epoch: 22, acc: 0.7966666221618652, Loss 0.5707754126191139\n",
      "LSTM, rep: 0, epoch: 23, acc: 0.8633332252502441, Loss 0.485584115087986\n",
      "LSTM, rep: 0, epoch: 24, acc: 0.8633332848548889, Loss 0.43172444850206376\n",
      "LSTM, rep: 0, epoch: 25, acc: 0.9033331274986267, Loss 0.3706925691664219\n",
      "LSTM, rep: 0, epoch: 26, acc: 0.9233332872390747, Loss 0.33409855410456657\n",
      "LSTM, rep: 0, epoch: 27, acc: 0.8799999356269836, Loss 0.34357385471463203\n",
      "LSTM, rep: 0, epoch: 28, acc: 0.9533332586288452, Loss 0.3051160731911659\n",
      "LSTM, rep: 0, epoch: 29, acc: 0.9533332586288452, Loss 0.28943339496850967\n",
      "LSTM, rep: 0, epoch: 30, acc: 0.9333332180976868, Loss 0.2791560135781765\n",
      "LSTM, rep: 0, epoch: 31, acc: 0.9833332896232605, Loss 0.2408258755505085\n",
      "LSTM                 Rep: 0   Epoch: 1     Acc: 0.9833 Params: min_length: 10, max_length: 15, fill: 0, value_1: -1, value_2: 1 Time: 8.51 sec\n",
      "NetLSTMWithAttention, rep: 0, epoch: 1, acc: 0.4766666889190674, Loss 1.0305030179023742\n",
      "NetLSTMWithAttention, rep: 0, epoch: 2, acc: 0.47999998927116394, Loss 1.0102443784475326\n",
      "NetLSTMWithAttention, rep: 0, epoch: 3, acc: 0.47666653990745544, Loss 1.0105786979198457\n",
      "NetLSTMWithAttention, rep: 0, epoch: 4, acc: 0.559999942779541, Loss 0.987794439792633\n",
      "NetLSTMWithAttention, rep: 0, epoch: 5, acc: 0.5100000500679016, Loss 0.9989811688661575\n",
      "NetLSTMWithAttention, rep: 0, epoch: 6, acc: 0.6166667938232422, Loss 0.9057459586858749\n",
      "NetLSTMWithAttention, rep: 0, epoch: 7, acc: 0.7166666984558105, Loss 0.7347485181689263\n",
      "NetLSTMWithAttention, rep: 0, epoch: 8, acc: 0.6600000858306885, Loss 0.7641070595383644\n",
      "NetLSTMWithAttention, rep: 0, epoch: 9, acc: 0.6499999165534973, Loss 0.749124457538128\n",
      "NetLSTMWithAttention, rep: 0, epoch: 10, acc: 0.6266666650772095, Loss 0.7703128251433372\n",
      "NetLSTMWithAttention, rep: 0, epoch: 11, acc: 0.6733333468437195, Loss 0.7272103762626648\n",
      "NetLSTMWithAttention, rep: 0, epoch: 12, acc: 0.6633333563804626, Loss 0.690109491944313\n",
      "NetLSTMWithAttention, rep: 0, epoch: 13, acc: 0.6833334565162659, Loss 0.6711974114179611\n",
      "NetLSTMWithAttention, rep: 0, epoch: 14, acc: 0.6800000071525574, Loss 0.6740035676956176\n",
      "NetLSTMWithAttention, rep: 0, epoch: 15, acc: 0.6466666460037231, Loss 0.6801521211862565\n",
      "NetLSTMWithAttention, rep: 0, epoch: 16, acc: 0.6533333659172058, Loss 0.6769012546539307\n",
      "NetLSTMWithAttention, rep: 0, epoch: 17, acc: 0.6566668748855591, Loss 0.6763356328010559\n",
      "NetLSTMWithAttention, rep: 0, epoch: 18, acc: 0.6766667366027832, Loss 0.6697151321172714\n",
      "NetLSTMWithAttention, rep: 0, epoch: 19, acc: 0.6366667151451111, Loss 0.6740357053279876\n",
      "NetLSTMWithAttention, rep: 0, epoch: 20, acc: 0.6433334350585938, Loss 0.6720484685897827\n",
      "NetLSTMWithAttention, rep: 0, epoch: 21, acc: 0.6933333873748779, Loss 0.6684271901845932\n",
      "NetLSTMWithAttention, rep: 0, epoch: 22, acc: 0.6633332967758179, Loss 0.6701586037874222\n",
      "NetLSTMWithAttention, rep: 0, epoch: 23, acc: 0.68666672706604, Loss 0.6652128505706787\n",
      "NetLSTMWithAttention, rep: 0, epoch: 24, acc: 0.68666672706604, Loss 0.6651032000780106\n",
      "NetLSTMWithAttention, rep: 0, epoch: 25, acc: 0.676666796207428, Loss 0.6704992377758026\n",
      "NetLSTMWithAttention, rep: 0, epoch: 26, acc: 0.6699998378753662, Loss 0.6712863886356354\n",
      "NetLSTMWithAttention, rep: 0, epoch: 27, acc: 0.6499999761581421, Loss 0.6704341065883637\n",
      "NetLSTMWithAttention, rep: 0, epoch: 28, acc: 0.6933335065841675, Loss 0.6675101512670517\n",
      "NetLSTMWithAttention, rep: 0, epoch: 29, acc: 0.676666796207428, Loss 0.6644868522882461\n",
      "NetLSTMWithAttention, rep: 0, epoch: 30, acc: 0.7366666197776794, Loss 0.658626601099968\n",
      "NetLSTMWithAttention, rep: 0, epoch: 31, acc: 0.6266667246818542, Loss 0.6776495999097825\n",
      "NetLSTMWithAttention, rep: 0, epoch: 32, acc: 0.6966666579246521, Loss 0.6635004901885986\n",
      "NetLSTMWithAttention, rep: 0, epoch: 33, acc: 0.6833334565162659, Loss 0.6619567424058914\n",
      "NetLSTMWithAttention, rep: 0, epoch: 34, acc: 0.643333375453949, Loss 0.6678247863054275\n",
      "NetLSTMWithAttention, rep: 0, epoch: 35, acc: 0.7033334970474243, Loss 0.6609124004840851\n",
      "NetLSTMWithAttention, rep: 0, epoch: 36, acc: 0.7133334279060364, Loss 0.6489382743835449\n",
      "NetLSTMWithAttention, rep: 0, epoch: 37, acc: 0.6833334565162659, Loss 0.7532728344202042\n",
      "NetLSTMWithAttention, rep: 0, epoch: 38, acc: 0.5766667127609253, Loss 1.2623541867733001\n",
      "NetLSTMWithAttention, rep: 0, epoch: 39, acc: 0.6033332347869873, Loss 0.932869216799736\n",
      "NetLSTMWithAttention, rep: 0, epoch: 40, acc: 0.7133333683013916, Loss 0.7235819369554519\n",
      "NetLSTMWithAttention, rep: 0, epoch: 41, acc: 0.7833333015441895, Loss 0.6005260941386222\n",
      "NetLSTMWithAttention, rep: 0, epoch: 42, acc: 0.8066667318344116, Loss 0.5141303309798241\n",
      "NetLSTMWithAttention, rep: 0, epoch: 43, acc: 0.7866666316986084, Loss 0.5442273354530335\n",
      "NetLSTMWithAttention, rep: 0, epoch: 44, acc: 0.6000001430511475, Loss 1.1564576908946038\n",
      "NetLSTMWithAttention, rep: 0, epoch: 45, acc: 0.6000001430511475, Loss 1.11911433249712\n",
      "NetLSTMWithAttention, rep: 0, epoch: 46, acc: 0.6099998354911804, Loss 1.0419311833381653\n",
      "NetLSTMWithAttention, rep: 0, epoch: 47, acc: 0.6300002932548523, Loss 0.843992504477501\n",
      "NetLSTMWithAttention, rep: 0, epoch: 48, acc: 0.7133334279060364, Loss 0.7573632863163948\n",
      "NetLSTMWithAttention, rep: 0, epoch: 49, acc: 0.7400001287460327, Loss 0.7045390382409096\n",
      "NetLSTMWithAttention, rep: 0, epoch: 50, acc: 0.7733333110809326, Loss 0.5746882775425911\n",
      "NetLSTMWithAttention, rep: 0, epoch: 51, acc: 0.7300000190734863, Loss 0.5974441066384315\n",
      "NetLSTMWithAttention, rep: 0, epoch: 52, acc: 0.7400000095367432, Loss 0.5642862364649772\n",
      "NetLSTMWithAttention, rep: 0, epoch: 53, acc: 0.7500000596046448, Loss 0.5611833670735359\n",
      "NetLSTMWithAttention, rep: 0, epoch: 54, acc: 0.7899999022483826, Loss 0.5266953605413437\n",
      "NetLSTMWithAttention, rep: 0, epoch: 55, acc: 0.7533332109451294, Loss 0.5532008272409439\n",
      "NetLSTMWithAttention, rep: 0, epoch: 56, acc: 0.7433332204818726, Loss 0.5295796605944634\n",
      "NetLSTMWithAttention, rep: 0, epoch: 57, acc: 0.7800000905990601, Loss 0.4957860067486763\n",
      "NetLSTMWithAttention, rep: 0, epoch: 58, acc: 0.7466665506362915, Loss 0.5130368223786355\n",
      "NetLSTMWithAttention, rep: 0, epoch: 59, acc: 0.8299999237060547, Loss 0.4781394827365875\n",
      "NetLSTMWithAttention, rep: 0, epoch: 60, acc: 0.8400001525878906, Loss 0.45937212467193606\n",
      "NetLSTMWithAttention, rep: 0, epoch: 61, acc: 0.8266664743423462, Loss 0.43530640691518785\n",
      "NetLSTMWithAttention, rep: 0, epoch: 62, acc: 0.8699999451637268, Loss 0.40358016818761827\n",
      "NetLSTMWithAttention, rep: 0, epoch: 63, acc: 0.8599998950958252, Loss 0.3959874352812767\n",
      "NetLSTMWithAttention, rep: 0, epoch: 64, acc: 0.7933333516120911, Loss 0.617099170088768\n",
      "NetLSTMWithAttention, rep: 0, epoch: 65, acc: 0.669999897480011, Loss 0.8998356124758721\n",
      "NetLSTMWithAttention, rep: 0, epoch: 66, acc: 0.7399999499320984, Loss 0.6866441258788109\n",
      "NetLSTMWithAttention, rep: 0, epoch: 67, acc: 0.7433334589004517, Loss 0.5756881174445152\n",
      "NetLSTMWithAttention, rep: 0, epoch: 68, acc: 0.7200000882148743, Loss 0.6098719236254692\n",
      "NetLSTMWithAttention, rep: 0, epoch: 69, acc: 0.7766665816307068, Loss 0.519863637983799\n",
      "NetLSTMWithAttention, rep: 0, epoch: 70, acc: 0.7799997925758362, Loss 0.5108309611678123\n",
      "NetLSTMWithAttention, rep: 0, epoch: 71, acc: 0.8399998545646667, Loss 0.4672935277223587\n",
      "NetLSTMWithAttention, rep: 0, epoch: 72, acc: 0.8166666626930237, Loss 0.4487990695238113\n",
      "NetLSTMWithAttention, rep: 0, epoch: 73, acc: 0.8299999237060547, Loss 0.4254980328679085\n",
      "NetLSTMWithAttention, rep: 0, epoch: 74, acc: 0.8133334517478943, Loss 0.39988081008195875\n",
      "NetLSTMWithAttention, rep: 0, epoch: 75, acc: 0.8199998736381531, Loss 0.38225077375769617\n",
      "NetLSTMWithAttention, rep: 0, epoch: 76, acc: 0.8633332848548889, Loss 0.37003010541200637\n",
      "NetLSTMWithAttention, rep: 0, epoch: 77, acc: 0.6966667771339417, Loss 0.8212462648749351\n",
      "NetLSTMWithAttention, rep: 0, epoch: 78, acc: 0.7766664624214172, Loss 0.5414716091752052\n",
      "NetLSTMWithAttention, rep: 0, epoch: 79, acc: 0.8633330464363098, Loss 0.38867936193943026\n",
      "NetLSTMWithAttention, rep: 0, epoch: 80, acc: 0.8466663956642151, Loss 0.3614034599065781\n",
      "NetLSTMWithAttention, rep: 0, epoch: 81, acc: 0.8466666340827942, Loss 0.3604086239635944\n",
      "NetLSTMWithAttention, rep: 0, epoch: 82, acc: 0.8299997448921204, Loss 0.3551986888051033\n",
      "NetLSTMWithAttention, rep: 0, epoch: 83, acc: 0.8533332943916321, Loss 0.3584822432696819\n",
      "NetLSTMWithAttention, rep: 0, epoch: 84, acc: 0.7966665625572205, Loss 0.3668968303501606\n",
      "NetLSTMWithAttention, rep: 0, epoch: 85, acc: 0.8566665649414062, Loss 0.34359409391880036\n",
      "NetLSTMWithAttention, rep: 0, epoch: 86, acc: 0.8266664743423462, Loss 0.35721720203757285\n",
      "NetLSTMWithAttention, rep: 0, epoch: 87, acc: 0.8399999141693115, Loss 0.34710625410079954\n",
      "NetLSTMWithAttention, rep: 0, epoch: 88, acc: 0.8499996662139893, Loss 0.34545649111270904\n",
      "NetLSTMWithAttention, rep: 0, epoch: 89, acc: 0.8633332848548889, Loss 0.3284152342379093\n",
      "NetLSTMWithAttention, rep: 0, epoch: 90, acc: 0.8366666436195374, Loss 0.34054028138518333\n",
      "NetLSTMWithAttention, rep: 0, epoch: 91, acc: 0.8333331346511841, Loss 0.341272089779377\n",
      "NetLSTMWithAttention, rep: 0, epoch: 92, acc: 0.8133330345153809, Loss 0.3463733750581741\n",
      "NetLSTMWithAttention, rep: 0, epoch: 93, acc: 0.8166665434837341, Loss 0.3556086175143719\n",
      "NetLSTMWithAttention, rep: 0, epoch: 94, acc: 0.7999998331069946, Loss 0.34386444121599197\n",
      "NetLSTMWithAttention, rep: 0, epoch: 95, acc: 0.8200000524520874, Loss 0.35613836616277694\n",
      "NetLSTMWithAttention, rep: 0, epoch: 96, acc: 0.8666666150093079, Loss 0.32654204189777375\n",
      "NetLSTMWithAttention, rep: 0, epoch: 97, acc: 0.8333331346511841, Loss 0.350739336758852\n",
      "NetLSTMWithAttention, rep: 0, epoch: 98, acc: 0.8433331847190857, Loss 0.3372648099064827\n",
      "NetLSTMWithAttention, rep: 0, epoch: 99, acc: 0.8433331847190857, Loss 0.337547769844532\n",
      "NetLSTMWithAttention, rep: 0, epoch: 100, acc: 0.8166666626930237, Loss 0.34372675001621245\n",
      "NetLSTMWithAttention, rep: 0, epoch: 101, acc: 0.84333336353302, Loss 0.33910837292671203\n",
      "NetLSTMWithAttention, rep: 0, epoch: 102, acc: 0.8733333349227905, Loss 0.3312433210015297\n",
      "NetLSTMWithAttention, rep: 0, epoch: 103, acc: 0.8666665554046631, Loss 0.3357429213821888\n",
      "NetLSTMWithAttention, rep: 0, epoch: 104, acc: 0.8633334636688232, Loss 0.33754575297236444\n",
      "NetLSTMWithAttention, rep: 0, epoch: 105, acc: 0.8533332943916321, Loss 0.3386375069618225\n",
      "NetLSTMWithAttention, rep: 0, epoch: 106, acc: 0.8533332943916321, Loss 0.3354437929391861\n",
      "NetLSTMWithAttention, rep: 0, epoch: 107, acc: 0.8066668510437012, Loss 0.6568535846471787\n",
      "NetLSTMWithAttention, rep: 0, epoch: 108, acc: 0.6266666650772095, Loss 0.9904789251089096\n",
      "NetLSTMWithAttention, rep: 0, epoch: 109, acc: 0.6933333873748779, Loss 0.8514222407341003\n",
      "NetLSTMWithAttention, rep: 0, epoch: 110, acc: 0.6666667461395264, Loss 0.8499979808926582\n",
      "NetLSTMWithAttention, rep: 0, epoch: 111, acc: 0.7199998497962952, Loss 0.6733375760912895\n",
      "NetLSTMWithAttention, rep: 0, epoch: 112, acc: 0.7333333492279053, Loss 0.6962313237786293\n",
      "NetLSTMWithAttention, rep: 0, epoch: 113, acc: 0.7533332705497742, Loss 0.7082322549819946\n",
      "NetLSTMWithAttention, rep: 0, epoch: 114, acc: 0.8199999332427979, Loss 0.5771830382943154\n",
      "NetLSTMWithAttention, rep: 0, epoch: 115, acc: 0.8199999928474426, Loss 0.5349655732512474\n",
      "NetLSTMWithAttention, rep: 0, epoch: 116, acc: 0.8333332538604736, Loss 0.4082933449745178\n",
      "NetLSTMWithAttention, rep: 0, epoch: 117, acc: 0.8533332943916321, Loss 0.37624673649668694\n",
      "NetLSTMWithAttention, rep: 0, epoch: 118, acc: 0.8666666150093079, Loss 0.3435712166130543\n",
      "NetLSTMWithAttention, rep: 0, epoch: 119, acc: 0.8666666150093079, Loss 0.34474734008312224\n",
      "NetLSTMWithAttention, rep: 0, epoch: 120, acc: 0.8666664958000183, Loss 0.34208065897226336\n",
      "NetLSTMWithAttention, rep: 0, epoch: 121, acc: 0.8766665458679199, Loss 0.334178833514452\n",
      "NetLSTMWithAttention, rep: 0, epoch: 122, acc: 0.8633331060409546, Loss 0.3342267294228077\n",
      "NetLSTMWithAttention, rep: 0, epoch: 123, acc: 0.8666664958000183, Loss 0.3444900791347027\n",
      "NetLSTMWithAttention, rep: 0, epoch: 124, acc: 0.8633332848548889, Loss 0.33084171324968337\n",
      "NetLSTMWithAttention, rep: 0, epoch: 125, acc: 0.8733331561088562, Loss 0.329463449716568\n",
      "NetLSTMWithAttention, rep: 0, epoch: 126, acc: 0.8499998450279236, Loss 0.3452051870524883\n",
      "NetLSTMWithAttention, rep: 0, epoch: 127, acc: 0.8999999165534973, Loss 0.2998740783333778\n",
      "NetLSTMWithAttention, rep: 0, epoch: 128, acc: 0.8533332943916321, Loss 0.3308905206620693\n",
      "NetLSTMWithAttention, rep: 0, epoch: 129, acc: 0.8633332252502441, Loss 0.3285359595716\n",
      "NetLSTMWithAttention, rep: 0, epoch: 130, acc: 0.8633330464363098, Loss 0.3115109747648239\n",
      "NetLSTMWithAttention, rep: 0, epoch: 131, acc: 0.916666567325592, Loss 0.27529643312096597\n",
      "NetLSTMWithAttention, rep: 0, epoch: 132, acc: 0.8666664958000183, Loss 0.31710191056132314\n",
      "NetLSTMWithAttention, rep: 0, epoch: 133, acc: 0.8933332562446594, Loss 0.2925748248398304\n",
      "NetLSTMWithAttention, rep: 0, epoch: 134, acc: 0.8899999260902405, Loss 0.28805368036031725\n",
      "NetLSTMWithAttention, rep: 0, epoch: 135, acc: 0.9033331274986267, Loss 0.2634593033790588\n",
      "NetLSTMWithAttention, rep: 0, epoch: 136, acc: 0.9166666269302368, Loss 0.28290570750832555\n",
      "NetLSTMWithAttention, rep: 0, epoch: 137, acc: 0.9133331775665283, Loss 0.2876484974473715\n",
      "NetLSTMWithAttention, rep: 0, epoch: 138, acc: 0.9066665172576904, Loss 0.30114104770123956\n",
      "NetLSTMWithAttention, rep: 0, epoch: 139, acc: 0.9100000262260437, Loss 0.2811090885102749\n",
      "NetLSTMWithAttention, rep: 0, epoch: 140, acc: 0.9233332872390747, Loss 0.25629560872912405\n",
      "NetLSTMWithAttention, rep: 0, epoch: 141, acc: 0.9133331179618835, Loss 0.24779111541807652\n",
      "NetLSTMWithAttention, rep: 0, epoch: 142, acc: 0.9266666173934937, Loss 0.2549544198811054\n",
      "NetLSTMWithAttention, rep: 0, epoch: 143, acc: 0.873333215713501, Loss 0.3082501598447561\n",
      "NetLSTMWithAttention, rep: 0, epoch: 144, acc: 0.9066665172576904, Loss 0.24378163486719132\n",
      "NetLSTMWithAttention, rep: 0, epoch: 145, acc: 0.9099998474121094, Loss 0.24369554601609708\n",
      "NetLSTMWithAttention, rep: 0, epoch: 146, acc: 0.9200000166893005, Loss 0.2202689977735281\n",
      "NetLSTMWithAttention, rep: 0, epoch: 147, acc: 0.93666672706604, Loss 0.22114618491381408\n",
      "NetLSTMWithAttention, rep: 0, epoch: 148, acc: 0.966666579246521, Loss 0.1956560877338052\n",
      "NetLSTMWithAttention, rep: 0, epoch: 149, acc: 0.9700000286102295, Loss 0.1710425929352641\n",
      "NetLSTMWithAttention Rep: 0   Epoch: 1     Acc: 0.9700 Params: min_length: 10, max_length: 15, fill: 0, value_1: -1, value_2: 1 Time: 52.35 sec\n",
      "GRU, rep: 0, epoch: 1, acc: 0.48666685819625854, Loss 1.02523896753788\n",
      "GRU, rep: 0, epoch: 2, acc: 0.5300000905990601, Loss 1.0018857324123382\n",
      "GRU, rep: 0, epoch: 3, acc: 0.48333343863487244, Loss 1.0002789461612702\n",
      "GRU, rep: 0, epoch: 4, acc: 0.5566667318344116, Loss 0.9904281145334244\n",
      "GRU, rep: 0, epoch: 5, acc: 0.6133334636688232, Loss 0.9573998820781707\n",
      "GRU, rep: 0, epoch: 6, acc: 0.6833334565162659, Loss 0.8065666154026985\n",
      "GRU, rep: 0, epoch: 7, acc: 0.6966667175292969, Loss 0.6930271765589714\n",
      "GRU, rep: 0, epoch: 8, acc: 0.6400002241134644, Loss 0.70828564286232\n",
      "GRU, rep: 0, epoch: 9, acc: 0.7100000977516174, Loss 0.6515958628058434\n",
      "GRU, rep: 0, epoch: 10, acc: 0.7033334374427795, Loss 0.631077940762043\n",
      "GRU, rep: 0, epoch: 11, acc: 0.7733333110809326, Loss 0.5669378468394279\n",
      "GRU, rep: 0, epoch: 12, acc: 0.7400000691413879, Loss 0.5725506871938706\n",
      "GRU, rep: 0, epoch: 13, acc: 0.8766665458679199, Loss 0.48698395878076556\n",
      "GRU, rep: 0, epoch: 14, acc: 0.919999897480011, Loss 0.3928943584859371\n",
      "GRU, rep: 0, epoch: 15, acc: 0.8999997973442078, Loss 0.3534683808684349\n",
      "GRU, rep: 0, epoch: 16, acc: 0.9399999380111694, Loss 0.29709902733564375\n",
      "GRU, rep: 0, epoch: 17, acc: 0.9299999475479126, Loss 0.2846369631588459\n",
      "GRU, rep: 0, epoch: 18, acc: 0.9133331775665283, Loss 0.27384301260113714\n",
      "GRU, rep: 0, epoch: 19, acc: 0.9466666579246521, Loss 0.21952839948236944\n",
      "GRU, rep: 0, epoch: 20, acc: 0.919999897480011, Loss 0.21701244615018367\n",
      "GRU, rep: 0, epoch: 21, acc: 0.9800000190734863, Loss 0.20313385128974915\n",
      "GRU                  Rep: 0   Epoch: 1     Acc: 0.9800 Params: min_length: 10, max_length: 15, fill: 0, value_1: -1, value_2: 1 Time: 9.06 sec\n",
      "NetGRUWithAttention, rep: 0, epoch: 1, acc: 0.5000000596046448, Loss 1.0064717131853103\n",
      "NetGRUWithAttention, rep: 0, epoch: 2, acc: 0.529999852180481, Loss 0.992520923614502\n",
      "NetGRUWithAttention, rep: 0, epoch: 3, acc: 0.5866668224334717, Loss 0.9756662064790725\n",
      "NetGRUWithAttention, rep: 0, epoch: 4, acc: 0.6833334565162659, Loss 0.9307179170846939\n",
      "NetGRUWithAttention, rep: 0, epoch: 5, acc: 0.6666668653488159, Loss 0.8371554166078568\n",
      "NetGRUWithAttention, rep: 0, epoch: 6, acc: 0.7033332586288452, Loss 0.6743041720986366\n",
      "NetGRUWithAttention, rep: 0, epoch: 7, acc: 0.7133333683013916, Loss 0.6448280864953995\n",
      "NetGRUWithAttention, rep: 0, epoch: 8, acc: 0.7733331918716431, Loss 0.6222861024737358\n",
      "NetGRUWithAttention, rep: 0, epoch: 9, acc: 0.8166666626930237, Loss 0.5008244647085667\n",
      "NetGRUWithAttention, rep: 0, epoch: 10, acc: 0.8399996757507324, Loss 0.37266127854585646\n",
      "NetGRUWithAttention, rep: 0, epoch: 11, acc: 0.8499998450279236, Loss 0.31719611749053\n",
      "NetGRUWithAttention, rep: 0, epoch: 12, acc: 0.8700000047683716, Loss 0.3133464419096708\n",
      "NetGRUWithAttention, rep: 0, epoch: 13, acc: 0.9066663980484009, Loss 0.2768058927357197\n",
      "NetGRUWithAttention, rep: 0, epoch: 14, acc: 0.93666672706604, Loss 0.22960546292364598\n",
      "NetGRUWithAttention, rep: 0, epoch: 15, acc: 0.9333333373069763, Loss 0.2221983079612255\n",
      "NetGRUWithAttention, rep: 0, epoch: 16, acc: 1.0, Loss 0.16813331484794616\n",
      "NetGRUWithAttention  Rep: 0   Epoch: 1     Acc: 1.0000 Params: min_length: 10, max_length: 15, fill: 0, value_1: -1, value_2: 1 Time: 7.77 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.5333330035209656, Loss 1.0003222560882568\n",
      "RNN, rep: 0, epoch: 2, acc: 0.463333398103714, Loss 1.0329830986261368\n",
      "RNN, rep: 0, epoch: 3, acc: 0.4933331608772278, Loss 1.0086588925123214\n",
      "RNN, rep: 0, epoch: 4, acc: 0.5100000500679016, Loss 1.005201169848442\n",
      "RNN, rep: 0, epoch: 5, acc: 0.5633333325386047, Loss 0.9905424445867539\n",
      "RNN, rep: 0, epoch: 6, acc: 0.4833335876464844, Loss 1.014833666086197\n",
      "RNN, rep: 0, epoch: 7, acc: 0.5166667699813843, Loss 1.0021838337182998\n",
      "RNN, rep: 0, epoch: 8, acc: 0.5366663932800293, Loss 0.9949116438627243\n",
      "RNN, rep: 0, epoch: 9, acc: 0.6000000238418579, Loss 0.9961185529828072\n",
      "RNN, rep: 0, epoch: 10, acc: 0.6033331751823425, Loss 0.9066495287418366\n",
      "RNN, rep: 0, epoch: 11, acc: 0.6466662883758545, Loss 0.8160843774676323\n",
      "RNN, rep: 0, epoch: 12, acc: 0.6700001358985901, Loss 0.742786838710308\n",
      "RNN, rep: 0, epoch: 13, acc: 0.660000205039978, Loss 0.7234957665205002\n",
      "RNN, rep: 0, epoch: 14, acc: 0.6800002455711365, Loss 0.7483833286166192\n",
      "RNN, rep: 0, epoch: 15, acc: 0.68666672706604, Loss 0.6828517591953278\n",
      "RNN, rep: 0, epoch: 16, acc: 0.7133334279060364, Loss 0.6737173500657082\n",
      "RNN, rep: 0, epoch: 17, acc: 0.7266666889190674, Loss 0.6565417063236236\n",
      "RNN, rep: 0, epoch: 18, acc: 0.7200000882148743, Loss 0.6508184441924095\n",
      "RNN, rep: 0, epoch: 19, acc: 0.6600003838539124, Loss 0.7193848952651024\n",
      "RNN, rep: 0, epoch: 20, acc: 0.730000376701355, Loss 0.6533745917677879\n",
      "RNN, rep: 0, epoch: 21, acc: 0.706666886806488, Loss 0.6549595606327057\n",
      "RNN, rep: 0, epoch: 22, acc: 0.6766668558120728, Loss 0.7145071437954903\n",
      "RNN, rep: 0, epoch: 23, acc: 0.720000147819519, Loss 0.6516559508442878\n",
      "RNN, rep: 0, epoch: 24, acc: 0.7599999308586121, Loss 0.6462296545505524\n",
      "RNN, rep: 0, epoch: 25, acc: 0.7133333683013916, Loss 0.701534676849842\n",
      "RNN, rep: 0, epoch: 26, acc: 0.7166666388511658, Loss 0.7027646964788437\n",
      "RNN, rep: 0, epoch: 27, acc: 0.7000002264976501, Loss 0.6619253522157669\n",
      "RNN, rep: 0, epoch: 28, acc: 0.6933337450027466, Loss 0.6626097708940506\n",
      "RNN, rep: 0, epoch: 29, acc: 0.676666796207428, Loss 0.7519863647222519\n",
      "RNN, rep: 0, epoch: 30, acc: 0.7066665887832642, Loss 0.6947295820713043\n",
      "RNN, rep: 0, epoch: 31, acc: 0.7300001382827759, Loss 0.6366419744491577\n",
      "RNN, rep: 0, epoch: 32, acc: 0.7366667985916138, Loss 0.6456606662273408\n",
      "RNN, rep: 0, epoch: 33, acc: 0.7433335185050964, Loss 0.6404648816585541\n",
      "RNN, rep: 0, epoch: 34, acc: 0.6700003147125244, Loss 0.6887392172217369\n",
      "RNN, rep: 0, epoch: 35, acc: 0.7266668081283569, Loss 0.6433381539583206\n",
      "RNN, rep: 0, epoch: 36, acc: 0.7400000095367432, Loss 0.6457298293709754\n",
      "RNN, rep: 0, epoch: 37, acc: 0.7400000691413879, Loss 0.6358760237693787\n",
      "RNN, rep: 0, epoch: 38, acc: 0.6700001358985901, Loss 0.9249108803272247\n",
      "RNN, rep: 0, epoch: 39, acc: 0.7366666197776794, Loss 0.638470604121685\n",
      "RNN, rep: 0, epoch: 40, acc: 0.7500001788139343, Loss 0.6254242739081383\n",
      "RNN, rep: 0, epoch: 41, acc: 0.6900001764297485, Loss 0.8055581042170524\n",
      "RNN, rep: 0, epoch: 42, acc: 0.7266665697097778, Loss 0.6309985417127609\n",
      "RNN, rep: 0, epoch: 43, acc: 0.7300000786781311, Loss 0.6314196571707725\n",
      "RNN, rep: 0, epoch: 44, acc: 0.7733333110809326, Loss 0.5971205100417137\n",
      "RNN, rep: 0, epoch: 45, acc: 0.7233335375785828, Loss 0.6309872871637344\n",
      "RNN, rep: 0, epoch: 46, acc: 0.7666668891906738, Loss 0.5998034691810608\n",
      "RNN, rep: 0, epoch: 47, acc: 0.7333332896232605, Loss 0.6325169810652733\n",
      "RNN, rep: 0, epoch: 48, acc: 0.7200000286102295, Loss 0.6479134121537209\n",
      "RNN, rep: 0, epoch: 49, acc: 0.7766666412353516, Loss 0.6673538190126419\n",
      "RNN, rep: 0, epoch: 50, acc: 0.7199999094009399, Loss 0.7207586225867272\n",
      "RNN, rep: 0, epoch: 51, acc: 0.7533335089683533, Loss 0.6030220058560372\n",
      "RNN, rep: 0, epoch: 52, acc: 0.746666669845581, Loss 0.6132884430885315\n",
      "RNN, rep: 0, epoch: 53, acc: 0.7933333516120911, Loss 0.5738830381631851\n",
      "RNN, rep: 0, epoch: 54, acc: 0.8033332228660583, Loss 0.5570405450463295\n",
      "RNN, rep: 0, epoch: 55, acc: 0.8233333826065063, Loss 0.5365234969556332\n",
      "RNN, rep: 0, epoch: 56, acc: 0.8033333420753479, Loss 0.573191061168909\n",
      "RNN, rep: 0, epoch: 57, acc: 0.7766668200492859, Loss 0.57524215310812\n",
      "RNN, rep: 0, epoch: 58, acc: 0.8033332228660583, Loss 0.5685341446101666\n",
      "RNN, rep: 0, epoch: 59, acc: 0.8366666436195374, Loss 0.5213771370053292\n",
      "RNN, rep: 0, epoch: 60, acc: 0.763333261013031, Loss 0.6105774341523648\n",
      "RNN, rep: 0, epoch: 61, acc: 0.8299999833106995, Loss 0.5410872477293015\n",
      "RNN, rep: 0, epoch: 62, acc: 0.8299998641014099, Loss 0.5904807418584823\n",
      "RNN, rep: 0, epoch: 63, acc: 0.8499998450279236, Loss 0.48800933107733724\n",
      "RNN, rep: 0, epoch: 64, acc: 0.8733334541320801, Loss 0.47905906826257705\n",
      "RNN, rep: 0, epoch: 65, acc: 0.8833332657814026, Loss 0.4535328610241413\n",
      "RNN, rep: 0, epoch: 66, acc: 0.9166667461395264, Loss 0.39218778513371944\n",
      "RNN, rep: 0, epoch: 67, acc: 0.8033332824707031, Loss 0.5956998067349195\n",
      "RNN, rep: 0, epoch: 68, acc: 0.8266667127609253, Loss 0.5015038275718688\n",
      "RNN, rep: 0, epoch: 69, acc: 0.8500002026557922, Loss 0.44514376677572726\n",
      "RNN, rep: 0, epoch: 70, acc: 0.8866666555404663, Loss 0.4053469754755497\n",
      "RNN, rep: 0, epoch: 71, acc: 0.9033333659172058, Loss 0.3629764404892921\n",
      "RNN, rep: 0, epoch: 72, acc: 0.893333375453949, Loss 0.3707259486615658\n",
      "RNN, rep: 0, epoch: 73, acc: 0.9033334255218506, Loss 0.37327796198427676\n",
      "RNN, rep: 0, epoch: 74, acc: 0.8799998760223389, Loss 0.3653100852668285\n",
      "RNN, rep: 0, epoch: 75, acc: 0.8833332657814026, Loss 0.3960390114039183\n",
      "RNN, rep: 0, epoch: 76, acc: 0.8866665363311768, Loss 0.3983435478806496\n",
      "RNN, rep: 0, epoch: 77, acc: 0.9266667366027832, Loss 0.37019018456339836\n",
      "RNN, rep: 0, epoch: 78, acc: 0.9433333873748779, Loss 0.3146514509618282\n",
      "RNN, rep: 0, epoch: 79, acc: 0.8933332562446594, Loss 0.4015127677470446\n",
      "RNN, rep: 0, epoch: 80, acc: 0.9033334255218506, Loss 0.3709393057972193\n",
      "RNN, rep: 0, epoch: 81, acc: 0.8599998950958252, Loss 0.4431002102047205\n",
      "RNN, rep: 0, epoch: 82, acc: 0.9266665577888489, Loss 0.2792421352118254\n",
      "RNN, rep: 0, epoch: 83, acc: 0.8733333349227905, Loss 0.41019352167844775\n",
      "RNN, rep: 0, epoch: 84, acc: 0.856666624546051, Loss 0.4176536886394024\n",
      "RNN, rep: 0, epoch: 85, acc: 0.7233331799507141, Loss 0.8079991631582379\n",
      "RNN, rep: 0, epoch: 86, acc: 0.6033332943916321, Loss 1.086304938532412\n",
      "RNN, rep: 0, epoch: 87, acc: 0.6033331751823425, Loss 1.04879523858428\n",
      "RNN, rep: 0, epoch: 88, acc: 0.6533331274986267, Loss 0.7992360625416041\n",
      "RNN, rep: 0, epoch: 89, acc: 0.7966665625572205, Loss 0.5398831756412983\n",
      "RNN, rep: 0, epoch: 90, acc: 0.8400000929832458, Loss 0.44641092862933873\n",
      "RNN, rep: 0, epoch: 91, acc: 0.8999999761581421, Loss 0.3767871751263738\n",
      "RNN, rep: 0, epoch: 92, acc: 0.8866667151451111, Loss 0.3754623368009925\n",
      "RNN, rep: 0, epoch: 93, acc: 0.9066666960716248, Loss 0.3632838727906346\n",
      "RNN, rep: 0, epoch: 94, acc: 0.8933334350585938, Loss 0.36375711642205716\n",
      "RNN, rep: 0, epoch: 95, acc: 0.8733333349227905, Loss 0.3688036947324872\n",
      "RNN, rep: 0, epoch: 96, acc: 0.8933334350585938, Loss 0.3432219078764319\n",
      "RNN, rep: 0, epoch: 97, acc: 0.9299999475479126, Loss 0.28327624332159757\n",
      "RNN, rep: 0, epoch: 98, acc: 0.9233335256576538, Loss 0.26506161388009786\n",
      "RNN, rep: 0, epoch: 99, acc: 0.9466665387153625, Loss 0.28856439217925073\n",
      "RNN, rep: 0, epoch: 100, acc: 0.8233332633972168, Loss 0.563349390104413\n",
      "RNN, rep: 0, epoch: 101, acc: 0.8866667747497559, Loss 0.3012126210331917\n",
      "RNN, rep: 0, epoch: 102, acc: 0.9200000166893005, Loss 0.2996154377609491\n",
      "RNN, rep: 0, epoch: 103, acc: 0.9533331990242004, Loss 0.2316099000349641\n",
      "RNN, rep: 0, epoch: 104, acc: 0.9500000476837158, Loss 0.2524000297486782\n",
      "RNN, rep: 0, epoch: 105, acc: 0.9200000762939453, Loss 0.3525814023055136\n",
      "RNN, rep: 0, epoch: 106, acc: 0.8800000548362732, Loss 0.366358412951231\n",
      "RNN, rep: 0, epoch: 107, acc: 0.9533332586288452, Loss 0.27143374996259806\n",
      "RNN, rep: 0, epoch: 108, acc: 0.9133334159851074, Loss 0.34444273468106984\n",
      "RNN, rep: 0, epoch: 109, acc: 0.919999897480011, Loss 0.2769188164174557\n",
      "RNN, rep: 0, epoch: 110, acc: 0.9900000095367432, Loss 0.10963949458673597\n",
      "RNN                  Rep: 0   Epoch: 1     Acc: 0.9900 Params: min_length: 20, max_length: 20, fill: 0, value_1: -1, value_2: 1 Time: 34.36 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.46000000834465027, Loss 1.0154993498325349\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.47999992966651917, Loss 1.0037817996740341\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5599998235702515, Loss 0.9908071929216384\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.54666668176651, Loss 0.9817169409990311\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.6900002360343933, Loss 0.9333659869432449\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.6733333468437195, Loss 0.8185397854447365\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.6899999976158142, Loss 0.7557679611444473\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.7000000476837158, Loss 0.6836654132604599\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.6666668653488159, Loss 0.6775024914741516\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.6833336353302002, Loss 0.6789329606294632\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.7100000977516174, Loss 0.6648351770639419\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.7166668176651001, Loss 0.6612250506877899\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.7099998593330383, Loss 0.7132656586170196\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.6666666269302368, Loss 0.7301940402388573\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.7533332109451294, Loss 0.6454958158731461\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.7300001382827759, Loss 0.6416791781783104\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.7400000691413879, Loss 0.622568573653698\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.779999852180481, Loss 0.5192347151041031\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.7866665124893188, Loss 0.44185659795999527\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.8266666531562805, Loss 0.3625838154554367\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.8399996161460876, Loss 0.34701345637440684\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.8066664934158325, Loss 0.3683453018963337\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.8333333730697632, Loss 0.34167673528194425\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.81333327293396, Loss 0.4648467734456062\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.7766664624214172, Loss 0.5433910495042801\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.8566665649414062, Loss 0.3371378734707832\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.8566665649414062, Loss 0.329214343726635\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.8966664671897888, Loss 0.32651702702045443\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.8833332061767578, Loss 0.31424456551671026\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.929999828338623, Loss 0.2982893915474415\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.9433333873748779, Loss 0.2878269974887371\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.9433332085609436, Loss 0.2588667909801006\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.9333332777023315, Loss 0.21934739366173744\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 1.0, Loss 0.20498586907982827\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 1     Acc: 1.0000 Params: min_length: 20, max_length: 20, fill: 0, value_1: -1, value_2: 1 Time: 12.79 sec\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 1, acc: 0.4500000774860382, Loss 1.0387886613607407\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 2, acc: 0.5633334517478943, Loss 0.9959626132249833\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 3, acc: 0.5633335113525391, Loss 0.930944567322731\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 4, acc: 0.6633334159851074, Loss 0.7225309202075004\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 5, acc: 0.6800000071525574, Loss 0.6826514887809754\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 6, acc: 0.6966665387153625, Loss 0.6692260009050369\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 7, acc: 0.7433333396911621, Loss 0.6120648020505906\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 8, acc: 0.8500000238418579, Loss 0.5004087939858437\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 9, acc: 0.8366666436195374, Loss 0.4304871529340744\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 10, acc: 0.8233332633972168, Loss 0.39369339764118194\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 11, acc: 0.8499997854232788, Loss 0.3674747243523598\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 12, acc: 0.8333333730697632, Loss 0.35850330397486685\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 13, acc: 0.8433333039283752, Loss 0.3520524501800537\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 14, acc: 0.8233330249786377, Loss 0.35458529874682426\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 15, acc: 0.8366665840148926, Loss 0.3441802990436554\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 16, acc: 0.809999942779541, Loss 0.3444840270280838\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 17, acc: 0.8266664147377014, Loss 0.34258562624454497\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 18, acc: 0.856666624546051, Loss 0.3381583881378174\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 19, acc: 0.8533332347869873, Loss 0.33375301480293273\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 20, acc: 0.8933331966400146, Loss 0.3154661180078983\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 21, acc: 0.896666407585144, Loss 0.31556851491332055\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 22, acc: 0.9033333659172058, Loss 0.29911785408854485\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 23, acc: 0.8999998569488525, Loss 0.28180997669696806\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 24, acc: 0.8966664671897888, Loss 0.27928848698735237\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 25, acc: 0.9166667461395264, Loss 0.23736522778868674\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 26, acc: 0.9266665577888489, Loss 0.21437415070831775\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 27, acc: 0.9299999475479126, Loss 0.19051793117076157\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 28, acc: 0.9399999976158142, Loss 0.23408123798668384\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 29, acc: 0.9533333778381348, Loss 0.1823865919932723\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 30, acc: 1.0, Loss 0.13753628736361861\n",
      "NetRNNWithAttentionExpFirst Rep: 0   Epoch: 1     Acc: 1.0000 Params: min_length: 20, max_length: 20, fill: 0, value_1: -1, value_2: 1 Time: 11.34 sec\n",
      "LSTM, rep: 0, epoch: 1, acc: 0.5066665410995483, Loss 1.0038112169504165\n",
      "LSTM, rep: 0, epoch: 2, acc: 0.5099998116493225, Loss 1.0090290796756745\n",
      "LSTM, rep: 0, epoch: 3, acc: 0.4900001585483551, Loss 1.0046988701820374\n",
      "LSTM, rep: 0, epoch: 4, acc: 0.5333335399627686, Loss 0.9956287366151809\n",
      "LSTM, rep: 0, epoch: 5, acc: 0.5099998116493225, Loss 1.0070162361860275\n",
      "LSTM, rep: 0, epoch: 6, acc: 0.5066664218902588, Loss 1.0040076649188996\n",
      "LSTM, rep: 0, epoch: 7, acc: 0.5266664624214172, Loss 0.9981890779733658\n",
      "LSTM, rep: 0, epoch: 8, acc: 0.5433335304260254, Loss 0.9948994863033295\n",
      "LSTM, rep: 0, epoch: 9, acc: 0.5099997520446777, Loss 0.9938874912261962\n",
      "LSTM, rep: 0, epoch: 10, acc: 0.580000102519989, Loss 0.9784612661600113\n",
      "LSTM, rep: 0, epoch: 11, acc: 0.6899999976158142, Loss 0.8642235088348389\n",
      "LSTM, rep: 0, epoch: 12, acc: 0.6733333468437195, Loss 0.7921666091680527\n",
      "LSTM, rep: 0, epoch: 13, acc: 0.6566666960716248, Loss 0.820823116004467\n",
      "LSTM, rep: 0, epoch: 14, acc: 0.7200000286102295, Loss 0.7411864832043648\n",
      "LSTM, rep: 0, epoch: 15, acc: 0.690000057220459, Loss 0.7520857551693916\n",
      "LSTM, rep: 0, epoch: 16, acc: 0.6433334350585938, Loss 0.7580170455574989\n",
      "LSTM, rep: 0, epoch: 17, acc: 0.660000205039978, Loss 0.7160494947433471\n",
      "LSTM, rep: 0, epoch: 18, acc: 0.6700000762939453, Loss 0.7147084480524063\n",
      "LSTM, rep: 0, epoch: 19, acc: 0.6966667771339417, Loss 0.6057605451345444\n",
      "LSTM, rep: 0, epoch: 20, acc: 0.8233332633972168, Loss 0.5342057690024375\n",
      "LSTM, rep: 0, epoch: 21, acc: 0.8266665935516357, Loss 0.48851056963205336\n",
      "LSTM, rep: 0, epoch: 22, acc: 0.8066666126251221, Loss 0.47863706707954407\n",
      "LSTM, rep: 0, epoch: 23, acc: 0.8933331966400146, Loss 0.38845945775508883\n",
      "LSTM, rep: 0, epoch: 24, acc: 0.9166665077209473, Loss 0.35866296514868734\n",
      "LSTM, rep: 0, epoch: 25, acc: 0.9166663885116577, Loss 0.34240921765565874\n",
      "LSTM, rep: 0, epoch: 26, acc: 0.9066665172576904, Loss 0.3265658801794052\n",
      "LSTM, rep: 0, epoch: 27, acc: 0.9499999284744263, Loss 0.2676177801191807\n",
      "LSTM, rep: 0, epoch: 28, acc: 0.9433332085609436, Loss 0.2952104276418686\n",
      "LSTM, rep: 0, epoch: 29, acc: 0.9433332681655884, Loss 0.26008662559092044\n",
      "LSTM, rep: 0, epoch: 30, acc: 0.9666666984558105, Loss 0.2316635736823082\n",
      "LSTM, rep: 0, epoch: 31, acc: 0.9866665601730347, Loss 0.18955602444708347\n",
      "LSTM                 Rep: 0   Epoch: 1     Acc: 0.9867 Params: min_length: 20, max_length: 20, fill: 0, value_1: -1, value_2: 1 Time: 8.44 sec\n",
      "NetLSTMWithAttention, rep: 0, epoch: 1, acc: 0.533333420753479, Loss 1.0216185349225997\n",
      "NetLSTMWithAttention, rep: 0, epoch: 2, acc: 0.4966665506362915, Loss 1.0471159934997558\n",
      "NetLSTMWithAttention, rep: 0, epoch: 3, acc: 0.46999990940093994, Loss 1.0224275451898575\n",
      "NetLSTMWithAttention, rep: 0, epoch: 4, acc: 0.5433334708213806, Loss 0.9943861544132233\n",
      "NetLSTMWithAttention, rep: 0, epoch: 5, acc: 0.5233333706855774, Loss 1.0009807574748992\n",
      "NetLSTMWithAttention, rep: 0, epoch: 6, acc: 0.5866667032241821, Loss 0.9759479564428329\n",
      "NetLSTMWithAttention, rep: 0, epoch: 7, acc: 0.6333335638046265, Loss 0.9028459423780442\n",
      "NetLSTMWithAttention, rep: 0, epoch: 8, acc: 0.7033333778381348, Loss 0.8047069942951203\n",
      "NetLSTMWithAttention, rep: 0, epoch: 9, acc: 0.6966667771339417, Loss 0.7040015971660614\n",
      "NetLSTMWithAttention, rep: 0, epoch: 10, acc: 0.6400001645088196, Loss 0.7081015992164612\n",
      "NetLSTMWithAttention, rep: 0, epoch: 11, acc: 0.690000057220459, Loss 0.6742099750041962\n",
      "NetLSTMWithAttention, rep: 0, epoch: 12, acc: 0.6833334565162659, Loss 0.6597966948151588\n",
      "NetLSTMWithAttention, rep: 0, epoch: 13, acc: 0.7766664624214172, Loss 0.6000043925642967\n",
      "NetLSTMWithAttention, rep: 0, epoch: 14, acc: 0.8299998641014099, Loss 0.47846936866641043\n",
      "NetLSTMWithAttention, rep: 0, epoch: 15, acc: 0.8333332538604736, Loss 0.43649642914533615\n",
      "NetLSTMWithAttention, rep: 0, epoch: 16, acc: 0.6666668057441711, Loss 0.8233495907485485\n",
      "NetLSTMWithAttention, rep: 0, epoch: 17, acc: 0.6266666054725647, Loss 0.8569923010468483\n",
      "NetLSTMWithAttention, rep: 0, epoch: 18, acc: 0.6966665387153625, Loss 0.7536668363213539\n",
      "NetLSTMWithAttention, rep: 0, epoch: 19, acc: 0.7066667079925537, Loss 0.7406977486610412\n",
      "NetLSTMWithAttention, rep: 0, epoch: 20, acc: 0.8633334636688232, Loss 0.43064733982086184\n",
      "NetLSTMWithAttention, rep: 0, epoch: 21, acc: 0.8599998950958252, Loss 0.3801973378658295\n",
      "NetLSTMWithAttention, rep: 0, epoch: 22, acc: 0.8399999141693115, Loss 0.37922309920191766\n",
      "NetLSTMWithAttention, rep: 0, epoch: 23, acc: 0.8133332133293152, Loss 0.4016912406682968\n",
      "NetLSTMWithAttention, rep: 0, epoch: 24, acc: 0.7966666221618652, Loss 0.5287060672044754\n",
      "NetLSTMWithAttention, rep: 0, epoch: 25, acc: 0.6766668558120728, Loss 0.8943133553862572\n",
      "NetLSTMWithAttention, rep: 0, epoch: 26, acc: 0.6333333849906921, Loss 0.9183274456858634\n",
      "NetLSTMWithAttention, rep: 0, epoch: 27, acc: 0.6433336138725281, Loss 0.7806569263339043\n",
      "NetLSTMWithAttention, rep: 0, epoch: 28, acc: 0.7999998331069946, Loss 0.47624270677566527\n",
      "NetLSTMWithAttention, rep: 0, epoch: 29, acc: 0.8233332633972168, Loss 0.38879898846149447\n",
      "NetLSTMWithAttention, rep: 0, epoch: 30, acc: 0.8666665554046631, Loss 0.3627481356263161\n",
      "NetLSTMWithAttention, rep: 0, epoch: 31, acc: 0.8233332633972168, Loss 0.3666536805033684\n",
      "NetLSTMWithAttention, rep: 0, epoch: 32, acc: 0.8433331847190857, Loss 0.344968156516552\n",
      "NetLSTMWithAttention, rep: 0, epoch: 33, acc: 0.8933331966400146, Loss 0.32519594371318816\n",
      "NetLSTMWithAttention, rep: 0, epoch: 34, acc: 0.90666663646698, Loss 0.3254069350659847\n",
      "NetLSTMWithAttention, rep: 0, epoch: 35, acc: 0.9066663980484009, Loss 0.31465902492403985\n",
      "NetLSTMWithAttention, rep: 0, epoch: 36, acc: 0.8999999165534973, Loss 0.29972308948636056\n",
      "NetLSTMWithAttention, rep: 0, epoch: 37, acc: 0.9199997782707214, Loss 0.2887928321957588\n",
      "NetLSTMWithAttention, rep: 0, epoch: 38, acc: 0.9166665077209473, Loss 0.2785669507086277\n",
      "NetLSTMWithAttention, rep: 0, epoch: 39, acc: 0.9133331179618835, Loss 0.2754424886405468\n",
      "NetLSTMWithAttention, rep: 0, epoch: 40, acc: 0.9233332276344299, Loss 0.25609428107738497\n",
      "NetLSTMWithAttention, rep: 0, epoch: 41, acc: 0.8933332562446594, Loss 0.26934436976909637\n",
      "NetLSTMWithAttention, rep: 0, epoch: 42, acc: 0.9299999475479126, Loss 0.22950747691094875\n",
      "NetLSTMWithAttention, rep: 0, epoch: 43, acc: 0.9066665172576904, Loss 0.24445620544254779\n",
      "NetLSTMWithAttention, rep: 0, epoch: 44, acc: 0.9166667461395264, Loss 0.22657919690012931\n",
      "NetLSTMWithAttention, rep: 0, epoch: 45, acc: 0.9299997687339783, Loss 0.22302290476858616\n",
      "NetLSTMWithAttention, rep: 0, epoch: 46, acc: 0.9066665172576904, Loss 0.23973304107785226\n",
      "NetLSTMWithAttention, rep: 0, epoch: 47, acc: 0.9066665172576904, Loss 0.22530959285795688\n",
      "NetLSTMWithAttention, rep: 0, epoch: 48, acc: 0.9300000071525574, Loss 0.20639427181333303\n",
      "NetLSTMWithAttention, rep: 0, epoch: 49, acc: 0.9266665577888489, Loss 0.21631532341241835\n",
      "NetLSTMWithAttention, rep: 0, epoch: 50, acc: 0.9099999070167542, Loss 0.21080230675637723\n",
      "NetLSTMWithAttention, rep: 0, epoch: 51, acc: 0.9266666173934937, Loss 0.20438562888652087\n",
      "NetLSTMWithAttention, rep: 0, epoch: 52, acc: 0.9266666173934937, Loss 0.19659244224429132\n",
      "NetLSTMWithAttention, rep: 0, epoch: 53, acc: 0.9299997687339783, Loss 0.20095836939290165\n",
      "NetLSTMWithAttention, rep: 0, epoch: 54, acc: 0.903333306312561, Loss 0.20217454930767417\n",
      "NetLSTMWithAttention, rep: 0, epoch: 55, acc: 0.9299999475479126, Loss 0.1869196893274784\n",
      "NetLSTMWithAttention, rep: 0, epoch: 56, acc: 0.9233330488204956, Loss 0.18863406825810672\n",
      "NetLSTMWithAttention, rep: 0, epoch: 57, acc: 0.9166663885116577, Loss 0.20222099421545864\n",
      "NetLSTMWithAttention, rep: 0, epoch: 58, acc: 0.9166666269302368, Loss 0.19583273712545635\n",
      "NetLSTMWithAttention, rep: 0, epoch: 59, acc: 0.8999998569488525, Loss 0.20726343033835293\n",
      "NetLSTMWithAttention, rep: 0, epoch: 60, acc: 0.8899999260902405, Loss 0.21601712500676512\n",
      "NetLSTMWithAttention, rep: 0, epoch: 61, acc: 0.9133332967758179, Loss 0.19810376904904842\n",
      "NetLSTMWithAttention, rep: 0, epoch: 62, acc: 0.8933331966400146, Loss 0.20026116395369173\n",
      "NetLSTMWithAttention, rep: 0, epoch: 63, acc: 0.9066665172576904, Loss 0.22081541579216718\n",
      "NetLSTMWithAttention, rep: 0, epoch: 64, acc: 0.9266664981842041, Loss 0.16999975162558256\n",
      "NetLSTMWithAttention, rep: 0, epoch: 65, acc: 0.8933332562446594, Loss 0.21794963127933442\n",
      "NetLSTMWithAttention, rep: 0, epoch: 66, acc: 0.9066665172576904, Loss 0.17975744330324234\n",
      "NetLSTMWithAttention, rep: 0, epoch: 67, acc: 0.9199998378753662, Loss 0.20114786612801253\n",
      "NetLSTMWithAttention, rep: 0, epoch: 68, acc: 0.8999999761581421, Loss 0.18857777984812857\n",
      "NetLSTMWithAttention, rep: 0, epoch: 69, acc: 0.886666476726532, Loss 0.22426394380629064\n",
      "NetLSTMWithAttention, rep: 0, epoch: 70, acc: 0.9300000071525574, Loss 0.1469185938127339\n",
      "NetLSTMWithAttention, rep: 0, epoch: 71, acc: 0.9133331775665283, Loss 0.2048346238862723\n",
      "NetLSTMWithAttention, rep: 0, epoch: 72, acc: 0.9233332872390747, Loss 0.17394728646613658\n",
      "NetLSTMWithAttention, rep: 0, epoch: 73, acc: 0.9366664886474609, Loss 0.18682305986993014\n",
      "NetLSTMWithAttention, rep: 0, epoch: 74, acc: 0.8400000929832458, Loss 0.4091840026061982\n",
      "NetLSTMWithAttention, rep: 0, epoch: 75, acc: 0.8733333349227905, Loss 0.3341407231148332\n",
      "NetLSTMWithAttention, rep: 0, epoch: 76, acc: 0.7866665720939636, Loss 0.6844685911107808\n",
      "NetLSTMWithAttention, rep: 0, epoch: 77, acc: 0.7000001668930054, Loss 0.8776551643945276\n",
      "NetLSTMWithAttention, rep: 0, epoch: 78, acc: 0.6966667175292969, Loss 0.829555690176785\n",
      "NetLSTMWithAttention, rep: 0, epoch: 79, acc: 0.7500000596046448, Loss 0.7609866294078529\n",
      "NetLSTMWithAttention, rep: 0, epoch: 80, acc: 0.6933332085609436, Loss 0.7844024378806352\n",
      "NetLSTMWithAttention, rep: 0, epoch: 81, acc: 0.7999999523162842, Loss 0.4618760134279728\n",
      "NetLSTMWithAttention, rep: 0, epoch: 82, acc: 0.8333332538604736, Loss 0.39378890350461004\n",
      "NetLSTMWithAttention, rep: 0, epoch: 83, acc: 0.8499999046325684, Loss 0.38859427532181146\n",
      "NetLSTMWithAttention, rep: 0, epoch: 84, acc: 0.8500000238418579, Loss 0.43266536433249714\n",
      "NetLSTMWithAttention, rep: 0, epoch: 85, acc: 0.8399999737739563, Loss 0.45455847747623923\n",
      "NetLSTMWithAttention, rep: 0, epoch: 86, acc: 0.8199998736381531, Loss 0.4881374554336071\n",
      "NetLSTMWithAttention, rep: 0, epoch: 87, acc: 0.8299999237060547, Loss 0.4536910892277956\n",
      "NetLSTMWithAttention, rep: 0, epoch: 88, acc: 0.8266664743423462, Loss 0.4250385566800833\n",
      "NetLSTMWithAttention, rep: 0, epoch: 89, acc: 0.8466666340827942, Loss 0.33980644792318343\n",
      "NetLSTMWithAttention, rep: 0, epoch: 90, acc: 0.8799997568130493, Loss 0.3219262179732323\n",
      "NetLSTMWithAttention, rep: 0, epoch: 91, acc: 0.8899998664855957, Loss 0.30511815145611765\n",
      "NetLSTMWithAttention, rep: 0, epoch: 92, acc: 0.8799999356269836, Loss 0.3296643579006195\n",
      "NetLSTMWithAttention, rep: 0, epoch: 93, acc: 0.839999794960022, Loss 0.3453137156739831\n",
      "NetLSTMWithAttention, rep: 0, epoch: 94, acc: 0.839999794960022, Loss 0.32151282899081707\n",
      "NetLSTMWithAttention, rep: 0, epoch: 95, acc: 0.7300002574920654, Loss 0.5744878196716309\n",
      "NetLSTMWithAttention, rep: 0, epoch: 96, acc: 0.6166666150093079, Loss 0.7444352596998215\n",
      "NetLSTMWithAttention, rep: 0, epoch: 97, acc: 0.6366666555404663, Loss 0.6215895890444517\n",
      "NetLSTMWithAttention, rep: 0, epoch: 98, acc: 0.7000001668930054, Loss 0.5534955760091543\n",
      "NetLSTMWithAttention, rep: 0, epoch: 99, acc: 0.7566667795181274, Loss 0.5396043809503317\n",
      "NetLSTMWithAttention, rep: 0, epoch: 100, acc: 0.76666659116745, Loss 0.49260150715708734\n",
      "NetLSTMWithAttention, rep: 0, epoch: 101, acc: 0.7400000691413879, Loss 0.5096575296670198\n",
      "NetLSTMWithAttention, rep: 0, epoch: 102, acc: 0.8466668128967285, Loss 0.431639369353652\n",
      "NetLSTMWithAttention, rep: 0, epoch: 103, acc: 0.7766664624214172, Loss 0.5225810396671295\n",
      "NetLSTMWithAttention, rep: 0, epoch: 104, acc: 0.8766664862632751, Loss 0.4551626301556826\n",
      "NetLSTMWithAttention, rep: 0, epoch: 105, acc: 0.8566665649414062, Loss 0.4635170967131853\n",
      "NetLSTMWithAttention, rep: 0, epoch: 106, acc: 0.8533332347869873, Loss 0.44777782648801806\n",
      "NetLSTMWithAttention, rep: 0, epoch: 107, acc: 0.8366664052009583, Loss 0.42205166883766654\n",
      "NetLSTMWithAttention, rep: 0, epoch: 108, acc: 0.8699999451637268, Loss 0.38307527795434\n",
      "NetLSTMWithAttention, rep: 0, epoch: 109, acc: 0.8366665840148926, Loss 0.4052090809494257\n",
      "NetLSTMWithAttention, rep: 0, epoch: 110, acc: 0.8433333039283752, Loss 0.3948253509402275\n",
      "NetLSTMWithAttention, rep: 0, epoch: 111, acc: 0.84333336353302, Loss 0.3852455247938633\n",
      "NetLSTMWithAttention, rep: 0, epoch: 112, acc: 0.8399998545646667, Loss 0.391588224992156\n",
      "NetLSTMWithAttention, rep: 0, epoch: 113, acc: 0.8766664266586304, Loss 0.357271875590086\n",
      "NetLSTMWithAttention, rep: 0, epoch: 114, acc: 0.8233331441879272, Loss 0.3875786916911602\n",
      "NetLSTMWithAttention, rep: 0, epoch: 115, acc: 0.8999999761581421, Loss 0.37522696852684023\n",
      "NetLSTMWithAttention, rep: 0, epoch: 116, acc: 0.9266666173934937, Loss 0.3492283672094345\n",
      "NetLSTMWithAttention, rep: 0, epoch: 117, acc: 0.9233331084251404, Loss 0.32806991554796694\n",
      "NetLSTMWithAttention, rep: 0, epoch: 118, acc: 0.9366666674613953, Loss 0.33365558728575706\n",
      "NetLSTMWithAttention, rep: 0, epoch: 119, acc: 0.9433332085609436, Loss 0.306915478259325\n",
      "NetLSTMWithAttention, rep: 0, epoch: 120, acc: 0.9566665887832642, Loss 0.27262587249279024\n",
      "NetLSTMWithAttention, rep: 0, epoch: 121, acc: 0.90666663646698, Loss 0.30787655904889105\n",
      "NetLSTMWithAttention, rep: 0, epoch: 122, acc: 0.903333306312561, Loss 0.28773876987397673\n",
      "NetLSTMWithAttention, rep: 0, epoch: 123, acc: 0.9066665768623352, Loss 0.27280562683939935\n",
      "NetLSTMWithAttention, rep: 0, epoch: 124, acc: 0.8999999761581421, Loss 0.27320480428636074\n",
      "NetLSTMWithAttention, rep: 0, epoch: 125, acc: 0.9299999475479126, Loss 0.2365284690260887\n",
      "NetLSTMWithAttention, rep: 0, epoch: 126, acc: 0.9233332872390747, Loss 0.24067328557372092\n",
      "NetLSTMWithAttention, rep: 0, epoch: 127, acc: 0.9133331179618835, Loss 0.2484427972137928\n",
      "NetLSTMWithAttention, rep: 0, epoch: 128, acc: 0.9266665577888489, Loss 0.23002443328499794\n",
      "NetLSTMWithAttention, rep: 0, epoch: 129, acc: 0.9366664886474609, Loss 0.23511807911098004\n",
      "NetLSTMWithAttention, rep: 0, epoch: 130, acc: 0.9399999380111694, Loss 0.18665597684681415\n",
      "NetLSTMWithAttention, rep: 0, epoch: 131, acc: 0.8899999856948853, Loss 0.22571327678859235\n",
      "NetLSTMWithAttention, rep: 0, epoch: 132, acc: 0.9099999070167542, Loss 0.1969053864851594\n",
      "NetLSTMWithAttention, rep: 0, epoch: 133, acc: 0.8699997663497925, Loss 0.2206759862974286\n",
      "NetLSTMWithAttention, rep: 0, epoch: 134, acc: 0.9700000286102295, Loss 0.17919438693672418\n",
      "NetLSTMWithAttention Rep: 0   Epoch: 1     Acc: 0.9700 Params: min_length: 20, max_length: 20, fill: 0, value_1: -1, value_2: 1 Time: 48.44 sec\n",
      "GRU, rep: 0, epoch: 1, acc: 0.43333321809768677, Loss 1.0426215875148772\n",
      "GRU, rep: 0, epoch: 2, acc: 0.4999999701976776, Loss 0.9996474313735962\n",
      "GRU, rep: 0, epoch: 3, acc: 0.5133333802223206, Loss 0.9965901976823807\n",
      "GRU, rep: 0, epoch: 4, acc: 0.5233332514762878, Loss 0.9938716298341751\n",
      "GRU, rep: 0, epoch: 5, acc: 0.5299997925758362, Loss 0.9849961292743683\n",
      "GRU, rep: 0, epoch: 6, acc: 0.5300000309944153, Loss 0.9854937815666198\n",
      "GRU, rep: 0, epoch: 7, acc: 0.5866668224334717, Loss 0.9544576871395111\n",
      "GRU, rep: 0, epoch: 8, acc: 0.6933335661888123, Loss 0.8175563123822213\n",
      "GRU, rep: 0, epoch: 9, acc: 0.7100000977516174, Loss 0.7028739288449287\n",
      "GRU, rep: 0, epoch: 10, acc: 0.783333420753479, Loss 0.6285942640900611\n",
      "GRU, rep: 0, epoch: 11, acc: 0.8966665863990784, Loss 0.4613823515176773\n",
      "GRU, rep: 0, epoch: 12, acc: 0.8533332347869873, Loss 0.3941844379901886\n",
      "GRU, rep: 0, epoch: 13, acc: 0.9100000262260437, Loss 0.3372092568874359\n",
      "GRU, rep: 0, epoch: 14, acc: 0.9166667461395264, Loss 0.3169433286786079\n",
      "GRU, rep: 0, epoch: 15, acc: 0.9266666173934937, Loss 0.28678150594234464\n",
      "GRU, rep: 0, epoch: 16, acc: 0.9599999189376831, Loss 0.2621814550459385\n",
      "GRU, rep: 0, epoch: 17, acc: 1.0, Loss 0.2199337887018919\n",
      "GRU                  Rep: 0   Epoch: 1     Acc: 1.0000 Params: min_length: 20, max_length: 20, fill: 0, value_1: -1, value_2: 1 Time: 9.22 sec\n",
      "NetGRUWithAttention, rep: 0, epoch: 1, acc: 0.5033331513404846, Loss 1.0349400860071183\n",
      "NetGRUWithAttention, rep: 0, epoch: 2, acc: 0.5100000500679016, Loss 1.0044898653030396\n",
      "NetGRUWithAttention, rep: 0, epoch: 3, acc: 0.47666656970977783, Loss 1.0140679544210434\n",
      "NetGRUWithAttention, rep: 0, epoch: 4, acc: 0.573333203792572, Loss 0.9879103219509124\n",
      "NetGRUWithAttention, rep: 0, epoch: 5, acc: 0.5733335018157959, Loss 0.9698575693368912\n",
      "NetGRUWithAttention, rep: 0, epoch: 6, acc: 0.5466665029525757, Loss 0.9844482290744782\n",
      "NetGRUWithAttention, rep: 0, epoch: 7, acc: 0.5833332538604736, Loss 0.9553694880008697\n",
      "NetGRUWithAttention, rep: 0, epoch: 8, acc: 0.60999995470047, Loss 0.9334197628498078\n",
      "NetGRUWithAttention, rep: 0, epoch: 9, acc: 0.630000114440918, Loss 0.8088412716984749\n",
      "NetGRUWithAttention, rep: 0, epoch: 10, acc: 0.6933335661888123, Loss 0.706673823595047\n",
      "NetGRUWithAttention, rep: 0, epoch: 11, acc: 0.7699999809265137, Loss 0.7126722288131714\n",
      "NetGRUWithAttention, rep: 0, epoch: 12, acc: 0.8033332228660583, Loss 0.6266499063372613\n",
      "NetGRUWithAttention, rep: 0, epoch: 13, acc: 0.7633333802223206, Loss 0.5534166674315929\n",
      "NetGRUWithAttention, rep: 0, epoch: 14, acc: 0.8066667318344116, Loss 0.5788366237282753\n",
      "NetGRUWithAttention, rep: 0, epoch: 15, acc: 0.8366667032241821, Loss 0.48842188611626625\n",
      "NetGRUWithAttention, rep: 0, epoch: 16, acc: 0.9433332085609436, Loss 0.32619376555085183\n",
      "NetGRUWithAttention, rep: 0, epoch: 17, acc: 0.9799998998641968, Loss 0.21328060813248156\n",
      "NetGRUWithAttention  Rep: 0   Epoch: 1     Acc: 0.9800 Params: min_length: 20, max_length: 20, fill: 0, value_1: -1, value_2: 1 Time: 10.40 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.47666671872138977, Loss 1.0886662328243255\n",
      "RNN, rep: 0, epoch: 2, acc: 0.4899999499320984, Loss 1.0226336550712585\n",
      "RNN, rep: 0, epoch: 3, acc: 0.48333314061164856, Loss 1.0070444285869598\n",
      "RNN, rep: 0, epoch: 4, acc: 0.5133331418037415, Loss 1.0094038558006286\n",
      "RNN, rep: 0, epoch: 5, acc: 0.5233334898948669, Loss 1.0040843987464905\n",
      "RNN, rep: 0, epoch: 6, acc: 0.4933334290981293, Loss 1.0122872704267503\n",
      "RNN, rep: 0, epoch: 7, acc: 0.506666362285614, Loss 1.0057947915792464\n",
      "RNN, rep: 0, epoch: 8, acc: 0.5066666007041931, Loss 1.0056182026863099\n",
      "RNN, rep: 0, epoch: 9, acc: 0.45666658878326416, Loss 1.0065570962429047\n",
      "RNN, rep: 0, epoch: 10, acc: 0.506666898727417, Loss 1.0017349517345429\n",
      "RNN, rep: 0, epoch: 11, acc: 0.5466668009757996, Loss 0.9882767200469971\n",
      "RNN, rep: 0, epoch: 12, acc: 0.46333324909210205, Loss 1.0120576626062394\n",
      "RNN, rep: 0, epoch: 13, acc: 0.4899998903274536, Loss 1.0007481974363328\n",
      "RNN, rep: 0, epoch: 14, acc: 0.5333333611488342, Loss 1.0073720985651016\n",
      "RNN, rep: 0, epoch: 15, acc: 0.5533334612846375, Loss 0.9934290933609009\n",
      "RNN, rep: 0, epoch: 16, acc: 0.4866666793823242, Loss 0.9952640110254287\n",
      "RNN, rep: 0, epoch: 17, acc: 0.5800001621246338, Loss 0.9617819482088089\n",
      "RNN, rep: 0, epoch: 18, acc: 0.48666664958000183, Loss 1.0231647211313248\n",
      "RNN, rep: 0, epoch: 19, acc: 0.5233333110809326, Loss 0.9962285727262497\n",
      "RNN, rep: 0, epoch: 20, acc: 0.49333325028419495, Loss 1.0099441313743591\n",
      "RNN, rep: 0, epoch: 21, acc: 0.550000011920929, Loss 0.971941031217575\n",
      "RNN, rep: 0, epoch: 22, acc: 0.646666944026947, Loss 0.8465626382827759\n",
      "RNN, rep: 0, epoch: 23, acc: 0.5966669321060181, Loss 0.9503200313448906\n",
      "RNN, rep: 0, epoch: 24, acc: 0.5500003695487976, Loss 0.9421938931941987\n",
      "RNN, rep: 0, epoch: 25, acc: 0.6333334445953369, Loss 0.8503720378875732\n",
      "RNN, rep: 0, epoch: 26, acc: 0.6566669344902039, Loss 0.8509796503186225\n",
      "RNN, rep: 0, epoch: 27, acc: 0.6733335256576538, Loss 0.8368269827961922\n",
      "RNN, rep: 0, epoch: 28, acc: 0.6766670346260071, Loss 0.7812900421023369\n",
      "RNN, rep: 0, epoch: 29, acc: 0.5999999642372131, Loss 0.9135539263486863\n",
      "RNN, rep: 0, epoch: 30, acc: 0.5400003790855408, Loss 1.0301593416929244\n",
      "RNN, rep: 0, epoch: 31, acc: 0.5466669201850891, Loss 1.0046980369091034\n",
      "RNN, rep: 0, epoch: 32, acc: 0.5566668510437012, Loss 0.9920364248752594\n",
      "RNN, rep: 0, epoch: 33, acc: 0.5100002884864807, Loss 1.0147621601819992\n",
      "RNN, rep: 0, epoch: 34, acc: 0.5233330726623535, Loss 1.0082834792137145\n",
      "RNN, rep: 0, epoch: 35, acc: 0.4966665208339691, Loss 1.0084205251932143\n",
      "RNN, rep: 0, epoch: 36, acc: 0.5066667795181274, Loss 1.0066201692819596\n",
      "RNN, rep: 0, epoch: 37, acc: 0.5033330321311951, Loss 1.0063484936952591\n",
      "RNN, rep: 0, epoch: 38, acc: 0.4699999690055847, Loss 1.0112508338689805\n",
      "RNN, rep: 0, epoch: 39, acc: 0.5166663527488708, Loss 1.0004068094491958\n",
      "RNN, rep: 0, epoch: 40, acc: 0.5, Loss 1.018401643037796\n",
      "RNN, rep: 0, epoch: 41, acc: 0.5166664719581604, Loss 1.001607182621956\n",
      "RNN, rep: 0, epoch: 42, acc: 0.5366664528846741, Loss 0.9868068224191666\n",
      "RNN, rep: 0, epoch: 43, acc: 0.5033332705497742, Loss 1.0051502585411072\n",
      "RNN, rep: 0, epoch: 44, acc: 0.51666659116745, Loss 1.0050000905990601\n",
      "RNN, rep: 0, epoch: 45, acc: 0.4533332586288452, Loss 1.0092532700300216\n",
      "RNN, rep: 0, epoch: 46, acc: 0.5366666913032532, Loss 0.9973487830162049\n",
      "RNN, rep: 0, epoch: 47, acc: 0.5233330726623535, Loss 1.0030020797252654\n",
      "RNN, rep: 0, epoch: 48, acc: 0.4600001573562622, Loss 1.0175700396299363\n",
      "RNN, rep: 0, epoch: 49, acc: 0.5000001788139343, Loss 1.0013475620746612\n",
      "RNN, rep: 0, epoch: 50, acc: 0.5933334231376648, Loss 0.9818938672542572\n",
      "RNN, rep: 0, epoch: 51, acc: 0.51666659116745, Loss 0.9991670089960099\n",
      "RNN, rep: 0, epoch: 52, acc: 0.4899999499320984, Loss 1.0121553093194962\n",
      "RNN, rep: 0, epoch: 53, acc: 0.5499996542930603, Loss 0.9846445953845978\n",
      "RNN, rep: 0, epoch: 54, acc: 0.506666898727417, Loss 0.9894380211830139\n",
      "RNN, rep: 0, epoch: 55, acc: 0.5333335399627686, Loss 1.0001415205001831\n",
      "RNN, rep: 0, epoch: 56, acc: 0.4933333992958069, Loss 1.0104676032066344\n",
      "RNN, rep: 0, epoch: 57, acc: 0.5566666722297668, Loss 0.9833066183328628\n",
      "RNN, rep: 0, epoch: 58, acc: 0.47333359718322754, Loss 1.0302269685268401\n",
      "RNN, rep: 0, epoch: 59, acc: 0.4966665506362915, Loss 1.0002076214551925\n",
      "RNN, rep: 0, epoch: 60, acc: 0.5233331918716431, Loss 0.9933864539861679\n",
      "RNN, rep: 0, epoch: 61, acc: 0.5300001502037048, Loss 0.9876456707715988\n",
      "RNN, rep: 0, epoch: 62, acc: 0.5099996328353882, Loss 1.0010649412870407\n",
      "RNN, rep: 0, epoch: 63, acc: 0.5333335399627686, Loss 1.0033523279428482\n",
      "RNN, rep: 0, epoch: 64, acc: 0.5300002098083496, Loss 0.9993310576677322\n",
      "RNN, rep: 0, epoch: 65, acc: 0.483333557844162, Loss 1.0069744992256164\n",
      "RNN, rep: 0, epoch: 66, acc: 0.5433332324028015, Loss 0.9917488545179367\n",
      "RNN, rep: 0, epoch: 67, acc: 0.5266667604446411, Loss 0.9924231398105622\n",
      "RNN, rep: 0, epoch: 68, acc: 0.5633335113525391, Loss 0.987508218884468\n",
      "RNN, rep: 0, epoch: 69, acc: 0.5233336091041565, Loss 1.0046224999427795\n",
      "RNN, rep: 0, epoch: 70, acc: 0.6100000143051147, Loss 0.9606764787435531\n",
      "RNN, rep: 0, epoch: 71, acc: 0.5600000619888306, Loss 0.934385367333889\n",
      "RNN, rep: 0, epoch: 72, acc: 0.4566665291786194, Loss 1.0223987573385238\n",
      "RNN, rep: 0, epoch: 73, acc: 0.5199999213218689, Loss 0.9957747566699982\n",
      "RNN, rep: 0, epoch: 74, acc: 0.5433332324028015, Loss 0.991226669549942\n",
      "RNN, rep: 0, epoch: 75, acc: 0.6833334565162659, Loss 0.8242805668711662\n",
      "RNN, rep: 0, epoch: 76, acc: 0.6833335757255554, Loss 0.7797539201378823\n",
      "RNN, rep: 0, epoch: 77, acc: 0.6666667461395264, Loss 0.8140060386061668\n",
      "RNN, rep: 0, epoch: 78, acc: 0.6333335638046265, Loss 0.8977069681882859\n",
      "RNN, rep: 0, epoch: 79, acc: 0.7233335375785828, Loss 0.7450066062808037\n",
      "RNN, rep: 0, epoch: 80, acc: 0.7033334970474243, Loss 0.7160000053048133\n",
      "RNN, rep: 0, epoch: 81, acc: 0.7266665697097778, Loss 0.7313228675723076\n",
      "RNN, rep: 0, epoch: 82, acc: 0.7033334374427795, Loss 0.7448996326327324\n",
      "RNN, rep: 0, epoch: 83, acc: 0.6466666460037231, Loss 0.817911049425602\n",
      "RNN, rep: 0, epoch: 84, acc: 0.6600000858306885, Loss 0.7222106537222862\n",
      "RNN, rep: 0, epoch: 85, acc: 0.7033334374427795, Loss 0.6812363156676292\n",
      "RNN, rep: 0, epoch: 86, acc: 0.6866667866706848, Loss 0.6770708891749382\n",
      "RNN, rep: 0, epoch: 87, acc: 0.6433334350585938, Loss 0.710155058503151\n",
      "RNN, rep: 0, epoch: 88, acc: 0.7166668176651001, Loss 0.6670477703213692\n",
      "RNN, rep: 0, epoch: 89, acc: 0.6933333873748779, Loss 0.6788309127092361\n",
      "RNN, rep: 0, epoch: 90, acc: 0.6533334851264954, Loss 0.6909072974324226\n",
      "RNN, rep: 0, epoch: 91, acc: 0.6966668963432312, Loss 0.7238418033719063\n",
      "RNN, rep: 0, epoch: 92, acc: 0.6066668033599854, Loss 0.9825579938292504\n",
      "RNN, rep: 0, epoch: 93, acc: 0.6866668462753296, Loss 0.6879949739575386\n",
      "RNN, rep: 0, epoch: 94, acc: 0.676666796207428, Loss 0.6851038211584091\n",
      "RNN, rep: 0, epoch: 95, acc: 0.6600003242492676, Loss 0.700335547029972\n",
      "RNN, rep: 0, epoch: 96, acc: 0.6833335757255554, Loss 0.6814065918326377\n",
      "RNN, rep: 0, epoch: 97, acc: 0.6866667866706848, Loss 0.6580088207125664\n",
      "RNN, rep: 0, epoch: 98, acc: 0.7300000190734863, Loss 0.6498279210925102\n",
      "RNN, rep: 0, epoch: 99, acc: 0.7300000786781311, Loss 0.649687249660492\n",
      "RNN, rep: 0, epoch: 100, acc: 0.6566665768623352, Loss 0.6738866594433784\n",
      "RNN, rep: 0, epoch: 101, acc: 0.7133333086967468, Loss 0.6285753265023232\n",
      "RNN, rep: 0, epoch: 102, acc: 0.6733332276344299, Loss 0.6680023325979709\n",
      "RNN, rep: 0, epoch: 103, acc: 0.7066666483879089, Loss 0.6503174310922623\n",
      "RNN, rep: 0, epoch: 104, acc: 0.6899999976158142, Loss 0.6443597114086151\n",
      "RNN, rep: 0, epoch: 105, acc: 0.6833334565162659, Loss 0.6674782386422158\n",
      "RNN, rep: 0, epoch: 106, acc: 0.7100001573562622, Loss 0.6344255417585373\n",
      "RNN, rep: 0, epoch: 107, acc: 0.7266666889190674, Loss 0.6503426003456115\n",
      "RNN, rep: 0, epoch: 108, acc: 0.6266668438911438, Loss 0.8436428719758987\n",
      "RNN, rep: 0, epoch: 109, acc: 0.6666668653488159, Loss 0.7470460081100464\n",
      "RNN, rep: 0, epoch: 110, acc: 0.7033332586288452, Loss 0.7036841720342636\n",
      "RNN, rep: 0, epoch: 111, acc: 0.6766664981842041, Loss 0.6869657284021378\n",
      "RNN, rep: 0, epoch: 112, acc: 0.6833333373069763, Loss 0.6774116605520248\n",
      "RNN, rep: 0, epoch: 113, acc: 0.7133333683013916, Loss 0.6624433261156082\n",
      "RNN, rep: 0, epoch: 114, acc: 0.6666666269302368, Loss 0.6684572678804398\n",
      "RNN, rep: 0, epoch: 115, acc: 0.6533334851264954, Loss 0.682517825961113\n",
      "RNN, rep: 0, epoch: 116, acc: 0.6800002455711365, Loss 0.6633077922463417\n",
      "RNN, rep: 0, epoch: 117, acc: 0.6299999356269836, Loss 0.6943483355641366\n",
      "RNN, rep: 0, epoch: 118, acc: 0.7066666483879089, Loss 0.6595415300130845\n",
      "RNN, rep: 0, epoch: 119, acc: 0.7133333683013916, Loss 0.6547129240632057\n",
      "RNN, rep: 0, epoch: 120, acc: 0.6766666173934937, Loss 0.673155566751957\n",
      "RNN, rep: 0, epoch: 121, acc: 0.6666666269302368, Loss 0.6728160014748573\n",
      "RNN, rep: 0, epoch: 122, acc: 0.7300000190734863, Loss 0.6474583727121354\n",
      "RNN, rep: 0, epoch: 123, acc: 0.6599997878074646, Loss 0.6798307871818543\n",
      "RNN, rep: 0, epoch: 124, acc: 0.7100001573562622, Loss 0.6727936083078384\n",
      "RNN, rep: 0, epoch: 125, acc: 0.6399998664855957, Loss 0.6927083760499955\n",
      "RNN, rep: 0, epoch: 126, acc: 0.6899999380111694, Loss 0.6713792619109153\n",
      "RNN, rep: 0, epoch: 127, acc: 0.6833332777023315, Loss 0.6619635516405106\n",
      "RNN, rep: 0, epoch: 128, acc: 0.746666669845581, Loss 0.6434617587924003\n",
      "RNN, rep: 0, epoch: 129, acc: 0.6633332967758179, Loss 0.665820590853691\n",
      "RNN, rep: 0, epoch: 130, acc: 0.6666668653488159, Loss 0.66485231757164\n",
      "RNN, rep: 0, epoch: 131, acc: 0.7266665101051331, Loss 0.6409603697061539\n",
      "RNN, rep: 0, epoch: 132, acc: 0.7033332586288452, Loss 0.6604899567365646\n",
      "RNN, rep: 0, epoch: 133, acc: 0.6833332180976868, Loss 0.6617933735251427\n",
      "RNN, rep: 0, epoch: 134, acc: 0.6666666269302368, Loss 0.6474566458165646\n",
      "RNN, rep: 0, epoch: 135, acc: 0.6700002551078796, Loss 0.6559397920966148\n",
      "RNN, rep: 0, epoch: 136, acc: 0.7333332300186157, Loss 0.6449889141321182\n",
      "RNN, rep: 0, epoch: 137, acc: 0.6966664791107178, Loss 0.6613213163614273\n",
      "RNN, rep: 0, epoch: 138, acc: 0.7033332586288452, Loss 0.6667516353726387\n",
      "RNN, rep: 0, epoch: 139, acc: 0.683333158493042, Loss 0.6602370417118073\n",
      "RNN, rep: 0, epoch: 140, acc: 0.666666567325592, Loss 0.6545061910152435\n",
      "RNN, rep: 0, epoch: 141, acc: 0.6766666173934937, Loss 0.6546291723847389\n",
      "RNN, rep: 0, epoch: 142, acc: 0.6633331775665283, Loss 0.6599795567989349\n",
      "RNN, rep: 0, epoch: 143, acc: 0.7333333492279053, Loss 0.6450280231237412\n",
      "RNN, rep: 0, epoch: 144, acc: 0.7066666483879089, Loss 0.641631386578083\n",
      "RNN, rep: 0, epoch: 145, acc: 0.6999999284744263, Loss 0.6534320111572742\n",
      "RNN, rep: 0, epoch: 146, acc: 0.6866666674613953, Loss 0.6531667317450046\n",
      "RNN, rep: 0, epoch: 147, acc: 0.699999988079071, Loss 0.6505311970412732\n",
      "RNN, rep: 0, epoch: 148, acc: 0.7333335280418396, Loss 0.6246003811061382\n",
      "RNN, rep: 0, epoch: 149, acc: 0.6699998378753662, Loss 0.6619159030914307\n",
      "RNN, rep: 0, epoch: 150, acc: 0.6966665387153625, Loss 0.6354146337509156\n",
      "RNN, rep: 0, epoch: 151, acc: 0.7166666388511658, Loss 0.6544920365512371\n",
      "RNN, rep: 0, epoch: 152, acc: 0.7233333587646484, Loss 0.6311834815144539\n",
      "RNN, rep: 0, epoch: 153, acc: 0.7166666388511658, Loss 0.6430508859455586\n",
      "RNN, rep: 0, epoch: 154, acc: 0.6099998950958252, Loss 0.8316982246935367\n",
      "RNN, rep: 0, epoch: 155, acc: 0.6466667056083679, Loss 0.7829878398776055\n",
      "RNN, rep: 0, epoch: 156, acc: 0.7466668486595154, Loss 0.6247991220653057\n",
      "RNN, rep: 0, epoch: 157, acc: 0.7066670060157776, Loss 0.6488361942768097\n",
      "RNN, rep: 0, epoch: 158, acc: 0.6933332681655884, Loss 0.6469432948529721\n",
      "RNN, rep: 0, epoch: 159, acc: 0.7099999189376831, Loss 0.6450529932975769\n",
      "RNN, rep: 0, epoch: 160, acc: 0.73333340883255, Loss 0.6330721905827522\n",
      "RNN, rep: 0, epoch: 161, acc: 0.7333332896232605, Loss 0.6334463487565517\n",
      "RNN, rep: 0, epoch: 162, acc: 0.7466665506362915, Loss 0.6205467706918717\n",
      "RNN, rep: 0, epoch: 163, acc: 0.7433332204818726, Loss 0.6182613831758499\n",
      "RNN, rep: 0, epoch: 164, acc: 0.7366666197776794, Loss 0.6371393078565597\n",
      "RNN, rep: 0, epoch: 165, acc: 0.7766665816307068, Loss 0.6190535005927086\n",
      "RNN, rep: 0, epoch: 166, acc: 0.7033331990242004, Loss 0.6791412153840065\n",
      "RNN, rep: 0, epoch: 167, acc: 0.7333332300186157, Loss 0.6648968866467476\n",
      "RNN, rep: 0, epoch: 168, acc: 0.7166664600372314, Loss 0.6559725649654865\n",
      "RNN, rep: 0, epoch: 169, acc: 0.7066664695739746, Loss 0.6700613740086555\n",
      "RNN, rep: 0, epoch: 170, acc: 0.666666567325592, Loss 0.647613195180893\n",
      "RNN, rep: 0, epoch: 171, acc: 0.6399998664855957, Loss 0.654578615128994\n",
      "RNN, rep: 0, epoch: 172, acc: 0.6866665482521057, Loss 0.6411457987129688\n",
      "RNN, rep: 0, epoch: 173, acc: 0.7599999904632568, Loss 0.6054055388271808\n",
      "RNN, rep: 0, epoch: 174, acc: 0.6933332681655884, Loss 0.6373800143599511\n",
      "RNN, rep: 0, epoch: 175, acc: 0.763333261013031, Loss 0.5922836373746395\n",
      "RNN, rep: 0, epoch: 176, acc: 0.7200000286102295, Loss 0.6452620849013329\n",
      "RNN, rep: 0, epoch: 177, acc: 0.746666669845581, Loss 0.6239752987027168\n",
      "RNN, rep: 0, epoch: 178, acc: 0.7766667008399963, Loss 0.5925028492510319\n",
      "RNN, rep: 0, epoch: 179, acc: 0.7299998998641968, Loss 0.6310963553190231\n",
      "RNN, rep: 0, epoch: 180, acc: 0.7266666293144226, Loss 0.6255724044144153\n",
      "RNN, rep: 0, epoch: 181, acc: 0.7400001287460327, Loss 0.6274341924488545\n",
      "RNN, rep: 0, epoch: 182, acc: 0.7600001692771912, Loss 0.5905201131105423\n",
      "RNN, rep: 0, epoch: 183, acc: 0.736666738986969, Loss 0.6287080095708371\n",
      "RNN, rep: 0, epoch: 184, acc: 0.7366666197776794, Loss 0.6205044430494309\n",
      "RNN, rep: 0, epoch: 185, acc: 0.5733331441879272, Loss 1.1444816163927316\n",
      "RNN, rep: 0, epoch: 186, acc: 0.6233333349227905, Loss 0.9900376757979393\n",
      "RNN, rep: 0, epoch: 187, acc: 0.6766666173934937, Loss 0.7333022677898406\n",
      "RNN, rep: 0, epoch: 188, acc: 0.7433332800865173, Loss 0.6886257212609053\n",
      "RNN, rep: 0, epoch: 189, acc: 0.7266665697097778, Loss 0.6492776328325272\n",
      "RNN, rep: 0, epoch: 190, acc: 0.6833332777023315, Loss 0.9003430689871311\n",
      "RNN, rep: 0, epoch: 191, acc: 0.7266666889190674, Loss 0.7021380859613419\n",
      "RNN, rep: 0, epoch: 192, acc: 0.7233334183692932, Loss 0.6516838118433952\n",
      "RNN, rep: 0, epoch: 193, acc: 0.7300002574920654, Loss 0.6453942699730396\n",
      "RNN, rep: 0, epoch: 194, acc: 0.7333332300186157, Loss 0.646147842258215\n",
      "RNN, rep: 0, epoch: 195, acc: 0.7033331990242004, Loss 0.6859017790853977\n",
      "RNN, rep: 0, epoch: 196, acc: 0.7266666889190674, Loss 0.6455741690099239\n",
      "RNN, rep: 0, epoch: 197, acc: 0.7200000882148743, Loss 0.6296649092435836\n",
      "RNN, rep: 0, epoch: 198, acc: 0.746666669845581, Loss 0.6369273810088635\n",
      "RNN, rep: 0, epoch: 199, acc: 0.6966666579246521, Loss 0.717480272948742\n",
      "RNN, rep: 0, epoch: 200, acc: 0.6766664981842041, Loss 0.6650035925209522\n",
      "RNN, rep: 0, epoch: 201, acc: 0.6899999976158142, Loss 0.6604585042595863\n",
      "RNN, rep: 0, epoch: 202, acc: 0.7033331990242004, Loss 0.6582533264160156\n",
      "RNN, rep: 0, epoch: 203, acc: 0.7133333086967468, Loss 0.654957400560379\n",
      "RNN, rep: 0, epoch: 204, acc: 0.7099999189376831, Loss 0.7202851429581643\n",
      "RNN, rep: 0, epoch: 205, acc: 0.7199998497962952, Loss 0.7002597346156836\n",
      "RNN, rep: 0, epoch: 206, acc: 0.7733333706855774, Loss 0.6547126542776823\n",
      "RNN, rep: 0, epoch: 207, acc: 0.7333332300186157, Loss 0.671638452410698\n",
      "RNN, rep: 0, epoch: 208, acc: 0.7199999094009399, Loss 0.652211823835969\n",
      "RNN, rep: 0, epoch: 209, acc: 0.7500001788139343, Loss 0.6268356249481439\n",
      "RNN, rep: 0, epoch: 210, acc: 0.6999998688697815, Loss 0.6520956268906594\n",
      "RNN, rep: 0, epoch: 211, acc: 0.7466667890548706, Loss 0.6195099461078644\n",
      "RNN, rep: 0, epoch: 212, acc: 0.7666667103767395, Loss 0.6141598628461361\n",
      "RNN, rep: 0, epoch: 213, acc: 0.7233331799507141, Loss 0.6518410038948059\n",
      "RNN, rep: 0, epoch: 214, acc: 0.7233333587646484, Loss 0.6398367288708687\n",
      "RNN, rep: 0, epoch: 215, acc: 0.7133334279060364, Loss 0.6415695755183697\n",
      "RNN, rep: 0, epoch: 216, acc: 0.7266668677330017, Loss 0.6451920256018638\n",
      "RNN, rep: 0, epoch: 217, acc: 0.7066665887832642, Loss 0.6410786816477776\n",
      "RNN, rep: 0, epoch: 218, acc: 0.7133331894874573, Loss 0.6648783700168133\n",
      "RNN, rep: 0, epoch: 219, acc: 0.7433333396911621, Loss 0.6233371617645025\n",
      "RNN, rep: 0, epoch: 220, acc: 0.7300000190734863, Loss 0.6509422090649605\n",
      "RNN, rep: 0, epoch: 221, acc: 0.7033331990242004, Loss 0.6476799491047859\n",
      "RNN, rep: 0, epoch: 222, acc: 0.7133333086967468, Loss 0.6413473007082939\n",
      "RNN, rep: 0, epoch: 223, acc: 0.736666738986969, Loss 0.628777176067233\n",
      "RNN, rep: 0, epoch: 224, acc: 0.6700000166893005, Loss 0.6813911689817905\n",
      "RNN, rep: 0, epoch: 225, acc: 0.6866664290428162, Loss 0.6460793735831976\n",
      "RNN, rep: 0, epoch: 226, acc: 0.7266663908958435, Loss 0.6084020598232747\n",
      "RNN, rep: 0, epoch: 227, acc: 0.6466665863990784, Loss 0.6781596229970455\n",
      "RNN, rep: 0, epoch: 228, acc: 0.7300000190734863, Loss 0.6199053602665663\n",
      "RNN, rep: 0, epoch: 229, acc: 0.7166668176651001, Loss 0.6337271931767464\n",
      "RNN, rep: 0, epoch: 230, acc: 0.7300000190734863, Loss 0.6146437798440456\n",
      "RNN, rep: 0, epoch: 231, acc: 0.7533334493637085, Loss 0.5912001177668571\n",
      "RNN, rep: 0, epoch: 232, acc: 0.7466667890548706, Loss 0.6122800126671791\n",
      "RNN, rep: 0, epoch: 233, acc: 0.7533331513404846, Loss 0.6190808747708797\n",
      "RNN, rep: 0, epoch: 234, acc: 0.6833335757255554, Loss 0.7400823956727982\n",
      "RNN, rep: 0, epoch: 235, acc: 0.756666898727417, Loss 0.6421635004878045\n",
      "RNN, rep: 0, epoch: 236, acc: 0.7433333396911621, Loss 0.641578753143549\n",
      "RNN, rep: 0, epoch: 237, acc: 0.7566665410995483, Loss 0.5983710741996765\n",
      "RNN, rep: 0, epoch: 238, acc: 0.7333332896232605, Loss 0.5921002289652825\n",
      "RNN, rep: 0, epoch: 239, acc: 0.7833333611488342, Loss 0.628859059214592\n",
      "RNN, rep: 0, epoch: 240, acc: 0.7233334183692932, Loss 0.6203074692189694\n",
      "RNN, rep: 0, epoch: 241, acc: 0.75, Loss 0.6118585342913866\n",
      "RNN, rep: 0, epoch: 242, acc: 0.6899997591972351, Loss 0.6026635052263737\n",
      "RNN, rep: 0, epoch: 243, acc: 0.7633332014083862, Loss 0.6108397977054119\n",
      "RNN, rep: 0, epoch: 244, acc: 0.7133333086967468, Loss 0.6247828136384487\n",
      "RNN, rep: 0, epoch: 245, acc: 0.7466665506362915, Loss 0.6235020076483488\n",
      "RNN, rep: 0, epoch: 246, acc: 0.7699999809265137, Loss 0.5824568237364293\n",
      "RNN, rep: 0, epoch: 247, acc: 0.7400001287460327, Loss 0.601223818436265\n",
      "RNN, rep: 0, epoch: 248, acc: 0.7366665005683899, Loss 0.5908919004350901\n",
      "RNN, rep: 0, epoch: 249, acc: 0.6766666173934937, Loss 0.665490156263113\n",
      "RNN, rep: 0, epoch: 250, acc: 0.6900001764297485, Loss 0.6146468612551689\n",
      "RNN, rep: 0, epoch: 251, acc: 0.7799999117851257, Loss 0.5483642306178809\n",
      "RNN, rep: 0, epoch: 252, acc: 0.7800001502037048, Loss 0.5446735221147537\n",
      "RNN, rep: 0, epoch: 253, acc: 0.7966667413711548, Loss 0.5212410139292478\n",
      "RNN, rep: 0, epoch: 254, acc: 0.830000102519989, Loss 0.48566343188285827\n",
      "RNN, rep: 0, epoch: 255, acc: 0.8600000143051147, Loss 0.4480875235050917\n",
      "RNN, rep: 0, epoch: 256, acc: 0.7933335304260254, Loss 0.5802250128611922\n",
      "RNN, rep: 0, epoch: 257, acc: 0.7933332920074463, Loss 0.5212811649963259\n",
      "RNN, rep: 0, epoch: 258, acc: 0.8166666626930237, Loss 0.4830982961878181\n",
      "RNN, rep: 0, epoch: 259, acc: 0.873333215713501, Loss 0.47999882854521275\n",
      "RNN, rep: 0, epoch: 260, acc: 0.8399999737739563, Loss 0.4808631210029125\n",
      "RNN, rep: 0, epoch: 261, acc: 0.8199999332427979, Loss 0.5458271496742964\n",
      "RNN, rep: 0, epoch: 262, acc: 0.8433336019515991, Loss 0.4830043284222484\n",
      "RNN, rep: 0, epoch: 263, acc: 0.8500000238418579, Loss 0.4722685865685344\n",
      "RNN, rep: 0, epoch: 264, acc: 0.8866667151451111, Loss 0.3930224009975791\n",
      "RNN, rep: 0, epoch: 265, acc: 0.8633332252502441, Loss 0.40923502899706365\n",
      "RNN, rep: 0, epoch: 266, acc: 0.8833332657814026, Loss 0.3725771575421095\n",
      "RNN, rep: 0, epoch: 267, acc: 0.7966666221618652, Loss 0.6185789005272091\n",
      "RNN, rep: 0, epoch: 268, acc: 0.7133334279060364, Loss 0.8521371569857001\n",
      "RNN, rep: 0, epoch: 269, acc: 0.8066667318344116, Loss 0.6060869648680091\n",
      "RNN, rep: 0, epoch: 270, acc: 0.7399999499320984, Loss 0.7047294515371323\n",
      "RNN, rep: 0, epoch: 271, acc: 0.7333331108093262, Loss 0.6481504555419088\n",
      "RNN, rep: 0, epoch: 272, acc: 0.7766666412353516, Loss 0.5616242552921176\n",
      "RNN, rep: 0, epoch: 273, acc: 0.7266665697097778, Loss 0.5859820079244673\n",
      "RNN, rep: 0, epoch: 274, acc: 0.7633333802223206, Loss 0.5553454653453082\n",
      "RNN, rep: 0, epoch: 275, acc: 0.7666667103767395, Loss 0.533312409631908\n",
      "RNN, rep: 0, epoch: 276, acc: 0.7300000786781311, Loss 0.6322238992154599\n",
      "RNN, rep: 0, epoch: 277, acc: 0.6966665387153625, Loss 0.6798256543278695\n",
      "RNN, rep: 0, epoch: 278, acc: 0.6866667866706848, Loss 0.6534044884890318\n",
      "RNN, rep: 0, epoch: 279, acc: 0.6733332276344299, Loss 0.6374760150909424\n",
      "RNN, rep: 0, epoch: 280, acc: 0.710000216960907, Loss 0.6207878447324038\n",
      "RNN, rep: 0, epoch: 281, acc: 0.7133334875106812, Loss 0.623246042970568\n",
      "RNN, rep: 0, epoch: 282, acc: 0.7266666889190674, Loss 0.598079449813813\n",
      "RNN, rep: 0, epoch: 283, acc: 0.7866665720939636, Loss 0.5468715225532651\n",
      "RNN, rep: 0, epoch: 284, acc: 0.8000002503395081, Loss 0.5192862119525671\n",
      "RNN, rep: 0, epoch: 285, acc: 0.7499999403953552, Loss 0.6522260981053114\n",
      "RNN, rep: 0, epoch: 286, acc: 0.7133331894874573, Loss 0.7087124688923359\n",
      "RNN, rep: 0, epoch: 287, acc: 0.8033333420753479, Loss 0.5486859284341336\n",
      "RNN, rep: 0, epoch: 288, acc: 0.8466669321060181, Loss 0.506423939242959\n",
      "RNN, rep: 0, epoch: 289, acc: 0.8299999833106995, Loss 0.5135286454856396\n",
      "RNN, rep: 0, epoch: 290, acc: 0.8033332228660583, Loss 0.5539561288058757\n",
      "RNN, rep: 0, epoch: 291, acc: 0.9600000977516174, Loss 0.28816189281642435\n",
      "RNN, rep: 0, epoch: 292, acc: 0.8900001049041748, Loss 0.38601257763803004\n",
      "RNN, rep: 0, epoch: 293, acc: 0.8466666340827942, Loss 0.45291290126740935\n",
      "RNN, rep: 0, epoch: 294, acc: 0.9666666984558105, Loss 0.22779325660318137\n",
      "RNN, rep: 0, epoch: 295, acc: 0.8166667222976685, Loss 0.5219036039710044\n",
      "RNN, rep: 0, epoch: 296, acc: 0.8033333420753479, Loss 0.5269726791605354\n",
      "RNN, rep: 0, epoch: 297, acc: 0.8000001311302185, Loss 0.5816016208007931\n",
      "RNN, rep: 0, epoch: 298, acc: 0.8699999451637268, Loss 0.4444019664078951\n",
      "RNN, rep: 0, epoch: 299, acc: 0.93666672706604, Loss 0.2895320972986519\n",
      "RNN, rep: 0, epoch: 300, acc: 0.9433334469795227, Loss 0.24221248779445886\n",
      "RNN, rep: 0, epoch: 301, acc: 0.8833332657814026, Loss 0.3818425313103944\n",
      "RNN, rep: 0, epoch: 302, acc: 0.8633332252502441, Loss 0.3904164568148553\n",
      "RNN, rep: 0, epoch: 303, acc: 0.893333375453949, Loss 0.3311732736043632\n",
      "RNN, rep: 0, epoch: 304, acc: 0.8166667819023132, Loss 0.5678327409550548\n",
      "RNN, rep: 0, epoch: 305, acc: 0.84333336353302, Loss 0.5197097675129771\n",
      "RNN, rep: 0, epoch: 306, acc: 0.8733334541320801, Loss 0.4325326596200466\n",
      "RNN, rep: 0, epoch: 307, acc: 0.8800000548362732, Loss 0.40022757893428207\n",
      "RNN, rep: 0, epoch: 308, acc: 0.8899999856948853, Loss 0.38029761059209705\n",
      "RNN, rep: 0, epoch: 309, acc: 0.7966666221618652, Loss 0.5998782972153276\n",
      "RNN, rep: 0, epoch: 310, acc: 0.8899999260902405, Loss 0.3355064237769693\n",
      "RNN, rep: 0, epoch: 311, acc: 0.9633333683013916, Loss 0.21520093423314393\n",
      "RNN, rep: 0, epoch: 312, acc: 0.9833333492279053, Loss 0.13895969671197236\n",
      "RNN                  Rep: 0   Epoch: 1     Acc: 0.9833 Params: min_length: 20, max_length: 25, fill: 0, value_1: -1, value_2: 1 Time: 98.44 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.4733332693576813, Loss 1.0177692180871964\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.4833333194255829, Loss 1.0096275144815445\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.4566666781902313, Loss 1.0134083616733551\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5133333802223206, Loss 0.9910465157032013\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.5099999904632568, Loss 1.001922892332077\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.5533333420753479, Loss 0.9850766056776047\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.5833333730697632, Loss 0.9793291848897934\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.6300002932548523, Loss 0.9312928861379624\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.6733335256576538, Loss 0.7683969768881798\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.7233334183692932, Loss 0.6680562925338746\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.6366667151451111, Loss 0.8386645200848579\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.5566666722297668, Loss 0.9594489884376526\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.6033334136009216, Loss 0.8849894213676452\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.6666666269302368, Loss 0.6914789563417435\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.6799999475479126, Loss 0.6779618391394615\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.690000057220459, Loss 0.6757251161336899\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.6466668844223022, Loss 0.6821611848473549\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.6899999976158142, Loss 0.6695370423793793\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.5733333826065063, Loss 1.0919280707836152\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.5166667699813843, Loss 1.1904323452711105\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.5333333015441895, Loss 0.9986842572689056\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.5866667628288269, Loss 0.9541669577360153\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.6333332657814026, Loss 0.8912503427267074\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.6300000548362732, Loss 0.7936832123994827\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.6666666269302368, Loss 0.6967291575670242\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.6933335065841675, Loss 0.6706702756881714\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.7200000286102295, Loss 0.6636283439397812\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.699999988079071, Loss 0.6628710004687309\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.653333306312561, Loss 0.6734040397405624\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.7500000596046448, Loss 0.6615070217847824\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.6833335161209106, Loss 0.6440002018213272\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.7633334994316101, Loss 0.6366329711675643\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.8333331346511841, Loss 0.5783626541495324\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.8699999451637268, Loss 0.5163163548707962\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.8633330464363098, Loss 0.4606661415100098\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.856666624546051, Loss 0.40677227690815926\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.8999998569488525, Loss 0.3459209690988064\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.9300000667572021, Loss 0.3114156103879213\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.949999988079071, Loss 0.26927264742553236\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.9699999094009399, Loss 0.23148131251335144\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 1.0, Loss 0.20215161301195622\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 1     Acc: 1.0000 Params: min_length: 20, max_length: 25, fill: 0, value_1: -1, value_2: 1 Time: 15.71 sec\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 1, acc: 0.45333340764045715, Loss 1.0304889696836472\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 2, acc: 0.5000000596046448, Loss 1.0059999871253966\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 3, acc: 0.49333328008651733, Loss 1.0029047018289565\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 4, acc: 0.6033334136009216, Loss 0.9854972225427627\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 5, acc: 0.7666666507720947, Loss 0.7640978494286537\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 6, acc: 0.7966665625572205, Loss 0.5541656991839409\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 7, acc: 0.8399999141693115, Loss 0.41682754322886467\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 8, acc: 0.8366665840148926, Loss 0.3576918515563011\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 9, acc: 0.8499999046325684, Loss 0.3378401717543602\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 10, acc: 0.8766664862632751, Loss 0.32533731788396836\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 11, acc: 0.8899998664855957, Loss 0.3171055565774441\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 12, acc: 0.8699999451637268, Loss 0.2974622568488121\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 13, acc: 0.8933331966400146, Loss 0.2652875258028507\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 14, acc: 0.9166665077209473, Loss 0.25184035174548625\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 15, acc: 0.8933331966400146, Loss 0.3337469856441021\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 16, acc: 0.9399999976158142, Loss 0.21197145976126194\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 17, acc: 0.9133333563804626, Loss 0.3176253120973706\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 18, acc: 0.8533333539962769, Loss 0.459074938967824\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 19, acc: 0.9166667461395264, Loss 0.23453206531703472\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 20, acc: 0.9566667675971985, Loss 0.18557673215866088\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 21, acc: 0.979999840259552, Loss 0.15328043995425106\n",
      "NetRNNWithAttentionExpFirst Rep: 0   Epoch: 1     Acc: 0.9800 Params: min_length: 20, max_length: 25, fill: 0, value_1: -1, value_2: 1 Time: 8.61 sec\n",
      "LSTM, rep: 0, epoch: 1, acc: 0.4666668176651001, Loss 1.0637572795152663\n",
      "LSTM, rep: 0, epoch: 2, acc: 0.5100001692771912, Loss 1.0106382334232331\n",
      "LSTM, rep: 0, epoch: 3, acc: 0.4633333683013916, Loss 1.0207535195350648\n",
      "LSTM, rep: 0, epoch: 4, acc: 0.5099999308586121, Loss 1.0011065512895585\n",
      "LSTM, rep: 0, epoch: 5, acc: 0.5433333516120911, Loss 0.9917509412765503\n",
      "LSTM, rep: 0, epoch: 6, acc: 0.6299999952316284, Loss 0.9645587551593781\n",
      "LSTM, rep: 0, epoch: 7, acc: 0.6700000166893005, Loss 0.8979430669546127\n",
      "LSTM, rep: 0, epoch: 8, acc: 0.6433334946632385, Loss 0.8821914374828339\n",
      "LSTM, rep: 0, epoch: 9, acc: 0.73333340883255, Loss 0.7869774660468102\n",
      "LSTM, rep: 0, epoch: 10, acc: 0.6233334541320801, Loss 0.8606645163893699\n",
      "LSTM, rep: 0, epoch: 11, acc: 0.7066667675971985, Loss 0.7660714733600616\n",
      "LSTM, rep: 0, epoch: 12, acc: 0.6833333373069763, Loss 0.760837254524231\n",
      "LSTM, rep: 0, epoch: 13, acc: 0.6966665387153625, Loss 0.7345577335357666\n",
      "LSTM, rep: 0, epoch: 14, acc: 0.6666669249534607, Loss 0.7521396017074585\n",
      "LSTM, rep: 0, epoch: 15, acc: 0.6666668057441711, Loss 0.7171058231592178\n",
      "LSTM, rep: 0, epoch: 16, acc: 0.6899999976158142, Loss 0.6897914943099022\n",
      "LSTM, rep: 0, epoch: 17, acc: 0.7400001287460327, Loss 0.6798907873034478\n",
      "LSTM, rep: 0, epoch: 18, acc: 0.7733331918716431, Loss 0.6580241021513938\n",
      "LSTM, rep: 0, epoch: 19, acc: 0.6799999475479126, Loss 0.6970766118168831\n",
      "LSTM, rep: 0, epoch: 20, acc: 0.7400000691413879, Loss 0.6373449975252151\n",
      "LSTM, rep: 0, epoch: 21, acc: 0.783333420753479, Loss 0.5962160056829453\n",
      "LSTM, rep: 0, epoch: 22, acc: 0.7533332109451294, Loss 0.5578014355897903\n",
      "LSTM, rep: 0, epoch: 23, acc: 0.7799997925758362, Loss 0.5049840867519378\n",
      "LSTM, rep: 0, epoch: 24, acc: 0.7000002264976501, Loss 0.7352350553870202\n",
      "LSTM, rep: 0, epoch: 25, acc: 0.7000000476837158, Loss 0.605997143983841\n",
      "LSTM, rep: 0, epoch: 26, acc: 0.7833335399627686, Loss 0.5168069729208946\n",
      "LSTM, rep: 0, epoch: 27, acc: 0.8433330655097961, Loss 0.4772917807102203\n",
      "LSTM, rep: 0, epoch: 28, acc: 0.8166664838790894, Loss 0.43173901081085203\n",
      "LSTM, rep: 0, epoch: 29, acc: 0.7700001001358032, Loss 0.4223514145612717\n",
      "LSTM, rep: 0, epoch: 30, acc: 0.7833333015441895, Loss 0.42020104885101317\n",
      "LSTM, rep: 0, epoch: 31, acc: 0.8166667222976685, Loss 0.3898963749408722\n",
      "LSTM, rep: 0, epoch: 32, acc: 0.7799999713897705, Loss 0.4132007160782814\n",
      "LSTM, rep: 0, epoch: 33, acc: 0.8066665530204773, Loss 0.37240070223808286\n",
      "LSTM, rep: 0, epoch: 34, acc: 0.8333334922790527, Loss 0.35458017826080324\n",
      "LSTM, rep: 0, epoch: 35, acc: 0.8299999833106995, Loss 0.39090721279382706\n",
      "LSTM, rep: 0, epoch: 36, acc: 0.8766664266586304, Loss 0.34548259794712066\n",
      "LSTM, rep: 0, epoch: 37, acc: 0.8866667151451111, Loss 0.35459918424487114\n",
      "LSTM, rep: 0, epoch: 38, acc: 0.8799998760223389, Loss 0.35888502657413485\n",
      "LSTM, rep: 0, epoch: 39, acc: 0.8033333420753479, Loss 0.36061850786209104\n",
      "LSTM, rep: 0, epoch: 40, acc: 0.8666665554046631, Loss 0.332877167314291\n",
      "LSTM, rep: 0, epoch: 41, acc: 0.8199999928474426, Loss 0.36401025250554087\n",
      "LSTM, rep: 0, epoch: 42, acc: 0.8966665863990784, Loss 0.3310719358921051\n",
      "LSTM, rep: 0, epoch: 43, acc: 0.8233331441879272, Loss 0.5157217553257942\n",
      "LSTM, rep: 0, epoch: 44, acc: 0.8433330655097961, Loss 0.39942443057894705\n",
      "LSTM, rep: 0, epoch: 45, acc: 0.8533332347869873, Loss 0.33801125943660737\n",
      "LSTM, rep: 0, epoch: 46, acc: 0.8766665458679199, Loss 0.3227488324046135\n",
      "LSTM, rep: 0, epoch: 47, acc: 0.8299999833106995, Loss 0.33537198677659036\n",
      "LSTM, rep: 0, epoch: 48, acc: 0.9033330678939819, Loss 0.30424629464745523\n",
      "LSTM, rep: 0, epoch: 49, acc: 0.8833332061767578, Loss 0.31261935725808143\n",
      "LSTM, rep: 0, epoch: 50, acc: 0.9099998474121094, Loss 0.31582775101065635\n",
      "LSTM, rep: 0, epoch: 51, acc: 0.9166666269302368, Loss 0.30414568856358526\n",
      "LSTM, rep: 0, epoch: 52, acc: 0.9199998378753662, Loss 0.2971136975288391\n",
      "LSTM, rep: 0, epoch: 53, acc: 0.9133331179618835, Loss 0.3058460021018982\n",
      "LSTM, rep: 0, epoch: 54, acc: 0.9366666674613953, Loss 0.2782662086188793\n",
      "LSTM, rep: 0, epoch: 55, acc: 0.916666567325592, Loss 0.28256611302495005\n",
      "LSTM, rep: 0, epoch: 56, acc: 0.9166665077209473, Loss 0.29052319794893267\n",
      "LSTM, rep: 0, epoch: 57, acc: 0.9133330583572388, Loss 0.28684869796037676\n",
      "LSTM, rep: 0, epoch: 58, acc: 0.9099998474121094, Loss 0.2773023772239685\n",
      "LSTM, rep: 0, epoch: 59, acc: 0.916666567325592, Loss 0.26506232261657714\n",
      "LSTM, rep: 0, epoch: 60, acc: 0.9033331274986267, Loss 0.2774686089158058\n",
      "LSTM, rep: 0, epoch: 61, acc: 0.9099997878074646, Loss 0.2668283502012491\n",
      "LSTM, rep: 0, epoch: 62, acc: 0.9333332180976868, Loss 0.24424982644617557\n",
      "LSTM, rep: 0, epoch: 63, acc: 0.8899998664855957, Loss 0.28116174571216107\n",
      "LSTM, rep: 0, epoch: 64, acc: 0.903333306312561, Loss 0.26060124948620794\n",
      "LSTM, rep: 0, epoch: 65, acc: 0.8999998569488525, Loss 0.27042544074356556\n",
      "LSTM, rep: 0, epoch: 66, acc: 0.916666567325592, Loss 0.23704703986644746\n",
      "LSTM, rep: 0, epoch: 67, acc: 0.9199998378753662, Loss 0.22272922322154046\n",
      "LSTM, rep: 0, epoch: 68, acc: 0.9399999380111694, Loss 0.19183436274528504\n",
      "LSTM, rep: 0, epoch: 69, acc: 0.9199998378753662, Loss 0.21475858211517335\n",
      "LSTM, rep: 0, epoch: 70, acc: 0.916666567325592, Loss 0.2145212221518159\n",
      "LSTM, rep: 0, epoch: 71, acc: 0.9099999070167542, Loss 0.21422016736119986\n",
      "LSTM, rep: 0, epoch: 72, acc: 0.883333146572113, Loss 0.24345607202500105\n",
      "LSTM, rep: 0, epoch: 73, acc: 0.9366665482521057, Loss 0.1804318280518055\n",
      "LSTM, rep: 0, epoch: 74, acc: 0.9566666483879089, Loss 0.20361177638173103\n",
      "LSTM, rep: 0, epoch: 75, acc: 0.9333332180976868, Loss 0.1952473545074463\n",
      "LSTM, rep: 0, epoch: 76, acc: 0.9333333373069763, Loss 0.18864928089082242\n",
      "LSTM, rep: 0, epoch: 77, acc: 0.9633333086967468, Loss 0.15774424089118838\n",
      "LSTM, rep: 0, epoch: 78, acc: 0.9666666388511658, Loss 0.15296903962269426\n",
      "LSTM, rep: 0, epoch: 79, acc: 0.9499998688697815, Loss 0.16248837618157266\n",
      "LSTM, rep: 0, epoch: 80, acc: 0.9633333683013916, Loss 0.14694148138165475\n",
      "LSTM, rep: 0, epoch: 81, acc: 0.9566666483879089, Loss 0.1408499074354768\n",
      "LSTM, rep: 0, epoch: 82, acc: 0.9533332586288452, Loss 0.15095851281657816\n",
      "LSTM, rep: 0, epoch: 83, acc: 0.9599999189376831, Loss 0.12573750397190453\n",
      "LSTM, rep: 0, epoch: 84, acc: 0.9800000190734863, Loss 0.1098932590149343\n",
      "LSTM                 Rep: 0   Epoch: 1     Acc: 0.9800 Params: min_length: 20, max_length: 25, fill: 0, value_1: -1, value_2: 1 Time: 22.13 sec\n",
      "NetLSTMWithAttention, rep: 0, epoch: 1, acc: 0.47999995946884155, Loss 1.059838152527809\n",
      "NetLSTMWithAttention, rep: 0, epoch: 2, acc: 0.503333568572998, Loss 1.0193429344892502\n",
      "NetLSTMWithAttention, rep: 0, epoch: 3, acc: 0.5266667008399963, Loss 0.997388288974762\n",
      "NetLSTMWithAttention, rep: 0, epoch: 4, acc: 0.4833333194255829, Loss 1.0114828830957412\n",
      "NetLSTMWithAttention, rep: 0, epoch: 5, acc: 0.5700000524520874, Loss 0.9889607059955597\n",
      "NetLSTMWithAttention, rep: 0, epoch: 6, acc: 0.5733335018157959, Loss 0.979823734164238\n",
      "NetLSTMWithAttention, rep: 0, epoch: 7, acc: 0.6366668939590454, Loss 0.9391623914241791\n",
      "NetLSTMWithAttention, rep: 0, epoch: 8, acc: 0.6833334565162659, Loss 0.7976447173953056\n",
      "NetLSTMWithAttention, rep: 0, epoch: 9, acc: 0.6500000953674316, Loss 0.7563795453310013\n",
      "NetLSTMWithAttention, rep: 0, epoch: 10, acc: 0.6700001358985901, Loss 0.7040084785223008\n",
      "NetLSTMWithAttention, rep: 0, epoch: 11, acc: 0.7733334302902222, Loss 0.6518894439935684\n",
      "NetLSTMWithAttention, rep: 0, epoch: 12, acc: 0.7266666889190674, Loss 0.6742004802823067\n",
      "NetLSTMWithAttention, rep: 0, epoch: 13, acc: 0.8333333730697632, Loss 0.6000940763950348\n",
      "NetLSTMWithAttention, rep: 0, epoch: 14, acc: 0.8199999332427979, Loss 0.5667641896009445\n",
      "NetLSTMWithAttention, rep: 0, epoch: 15, acc: 0.8266665935516357, Loss 0.48116316080093385\n",
      "NetLSTMWithAttention, rep: 0, epoch: 16, acc: 0.8199998736381531, Loss 0.44277278929948805\n",
      "NetLSTMWithAttention, rep: 0, epoch: 17, acc: 0.873333215713501, Loss 0.39930727452039716\n",
      "NetLSTMWithAttention, rep: 0, epoch: 18, acc: 0.7800000905990601, Loss 0.5309008169174194\n",
      "NetLSTMWithAttention, rep: 0, epoch: 19, acc: 0.7600000500679016, Loss 0.5474516256153583\n",
      "NetLSTMWithAttention, rep: 0, epoch: 20, acc: 0.8233333826065063, Loss 0.37591959595680235\n",
      "NetLSTMWithAttention, rep: 0, epoch: 21, acc: 0.8299999833106995, Loss 0.36025547981262207\n",
      "NetLSTMWithAttention, rep: 0, epoch: 22, acc: 0.8666664958000183, Loss 0.34964312851428986\n",
      "NetLSTMWithAttention, rep: 0, epoch: 23, acc: 0.8499999046325684, Loss 0.34506355598568916\n",
      "NetLSTMWithAttention, rep: 0, epoch: 24, acc: 0.8533329963684082, Loss 0.3432947874069214\n",
      "NetLSTMWithAttention, rep: 0, epoch: 25, acc: 0.8299998641014099, Loss 0.35016834452748297\n",
      "NetLSTMWithAttention, rep: 0, epoch: 26, acc: 0.8700000047683716, Loss 0.33509791135787964\n",
      "NetLSTMWithAttention, rep: 0, epoch: 27, acc: 0.8499997854232788, Loss 0.34221242398023605\n",
      "NetLSTMWithAttention, rep: 0, epoch: 28, acc: 0.8299998641014099, Loss 0.3425729548931122\n",
      "NetLSTMWithAttention, rep: 0, epoch: 29, acc: 0.8366665840148926, Loss 0.33795738726854324\n",
      "NetLSTMWithAttention, rep: 0, epoch: 30, acc: 0.8500000238418579, Loss 0.33397899448871615\n",
      "NetLSTMWithAttention, rep: 0, epoch: 31, acc: 0.8366663455963135, Loss 0.3390433417260647\n",
      "NetLSTMWithAttention, rep: 0, epoch: 32, acc: 0.8566665053367615, Loss 0.32446912467479705\n",
      "NetLSTMWithAttention, rep: 0, epoch: 33, acc: 0.8499999046325684, Loss 0.3274018320441246\n",
      "NetLSTMWithAttention, rep: 0, epoch: 34, acc: 0.8933332562446594, Loss 0.31574872985482216\n",
      "NetLSTMWithAttention, rep: 0, epoch: 35, acc: 0.7800000905990601, Loss 0.5242202241718769\n",
      "NetLSTMWithAttention, rep: 0, epoch: 36, acc: 0.893333375453949, Loss 0.34260745108127594\n",
      "NetLSTMWithAttention, rep: 0, epoch: 37, acc: 0.8666667342185974, Loss 0.3323675927519798\n",
      "NetLSTMWithAttention, rep: 0, epoch: 38, acc: 0.8999997973442078, Loss 0.3118330691754818\n",
      "NetLSTMWithAttention, rep: 0, epoch: 39, acc: 0.8933332562446594, Loss 0.2992093861103058\n",
      "NetLSTMWithAttention, rep: 0, epoch: 40, acc: 0.8566663861274719, Loss 0.3293513701856136\n",
      "NetLSTMWithAttention, rep: 0, epoch: 41, acc: 0.8566665053367615, Loss 0.31594034522771836\n",
      "NetLSTMWithAttention, rep: 0, epoch: 42, acc: 0.8999998569488525, Loss 0.2957423317432404\n",
      "NetLSTMWithAttention, rep: 0, epoch: 43, acc: 0.9466664791107178, Loss 0.27529061421751977\n",
      "NetLSTMWithAttention, rep: 0, epoch: 44, acc: 0.9233333468437195, Loss 0.3062868100404739\n",
      "NetLSTMWithAttention, rep: 0, epoch: 45, acc: 0.7433332204818726, Loss 0.6913640870153904\n",
      "NetLSTMWithAttention, rep: 0, epoch: 46, acc: 0.7666667103767395, Loss 0.6491099616885185\n",
      "NetLSTMWithAttention, rep: 0, epoch: 47, acc: 0.7766667008399963, Loss 0.614850603044033\n",
      "NetLSTMWithAttention, rep: 0, epoch: 48, acc: 0.7966667413711548, Loss 0.49712071403861047\n",
      "NetLSTMWithAttention, rep: 0, epoch: 49, acc: 0.8866667151451111, Loss 0.34859279453754427\n",
      "NetLSTMWithAttention, rep: 0, epoch: 50, acc: 0.9133332967758179, Loss 0.3102321098744869\n",
      "NetLSTMWithAttention, rep: 0, epoch: 51, acc: 0.6633333563804626, Loss 0.8736004111170769\n",
      "NetLSTMWithAttention, rep: 0, epoch: 52, acc: 0.8566665649414062, Loss 0.3585342639684677\n",
      "NetLSTMWithAttention, rep: 0, epoch: 53, acc: 0.8966665863990784, Loss 0.3040791521966457\n",
      "NetLSTMWithAttention, rep: 0, epoch: 54, acc: 0.9099999070167542, Loss 0.31122921869158743\n",
      "NetLSTMWithAttention, rep: 0, epoch: 55, acc: 0.9099998474121094, Loss 0.2936295311152935\n",
      "NetLSTMWithAttention, rep: 0, epoch: 56, acc: 0.919999897480011, Loss 0.30013719126582145\n",
      "NetLSTMWithAttention, rep: 0, epoch: 57, acc: 0.8699997663497925, Loss 0.31063181191682815\n",
      "NetLSTMWithAttention, rep: 0, epoch: 58, acc: 0.9100000262260437, Loss 0.28014476627111434\n",
      "NetLSTMWithAttention, rep: 0, epoch: 59, acc: 0.9166665077209473, Loss 0.26812790349125865\n",
      "NetLSTMWithAttention, rep: 0, epoch: 60, acc: 0.8799999952316284, Loss 0.321336008310318\n",
      "NetLSTMWithAttention, rep: 0, epoch: 61, acc: 0.9333333373069763, Loss 0.2683479516208172\n",
      "NetLSTMWithAttention, rep: 0, epoch: 62, acc: 0.9399999380111694, Loss 0.23826491110026837\n",
      "NetLSTMWithAttention, rep: 0, epoch: 63, acc: 0.8999999165534973, Loss 0.3033537085354328\n",
      "NetLSTMWithAttention, rep: 0, epoch: 64, acc: 0.8699999451637268, Loss 0.294645666629076\n",
      "NetLSTMWithAttention, rep: 0, epoch: 65, acc: 0.9300000667572021, Loss 0.26704340979456903\n",
      "NetLSTMWithAttention, rep: 0, epoch: 66, acc: 0.9233332276344299, Loss 0.25608396753668783\n",
      "NetLSTMWithAttention, rep: 0, epoch: 67, acc: 0.9399998188018799, Loss 0.2562520013004541\n",
      "NetLSTMWithAttention, rep: 0, epoch: 68, acc: 0.9333332777023315, Loss 0.21962924011051654\n",
      "NetLSTMWithAttention, rep: 0, epoch: 69, acc: 0.9733332991600037, Loss 0.17142206512391567\n",
      "NetLSTMWithAttention Rep: 0   Epoch: 1     Acc: 0.9733 Params: min_length: 20, max_length: 25, fill: 0, value_1: -1, value_2: 1 Time: 26.15 sec\n",
      "GRU, rep: 0, epoch: 1, acc: 0.5266665816307068, Loss 1.0149419170618057\n",
      "GRU, rep: 0, epoch: 2, acc: 0.5799997448921204, Loss 0.9763970333337784\n",
      "GRU, rep: 0, epoch: 3, acc: 0.5733333826065063, Loss 0.9770159924030304\n",
      "GRU, rep: 0, epoch: 4, acc: 0.493333101272583, Loss 1.0256388574838637\n",
      "GRU, rep: 0, epoch: 5, acc: 0.5133331418037415, Loss 1.0086815416812898\n",
      "GRU, rep: 0, epoch: 6, acc: 0.49333298206329346, Loss 1.0061111450195312\n",
      "GRU, rep: 0, epoch: 7, acc: 0.51666659116745, Loss 1.0014194196462631\n",
      "GRU, rep: 0, epoch: 8, acc: 0.5499999523162842, Loss 0.9880744165182114\n",
      "GRU, rep: 0, epoch: 9, acc: 0.5266665816307068, Loss 1.002655370235443\n",
      "GRU, rep: 0, epoch: 10, acc: 0.5266668200492859, Loss 0.9986241549253464\n",
      "GRU, rep: 0, epoch: 11, acc: 0.5433331727981567, Loss 0.9885812842845917\n",
      "GRU, rep: 0, epoch: 12, acc: 0.5100002884864807, Loss 0.9860381817817688\n",
      "GRU, rep: 0, epoch: 13, acc: 0.5666664838790894, Loss 0.9677499842643738\n",
      "GRU, rep: 0, epoch: 14, acc: 0.6266666650772095, Loss 0.9240816289186478\n",
      "GRU, rep: 0, epoch: 15, acc: 0.6866668462753296, Loss 0.7473441940546036\n",
      "GRU, rep: 0, epoch: 16, acc: 0.7000000476837158, Loss 0.7040747421979904\n",
      "GRU, rep: 0, epoch: 17, acc: 0.7366669178009033, Loss 0.6611702579259873\n",
      "GRU, rep: 0, epoch: 18, acc: 0.7133333086967468, Loss 0.6586166483163833\n",
      "GRU, rep: 0, epoch: 19, acc: 0.7733331322669983, Loss 0.6230549177527428\n",
      "GRU, rep: 0, epoch: 20, acc: 0.839999794960022, Loss 0.5200005593895912\n",
      "GRU, rep: 0, epoch: 21, acc: 0.8466665148735046, Loss 0.44370375871658324\n",
      "GRU, rep: 0, epoch: 22, acc: 0.8999996781349182, Loss 0.3772214104235172\n",
      "GRU, rep: 0, epoch: 23, acc: 0.8866665363311768, Loss 0.3496921142935753\n",
      "GRU, rep: 0, epoch: 24, acc: 0.8899998664855957, Loss 0.3335996489226818\n",
      "GRU, rep: 0, epoch: 25, acc: 0.9599998593330383, Loss 0.3040784452855587\n",
      "GRU, rep: 0, epoch: 26, acc: 0.9566665887832642, Loss 0.28396795004606246\n",
      "GRU, rep: 0, epoch: 27, acc: 0.9499999284744263, Loss 0.25880332723259925\n",
      "GRU, rep: 0, epoch: 28, acc: 0.9566666483879089, Loss 0.24756585106253623\n",
      "GRU, rep: 0, epoch: 29, acc: 0.9700000882148743, Loss 0.20815471053123474\n",
      "GRU                  Rep: 0   Epoch: 1     Acc: 0.9700 Params: min_length: 20, max_length: 25, fill: 0, value_1: -1, value_2: 1 Time: 17.45 sec\n",
      "NetGRUWithAttention, rep: 0, epoch: 1, acc: 0.4933333098888397, Loss 1.0054314255714416\n",
      "NetGRUWithAttention, rep: 0, epoch: 2, acc: 0.43333330750465393, Loss 1.012472363114357\n",
      "NetGRUWithAttention, rep: 0, epoch: 3, acc: 0.4766666293144226, Loss 1.005511708855629\n",
      "NetGRUWithAttention, rep: 0, epoch: 4, acc: 0.5900000929832458, Loss 0.9843483471870422\n",
      "NetGRUWithAttention, rep: 0, epoch: 5, acc: 0.5466665029525757, Loss 0.9938480710983276\n",
      "NetGRUWithAttention, rep: 0, epoch: 6, acc: 0.5233334302902222, Loss 0.9845547449588775\n",
      "NetGRUWithAttention, rep: 0, epoch: 7, acc: 0.6666666269302368, Loss 0.9197862309217453\n",
      "NetGRUWithAttention, rep: 0, epoch: 8, acc: 0.7166666984558105, Loss 0.7526198589801788\n",
      "NetGRUWithAttention, rep: 0, epoch: 9, acc: 0.7100000977516174, Loss 0.6745524907112121\n",
      "NetGRUWithAttention, rep: 0, epoch: 10, acc: 0.7266666889190674, Loss 0.6750159350037575\n",
      "NetGRUWithAttention, rep: 0, epoch: 11, acc: 0.7366667985916138, Loss 0.6540798422694206\n",
      "NetGRUWithAttention, rep: 0, epoch: 12, acc: 0.6866669654846191, Loss 0.628453514277935\n",
      "NetGRUWithAttention, rep: 0, epoch: 13, acc: 0.7566666603088379, Loss 0.5927393883466721\n",
      "NetGRUWithAttention, rep: 0, epoch: 14, acc: 0.7533334493637085, Loss 0.5834573239088059\n",
      "NetGRUWithAttention, rep: 0, epoch: 15, acc: 0.7966666221618652, Loss 0.6074387058615685\n",
      "NetGRUWithAttention, rep: 0, epoch: 16, acc: 0.7866665720939636, Loss 0.5305315275490284\n",
      "NetGRUWithAttention, rep: 0, epoch: 17, acc: 0.8333331942558289, Loss 0.4815062391757965\n",
      "NetGRUWithAttention, rep: 0, epoch: 18, acc: 0.893333375453949, Loss 0.4801729021966457\n",
      "NetGRUWithAttention, rep: 0, epoch: 19, acc: 0.9233332276344299, Loss 0.3877309287339449\n",
      "NetGRUWithAttention, rep: 0, epoch: 20, acc: 0.9066663980484009, Loss 0.37101298809051514\n",
      "NetGRUWithAttention, rep: 0, epoch: 21, acc: 0.9333332777023315, Loss 0.3248467268794775\n",
      "NetGRUWithAttention, rep: 0, epoch: 22, acc: 0.9499998688697815, Loss 0.31894954830408095\n",
      "NetGRUWithAttention, rep: 0, epoch: 23, acc: 0.9533332586288452, Loss 0.27350222066044805\n",
      "NetGRUWithAttention, rep: 0, epoch: 24, acc: 0.8566665649414062, Loss 0.5071468085795641\n",
      "NetGRUWithAttention, rep: 0, epoch: 25, acc: 0.7599999904632568, Loss 0.6725055246800185\n",
      "NetGRUWithAttention, rep: 0, epoch: 26, acc: 0.7433333396911621, Loss 0.6723003752529622\n",
      "NetGRUWithAttention, rep: 0, epoch: 27, acc: 0.846666693687439, Loss 0.49524622347205877\n",
      "NetGRUWithAttention, rep: 0, epoch: 28, acc: 0.9333332777023315, Loss 0.28415870159864426\n",
      "NetGRUWithAttention, rep: 0, epoch: 29, acc: 0.8466666340827942, Loss 0.5425877510756254\n",
      "NetGRUWithAttention, rep: 0, epoch: 30, acc: 0.8833332061767578, Loss 0.4681331234425306\n",
      "NetGRUWithAttention, rep: 0, epoch: 31, acc: 0.9099998474121094, Loss 0.3988329631090164\n",
      "NetGRUWithAttention, rep: 0, epoch: 32, acc: 0.9266665577888489, Loss 0.30200312156230213\n",
      "NetGRUWithAttention, rep: 0, epoch: 33, acc: 0.9466666579246521, Loss 0.20212010115385057\n",
      "NetGRUWithAttention, rep: 0, epoch: 34, acc: 0.9499999284744263, Loss 0.17530262511223554\n",
      "NetGRUWithAttention, rep: 0, epoch: 35, acc: 0.9399999380111694, Loss 0.18541480880230665\n",
      "NetGRUWithAttention, rep: 0, epoch: 36, acc: 0.9499999284744263, Loss 0.1668511362373829\n",
      "NetGRUWithAttention, rep: 0, epoch: 37, acc: 0.9566664695739746, Loss 0.14316265223547817\n",
      "NetGRUWithAttention, rep: 0, epoch: 38, acc: 0.9666666388511658, Loss 0.11788465349003673\n",
      "NetGRUWithAttention, rep: 0, epoch: 39, acc: 0.9566667079925537, Loss 0.14309587551280856\n",
      "NetGRUWithAttention, rep: 0, epoch: 40, acc: 0.9633333086967468, Loss 0.12154671331867575\n",
      "NetGRUWithAttention, rep: 0, epoch: 41, acc: 0.9466667175292969, Loss 0.13460812913253903\n",
      "NetGRUWithAttention, rep: 0, epoch: 42, acc: 0.9533332586288452, Loss 0.11482934463769197\n",
      "NetGRUWithAttention, rep: 0, epoch: 43, acc: 0.9633331894874573, Loss 0.09562245754525066\n",
      "NetGRUWithAttention, rep: 0, epoch: 44, acc: 0.9666666984558105, Loss 0.09611945508979261\n",
      "NetGRUWithAttention, rep: 0, epoch: 45, acc: 0.9933332800865173, Loss 0.0927932277135551\n",
      "NetGRUWithAttention  Rep: 0   Epoch: 1     Acc: 0.9933 Params: min_length: 20, max_length: 25, fill: 0, value_1: -1, value_2: 1 Time: 29.80 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.46333304047584534, Loss 1.0479192399978638\n",
      "RNN, rep: 0, epoch: 2, acc: 0.5, Loss 1.0034494560956955\n",
      "RNN, rep: 0, epoch: 3, acc: 0.5000004172325134, Loss 0.9989190655946731\n",
      "RNN, rep: 0, epoch: 4, acc: 0.5233336091041565, Loss 1.0047341692447662\n",
      "RNN, rep: 0, epoch: 5, acc: 0.4966663718223572, Loss 1.0061812102794647\n",
      "RNN, rep: 0, epoch: 6, acc: 0.5, Loss 1.01361470580101\n",
      "RNN, rep: 0, epoch: 7, acc: 0.4433334469795227, Loss 1.018649132847786\n",
      "RNN, rep: 0, epoch: 8, acc: 0.4733333885669708, Loss 1.00524662733078\n",
      "RNN, rep: 0, epoch: 9, acc: 0.49000006914138794, Loss 1.0036357259750366\n",
      "RNN, rep: 0, epoch: 10, acc: 0.4533330798149109, Loss 1.012851213812828\n",
      "RNN, rep: 0, epoch: 11, acc: 0.5099998116493225, Loss 1.005985392332077\n",
      "RNN, rep: 0, epoch: 12, acc: 0.4666668176651001, Loss 1.0091964817047119\n",
      "RNN, rep: 0, epoch: 13, acc: 0.5033334493637085, Loss 1.0014064085483552\n",
      "RNN, rep: 0, epoch: 14, acc: 0.5033332109451294, Loss 1.0013321560621262\n",
      "RNN, rep: 0, epoch: 15, acc: 0.47333335876464844, Loss 1.0079488968849182\n",
      "RNN, rep: 0, epoch: 16, acc: 0.5199998617172241, Loss 1.0042899668216705\n",
      "RNN, rep: 0, epoch: 17, acc: 0.5533335208892822, Loss 0.983243755698204\n",
      "RNN, rep: 0, epoch: 18, acc: 0.5199996829032898, Loss 1.0023232984542847\n",
      "RNN, rep: 0, epoch: 19, acc: 0.5099999904632568, Loss 1.004997916817665\n",
      "RNN, rep: 0, epoch: 20, acc: 0.4900001883506775, Loss 1.0077252906560898\n",
      "RNN, rep: 0, epoch: 21, acc: 0.4699999988079071, Loss 1.010376779437065\n",
      "RNN, rep: 0, epoch: 22, acc: 0.479999840259552, Loss 1.002248083949089\n",
      "RNN, rep: 0, epoch: 23, acc: 0.5066664218902588, Loss 0.9877917176485062\n",
      "RNN, rep: 0, epoch: 24, acc: 0.5533335208892822, Loss 0.993849630355835\n",
      "RNN, rep: 0, epoch: 25, acc: 0.5400002598762512, Loss 0.9957626271247864\n",
      "RNN, rep: 0, epoch: 26, acc: 0.5266668200492859, Loss 0.9988862371444702\n",
      "RNN, rep: 0, epoch: 27, acc: 0.5266663432121277, Loss 0.9910094386339188\n",
      "RNN, rep: 0, epoch: 28, acc: 0.5099997520446777, Loss 1.0082495146989823\n",
      "RNN, rep: 0, epoch: 29, acc: 0.5133333802223206, Loss 1.000276969075203\n",
      "RNN, rep: 0, epoch: 30, acc: 0.5200003981590271, Loss 1.0022410702705384\n",
      "RNN, rep: 0, epoch: 31, acc: 0.5366666913032532, Loss 0.9994800251722336\n",
      "RNN, rep: 0, epoch: 32, acc: 0.5100000500679016, Loss 0.9873960703611374\n",
      "RNN, rep: 0, epoch: 33, acc: 0.48000019788742065, Loss 1.0168687885999679\n",
      "RNN, rep: 0, epoch: 34, acc: 0.4866665303707123, Loss 1.0027778178453446\n",
      "RNN, rep: 0, epoch: 35, acc: 0.5266666412353516, Loss 0.9985711127519608\n",
      "RNN, rep: 0, epoch: 36, acc: 0.48333343863487244, Loss 0.9942069113254547\n",
      "RNN, rep: 0, epoch: 37, acc: 0.543333113193512, Loss 0.9899648922681809\n",
      "RNN, rep: 0, epoch: 38, acc: 0.4833333194255829, Loss 1.0057914692163468\n",
      "RNN, rep: 0, epoch: 39, acc: 0.47666671872138977, Loss 1.008573939204216\n",
      "RNN, rep: 0, epoch: 40, acc: 0.49999991059303284, Loss 0.9994219130277634\n",
      "RNN, rep: 0, epoch: 41, acc: 0.4966663122177124, Loss 1.0079122579097748\n",
      "RNN, rep: 0, epoch: 42, acc: 0.49666666984558105, Loss 1.0010049188137053\n",
      "RNN, rep: 0, epoch: 43, acc: 0.5099997520446777, Loss 1.0007181721925735\n",
      "RNN, rep: 0, epoch: 44, acc: 0.5400002598762512, Loss 0.9987909829616547\n",
      "RNN, rep: 0, epoch: 45, acc: 0.5400002598762512, Loss 0.9908828634023666\n",
      "RNN, rep: 0, epoch: 46, acc: 0.5199997425079346, Loss 1.00549733877182\n",
      "RNN, rep: 0, epoch: 47, acc: 0.5066667795181274, Loss 0.9988673424720764\n",
      "RNN, rep: 0, epoch: 48, acc: 0.5100001692771912, Loss 1.0051415395736694\n",
      "RNN, rep: 0, epoch: 49, acc: 0.5100000500679016, Loss 0.9978869330883026\n",
      "RNN, rep: 0, epoch: 50, acc: 0.4833333492279053, Loss 1.0061769825220108\n",
      "RNN, rep: 0, epoch: 51, acc: 0.5233334898948669, Loss 0.9890195733308792\n",
      "RNN, rep: 0, epoch: 52, acc: 0.503333330154419, Loss 0.9951616698503494\n",
      "RNN, rep: 0, epoch: 53, acc: 0.48333340883255005, Loss 1.0059267264604568\n",
      "RNN, rep: 0, epoch: 54, acc: 0.5166665315628052, Loss 1.0010055249929428\n",
      "RNN, rep: 0, epoch: 55, acc: 0.5366668105125427, Loss 0.9890996223688125\n",
      "RNN, rep: 0, epoch: 56, acc: 0.5199997425079346, Loss 0.9931811743974686\n",
      "RNN, rep: 0, epoch: 57, acc: 0.5233333706855774, Loss 0.9897525376081466\n",
      "RNN, rep: 0, epoch: 58, acc: 0.5066667199134827, Loss 0.9961774563789367\n",
      "RNN, rep: 0, epoch: 59, acc: 0.526666522026062, Loss 0.9863487648963928\n",
      "RNN, rep: 0, epoch: 60, acc: 0.5266667008399963, Loss 1.0084055328369141\n",
      "RNN, rep: 0, epoch: 61, acc: 0.5366666316986084, Loss 0.9871776229143143\n",
      "RNN, rep: 0, epoch: 62, acc: 0.5499998331069946, Loss 0.9806471502780915\n",
      "RNN, rep: 0, epoch: 63, acc: 0.5533332228660583, Loss 0.9775686043500901\n",
      "RNN, rep: 0, epoch: 64, acc: 0.5499999523162842, Loss 0.9563786953687667\n",
      "RNN, rep: 0, epoch: 65, acc: 0.5333335995674133, Loss 0.9884116137027741\n",
      "RNN, rep: 0, epoch: 66, acc: 0.5733332633972168, Loss 0.9700264954566955\n",
      "RNN, rep: 0, epoch: 67, acc: 0.6533334851264954, Loss 0.8726577907800674\n",
      "RNN, rep: 0, epoch: 68, acc: 0.5566668510437012, Loss 1.0018078935146333\n",
      "RNN, rep: 0, epoch: 69, acc: 0.5100000500679016, Loss 1.01973342359066\n",
      "RNN, rep: 0, epoch: 70, acc: 0.550000011920929, Loss 0.9802351975440979\n",
      "RNN, rep: 0, epoch: 71, acc: 0.5066667199134827, Loss 0.9997839176654816\n",
      "RNN, rep: 0, epoch: 72, acc: 0.5666667222976685, Loss 0.9843512046337127\n",
      "RNN, rep: 0, epoch: 73, acc: 0.5666666030883789, Loss 0.9681503748893738\n",
      "RNN, rep: 0, epoch: 74, acc: 0.5066666603088379, Loss 0.9959652930498123\n",
      "RNN, rep: 0, epoch: 75, acc: 0.5466668009757996, Loss 0.9825567090511322\n",
      "RNN, rep: 0, epoch: 76, acc: 0.5333335399627686, Loss 0.9856544148921966\n",
      "RNN, rep: 0, epoch: 77, acc: 0.5666665434837341, Loss 0.9859902769327163\n",
      "RNN, rep: 0, epoch: 78, acc: 0.5666667222976685, Loss 0.9662072145938874\n",
      "RNN, rep: 0, epoch: 79, acc: 0.5666670203208923, Loss 0.9762152642011642\n",
      "RNN, rep: 0, epoch: 80, acc: 0.5533334016799927, Loss 0.9728559988737107\n",
      "RNN, rep: 0, epoch: 81, acc: 0.47999972105026245, Loss 1.0079903656244278\n",
      "RNN, rep: 0, epoch: 82, acc: 0.5166669487953186, Loss 0.984205179810524\n",
      "RNN, rep: 0, epoch: 83, acc: 0.5466666221618652, Loss 0.9630973863601685\n",
      "RNN, rep: 0, epoch: 84, acc: 0.5800001621246338, Loss 0.9371279403567314\n",
      "RNN, rep: 0, epoch: 85, acc: 0.5466665029525757, Loss 0.968752373456955\n",
      "RNN, rep: 0, epoch: 86, acc: 0.5700002312660217, Loss 0.9482214713096618\n",
      "RNN, rep: 0, epoch: 87, acc: 0.5866667032241821, Loss 0.9148821723461151\n",
      "RNN, rep: 0, epoch: 88, acc: 0.5666666626930237, Loss 0.9515837705135346\n",
      "RNN, rep: 0, epoch: 89, acc: 0.4699999690055847, Loss 1.0198162671923638\n",
      "RNN, rep: 0, epoch: 90, acc: 0.5366665720939636, Loss 0.9658068469166756\n",
      "RNN, rep: 0, epoch: 91, acc: 0.5933334827423096, Loss 0.9210103189945221\n",
      "RNN, rep: 0, epoch: 92, acc: 0.6366667151451111, Loss 0.8134696280956268\n",
      "RNN, rep: 0, epoch: 93, acc: 0.6066666841506958, Loss 0.9207748702168465\n",
      "RNN, rep: 0, epoch: 94, acc: 0.6800001263618469, Loss 0.7121613311767578\n",
      "RNN, rep: 0, epoch: 95, acc: 0.6966667771339417, Loss 0.687511939406395\n",
      "RNN, rep: 0, epoch: 96, acc: 0.7033333778381348, Loss 0.6729690897464752\n",
      "RNN, rep: 0, epoch: 97, acc: 0.7233333587646484, Loss 0.668593085706234\n",
      "RNN, rep: 0, epoch: 98, acc: 0.7099999785423279, Loss 0.6609071710705757\n",
      "RNN, rep: 0, epoch: 99, acc: 0.6433332562446594, Loss 0.6942820546030998\n",
      "RNN, rep: 0, epoch: 100, acc: 0.6500001549720764, Loss 0.6760218435525894\n",
      "RNN, rep: 0, epoch: 101, acc: 0.7133333086967468, Loss 0.6582118448615074\n",
      "RNN, rep: 0, epoch: 102, acc: 0.7000002264976501, Loss 0.6642648768424988\n",
      "RNN, rep: 0, epoch: 103, acc: 0.736666738986969, Loss 0.6377342227101326\n",
      "RNN, rep: 0, epoch: 104, acc: 0.6866666674613953, Loss 0.6925212028622627\n",
      "RNN, rep: 0, epoch: 105, acc: 0.7099999785423279, Loss 0.6412750110030174\n",
      "RNN, rep: 0, epoch: 106, acc: 0.7433333396911621, Loss 0.63084204018116\n",
      "RNN, rep: 0, epoch: 107, acc: 0.7633333802223206, Loss 0.6576619166135788\n",
      "RNN, rep: 0, epoch: 108, acc: 0.5099999904632568, Loss 1.1190499240159988\n",
      "RNN, rep: 0, epoch: 109, acc: 0.6133336424827576, Loss 0.9034235215187073\n",
      "RNN, rep: 0, epoch: 110, acc: 0.6633334159851074, Loss 0.8705154472589492\n",
      "RNN, rep: 0, epoch: 111, acc: 0.6899999976158142, Loss 0.67157968968153\n",
      "RNN, rep: 0, epoch: 112, acc: 0.7000000476837158, Loss 0.6654286068677903\n",
      "RNN, rep: 0, epoch: 113, acc: 0.7300000190734863, Loss 0.625361405313015\n",
      "RNN, rep: 0, epoch: 114, acc: 0.7166668772697449, Loss 0.6559187909960746\n",
      "RNN, rep: 0, epoch: 115, acc: 0.7733331918716431, Loss 0.5932851788401604\n",
      "RNN, rep: 0, epoch: 116, acc: 0.7766664028167725, Loss 0.6051572883129119\n",
      "RNN, rep: 0, epoch: 117, acc: 0.7833331227302551, Loss 0.5649933362007141\n",
      "RNN, rep: 0, epoch: 118, acc: 0.6400001049041748, Loss 0.952194412946701\n",
      "RNN, rep: 0, epoch: 119, acc: 0.5000001192092896, Loss 1.1005536377429963\n",
      "RNN, rep: 0, epoch: 120, acc: 0.4766666293144226, Loss 1.0034869754314422\n",
      "RNN, rep: 0, epoch: 121, acc: 0.5400000214576721, Loss 0.974518294930458\n",
      "RNN, rep: 0, epoch: 122, acc: 0.4833332896232605, Loss 0.9913392913341522\n",
      "RNN, rep: 0, epoch: 123, acc: 0.5166666507720947, Loss 0.9879423326253891\n",
      "RNN, rep: 0, epoch: 124, acc: 0.570000171661377, Loss 0.890767621397972\n",
      "RNN, rep: 0, epoch: 125, acc: 0.7433332800865173, Loss 0.6411769050359726\n",
      "RNN, rep: 0, epoch: 126, acc: 0.7033334374427795, Loss 0.6525651371479034\n",
      "RNN, rep: 0, epoch: 127, acc: 0.7566665410995483, Loss 0.5755960223078728\n",
      "RNN, rep: 0, epoch: 128, acc: 0.7566667199134827, Loss 0.6177277275919915\n",
      "RNN, rep: 0, epoch: 129, acc: 0.7533332705497742, Loss 0.5949301469326019\n",
      "RNN, rep: 0, epoch: 130, acc: 0.75, Loss 0.6333648148179054\n",
      "RNN, rep: 0, epoch: 131, acc: 0.7733334898948669, Loss 0.5947726818919182\n",
      "RNN, rep: 0, epoch: 132, acc: 0.7466668486595154, Loss 0.6005109390616417\n",
      "RNN, rep: 0, epoch: 133, acc: 0.763333261013031, Loss 0.5884124544262886\n",
      "RNN, rep: 0, epoch: 134, acc: 0.7433334589004517, Loss 0.6152901795506477\n",
      "RNN, rep: 0, epoch: 135, acc: 0.7299998998641968, Loss 0.5899682831764221\n",
      "RNN, rep: 0, epoch: 136, acc: 0.7966667413711548, Loss 0.5274791803956032\n",
      "RNN, rep: 0, epoch: 137, acc: 0.8299999237060547, Loss 0.40264458283782006\n",
      "RNN, rep: 0, epoch: 138, acc: 0.8633332252502441, Loss 0.3716786162555218\n",
      "RNN, rep: 0, epoch: 139, acc: 0.873333215713501, Loss 0.333787332624197\n",
      "RNN, rep: 0, epoch: 140, acc: 0.9300000071525574, Loss 0.322112999856472\n",
      "RNN, rep: 0, epoch: 141, acc: 0.9066666960716248, Loss 0.33737373769283296\n",
      "RNN, rep: 0, epoch: 142, acc: 0.9533333778381348, Loss 0.2858158531785011\n",
      "RNN, rep: 0, epoch: 143, acc: 0.986666738986969, Loss 0.27009072780609134\n",
      "RNN                  Rep: 0   Epoch: 1     Acc: 0.9867 Params: min_length: 40, max_length: 40, fill: 0, value_1: -1, value_2: 1 Time: 63.59 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.5066666007041931, Loss 1.0040158212184906\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.48666656017303467, Loss 1.0231333190202714\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5000000596046448, Loss 1.0026811784505845\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5199999809265137, Loss 0.9953275567293167\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.5866666436195374, Loss 0.9800300741195679\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.6400002837181091, Loss 0.9353565621376038\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.6933333873748779, Loss 0.7727128112316132\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.6433334946632385, Loss 0.7303509256243705\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.6700000762939453, Loss 0.7152563402056694\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.6466667056083679, Loss 0.6935751363635063\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.6666668653488159, Loss 0.6624260538816452\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.7100000977516174, Loss 0.6364707991480827\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.7399999499320984, Loss 0.6499689435958862\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.7866666913032532, Loss 0.5851977580785751\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.8399999737739563, Loss 0.46990874528884885\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.8499999046325684, Loss 0.5018341436982154\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.8199999332427979, Loss 0.41435366719961164\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.8266665935516357, Loss 0.38102977246046066\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.8299998641014099, Loss 0.3818474218249321\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.8199999332427979, Loss 0.35510032653808593\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.8499997854232788, Loss 0.34460370019078257\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.7166668176651001, Loss 0.9148709118366242\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.6566668152809143, Loss 1.0810646998882294\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.646666944026947, Loss 0.9862849777936935\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.676666796207428, Loss 0.954003423154354\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.690000057220459, Loss 0.8778843986988067\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.6500000953674316, Loss 0.9052420854568481\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.6466667056083679, Loss 0.9276226913928985\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.6700002551078796, Loss 0.7852602159976959\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.6700000166893005, Loss 0.7829892379045487\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.720000147819519, Loss 0.7612064844369888\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.6566666960716248, Loss 0.7852802687883377\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.6333335041999817, Loss 0.7838957390189171\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.6300000548362732, Loss 0.775801892876625\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.6333334445953369, Loss 0.8565612018108368\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.5433335304260254, Loss 0.9764570069313049\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.6066668033599854, Loss 0.9332978355884552\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.6733334064483643, Loss 0.9240315419435501\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.59333336353302, Loss 0.9144935083389282\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.6600000858306885, Loss 0.8947219610214233\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.6666665077209473, Loss 0.8833964776992798\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.6833333373069763, Loss 0.8439805179834365\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.6666669249534607, Loss 0.8325456309318543\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.73333340883255, Loss 0.7631808620691299\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.6733333468437195, Loss 0.8006930774450303\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.6566668152809143, Loss 0.7776491877436638\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.660000205039978, Loss 0.7634549441933632\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.6733334064483643, Loss 0.7294104346632957\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.7699999809265137, Loss 0.673975969851017\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.7066667675971985, Loss 0.7144862899184227\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.6800000071525574, Loss 0.7075497388839722\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.699999988079071, Loss 0.6857906240224838\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.699999988079071, Loss 0.6385611191391944\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.6333334445953369, Loss 0.6636227351427079\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.6366666555404663, Loss 0.7037888988852501\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.7199999094009399, Loss 0.6491145583987236\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.6700000762939453, Loss 0.6376694241166114\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.6766667366027832, Loss 0.6479564177989959\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.6766667366027832, Loss 0.6470188695192337\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.6833335161209106, Loss 0.6411168622970581\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.6533334255218506, Loss 0.6752253609895706\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.643333375453949, Loss 0.6602649024128914\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.6666668057441711, Loss 0.6183219033479691\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.6766667366027832, Loss 0.647324680685997\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.6666667461395264, Loss 0.6246417742967606\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.6166666150093079, Loss 0.6690912052989006\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.6933335661888123, Loss 0.5904976093769073\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.6900002360343933, Loss 0.5935693097114563\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.676666796207428, Loss 0.6035863545536995\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.7766667008399963, Loss 0.5837915387749671\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.7733333706855774, Loss 0.5638506361842155\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.7799999713897705, Loss 0.5075385367870331\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.7233333587646484, Loss 0.606455902159214\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.753333330154419, Loss 0.5606231439113617\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.736666738986969, Loss 0.5873019388318061\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.7233335971832275, Loss 0.5608750972151756\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.7933332920074463, Loss 0.5353558263182641\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.7433332800865173, Loss 0.5485118654370308\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.753333330154419, Loss 0.5622028642892838\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.7300001382827759, Loss 0.5538415944576264\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.7533335089683533, Loss 0.5090059632062912\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.7966667413711548, Loss 0.5223726958036423\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.8399999141693115, Loss 0.4643192654848099\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.8500000238418579, Loss 0.3734390406310558\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.856666624546051, Loss 0.3497121177613735\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.7900000214576721, Loss 0.5279627622663975\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.5466666221618652, Loss 1.3055913218855857\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.5466667413711548, Loss 1.2434065932035445\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.6233334541320801, Loss 1.1729924336075783\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.5366668105125427, Loss 1.297532596886158\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.5433335304260254, Loss 1.010813680589199\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.8299996852874756, Loss 0.531077711880207\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.8166667222976685, Loss 0.48466413497924804\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.8266666531562805, Loss 0.4569658076763153\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.8533334136009216, Loss 0.4111659696698189\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.8233332633972168, Loss 0.3719256818294525\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.8466665744781494, Loss 0.35664098292589186\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.8333331942558289, Loss 0.3468747413158417\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.8233332633972168, Loss 0.3485472822189331\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.9066663980484009, Loss 0.33678015649318693\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.8333331942558289, Loss 0.3409651863574982\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.8433331847190857, Loss 0.33996865659952163\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.8933331370353699, Loss 0.3375659701228142\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.8899997472763062, Loss 0.32002220302820206\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.8199999332427979, Loss 0.49179899111390113\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.7566666603088379, Loss 0.6291408431529999\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.853333055973053, Loss 0.3889593318104744\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.836666464805603, Loss 0.3313517270982265\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.8333332538604736, Loss 0.33364197909832\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.9066665768623352, Loss 0.32661877349019053\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.8599998354911804, Loss 0.3219904202222824\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.8433331251144409, Loss 0.32295344278216365\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.8666664361953735, Loss 0.3230458781123161\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.8966664671897888, Loss 0.30823414877057076\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.9200000166893005, Loss 0.2938487820327282\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.8899999260902405, Loss 0.2989738444983959\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.8699998259544373, Loss 0.30994668900966643\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.9166666269302368, Loss 0.2711186797916889\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.8466665148735046, Loss 0.42664565190672876\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.8866666555404663, Loss 0.2929160215705633\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.9200000762939453, Loss 0.2658795078098774\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.9099998474121094, Loss 0.259634889960289\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.5533334612846375, Loss 1.132919247969985\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.6299999952316284, Loss 0.8925351230800151\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.6200000643730164, Loss 0.8575383891165257\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.6566668152809143, Loss 0.8416847172379494\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.7166666984558105, Loss 0.6297728535532952\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.7366666197776794, Loss 0.6169906857609749\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.68666672706604, Loss 0.7346237941086292\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.73333340883255, Loss 0.5717894485592843\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.6733334064483643, Loss 0.7250554448366165\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.7633334398269653, Loss 0.5523664619028569\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.746666669845581, Loss 0.6414786957204341\n",
      "NetRNNWithAttention, rep: 0, epoch: 134, acc: 0.6400001645088196, Loss 0.8577901947498322\n",
      "NetRNNWithAttention, rep: 0, epoch: 135, acc: 0.6299998760223389, Loss 0.9011209863424301\n",
      "NetRNNWithAttention, rep: 0, epoch: 136, acc: 0.8233332633972168, Loss 0.48476456001400947\n",
      "NetRNNWithAttention, rep: 0, epoch: 137, acc: 0.763333261013031, Loss 0.4674392598867416\n",
      "NetRNNWithAttention, rep: 0, epoch: 138, acc: 0.8566665649414062, Loss 0.3989157471060753\n",
      "NetRNNWithAttention, rep: 0, epoch: 139, acc: 0.8066665530204773, Loss 0.40887667670845984\n",
      "NetRNNWithAttention, rep: 0, epoch: 140, acc: 0.8699999451637268, Loss 0.3906030842661858\n",
      "NetRNNWithAttention, rep: 0, epoch: 141, acc: 0.8833333849906921, Loss 0.3633290410041809\n",
      "NetRNNWithAttention, rep: 0, epoch: 142, acc: 0.8866665363311768, Loss 0.3654015189409256\n",
      "NetRNNWithAttention, rep: 0, epoch: 143, acc: 0.8866665363311768, Loss 0.3529561737179756\n",
      "NetRNNWithAttention, rep: 0, epoch: 144, acc: 0.8966666460037231, Loss 0.3436965054273605\n",
      "NetRNNWithAttention, rep: 0, epoch: 145, acc: 0.8799998760223389, Loss 0.34572118923068046\n",
      "NetRNNWithAttention, rep: 0, epoch: 146, acc: 0.8766664862632751, Loss 0.32289929687976837\n",
      "NetRNNWithAttention, rep: 0, epoch: 147, acc: 0.8933334350585938, Loss 0.31229324474930764\n",
      "NetRNNWithAttention, rep: 0, epoch: 148, acc: 0.8199998736381531, Loss 0.35730064317584037\n",
      "NetRNNWithAttention, rep: 0, epoch: 149, acc: 0.8366667032241821, Loss 0.3346439503133297\n",
      "NetRNNWithAttention, rep: 0, epoch: 150, acc: 0.8466665744781494, Loss 0.33598567977547644\n",
      "NetRNNWithAttention, rep: 0, epoch: 151, acc: 0.9166666269302368, Loss 0.3154952962696552\n",
      "NetRNNWithAttention, rep: 0, epoch: 152, acc: 0.8966664671897888, Loss 0.3246455334126949\n",
      "NetRNNWithAttention, rep: 0, epoch: 153, acc: 0.9099997878074646, Loss 0.3188064843416214\n",
      "NetRNNWithAttention, rep: 0, epoch: 154, acc: 0.8899999260902405, Loss 0.30923782467842104\n",
      "NetRNNWithAttention, rep: 0, epoch: 155, acc: 0.9300000071525574, Loss 0.3034991391003132\n",
      "NetRNNWithAttention, rep: 0, epoch: 156, acc: 0.9166666269302368, Loss 0.29565365687012674\n",
      "NetRNNWithAttention, rep: 0, epoch: 157, acc: 0.8733332753181458, Loss 0.3088169749081135\n",
      "NetRNNWithAttention, rep: 0, epoch: 158, acc: 0.8999998569488525, Loss 0.2873740014433861\n",
      "NetRNNWithAttention, rep: 0, epoch: 159, acc: 0.8933332562446594, Loss 0.30034130468964576\n",
      "NetRNNWithAttention, rep: 0, epoch: 160, acc: 0.896666407585144, Loss 0.2891602125763893\n",
      "NetRNNWithAttention, rep: 0, epoch: 161, acc: 0.8433331847190857, Loss 0.3155696405470371\n",
      "NetRNNWithAttention, rep: 0, epoch: 162, acc: 0.9133332967758179, Loss 0.2820200538635254\n",
      "NetRNNWithAttention, rep: 0, epoch: 163, acc: 0.9333332777023315, Loss 0.26256968200206754\n",
      "NetRNNWithAttention, rep: 0, epoch: 164, acc: 0.940000057220459, Loss 0.25776253268122673\n",
      "NetRNNWithAttention, rep: 0, epoch: 165, acc: 0.9233333468437195, Loss 0.2552042396366596\n",
      "NetRNNWithAttention, rep: 0, epoch: 166, acc: 0.8833332061767578, Loss 0.34865622632205484\n",
      "NetRNNWithAttention, rep: 0, epoch: 167, acc: 0.919999897480011, Loss 0.24222674913704395\n",
      "NetRNNWithAttention, rep: 0, epoch: 168, acc: 0.9166665077209473, Loss 0.2388507730513811\n",
      "NetRNNWithAttention, rep: 0, epoch: 169, acc: 0.9133332967758179, Loss 0.22747644297778608\n",
      "NetRNNWithAttention, rep: 0, epoch: 170, acc: 0.9233332276344299, Loss 0.217105892598629\n",
      "NetRNNWithAttention, rep: 0, epoch: 171, acc: 0.9366665482521057, Loss 0.19939799539744854\n",
      "NetRNNWithAttention, rep: 0, epoch: 172, acc: 0.9199997782707214, Loss 0.2046126541495323\n",
      "NetRNNWithAttention, rep: 0, epoch: 173, acc: 0.9299997687339783, Loss 0.20955610312521458\n",
      "NetRNNWithAttention, rep: 0, epoch: 174, acc: 0.9199998378753662, Loss 0.18241991486400366\n",
      "NetRNNWithAttention, rep: 0, epoch: 175, acc: 0.9466666579246521, Loss 0.17071837823837996\n",
      "NetRNNWithAttention, rep: 0, epoch: 176, acc: 0.9566667079925537, Loss 0.1610943290963769\n",
      "NetRNNWithAttention, rep: 0, epoch: 177, acc: 0.9566665887832642, Loss 0.13762474369257688\n",
      "NetRNNWithAttention, rep: 0, epoch: 178, acc: 0.9566666483879089, Loss 0.14544247161597013\n",
      "NetRNNWithAttention, rep: 0, epoch: 179, acc: 0.9766666293144226, Loss 0.1561491095274687\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 1     Acc: 0.9767 Params: min_length: 40, max_length: 40, fill: 0, value_1: -1, value_2: 1 Time: 91.38 sec\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 1, acc: 0.49999991059303284, Loss 1.0370372480154038\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 2, acc: 0.46333327889442444, Loss 1.0133853077888488\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 3, acc: 0.4599999487400055, Loss 1.0040058135986327\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 4, acc: 0.5099999904632568, Loss 1.0037263667583465\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 5, acc: 0.49666664004325867, Loss 1.002620025873184\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 6, acc: 0.5433334112167358, Loss 1.001632078886032\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 7, acc: 0.6500000953674316, Loss 0.8060089719295501\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 8, acc: 0.6633334159851074, Loss 0.7034384900331497\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 9, acc: 0.6733334064483643, Loss 0.6876044827699661\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 10, acc: 0.7033334970474243, Loss 0.6635791015625\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 11, acc: 0.7133333683013916, Loss 0.6468343338370324\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 12, acc: 0.7799999117851257, Loss 0.6381220123171807\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 13, acc: 0.7366666197776794, Loss 0.6282282078266144\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 14, acc: 0.8233332633972168, Loss 0.5117278853058815\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 15, acc: 0.8299998641014099, Loss 0.4209360286593437\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 16, acc: 0.8499999046325684, Loss 0.37555511206388476\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 17, acc: 0.8533332347869873, Loss 0.3483045481145382\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 18, acc: 0.8433331847190857, Loss 0.3379511457681656\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 19, acc: 0.8533332943916321, Loss 0.3391326054930687\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 20, acc: 0.8733333349227905, Loss 0.3227353921532631\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 21, acc: 0.8499998450279236, Loss 0.3617050936818123\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 22, acc: 0.9233331084251404, Loss 0.31724934965372087\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 23, acc: 0.9066665768623352, Loss 0.31566384389996527\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 24, acc: 0.9233330488204956, Loss 0.2836546967178583\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 25, acc: 0.9166663885116577, Loss 0.2857210810482502\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 26, acc: 0.9200000166893005, Loss 0.28086229622364045\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 27, acc: 0.9466664791107178, Loss 0.2521003817021847\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 28, acc: 0.9599999189376831, Loss 0.22271205618977546\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 29, acc: 0.9733332991600037, Loss 0.21489802680909634\n",
      "NetRNNWithAttentionExpFirst Rep: 0   Epoch: 1     Acc: 0.9733 Params: min_length: 40, max_length: 40, fill: 0, value_1: -1, value_2: 1 Time: 14.55 sec\n",
      "LSTM, rep: 0, epoch: 1, acc: 0.46000003814697266, Loss 1.116091766357422\n",
      "LSTM, rep: 0, epoch: 2, acc: 0.4966667592525482, Loss 1.0360008764266968\n",
      "LSTM, rep: 0, epoch: 3, acc: 0.5200001001358032, Loss 1.0139220798015594\n",
      "LSTM, rep: 0, epoch: 4, acc: 0.5033332705497742, Loss 1.0250825601816178\n",
      "LSTM, rep: 0, epoch: 5, acc: 0.5299999117851257, Loss 0.9958343148231507\n",
      "LSTM, rep: 0, epoch: 6, acc: 0.5066666603088379, Loss 1.0289030408859252\n",
      "LSTM, rep: 0, epoch: 7, acc: 0.529999852180481, Loss 1.0020825099945068\n",
      "LSTM, rep: 0, epoch: 8, acc: 0.5433330535888672, Loss 0.9969058561325074\n",
      "LSTM, rep: 0, epoch: 9, acc: 0.4866664409637451, Loss 1.0137003427743911\n",
      "LSTM, rep: 0, epoch: 10, acc: 0.5266667008399963, Loss 0.9992433220148087\n",
      "LSTM, rep: 0, epoch: 11, acc: 0.473333477973938, Loss 1.0122951030731202\n",
      "LSTM, rep: 0, epoch: 12, acc: 0.4633335471153259, Loss 1.008432509303093\n",
      "LSTM, rep: 0, epoch: 13, acc: 0.5366665720939636, Loss 0.9914083617925644\n",
      "LSTM, rep: 0, epoch: 14, acc: 0.49666664004325867, Loss 0.9978888893127441\n",
      "LSTM, rep: 0, epoch: 15, acc: 0.47333332896232605, Loss 1.0048802191019057\n",
      "LSTM, rep: 0, epoch: 16, acc: 0.5466665029525757, Loss 0.9967350518703461\n",
      "LSTM, rep: 0, epoch: 17, acc: 0.5100001096725464, Loss 1.0054294419288636\n",
      "LSTM, rep: 0, epoch: 18, acc: 0.5433333516120911, Loss 0.9888117361068726\n",
      "LSTM, rep: 0, epoch: 19, acc: 0.5233332514762878, Loss 0.9997224974632263\n",
      "LSTM, rep: 0, epoch: 20, acc: 0.539999783039093, Loss 0.9859377676248551\n",
      "LSTM, rep: 0, epoch: 21, acc: 0.5833331346511841, Loss 0.9846174800395966\n",
      "LSTM, rep: 0, epoch: 22, acc: 0.4566664397716522, Loss 1.0154629343748092\n",
      "LSTM, rep: 0, epoch: 23, acc: 0.5099998116493225, Loss 0.9994612097740173\n",
      "LSTM, rep: 0, epoch: 24, acc: 0.5466663837432861, Loss 0.9908558195829391\n",
      "LSTM, rep: 0, epoch: 25, acc: 0.5699998736381531, Loss 0.9803553473949432\n",
      "LSTM, rep: 0, epoch: 26, acc: 0.5533329844474792, Loss 0.9727127176523208\n",
      "LSTM, rep: 0, epoch: 27, acc: 0.5199999809265137, Loss 0.9938611602783203\n",
      "LSTM, rep: 0, epoch: 28, acc: 0.5500000715255737, Loss 0.995784302353859\n",
      "LSTM, rep: 0, epoch: 29, acc: 0.5533334612846375, Loss 0.9856073021888733\n",
      "LSTM, rep: 0, epoch: 30, acc: 0.5300002098083496, Loss 0.990436732172966\n",
      "LSTM, rep: 0, epoch: 31, acc: 0.5833334922790527, Loss 0.9759452396631241\n",
      "LSTM, rep: 0, epoch: 32, acc: 0.6000000238418579, Loss 0.9493440675735474\n",
      "LSTM, rep: 0, epoch: 33, acc: 0.5633333325386047, Loss 0.9574049925804138\n",
      "LSTM, rep: 0, epoch: 34, acc: 0.6233334541320801, Loss 0.9312710785865783\n",
      "LSTM, rep: 0, epoch: 35, acc: 0.603333592414856, Loss 0.8873871994018555\n",
      "LSTM, rep: 0, epoch: 36, acc: 0.6733334064483643, Loss 0.8116551798582077\n",
      "LSTM, rep: 0, epoch: 37, acc: 0.6600003838539124, Loss 0.8039607393741608\n",
      "LSTM, rep: 0, epoch: 38, acc: 0.6600003242492676, Loss 0.8169240140914917\n",
      "LSTM, rep: 0, epoch: 39, acc: 0.7166668176651001, Loss 0.7550628525018692\n",
      "LSTM, rep: 0, epoch: 40, acc: 0.663333535194397, Loss 0.7534713324904442\n",
      "LSTM, rep: 0, epoch: 41, acc: 0.6433336734771729, Loss 0.7952714467048645\n",
      "LSTM, rep: 0, epoch: 42, acc: 0.6733335256576538, Loss 0.7063941696286201\n",
      "LSTM, rep: 0, epoch: 43, acc: 0.6766667366027832, Loss 0.6985789424180985\n",
      "LSTM, rep: 0, epoch: 44, acc: 0.6800002455711365, Loss 0.6993290141224862\n",
      "LSTM, rep: 0, epoch: 45, acc: 0.646666944026947, Loss 0.7544289416074753\n",
      "LSTM, rep: 0, epoch: 46, acc: 0.5000001788139343, Loss 1.1318209129571914\n",
      "LSTM, rep: 0, epoch: 47, acc: 0.6633332967758179, Loss 0.7336234837770462\n",
      "LSTM, rep: 0, epoch: 48, acc: 0.6700002551078796, Loss 0.6925959420204163\n",
      "LSTM, rep: 0, epoch: 49, acc: 0.6666666269302368, Loss 0.6868505531549454\n",
      "LSTM, rep: 0, epoch: 50, acc: 0.6533333659172058, Loss 0.6864049357175827\n",
      "LSTM, rep: 0, epoch: 51, acc: 0.6333334445953369, Loss 0.6876313787698746\n",
      "LSTM, rep: 0, epoch: 52, acc: 0.7200000286102295, Loss 0.6676187014579773\n",
      "LSTM, rep: 0, epoch: 53, acc: 0.6566666960716248, Loss 0.6766430175304413\n",
      "LSTM, rep: 0, epoch: 54, acc: 0.6766666173934937, Loss 0.6760567224025726\n",
      "LSTM, rep: 0, epoch: 55, acc: 0.6866666674613953, Loss 0.6656572645902634\n",
      "LSTM, rep: 0, epoch: 56, acc: 0.7100001573562622, Loss 0.6614749145507812\n",
      "LSTM, rep: 0, epoch: 57, acc: 0.6733333468437195, Loss 0.6695112749934197\n",
      "LSTM, rep: 0, epoch: 58, acc: 0.676666796207428, Loss 0.6775790286064148\n",
      "LSTM, rep: 0, epoch: 59, acc: 0.6700000762939453, Loss 0.6688569676876068\n",
      "LSTM, rep: 0, epoch: 60, acc: 0.6799999475479126, Loss 0.6717131125926972\n",
      "LSTM, rep: 0, epoch: 61, acc: 0.7233334183692932, Loss 0.6589546138048172\n",
      "LSTM, rep: 0, epoch: 62, acc: 0.6500001549720764, Loss 0.6745526748895645\n",
      "LSTM, rep: 0, epoch: 63, acc: 0.6666666269302368, Loss 0.6720416760444641\n",
      "LSTM, rep: 0, epoch: 64, acc: 0.6966667771339417, Loss 0.6637530428171158\n",
      "LSTM, rep: 0, epoch: 65, acc: 0.653333306312561, Loss 0.6740306347608567\n",
      "LSTM, rep: 0, epoch: 66, acc: 0.7066666483879089, Loss 0.6623500353097915\n",
      "LSTM, rep: 0, epoch: 67, acc: 0.68666672706604, Loss 0.662612769305706\n",
      "LSTM, rep: 0, epoch: 68, acc: 0.7000001668930054, Loss 0.6544723355770111\n",
      "LSTM, rep: 0, epoch: 69, acc: 0.6433334946632385, Loss 0.6718868088722229\n",
      "LSTM, rep: 0, epoch: 70, acc: 0.6733335852622986, Loss 0.6657128143310547\n",
      "LSTM, rep: 0, epoch: 71, acc: 0.6866669654846191, Loss 0.6768973323702813\n",
      "LSTM, rep: 0, epoch: 72, acc: 0.7133334875106812, Loss 0.6623281428217888\n",
      "LSTM, rep: 0, epoch: 73, acc: 0.6666668653488159, Loss 0.6682124465703965\n",
      "LSTM, rep: 0, epoch: 74, acc: 0.6566668748855591, Loss 0.6704669368267059\n",
      "LSTM, rep: 0, epoch: 75, acc: 0.6899999380111694, Loss 0.6596718895435333\n",
      "LSTM, rep: 0, epoch: 76, acc: 0.6599999070167542, Loss 0.666625440120697\n",
      "LSTM, rep: 0, epoch: 77, acc: 0.6866667866706848, Loss 0.6650630062818528\n",
      "LSTM, rep: 0, epoch: 78, acc: 0.669999897480011, Loss 0.6649410527944565\n",
      "LSTM, rep: 0, epoch: 79, acc: 0.6766665577888489, Loss 0.6680508577823638\n",
      "LSTM, rep: 0, epoch: 80, acc: 0.7166666984558105, Loss 0.6586178243160248\n",
      "LSTM, rep: 0, epoch: 81, acc: 0.666666567325592, Loss 0.6868515586853028\n",
      "LSTM, rep: 0, epoch: 82, acc: 0.73333340883255, Loss 0.651315501332283\n",
      "LSTM, rep: 0, epoch: 83, acc: 0.6933334469795227, Loss 0.6550628215074539\n",
      "LSTM, rep: 0, epoch: 84, acc: 0.7133333683013916, Loss 0.6395223221182823\n",
      "LSTM, rep: 0, epoch: 85, acc: 0.7200002074241638, Loss 0.6173942396044731\n",
      "LSTM, rep: 0, epoch: 86, acc: 0.7100000977516174, Loss 0.6231878408789635\n",
      "LSTM, rep: 0, epoch: 87, acc: 0.7266666293144226, Loss 0.6043595984578133\n",
      "LSTM, rep: 0, epoch: 88, acc: 0.7433332800865173, Loss 0.5635463538765907\n",
      "LSTM, rep: 0, epoch: 89, acc: 0.7633332014083862, Loss 0.5655712485313416\n",
      "LSTM, rep: 0, epoch: 90, acc: 0.7733331918716431, Loss 0.5489215585589409\n",
      "LSTM, rep: 0, epoch: 91, acc: 0.7333332896232605, Loss 0.6544495618343353\n",
      "LSTM, rep: 0, epoch: 92, acc: 0.8333332538604736, Loss 0.49009720027446746\n",
      "LSTM, rep: 0, epoch: 93, acc: 0.8133334517478943, Loss 0.48351681992411616\n",
      "LSTM, rep: 0, epoch: 94, acc: 0.8166666626930237, Loss 0.4646101920306683\n",
      "LSTM, rep: 0, epoch: 95, acc: 0.76666659116745, Loss 0.5066306939721108\n",
      "LSTM, rep: 0, epoch: 96, acc: 0.6433334350585938, Loss 0.9531934505701065\n",
      "LSTM, rep: 0, epoch: 97, acc: 0.6833333373069763, Loss 0.9329659593105316\n",
      "LSTM, rep: 0, epoch: 98, acc: 0.7333333492279053, Loss 0.6661573719978332\n",
      "LSTM, rep: 0, epoch: 99, acc: 0.8799999356269836, Loss 0.4625359410047531\n",
      "LSTM, rep: 0, epoch: 100, acc: 0.8733334541320801, Loss 0.38303193703293803\n",
      "LSTM, rep: 0, epoch: 101, acc: 0.8299997448921204, Loss 0.4012782022356987\n",
      "LSTM, rep: 0, epoch: 102, acc: 0.8100000023841858, Loss 0.48600842610001566\n",
      "LSTM, rep: 0, epoch: 103, acc: 0.8666665554046631, Loss 0.42058183684945105\n",
      "LSTM, rep: 0, epoch: 104, acc: 0.8833332061767578, Loss 0.35703994899988173\n",
      "LSTM, rep: 0, epoch: 105, acc: 0.8866665363311768, Loss 0.3486084322631359\n",
      "LSTM, rep: 0, epoch: 106, acc: 0.8833332061767578, Loss 0.34766782611608504\n",
      "LSTM, rep: 0, epoch: 107, acc: 0.8466663956642151, Loss 0.36808972150087355\n",
      "LSTM, rep: 0, epoch: 108, acc: 0.9033331871032715, Loss 0.36167801916599274\n",
      "LSTM, rep: 0, epoch: 109, acc: 0.8166664838790894, Loss 0.5218535913527012\n",
      "LSTM, rep: 0, epoch: 110, acc: 0.8933331370353699, Loss 0.3449470791220665\n",
      "LSTM, rep: 0, epoch: 111, acc: 0.8866667151451111, Loss 0.338295166939497\n",
      "LSTM, rep: 0, epoch: 112, acc: 0.8799999356269836, Loss 0.3430439095199108\n",
      "LSTM, rep: 0, epoch: 113, acc: 0.8799997568130493, Loss 0.32009328454732894\n",
      "LSTM, rep: 0, epoch: 114, acc: 0.8966665863990784, Loss 0.3242419721186161\n",
      "LSTM, rep: 0, epoch: 115, acc: 0.81333327293396, Loss 0.501552544683218\n",
      "LSTM, rep: 0, epoch: 116, acc: 0.90666663646698, Loss 0.2968932019174099\n",
      "LSTM, rep: 0, epoch: 117, acc: 0.8399999141693115, Loss 0.42168562069535254\n",
      "LSTM, rep: 0, epoch: 118, acc: 0.8866667151451111, Loss 0.29866345420479773\n",
      "LSTM, rep: 0, epoch: 119, acc: 0.8966665863990784, Loss 0.2934999765455723\n",
      "LSTM, rep: 0, epoch: 120, acc: 0.8766665458679199, Loss 0.3018295767903328\n",
      "LSTM, rep: 0, epoch: 121, acc: 0.9233332276344299, Loss 0.2640115708112717\n",
      "LSTM, rep: 0, epoch: 122, acc: 0.9266664981842041, Loss 0.2881204731762409\n",
      "LSTM, rep: 0, epoch: 123, acc: 0.7466665506362915, Loss 0.71321754001081\n",
      "LSTM, rep: 0, epoch: 124, acc: 0.7200000882148743, Loss 0.7473312958329916\n",
      "LSTM, rep: 0, epoch: 125, acc: 0.7333332300186157, Loss 0.6483509158343077\n",
      "LSTM, rep: 0, epoch: 126, acc: 0.6966666579246521, Loss 0.7408795467764139\n",
      "LSTM, rep: 0, epoch: 127, acc: 0.7200000882148743, Loss 0.6561517246067524\n",
      "LSTM, rep: 0, epoch: 128, acc: 0.7266666889190674, Loss 0.6350703205168248\n",
      "LSTM, rep: 0, epoch: 129, acc: 0.7333332896232605, Loss 0.6029770331084728\n",
      "LSTM, rep: 0, epoch: 130, acc: 0.6900001764297485, Loss 0.6445950990915299\n",
      "LSTM, rep: 0, epoch: 131, acc: 0.7533332705497742, Loss 0.5307408377528191\n",
      "LSTM, rep: 0, epoch: 132, acc: 0.7399999499320984, Loss 0.5688216370344162\n",
      "LSTM, rep: 0, epoch: 133, acc: 0.7399998307228088, Loss 0.5756832893192768\n",
      "LSTM, rep: 0, epoch: 134, acc: 0.8133333325386047, Loss 0.5219038890302181\n",
      "LSTM, rep: 0, epoch: 135, acc: 0.7433332800865173, Loss 0.5483702205121517\n",
      "LSTM, rep: 0, epoch: 136, acc: 0.7733333706855774, Loss 0.550275992155075\n",
      "LSTM, rep: 0, epoch: 137, acc: 0.8600000739097595, Loss 0.5142623071372509\n",
      "LSTM, rep: 0, epoch: 138, acc: 0.9233332872390747, Loss 0.3709508036077023\n",
      "LSTM, rep: 0, epoch: 139, acc: 0.9333333373069763, Loss 0.31110209569334984\n",
      "LSTM, rep: 0, epoch: 140, acc: 0.9366666674613953, Loss 0.2931180939078331\n",
      "LSTM, rep: 0, epoch: 141, acc: 0.9166668057441711, Loss 0.3401225084066391\n",
      "LSTM, rep: 0, epoch: 142, acc: 0.9333332180976868, Loss 0.28924080930650237\n",
      "LSTM, rep: 0, epoch: 143, acc: 0.9233333468437195, Loss 0.2953410930931568\n",
      "LSTM, rep: 0, epoch: 144, acc: 0.7966665625572205, Loss 0.5897291205823422\n",
      "LSTM, rep: 0, epoch: 145, acc: 0.7100000977516174, Loss 0.7017239739000797\n",
      "LSTM, rep: 0, epoch: 146, acc: 0.7499998211860657, Loss 0.6051994760334491\n",
      "LSTM, rep: 0, epoch: 147, acc: 0.9033331871032715, Loss 0.3769777126610279\n",
      "LSTM, rep: 0, epoch: 148, acc: 0.9266665577888489, Loss 0.30622398361563685\n",
      "LSTM, rep: 0, epoch: 149, acc: 0.9200000166893005, Loss 0.28841404043138025\n",
      "LSTM, rep: 0, epoch: 150, acc: 0.9433332681655884, Loss 0.24866543784737588\n",
      "LSTM, rep: 0, epoch: 151, acc: 0.929999828338623, Loss 0.23122753329575063\n",
      "LSTM, rep: 0, epoch: 152, acc: 0.9099999070167542, Loss 0.24230181977152823\n",
      "LSTM, rep: 0, epoch: 153, acc: 0.9299999475479126, Loss 0.24226770550012589\n",
      "LSTM, rep: 0, epoch: 154, acc: 0.8766665458679199, Loss 0.28068205527961254\n",
      "LSTM, rep: 0, epoch: 155, acc: 0.9266667366027832, Loss 0.2701889145374298\n",
      "LSTM, rep: 0, epoch: 156, acc: 0.9599998593330383, Loss 0.20790119748562574\n",
      "LSTM, rep: 0, epoch: 157, acc: 0.9233333468437195, Loss 0.22590004291385413\n",
      "LSTM, rep: 0, epoch: 158, acc: 0.9366666674613953, Loss 0.21707938697189091\n",
      "LSTM, rep: 0, epoch: 159, acc: 0.9266665577888489, Loss 0.24387858841568233\n",
      "LSTM, rep: 0, epoch: 160, acc: 0.9533333778381348, Loss 0.20981757581233978\n",
      "LSTM, rep: 0, epoch: 161, acc: 0.9566666483879089, Loss 0.16284577168524264\n",
      "LSTM, rep: 0, epoch: 162, acc: 0.9366665482521057, Loss 0.17263171516358852\n",
      "LSTM, rep: 0, epoch: 163, acc: 0.9599999189376831, Loss 0.15772169657051563\n",
      "LSTM, rep: 0, epoch: 164, acc: 0.9266665577888489, Loss 0.19192668545991182\n",
      "LSTM, rep: 0, epoch: 165, acc: 0.9833332896232605, Loss 0.1426944988593459\n",
      "LSTM                 Rep: 0   Epoch: 1     Acc: 0.9833 Params: min_length: 40, max_length: 40, fill: 0, value_1: -1, value_2: 1 Time: 47.51 sec\n",
      "NetLSTMWithAttention, rep: 0, epoch: 1, acc: 0.5566667318344116, Loss 1.0430570900440217\n",
      "NetLSTMWithAttention, rep: 0, epoch: 2, acc: 0.5033332705497742, Loss 1.0362979513406754\n",
      "NetLSTMWithAttention, rep: 0, epoch: 3, acc: 0.5133333802223206, Loss 1.0105564373731613\n",
      "NetLSTMWithAttention, rep: 0, epoch: 4, acc: 0.503333330154419, Loss 1.0037980484962463\n",
      "NetLSTMWithAttention, rep: 0, epoch: 5, acc: 0.4733332693576813, Loss 1.0089573967456817\n",
      "NetLSTMWithAttention, rep: 0, epoch: 6, acc: 0.44999992847442627, Loss 1.0056190925836563\n",
      "NetLSTMWithAttention, rep: 0, epoch: 7, acc: 0.51666659116745, Loss 0.9983783113956451\n",
      "NetLSTMWithAttention, rep: 0, epoch: 8, acc: 0.5533334612846375, Loss 0.9905894535779953\n",
      "NetLSTMWithAttention, rep: 0, epoch: 9, acc: 0.5400000810623169, Loss 0.9908663892745971\n",
      "NetLSTMWithAttention, rep: 0, epoch: 10, acc: 0.5533333420753479, Loss 0.9949517488479614\n",
      "NetLSTMWithAttention, rep: 0, epoch: 11, acc: 0.5400001406669617, Loss 0.9855145978927612\n",
      "NetLSTMWithAttention, rep: 0, epoch: 12, acc: 0.6100001335144043, Loss 0.9381468021869659\n",
      "NetLSTMWithAttention, rep: 0, epoch: 13, acc: 0.5733333230018616, Loss 0.9505597567558288\n",
      "NetLSTMWithAttention, rep: 0, epoch: 14, acc: 0.6500002145767212, Loss 0.8517520135641098\n",
      "NetLSTMWithAttention, rep: 0, epoch: 15, acc: 0.6600000262260437, Loss 0.7745254093408585\n",
      "NetLSTMWithAttention, rep: 0, epoch: 16, acc: 0.6800002455711365, Loss 0.7260065194964409\n",
      "NetLSTMWithAttention, rep: 0, epoch: 17, acc: 0.6833334565162659, Loss 0.7232302868366242\n",
      "NetLSTMWithAttention, rep: 0, epoch: 18, acc: 0.6633333563804626, Loss 0.7075029715895653\n",
      "NetLSTMWithAttention, rep: 0, epoch: 19, acc: 0.7233334183692932, Loss 0.6713439282774926\n",
      "NetLSTMWithAttention, rep: 0, epoch: 20, acc: 0.7000000476837158, Loss 0.6829626613855362\n",
      "NetLSTMWithAttention, rep: 0, epoch: 21, acc: 0.6333335041999817, Loss 0.7066531485319137\n",
      "NetLSTMWithAttention, rep: 0, epoch: 22, acc: 0.6666666269302368, Loss 0.6802979868650436\n",
      "NetLSTMWithAttention, rep: 0, epoch: 23, acc: 0.6733334064483643, Loss 0.6817689520120621\n",
      "NetLSTMWithAttention, rep: 0, epoch: 24, acc: 0.7033336758613586, Loss 0.6737187922000885\n",
      "NetLSTMWithAttention, rep: 0, epoch: 25, acc: 0.6866666674613953, Loss 0.6714838236570358\n",
      "NetLSTMWithAttention, rep: 0, epoch: 26, acc: 0.6266665458679199, Loss 0.8411741924285888\n",
      "NetLSTMWithAttention, rep: 0, epoch: 27, acc: 0.503333330154419, Loss 1.231255326271057\n",
      "NetLSTMWithAttention, rep: 0, epoch: 28, acc: 0.533333420753479, Loss 1.0520291310548782\n",
      "NetLSTMWithAttention, rep: 0, epoch: 29, acc: 0.7133334279060364, Loss 0.8344794619083404\n",
      "NetLSTMWithAttention, rep: 0, epoch: 30, acc: 0.6600000858306885, Loss 0.7345214939117432\n",
      "NetLSTMWithAttention, rep: 0, epoch: 31, acc: 0.6666668057441711, Loss 0.6912975043058396\n",
      "NetLSTMWithAttention, rep: 0, epoch: 32, acc: 0.7166666388511658, Loss 0.6679595005512238\n",
      "NetLSTMWithAttention, rep: 0, epoch: 33, acc: 0.6900001764297485, Loss 0.6765313282608986\n",
      "NetLSTMWithAttention, rep: 0, epoch: 34, acc: 0.6533334851264954, Loss 0.6794506418704986\n",
      "NetLSTMWithAttention, rep: 0, epoch: 35, acc: 0.7000001668930054, Loss 0.6596294218301773\n",
      "NetLSTMWithAttention, rep: 0, epoch: 36, acc: 0.6866665482521057, Loss 0.6740859633684159\n",
      "NetLSTMWithAttention, rep: 0, epoch: 37, acc: 0.6800000667572021, Loss 0.6687597006559372\n",
      "NetLSTMWithAttention, rep: 0, epoch: 38, acc: 0.7000000476837158, Loss 0.6656598716974258\n",
      "NetLSTMWithAttention, rep: 0, epoch: 39, acc: 0.6666668057441711, Loss 0.6713682234287262\n",
      "NetLSTMWithAttention, rep: 0, epoch: 40, acc: 0.669999897480011, Loss 0.7331810188293457\n",
      "NetLSTMWithAttention, rep: 0, epoch: 41, acc: 0.45333346724510193, Loss 1.3219599092006684\n",
      "NetLSTMWithAttention, rep: 0, epoch: 42, acc: 0.5133333802223206, Loss 1.1090058624744414\n",
      "NetLSTMWithAttention, rep: 0, epoch: 43, acc: 0.6633335947990417, Loss 0.8990431469678879\n",
      "NetLSTMWithAttention, rep: 0, epoch: 44, acc: 0.6899999976158142, Loss 0.7337579435110092\n",
      "NetLSTMWithAttention, rep: 0, epoch: 45, acc: 0.6966665387153625, Loss 0.6721185892820358\n",
      "NetLSTMWithAttention, rep: 0, epoch: 46, acc: 0.7499999403953552, Loss 0.6548333609104157\n",
      "NetLSTMWithAttention, rep: 0, epoch: 47, acc: 0.770000159740448, Loss 0.6284807419776917\n",
      "NetLSTMWithAttention, rep: 0, epoch: 48, acc: 0.7566667795181274, Loss 0.6117171102762222\n",
      "NetLSTMWithAttention, rep: 0, epoch: 49, acc: 0.7566665410995483, Loss 0.6039063853025436\n",
      "NetLSTMWithAttention, rep: 0, epoch: 50, acc: 0.7233333587646484, Loss 0.6108894911408425\n",
      "NetLSTMWithAttention, rep: 0, epoch: 51, acc: 0.7599999904632568, Loss 0.5858209067583084\n",
      "NetLSTMWithAttention, rep: 0, epoch: 52, acc: 0.7633333802223206, Loss 0.5896340185403823\n",
      "NetLSTMWithAttention, rep: 0, epoch: 53, acc: 0.6733333468437195, Loss 0.6749192124605179\n",
      "NetLSTMWithAttention, rep: 0, epoch: 54, acc: 0.7000002264976501, Loss 0.6802064713835716\n",
      "NetLSTMWithAttention, rep: 0, epoch: 55, acc: 0.690000057220459, Loss 0.6574345633387566\n",
      "NetLSTMWithAttention, rep: 0, epoch: 56, acc: 0.7466667890548706, Loss 0.6357566502690315\n",
      "NetLSTMWithAttention, rep: 0, epoch: 57, acc: 0.6366667747497559, Loss 0.7654720342159271\n",
      "NetLSTMWithAttention, rep: 0, epoch: 58, acc: 0.7266668081283569, Loss 0.6128791618347168\n",
      "NetLSTMWithAttention, rep: 0, epoch: 59, acc: 0.8266665935516357, Loss 0.46959364116191865\n",
      "NetLSTMWithAttention, rep: 0, epoch: 60, acc: 0.8599998950958252, Loss 0.430862700343132\n",
      "NetLSTMWithAttention, rep: 0, epoch: 61, acc: 0.81333327293396, Loss 0.42465807050466536\n",
      "NetLSTMWithAttention, rep: 0, epoch: 62, acc: 0.7433333396911621, Loss 0.5705530932545662\n",
      "NetLSTMWithAttention, rep: 0, epoch: 63, acc: 0.5166666507720947, Loss 1.3696251174807548\n",
      "NetLSTMWithAttention, rep: 0, epoch: 64, acc: 0.4833332300186157, Loss 1.455381904244423\n",
      "NetLSTMWithAttention, rep: 0, epoch: 65, acc: 0.49333325028419495, Loss 1.2784857791662216\n",
      "NetLSTMWithAttention, rep: 0, epoch: 66, acc: 0.49666664004325867, Loss 1.1979439431428909\n",
      "NetLSTMWithAttention, rep: 0, epoch: 67, acc: 0.533333420753479, Loss 1.2597633856534958\n",
      "NetLSTMWithAttention, rep: 0, epoch: 68, acc: 0.4999999701976776, Loss 1.3380750358104705\n",
      "NetLSTMWithAttention, rep: 0, epoch: 69, acc: 0.5133333802223206, Loss 1.3738126701116562\n",
      "NetLSTMWithAttention, rep: 0, epoch: 70, acc: 0.48333340883255005, Loss 1.3742268019914627\n",
      "NetLSTMWithAttention, rep: 0, epoch: 71, acc: 0.5166667699813843, Loss 1.191720290184021\n",
      "NetLSTMWithAttention, rep: 0, epoch: 72, acc: 0.46000003814697266, Loss 1.3076665055751802\n",
      "NetLSTMWithAttention, rep: 0, epoch: 73, acc: 0.4833333194255829, Loss 1.059579826593399\n",
      "NetLSTMWithAttention, rep: 0, epoch: 74, acc: 0.4666667878627777, Loss 1.0151794373989105\n",
      "NetLSTMWithAttention, rep: 0, epoch: 75, acc: 0.526666522026062, Loss 1.0027290964126587\n",
      "NetLSTMWithAttention, rep: 0, epoch: 76, acc: 0.4766666293144226, Loss 1.0072294139862061\n",
      "NetLSTMWithAttention, rep: 0, epoch: 77, acc: 0.46999990940093994, Loss 1.001812415122986\n",
      "NetLSTMWithAttention, rep: 0, epoch: 78, acc: 0.5066667199134827, Loss 1.0012118047475815\n",
      "NetLSTMWithAttention, rep: 0, epoch: 79, acc: 0.4733333885669708, Loss 1.0068261921405792\n",
      "NetLSTMWithAttention, rep: 0, epoch: 80, acc: 0.5533333420753479, Loss 0.9976248604059219\n",
      "NetLSTMWithAttention, rep: 0, epoch: 81, acc: 0.45333331823349, Loss 1.0053572982549668\n",
      "NetLSTMWithAttention, rep: 0, epoch: 82, acc: 0.5033333897590637, Loss 1.0013928139209747\n",
      "NetLSTMWithAttention, rep: 0, epoch: 83, acc: 0.5266667008399963, Loss 0.9983287727832795\n",
      "NetLSTMWithAttention, rep: 0, epoch: 84, acc: 0.513333261013031, Loss 0.9951341664791107\n",
      "NetLSTMWithAttention, rep: 0, epoch: 85, acc: 0.5366666913032532, Loss 1.0012730091810227\n",
      "NetLSTMWithAttention, rep: 0, epoch: 86, acc: 0.5133333206176758, Loss 1.0021335524320603\n",
      "NetLSTMWithAttention, rep: 0, epoch: 87, acc: 0.4833333194255829, Loss 1.0047178888320922\n",
      "NetLSTMWithAttention, rep: 0, epoch: 88, acc: 0.47999992966651917, Loss 1.0042496877908706\n",
      "NetLSTMWithAttention, rep: 0, epoch: 89, acc: 0.5100000500679016, Loss 0.9988136255741119\n",
      "NetLSTMWithAttention, rep: 0, epoch: 90, acc: 0.5533334016799927, Loss 0.9873633933067322\n",
      "NetLSTMWithAttention, rep: 0, epoch: 91, acc: 0.5166666507720947, Loss 0.9969968289136887\n",
      "NetLSTMWithAttention, rep: 0, epoch: 92, acc: 0.48666658997535706, Loss 1.0094624865055084\n",
      "NetLSTMWithAttention, rep: 0, epoch: 93, acc: 0.5299999713897705, Loss 0.9984342056512833\n",
      "NetLSTMWithAttention, rep: 0, epoch: 94, acc: 0.5199999809265137, Loss 0.9951102620363236\n",
      "NetLSTMWithAttention, rep: 0, epoch: 95, acc: 0.5266666412353516, Loss 0.9968396729230881\n",
      "NetLSTMWithAttention, rep: 0, epoch: 96, acc: 0.5066666007041931, Loss 0.9981702154874802\n",
      "NetLSTMWithAttention, rep: 0, epoch: 97, acc: 0.5633333325386047, Loss 0.9895486426353455\n",
      "NetLSTMWithAttention, rep: 0, epoch: 98, acc: 0.613333523273468, Loss 0.9680516934394836\n",
      "NetLSTMWithAttention, rep: 0, epoch: 99, acc: 0.7200000286102295, Loss 0.7932069364190102\n",
      "NetLSTMWithAttention, rep: 0, epoch: 100, acc: 0.6366667747497559, Loss 0.7759453976154327\n",
      "NetLSTMWithAttention, rep: 0, epoch: 101, acc: 0.6766666173934937, Loss 0.7097952139377594\n",
      "NetLSTMWithAttention, rep: 0, epoch: 102, acc: 0.6633334159851074, Loss 0.7039899799227715\n",
      "NetLSTMWithAttention, rep: 0, epoch: 103, acc: 0.6566669344902039, Loss 0.6993219488859177\n",
      "NetLSTMWithAttention, rep: 0, epoch: 104, acc: 0.6466666460037231, Loss 0.7059186914563179\n",
      "NetLSTMWithAttention, rep: 0, epoch: 105, acc: 0.663333535194397, Loss 0.6835826480388641\n",
      "NetLSTMWithAttention, rep: 0, epoch: 106, acc: 0.6800000071525574, Loss 0.6763351771235466\n",
      "NetLSTMWithAttention, rep: 0, epoch: 107, acc: 0.663333535194397, Loss 0.6853556019067765\n",
      "NetLSTMWithAttention, rep: 0, epoch: 108, acc: 0.6433334350585938, Loss 0.6771608823537827\n",
      "NetLSTMWithAttention, rep: 0, epoch: 109, acc: 0.670000433921814, Loss 0.6738147586584091\n",
      "NetLSTMWithAttention, rep: 0, epoch: 110, acc: 0.6533331871032715, Loss 0.6808840990066528\n",
      "NetLSTMWithAttention, rep: 0, epoch: 111, acc: 0.676666796207428, Loss 0.6734107303619384\n",
      "NetLSTMWithAttention, rep: 0, epoch: 112, acc: 0.6900001764297485, Loss 0.6696460390090943\n",
      "NetLSTMWithAttention, rep: 0, epoch: 113, acc: 0.6766670346260071, Loss 0.6711797416210175\n",
      "NetLSTMWithAttention, rep: 0, epoch: 114, acc: 0.68666672706604, Loss 0.666726245880127\n",
      "NetLSTMWithAttention, rep: 0, epoch: 115, acc: 0.636667013168335, Loss 0.6725323122739791\n",
      "NetLSTMWithAttention, rep: 0, epoch: 116, acc: 0.7066667079925537, Loss 0.6662181639671325\n",
      "NetLSTMWithAttention, rep: 0, epoch: 117, acc: 0.6933334469795227, Loss 0.6663921576738357\n",
      "NetLSTMWithAttention, rep: 0, epoch: 118, acc: 0.6800001263618469, Loss 0.6589180862903595\n",
      "NetLSTMWithAttention, rep: 0, epoch: 119, acc: 0.6966666579246521, Loss 0.658280873298645\n",
      "NetLSTMWithAttention, rep: 0, epoch: 120, acc: 0.6966667771339417, Loss 0.6529599702358246\n",
      "NetLSTMWithAttention, rep: 0, epoch: 121, acc: 0.6400001049041748, Loss 0.6718658262491226\n",
      "NetLSTMWithAttention, rep: 0, epoch: 122, acc: 0.6566665768623352, Loss 0.6616752713918685\n",
      "NetLSTMWithAttention, rep: 0, epoch: 123, acc: 0.6733334064483643, Loss 0.657446865439415\n",
      "NetLSTMWithAttention, rep: 0, epoch: 124, acc: 0.7366667985916138, Loss 0.6540735083818435\n",
      "NetLSTMWithAttention, rep: 0, epoch: 125, acc: 0.7566665410995483, Loss 0.6467528992891312\n",
      "NetLSTMWithAttention, rep: 0, epoch: 126, acc: 0.8299999237060547, Loss 0.6290003728866577\n",
      "NetLSTMWithAttention, rep: 0, epoch: 127, acc: 0.8299999237060547, Loss 0.6009393161535264\n",
      "NetLSTMWithAttention, rep: 0, epoch: 128, acc: 0.8399999141693115, Loss 0.5710791537165641\n",
      "NetLSTMWithAttention, rep: 0, epoch: 129, acc: 0.8233334422111511, Loss 0.5372095787525177\n",
      "NetLSTMWithAttention, rep: 0, epoch: 130, acc: 0.7999998331069946, Loss 0.5005605217814445\n",
      "NetLSTMWithAttention, rep: 0, epoch: 131, acc: 0.8100000023841858, Loss 0.48112538576126096\n",
      "NetLSTMWithAttention, rep: 0, epoch: 132, acc: 0.7733331918716431, Loss 0.4865598395466805\n",
      "NetLSTMWithAttention, rep: 0, epoch: 133, acc: 0.8633332252502441, Loss 0.46626350909471515\n",
      "NetLSTMWithAttention, rep: 0, epoch: 134, acc: 0.8966665863990784, Loss 0.4504771000146866\n",
      "NetLSTMWithAttention, rep: 0, epoch: 135, acc: 0.8599998354911804, Loss 0.4427261739969254\n",
      "NetLSTMWithAttention, rep: 0, epoch: 136, acc: 0.8499999046325684, Loss 0.4358178412914276\n",
      "NetLSTMWithAttention, rep: 0, epoch: 137, acc: 0.8666665554046631, Loss 0.40602770864963533\n",
      "NetLSTMWithAttention, rep: 0, epoch: 138, acc: 0.853333055973053, Loss 0.4011806458234787\n",
      "NetLSTMWithAttention, rep: 0, epoch: 139, acc: 0.8633332252502441, Loss 0.40155792504549026\n",
      "NetLSTMWithAttention, rep: 0, epoch: 140, acc: 0.8666664958000183, Loss 0.3794281804561615\n",
      "NetLSTMWithAttention, rep: 0, epoch: 141, acc: 0.8566663861274719, Loss 0.38014478117227557\n",
      "NetLSTMWithAttention, rep: 0, epoch: 142, acc: 0.8633332252502441, Loss 0.36594923451542855\n",
      "NetLSTMWithAttention, rep: 0, epoch: 143, acc: 0.8466665744781494, Loss 0.41280161783099173\n",
      "NetLSTMWithAttention, rep: 0, epoch: 144, acc: 0.8866667151451111, Loss 0.3928956981003284\n",
      "NetLSTMWithAttention, rep: 0, epoch: 145, acc: 0.8799999952316284, Loss 0.359876117259264\n",
      "NetLSTMWithAttention, rep: 0, epoch: 146, acc: 0.8666664361953735, Loss 0.3606388175487518\n",
      "NetLSTMWithAttention, rep: 0, epoch: 147, acc: 0.9066663980484009, Loss 0.33238027766346934\n",
      "NetLSTMWithAttention, rep: 0, epoch: 148, acc: 0.8566663861274719, Loss 0.368437939286232\n",
      "NetLSTMWithAttention, rep: 0, epoch: 149, acc: 0.8733332753181458, Loss 0.3544710439443588\n",
      "NetLSTMWithAttention, rep: 0, epoch: 150, acc: 0.8733333349227905, Loss 0.34674001470208166\n",
      "NetLSTMWithAttention, rep: 0, epoch: 151, acc: 0.8566665053367615, Loss 0.3502346298098564\n",
      "NetLSTMWithAttention, rep: 0, epoch: 152, acc: 0.856666624546051, Loss 0.35123694270849226\n",
      "NetLSTMWithAttention, rep: 0, epoch: 153, acc: 0.8466665744781494, Loss 0.35003745794296265\n",
      "NetLSTMWithAttention, rep: 0, epoch: 154, acc: 0.8499998450279236, Loss 0.33498088642954826\n",
      "NetLSTMWithAttention, rep: 0, epoch: 155, acc: 0.8666664958000183, Loss 0.3277143043279648\n",
      "NetLSTMWithAttention, rep: 0, epoch: 156, acc: 0.8600000143051147, Loss 0.32788424119353293\n",
      "NetLSTMWithAttention, rep: 0, epoch: 157, acc: 0.8799996972084045, Loss 0.3204642589390278\n",
      "NetLSTMWithAttention, rep: 0, epoch: 158, acc: 0.8766665458679199, Loss 0.31296211168169974\n",
      "NetLSTMWithAttention, rep: 0, epoch: 159, acc: 0.9033331871032715, Loss 0.30655529364943507\n",
      "NetLSTMWithAttention, rep: 0, epoch: 160, acc: 0.9233332872390747, Loss 0.2896322204172611\n",
      "NetLSTMWithAttention, rep: 0, epoch: 161, acc: 0.9299997091293335, Loss 0.2756793765723705\n",
      "NetLSTMWithAttention, rep: 0, epoch: 162, acc: 0.8799999356269836, Loss 0.3013978786021471\n",
      "NetLSTMWithAttention, rep: 0, epoch: 163, acc: 0.8933331966400146, Loss 0.2784109172970057\n",
      "NetLSTMWithAttention, rep: 0, epoch: 164, acc: 0.9233330488204956, Loss 0.25510435350239274\n",
      "NetLSTMWithAttention, rep: 0, epoch: 165, acc: 0.9199997782707214, Loss 0.25228270396590236\n",
      "NetLSTMWithAttention, rep: 0, epoch: 166, acc: 0.9399998188018799, Loss 0.22318688251078128\n",
      "NetLSTMWithAttention, rep: 0, epoch: 167, acc: 0.9366666674613953, Loss 0.2136270759254694\n",
      "NetLSTMWithAttention, rep: 0, epoch: 168, acc: 0.9033331274986267, Loss 0.23783111050724984\n",
      "NetLSTMWithAttention, rep: 0, epoch: 169, acc: 0.9366664290428162, Loss 0.19954501383006573\n",
      "NetLSTMWithAttention, rep: 0, epoch: 170, acc: 0.9366665482521057, Loss 0.20808974511921405\n",
      "NetLSTMWithAttention, rep: 0, epoch: 171, acc: 0.9699999094009399, Loss 0.18090402029454708\n",
      "NetLSTMWithAttention, rep: 0, epoch: 172, acc: 0.9499999284744263, Loss 0.18499920204281806\n",
      "NetLSTMWithAttention, rep: 0, epoch: 173, acc: 0.9700000286102295, Loss 0.18188304744660855\n",
      "NetLSTMWithAttention Rep: 0   Epoch: 1     Acc: 0.9700 Params: min_length: 40, max_length: 40, fill: 0, value_1: -1, value_2: 1 Time: 68.47 sec\n",
      "GRU, rep: 0, epoch: 1, acc: 0.5199998617172241, Loss 1.0096642243862153\n",
      "GRU, rep: 0, epoch: 2, acc: 0.5066666603088379, Loss 1.0439648395776748\n",
      "GRU, rep: 0, epoch: 3, acc: 0.569999635219574, Loss 0.9822861927747727\n",
      "GRU, rep: 0, epoch: 4, acc: 0.46333348751068115, Loss 1.030852311849594\n",
      "GRU, rep: 0, epoch: 5, acc: 0.49666687846183777, Loss 1.0096912896633148\n",
      "GRU, rep: 0, epoch: 6, acc: 0.5000001192092896, Loss 1.0044905531406403\n",
      "GRU, rep: 0, epoch: 7, acc: 0.5066666603088379, Loss 1.006671702861786\n",
      "GRU, rep: 0, epoch: 8, acc: 0.5366666913032532, Loss 0.9977210474014282\n",
      "GRU, rep: 0, epoch: 9, acc: 0.5333333611488342, Loss 0.9986757379770279\n",
      "GRU, rep: 0, epoch: 10, acc: 0.48666658997535706, Loss 1.0099906373023986\n",
      "GRU, rep: 0, epoch: 11, acc: 0.5133333206176758, Loss 0.9987443959712983\n",
      "GRU, rep: 0, epoch: 12, acc: 0.5666664242744446, Loss 0.9903646981716157\n",
      "GRU, rep: 0, epoch: 13, acc: 0.5299999117851257, Loss 0.9912324565649032\n",
      "GRU, rep: 0, epoch: 14, acc: 0.5166665315628052, Loss 0.9885535675287247\n",
      "GRU, rep: 0, epoch: 15, acc: 0.5099999904632568, Loss 0.9970493686199188\n",
      "GRU, rep: 0, epoch: 16, acc: 0.6066668033599854, Loss 0.9512130415439606\n",
      "GRU, rep: 0, epoch: 17, acc: 0.7166668176651001, Loss 0.7332813280820847\n",
      "GRU, rep: 0, epoch: 18, acc: 0.6500001549720764, Loss 0.7028944057226181\n",
      "GRU, rep: 0, epoch: 19, acc: 0.6600001454353333, Loss 0.690986521244049\n",
      "GRU, rep: 0, epoch: 20, acc: 0.6800000667572021, Loss 0.6755467510223389\n",
      "GRU, rep: 0, epoch: 21, acc: 0.6733334064483643, Loss 0.6591078028082847\n",
      "GRU, rep: 0, epoch: 22, acc: 0.6600003242492676, Loss 0.6776628792285919\n",
      "GRU, rep: 0, epoch: 23, acc: 0.6433331370353699, Loss 0.6833807650208473\n",
      "GRU, rep: 0, epoch: 24, acc: 0.6633334159851074, Loss 0.670867400765419\n",
      "GRU, rep: 0, epoch: 25, acc: 0.6933333873748779, Loss 0.6714848417043686\n",
      "GRU, rep: 0, epoch: 26, acc: 0.6899999976158142, Loss 0.6612856525182724\n",
      "GRU, rep: 0, epoch: 27, acc: 0.6866666674613953, Loss 0.6703231272101402\n",
      "GRU, rep: 0, epoch: 28, acc: 0.7033333778381348, Loss 0.6644702133536339\n",
      "GRU, rep: 0, epoch: 29, acc: 0.6300001740455627, Loss 0.6832907256484032\n",
      "GRU, rep: 0, epoch: 30, acc: 0.7133334279060364, Loss 0.660067241191864\n",
      "GRU, rep: 0, epoch: 31, acc: 0.6866666674613953, Loss 0.6573200124502182\n",
      "GRU, rep: 0, epoch: 32, acc: 0.7066667675971985, Loss 0.6486630770564079\n",
      "GRU, rep: 0, epoch: 33, acc: 0.8033334612846375, Loss 0.5952433770895005\n",
      "GRU, rep: 0, epoch: 34, acc: 0.8333331942558289, Loss 0.519350061416626\n",
      "GRU, rep: 0, epoch: 35, acc: 0.8266663551330566, Loss 0.3939800463616848\n",
      "GRU, rep: 0, epoch: 36, acc: 0.8433331847190857, Loss 0.3625773033499718\n",
      "GRU, rep: 0, epoch: 37, acc: 0.8733330368995667, Loss 0.3462083496153355\n",
      "GRU, rep: 0, epoch: 38, acc: 0.8999999165534973, Loss 0.32536524698138236\n",
      "GRU, rep: 0, epoch: 39, acc: 0.90666663646698, Loss 0.3116480764746666\n",
      "GRU, rep: 0, epoch: 40, acc: 0.9499997496604919, Loss 0.2780179233849049\n",
      "GRU, rep: 0, epoch: 41, acc: 0.9800000190734863, Loss 0.2438409897685051\n",
      "GRU                  Rep: 0   Epoch: 1     Acc: 0.9800 Params: min_length: 40, max_length: 40, fill: 0, value_1: -1, value_2: 1 Time: 39.44 sec\n",
      "NetGRUWithAttention, rep: 0, epoch: 1, acc: 0.46333327889442444, Loss 1.0276420164108275\n",
      "NetGRUWithAttention, rep: 0, epoch: 2, acc: 0.5299999713897705, Loss 0.9961839938163757\n",
      "NetGRUWithAttention, rep: 0, epoch: 3, acc: 0.4933333098888397, Loss 1.0041844463348388\n",
      "NetGRUWithAttention, rep: 0, epoch: 4, acc: 0.503333330154419, Loss 1.0095296329259873\n",
      "NetGRUWithAttention, rep: 0, epoch: 5, acc: 0.5066666603088379, Loss 1.0025897240638733\n",
      "NetGRUWithAttention, rep: 0, epoch: 6, acc: 0.49666649103164673, Loss 1.010342810153961\n",
      "NetGRUWithAttention, rep: 0, epoch: 7, acc: 0.5366666316986084, Loss 0.9948266565799713\n",
      "NetGRUWithAttention, rep: 0, epoch: 8, acc: 0.5133333802223206, Loss 0.9990881186723709\n",
      "NetGRUWithAttention, rep: 0, epoch: 9, acc: 0.5499998331069946, Loss 0.9914431822299957\n",
      "NetGRUWithAttention, rep: 0, epoch: 10, acc: 0.5100000500679016, Loss 0.992172183394432\n",
      "NetGRUWithAttention, rep: 0, epoch: 11, acc: 0.613333523273468, Loss 0.9717323386669159\n",
      "NetGRUWithAttention, rep: 0, epoch: 12, acc: 0.6600000858306885, Loss 0.9102381128072738\n",
      "NetGRUWithAttention, rep: 0, epoch: 13, acc: 0.699999988079071, Loss 0.8059208935499191\n",
      "NetGRUWithAttention, rep: 0, epoch: 14, acc: 0.6500000953674316, Loss 0.7550632669031621\n",
      "NetGRUWithAttention, rep: 0, epoch: 15, acc: 0.6966667175292969, Loss 0.6785880416631699\n",
      "NetGRUWithAttention, rep: 0, epoch: 16, acc: 0.68666672706604, Loss 0.6583759081363678\n",
      "NetGRUWithAttention, rep: 0, epoch: 17, acc: 0.7866666913032532, Loss 0.5889465552568436\n",
      "NetGRUWithAttention, rep: 0, epoch: 18, acc: 0.7233332991600037, Loss 0.6339250339567661\n",
      "NetGRUWithAttention, rep: 0, epoch: 19, acc: 0.7799999713897705, Loss 0.5354334498941898\n",
      "NetGRUWithAttention, rep: 0, epoch: 20, acc: 0.7866665720939636, Loss 0.48549643956124783\n",
      "NetGRUWithAttention, rep: 0, epoch: 21, acc: 0.7833333015441895, Loss 0.5334022089838981\n",
      "NetGRUWithAttention, rep: 0, epoch: 22, acc: 0.75, Loss 0.5120645310729742\n",
      "NetGRUWithAttention, rep: 0, epoch: 23, acc: 0.7899999022483826, Loss 0.5018758919090033\n",
      "NetGRUWithAttention, rep: 0, epoch: 24, acc: 0.8433333039283752, Loss 0.4250874134898186\n",
      "NetGRUWithAttention, rep: 0, epoch: 25, acc: 0.919999897480011, Loss 0.3547975055873394\n",
      "NetGRUWithAttention, rep: 0, epoch: 26, acc: 0.9599999189376831, Loss 0.2931719509512186\n",
      "NetGRUWithAttention, rep: 0, epoch: 27, acc: 0.9599999785423279, Loss 0.26693709898740053\n",
      "NetGRUWithAttention, rep: 0, epoch: 28, acc: 1.0, Loss 0.1761766419932246\n",
      "NetGRUWithAttention  Rep: 0   Epoch: 1     Acc: 1.0000 Params: min_length: 40, max_length: 40, fill: 0, value_1: -1, value_2: 1 Time: 33.33 sec\n",
      "RNN, rep: 0, epoch: 1, acc: 0.5066667795181274, Loss 1.0056702369451522\n",
      "RNN, rep: 0, epoch: 2, acc: 0.49666714668273926, Loss 1.0169060218334198\n",
      "RNN, rep: 0, epoch: 3, acc: 0.5200001001358032, Loss 1.002636342048645\n",
      "RNN, rep: 0, epoch: 4, acc: 0.463333398103714, Loss 1.0205592864751816\n",
      "RNN, rep: 0, epoch: 5, acc: 0.4966667890548706, Loss 1.0020422166585923\n",
      "RNN, rep: 0, epoch: 6, acc: 0.5100000500679016, Loss 1.0070552098751069\n",
      "RNN, rep: 0, epoch: 7, acc: 0.5199999809265137, Loss 0.9978055632114411\n",
      "RNN, rep: 0, epoch: 8, acc: 0.513333261013031, Loss 0.9844819295406342\n",
      "RNN, rep: 0, epoch: 9, acc: 0.5133333206176758, Loss 1.0081565481424333\n",
      "RNN, rep: 0, epoch: 10, acc: 0.5033332705497742, Loss 1.0118752056360245\n",
      "RNN, rep: 0, epoch: 11, acc: 0.49999985098838806, Loss 1.0090118795633316\n",
      "RNN, rep: 0, epoch: 12, acc: 0.5099997520446777, Loss 1.0076840007305146\n",
      "RNN, rep: 0, epoch: 13, acc: 0.5166667699813843, Loss 1.0090784895420075\n",
      "RNN, rep: 0, epoch: 14, acc: 0.5166666507720947, Loss 1.0022745001316071\n",
      "RNN, rep: 0, epoch: 15, acc: 0.48333343863487244, Loss 1.0065868711471557\n",
      "RNN, rep: 0, epoch: 16, acc: 0.49666666984558105, Loss 1.005270892381668\n",
      "RNN, rep: 0, epoch: 17, acc: 0.5166663527488708, Loss 1.0020917296409606\n",
      "RNN, rep: 0, epoch: 18, acc: 0.5466669201850891, Loss 0.9999668443202973\n",
      "RNN, rep: 0, epoch: 19, acc: 0.4933331608772278, Loss 1.00629707634449\n",
      "RNN, rep: 0, epoch: 20, acc: 0.4966667592525482, Loss 1.004283053278923\n",
      "RNN, rep: 0, epoch: 21, acc: 0.5666667222976685, Loss 0.9879279464483262\n",
      "RNN, rep: 0, epoch: 22, acc: 0.516666829586029, Loss 1.0079627984762192\n",
      "RNN, rep: 0, epoch: 23, acc: 0.4866667091846466, Loss 1.0122089368104934\n",
      "RNN, rep: 0, epoch: 24, acc: 0.5633334517478943, Loss 0.9871259427070618\n",
      "RNN, rep: 0, epoch: 25, acc: 0.5666669607162476, Loss 0.9883997267484665\n",
      "RNN, rep: 0, epoch: 26, acc: 0.5633334517478943, Loss 0.9807770001888275\n",
      "RNN, rep: 0, epoch: 27, acc: 0.5400000214576721, Loss 1.001193838119507\n",
      "RNN, rep: 0, epoch: 28, acc: 0.6066667437553406, Loss 0.9447312945127487\n",
      "RNN, rep: 0, epoch: 29, acc: 0.5633334517478943, Loss 0.9607594788074494\n",
      "RNN, rep: 0, epoch: 30, acc: 0.49666649103164673, Loss 0.994721308350563\n",
      "RNN, rep: 0, epoch: 31, acc: 0.583333432674408, Loss 0.9028769952058792\n",
      "RNN, rep: 0, epoch: 32, acc: 0.6233335733413696, Loss 0.8719576960802078\n",
      "RNN, rep: 0, epoch: 33, acc: 0.479999840259552, Loss 1.0598153018951415\n",
      "RNN, rep: 0, epoch: 34, acc: 0.543333113193512, Loss 0.9987666004896164\n",
      "RNN, rep: 0, epoch: 35, acc: 0.5166666507720947, Loss 0.9902594810724259\n",
      "RNN, rep: 0, epoch: 36, acc: 0.5133334994316101, Loss 0.9900211504101754\n",
      "RNN, rep: 0, epoch: 37, acc: 0.4566667079925537, Loss 1.0132996827363967\n",
      "RNN, rep: 0, epoch: 38, acc: 0.5166666507720947, Loss 0.9983650827407837\n",
      "RNN, rep: 0, epoch: 39, acc: 0.5533333420753479, Loss 0.9829172956943512\n",
      "RNN, rep: 0, epoch: 40, acc: 0.5099999904632568, Loss 0.982739906013012\n",
      "RNN, rep: 0, epoch: 41, acc: 0.5433334112167358, Loss 0.9810386970639229\n",
      "RNN, rep: 0, epoch: 42, acc: 0.49000003933906555, Loss 1.0113896107673646\n",
      "RNN, rep: 0, epoch: 43, acc: 0.5233332514762878, Loss 0.9715834054350853\n",
      "RNN, rep: 0, epoch: 44, acc: 0.5500002503395081, Loss 0.9750733160972596\n",
      "RNN, rep: 0, epoch: 45, acc: 0.5633330941200256, Loss 0.9687130591273307\n",
      "RNN, rep: 0, epoch: 46, acc: 0.6133336424827576, Loss 0.9040044677257538\n",
      "RNN, rep: 0, epoch: 47, acc: 0.6500000953674316, Loss 0.815599019229412\n",
      "RNN, rep: 0, epoch: 48, acc: 0.7066667675971985, Loss 0.7648325243592262\n",
      "RNN, rep: 0, epoch: 49, acc: 0.6733334064483643, Loss 0.7652016824483872\n",
      "RNN, rep: 0, epoch: 50, acc: 0.6000000238418579, Loss 0.9389040440320968\n",
      "RNN, rep: 0, epoch: 51, acc: 0.6500000953674316, Loss 0.7434327936172486\n",
      "RNN, rep: 0, epoch: 52, acc: 0.6533333659172058, Loss 0.7214281618595123\n",
      "RNN, rep: 0, epoch: 53, acc: 0.6666667461395264, Loss 0.729248805642128\n",
      "RNN, rep: 0, epoch: 54, acc: 0.690000057220459, Loss 0.7078884106874466\n",
      "RNN, rep: 0, epoch: 55, acc: 0.6500000953674316, Loss 0.6825911420583725\n",
      "RNN, rep: 0, epoch: 56, acc: 0.676666796207428, Loss 0.6860481876134873\n",
      "RNN, rep: 0, epoch: 57, acc: 0.6700000166893005, Loss 0.6778821873664856\n",
      "RNN, rep: 0, epoch: 58, acc: 0.6033332943916321, Loss 0.9391161191463471\n",
      "RNN, rep: 0, epoch: 59, acc: 0.5066666603088379, Loss 1.1974243932962418\n",
      "RNN, rep: 0, epoch: 60, acc: 0.4866665303707123, Loss 1.0293559050559997\n",
      "RNN, rep: 0, epoch: 61, acc: 0.5399998426437378, Loss 0.9866838347911835\n",
      "RNN, rep: 0, epoch: 62, acc: 0.48666682839393616, Loss 0.9885201668739318\n",
      "RNN, rep: 0, epoch: 63, acc: 0.5733332633972168, Loss 0.9450409519672394\n",
      "RNN, rep: 0, epoch: 64, acc: 0.6666667461395264, Loss 0.7678099209070206\n",
      "RNN, rep: 0, epoch: 65, acc: 0.6800000667572021, Loss 0.7141226762533188\n",
      "RNN, rep: 0, epoch: 66, acc: 0.6700000762939453, Loss 0.7015865290164948\n",
      "RNN, rep: 0, epoch: 67, acc: 0.6500000953674316, Loss 0.6896336716413498\n",
      "RNN, rep: 0, epoch: 68, acc: 0.6633335947990417, Loss 0.6859489518404007\n",
      "RNN, rep: 0, epoch: 69, acc: 0.6666668057441711, Loss 0.679652196764946\n",
      "RNN, rep: 0, epoch: 70, acc: 0.6466667652130127, Loss 0.680515940785408\n",
      "RNN, rep: 0, epoch: 71, acc: 0.6533334851264954, Loss 0.677172658443451\n",
      "RNN, rep: 0, epoch: 72, acc: 0.6900001764297485, Loss 0.6703603476285934\n",
      "RNN, rep: 0, epoch: 73, acc: 0.669999897480011, Loss 0.6707452738285065\n",
      "RNN, rep: 0, epoch: 74, acc: 0.690000057220459, Loss 0.6659207659959793\n",
      "RNN, rep: 0, epoch: 75, acc: 0.6199999451637268, Loss 0.6807450252771378\n",
      "RNN, rep: 0, epoch: 76, acc: 0.6433336138725281, Loss 0.6743735522031784\n",
      "RNN, rep: 0, epoch: 77, acc: 0.7099999189376831, Loss 0.6627614217996597\n",
      "RNN, rep: 0, epoch: 78, acc: 0.6833332777023315, Loss 0.6708338141441346\n",
      "RNN, rep: 0, epoch: 79, acc: 0.6933332681655884, Loss 0.7028118681907654\n",
      "RNN, rep: 0, epoch: 80, acc: 0.676666796207428, Loss 0.7151033928990365\n",
      "RNN, rep: 0, epoch: 81, acc: 0.7066665887832642, Loss 0.6829452815651894\n",
      "RNN, rep: 0, epoch: 82, acc: 0.6433334946632385, Loss 0.7165153411030769\n",
      "RNN, rep: 0, epoch: 83, acc: 0.6733333468437195, Loss 0.6817430037260056\n",
      "RNN, rep: 0, epoch: 84, acc: 0.6666668057441711, Loss 0.697948197722435\n",
      "RNN, rep: 0, epoch: 85, acc: 0.6766666173934937, Loss 0.6809437155723572\n",
      "RNN, rep: 0, epoch: 86, acc: 0.6666666269302368, Loss 0.6778007519245147\n",
      "RNN, rep: 0, epoch: 87, acc: 0.6333335638046265, Loss 0.6760436391830444\n",
      "RNN, rep: 0, epoch: 88, acc: 0.6966667175292969, Loss 0.6795538604259491\n",
      "RNN, rep: 0, epoch: 89, acc: 0.6800001263618469, Loss 0.6772986698150635\n",
      "RNN, rep: 0, epoch: 90, acc: 0.6300000548362732, Loss 0.8126791846752167\n",
      "RNN, rep: 0, epoch: 91, acc: 0.5400000214576721, Loss 1.188074875473976\n",
      "RNN, rep: 0, epoch: 92, acc: 0.5933334827423096, Loss 0.9480821651220321\n",
      "RNN, rep: 0, epoch: 93, acc: 0.6466667652130127, Loss 0.7025722801685333\n",
      "RNN, rep: 0, epoch: 94, acc: 0.6599997878074646, Loss 0.6946249306201935\n",
      "RNN, rep: 0, epoch: 95, acc: 0.7133333086967468, Loss 0.6691509169340134\n",
      "RNN, rep: 0, epoch: 96, acc: 0.6499999761581421, Loss 0.6810607308149338\n",
      "RNN, rep: 0, epoch: 97, acc: 0.6866664886474609, Loss 0.6826205933094025\n",
      "RNN, rep: 0, epoch: 98, acc: 0.6899999976158142, Loss 0.6787802594900131\n",
      "RNN, rep: 0, epoch: 99, acc: 0.6266666054725647, Loss 0.6821116501092911\n",
      "RNN, rep: 0, epoch: 100, acc: 0.6966666579246521, Loss 0.6689647310972213\n",
      "RNN, rep: 0, epoch: 101, acc: 0.6733332276344299, Loss 0.6678962931036949\n",
      "RNN, rep: 0, epoch: 102, acc: 0.676666796207428, Loss 0.6704481667280198\n",
      "RNN, rep: 0, epoch: 103, acc: 0.7033334374427795, Loss 0.6684492367506027\n",
      "RNN, rep: 0, epoch: 104, acc: 0.7033331990242004, Loss 0.6650125882029534\n",
      "RNN, rep: 0, epoch: 105, acc: 0.6333332657814026, Loss 0.6937564191222191\n",
      "RNN, rep: 0, epoch: 106, acc: 0.6766666173934937, Loss 0.6703949558734894\n",
      "RNN, rep: 0, epoch: 107, acc: 0.6566669344902039, Loss 0.6885102242231369\n",
      "RNN, rep: 0, epoch: 108, acc: 0.6666667461395264, Loss 0.6725766026973724\n",
      "RNN, rep: 0, epoch: 109, acc: 0.6633334159851074, Loss 0.678532547056675\n",
      "RNN, rep: 0, epoch: 110, acc: 0.633333146572113, Loss 0.6877159011363984\n",
      "RNN, rep: 0, epoch: 111, acc: 0.6633334159851074, Loss 0.6788569384813309\n",
      "RNN, rep: 0, epoch: 112, acc: 0.6766668558120728, Loss 0.66732441842556\n",
      "RNN, rep: 0, epoch: 113, acc: 0.6966667771339417, Loss 0.6607745650410652\n",
      "RNN, rep: 0, epoch: 114, acc: 0.6433332562446594, Loss 0.6830360627174378\n",
      "RNN, rep: 0, epoch: 115, acc: 0.690000057220459, Loss 0.6684451854228973\n",
      "RNN, rep: 0, epoch: 116, acc: 0.6733335256576538, Loss 0.6729267239570618\n",
      "RNN, rep: 0, epoch: 117, acc: 0.7100001573562622, Loss 0.6626474887132645\n",
      "RNN, rep: 0, epoch: 118, acc: 0.7000000476837158, Loss 0.6665514105558396\n",
      "RNN, rep: 0, epoch: 119, acc: 0.6433334350585938, Loss 0.6756077039241791\n",
      "RNN, rep: 0, epoch: 120, acc: 0.6866666674613953, Loss 0.6624549609422684\n",
      "RNN, rep: 0, epoch: 121, acc: 0.6633333563804626, Loss 0.6738988977670669\n",
      "RNN, rep: 0, epoch: 122, acc: 0.6399999856948853, Loss 0.675229429602623\n",
      "RNN, rep: 0, epoch: 123, acc: 0.6933333873748779, Loss 0.670107580423355\n",
      "RNN, rep: 0, epoch: 124, acc: 0.6233334541320801, Loss 0.6734946829080581\n",
      "RNN, rep: 0, epoch: 125, acc: 0.6833333373069763, Loss 0.6671907806396484\n",
      "RNN, rep: 0, epoch: 126, acc: 0.6633331775665283, Loss 0.6688693225383758\n",
      "RNN, rep: 0, epoch: 127, acc: 0.6999998688697815, Loss 0.6651525574922562\n",
      "RNN, rep: 0, epoch: 128, acc: 0.669999897480011, Loss 0.6692030090093612\n",
      "RNN, rep: 0, epoch: 129, acc: 0.6933331489562988, Loss 0.6671120303869248\n",
      "RNN, rep: 0, epoch: 130, acc: 0.6800000667572021, Loss 0.6708661073446274\n",
      "RNN, rep: 0, epoch: 131, acc: 0.7099999189376831, Loss 0.6628041446208954\n",
      "RNN, rep: 0, epoch: 132, acc: 0.643333375453949, Loss 0.6740728715062141\n",
      "RNN, rep: 0, epoch: 133, acc: 0.6966665387153625, Loss 0.6624081918597221\n",
      "RNN, rep: 0, epoch: 134, acc: 0.6833333373069763, Loss 0.6674267119169235\n",
      "RNN, rep: 0, epoch: 135, acc: 0.6399999260902405, Loss 0.6787227615714073\n",
      "RNN, rep: 0, epoch: 136, acc: 0.6400001049041748, Loss 0.6734415453672409\n",
      "RNN, rep: 0, epoch: 137, acc: 0.676666796207428, Loss 0.6683931934833527\n",
      "RNN, rep: 0, epoch: 138, acc: 0.646666944026947, Loss 0.6754096513986587\n",
      "RNN, rep: 0, epoch: 139, acc: 0.6600003838539124, Loss 0.6695936012268067\n",
      "RNN, rep: 0, epoch: 140, acc: 0.6566666960716248, Loss 0.6746014738082886\n",
      "RNN, rep: 0, epoch: 141, acc: 0.6633332967758179, Loss 0.6658962744474411\n",
      "RNN, rep: 0, epoch: 142, acc: 0.7000002264976501, Loss 0.6621681752800942\n",
      "RNN, rep: 0, epoch: 143, acc: 0.7066667079925537, Loss 0.6639927011728287\n",
      "RNN, rep: 0, epoch: 144, acc: 0.6299999952316284, Loss 0.6803471291065216\n",
      "RNN, rep: 0, epoch: 145, acc: 0.6566668748855591, Loss 0.6711934953927994\n",
      "RNN, rep: 0, epoch: 146, acc: 0.7266668081283569, Loss 0.6532143449783325\n",
      "RNN, rep: 0, epoch: 147, acc: 0.6666667461395264, Loss 0.6708294463157654\n",
      "RNN, rep: 0, epoch: 148, acc: 0.6766668558120728, Loss 0.6577807497978211\n",
      "RNN, rep: 0, epoch: 149, acc: 0.6666668653488159, Loss 0.6687743389606475\n",
      "RNN, rep: 0, epoch: 150, acc: 0.6899999380111694, Loss 0.6640074825286866\n",
      "RNN, rep: 0, epoch: 151, acc: 0.653333306312561, Loss 0.6814605689048767\n",
      "RNN, rep: 0, epoch: 152, acc: 0.7300000786781311, Loss 0.6524596893787384\n",
      "RNN, rep: 0, epoch: 153, acc: 0.7066666483879089, Loss 0.6516045930981637\n",
      "RNN, rep: 0, epoch: 154, acc: 0.6800000071525574, Loss 0.6689078837633133\n",
      "RNN, rep: 0, epoch: 155, acc: 0.6933332085609436, Loss 0.6647933533787728\n",
      "RNN, rep: 0, epoch: 156, acc: 0.7099998593330383, Loss 0.6668396630883217\n",
      "RNN, rep: 0, epoch: 157, acc: 0.6966666579246521, Loss 0.6605033984780312\n",
      "RNN, rep: 0, epoch: 158, acc: 0.6533334255218506, Loss 0.6722533488273621\n",
      "RNN, rep: 0, epoch: 159, acc: 0.6466667652130127, Loss 0.672104030251503\n",
      "RNN, rep: 0, epoch: 160, acc: 0.6899999380111694, Loss 0.663804250061512\n",
      "RNN, rep: 0, epoch: 161, acc: 0.6500001549720764, Loss 0.6689361727237702\n",
      "RNN, rep: 0, epoch: 162, acc: 0.6600003242492676, Loss 0.7244405746459961\n",
      "RNN, rep: 0, epoch: 163, acc: 0.630000114440918, Loss 0.8521629810333252\n",
      "RNN, rep: 0, epoch: 164, acc: 0.6800001263618469, Loss 0.6772302496433258\n",
      "RNN, rep: 0, epoch: 165, acc: 0.6700000762939453, Loss 0.7164086699485779\n",
      "RNN, rep: 0, epoch: 166, acc: 0.6900002360343933, Loss 0.6709329074621201\n",
      "RNN, rep: 0, epoch: 167, acc: 0.6700001358985901, Loss 0.6721090477705002\n",
      "RNN, rep: 0, epoch: 168, acc: 0.6100000143051147, Loss 0.7028000873327255\n",
      "RNN, rep: 0, epoch: 169, acc: 0.6366665363311768, Loss 0.671345631480217\n",
      "RNN, rep: 0, epoch: 170, acc: 0.7133333683013916, Loss 0.6635958558320999\n",
      "RNN, rep: 0, epoch: 171, acc: 0.6666667461395264, Loss 0.6707229036092758\n",
      "RNN, rep: 0, epoch: 172, acc: 0.6133334636688232, Loss 0.6800479710102081\n",
      "RNN, rep: 0, epoch: 173, acc: 0.6966665387153625, Loss 0.6630755519866943\n",
      "RNN, rep: 0, epoch: 174, acc: 0.706666886806488, Loss 0.6557528483867645\n",
      "RNN, rep: 0, epoch: 175, acc: 0.6733332872390747, Loss 0.6772672334313392\n",
      "RNN, rep: 0, epoch: 176, acc: 0.6400002837181091, Loss 0.6720965266227722\n",
      "RNN, rep: 0, epoch: 177, acc: 0.6666667461395264, Loss 0.6684672617912293\n",
      "RNN, rep: 0, epoch: 178, acc: 0.6500002145767212, Loss 0.6748399311304092\n",
      "RNN, rep: 0, epoch: 179, acc: 0.6466668844223022, Loss 0.6669394409656525\n",
      "RNN, rep: 0, epoch: 180, acc: 0.6833332777023315, Loss 0.6688901954889297\n",
      "RNN, rep: 0, epoch: 181, acc: 0.7066667675971985, Loss 0.6575195533037186\n",
      "RNN, rep: 0, epoch: 182, acc: 0.7033333778381348, Loss 0.6593631505966187\n",
      "RNN, rep: 0, epoch: 183, acc: 0.6566666960716248, Loss 0.6746642583608627\n",
      "RNN, rep: 0, epoch: 184, acc: 0.6966667175292969, Loss 0.6682460886240006\n",
      "RNN, rep: 0, epoch: 185, acc: 0.699999988079071, Loss 0.661946928203106\n",
      "RNN, rep: 0, epoch: 186, acc: 0.7133333683013916, Loss 0.6601972681283951\n",
      "RNN, rep: 0, epoch: 187, acc: 0.6666666269302368, Loss 0.6715150544047356\n",
      "RNN, rep: 0, epoch: 188, acc: 0.6800000071525574, Loss 0.6698853686451912\n",
      "RNN, rep: 0, epoch: 189, acc: 0.6599999070167542, Loss 0.6707915109395981\n",
      "RNN, rep: 0, epoch: 190, acc: 0.6933332681655884, Loss 0.658169336616993\n",
      "RNN, rep: 0, epoch: 191, acc: 0.6800002455711365, Loss 0.6719181284308433\n",
      "RNN, rep: 0, epoch: 192, acc: 0.6600000858306885, Loss 0.6630029821395874\n",
      "RNN, rep: 0, epoch: 193, acc: 0.6466668844223022, Loss 0.6769851046800613\n",
      "RNN, rep: 0, epoch: 194, acc: 0.6833334565162659, Loss 0.6700479155778885\n",
      "RNN, rep: 0, epoch: 195, acc: 0.6466667652130127, Loss 0.6708217674493789\n",
      "RNN, rep: 0, epoch: 196, acc: 0.6466665863990784, Loss 0.6723231816291809\n",
      "RNN, rep: 0, epoch: 197, acc: 0.6733332276344299, Loss 0.6673570668697357\n",
      "RNN, rep: 0, epoch: 198, acc: 0.6866666674613953, Loss 0.6623080489039421\n",
      "RNN, rep: 0, epoch: 199, acc: 0.6133332252502441, Loss 0.675402717590332\n",
      "RNN, rep: 0, epoch: 200, acc: 0.5933336019515991, Loss 0.6794986546039581\n",
      "RNN, rep: 0, epoch: 201, acc: 0.6866668462753296, Loss 0.6675493752956391\n",
      "RNN, rep: 0, epoch: 202, acc: 0.6933333873748779, Loss 0.6621681413054467\n",
      "RNN, rep: 0, epoch: 203, acc: 0.6600000858306885, Loss 0.6725065809488296\n",
      "RNN, rep: 0, epoch: 204, acc: 0.6833332180976868, Loss 0.6641043919324875\n",
      "RNN, rep: 0, epoch: 205, acc: 0.6666669249534607, Loss 0.6937189847230911\n",
      "RNN, rep: 0, epoch: 206, acc: 0.7233334183692932, Loss 0.6642508962750435\n",
      "RNN, rep: 0, epoch: 207, acc: 0.6433331370353699, Loss 0.7045973920822144\n",
      "RNN, rep: 0, epoch: 208, acc: 0.6566665768623352, Loss 0.6846226292848587\n",
      "RNN, rep: 0, epoch: 209, acc: 0.6366666555404663, Loss 0.6722672498226165\n",
      "RNN, rep: 0, epoch: 210, acc: 0.6966665387153625, Loss 0.670903645157814\n",
      "RNN, rep: 0, epoch: 211, acc: 0.6633333563804626, Loss 0.6955102336406708\n",
      "RNN, rep: 0, epoch: 212, acc: 0.6266667246818542, Loss 0.673859030008316\n",
      "RNN, rep: 0, epoch: 213, acc: 0.6800000071525574, Loss 0.6689147341251374\n",
      "RNN, rep: 0, epoch: 214, acc: 0.6500001549720764, Loss 0.6757676100730896\n",
      "RNN, rep: 0, epoch: 215, acc: 0.6966666579246521, Loss 0.6659164690971374\n",
      "RNN, rep: 0, epoch: 216, acc: 0.6700000166893005, Loss 0.6676652789115906\n",
      "RNN, rep: 0, epoch: 217, acc: 0.6766667366027832, Loss 0.6678661251068115\n",
      "RNN, rep: 0, epoch: 218, acc: 0.6566668152809143, Loss 0.672496167421341\n",
      "RNN, rep: 0, epoch: 219, acc: 0.653333306312561, Loss 0.6689421513676643\n",
      "RNN, rep: 0, epoch: 220, acc: 0.6700000166893005, Loss 0.6659840506315231\n",
      "RNN, rep: 0, epoch: 221, acc: 0.6533331274986267, Loss 0.6774760055541992\n",
      "RNN, rep: 0, epoch: 222, acc: 0.643333375453949, Loss 0.6762094306945801\n",
      "RNN, rep: 0, epoch: 223, acc: 0.6500000953674316, Loss 0.6712707090377807\n",
      "RNN, rep: 0, epoch: 224, acc: 0.669999897480011, Loss 0.6683213448524475\n",
      "RNN, rep: 0, epoch: 225, acc: 0.6599998474121094, Loss 0.6702475714683532\n",
      "RNN, rep: 0, epoch: 226, acc: 0.6666668653488159, Loss 0.669661363363266\n",
      "RNN, rep: 0, epoch: 227, acc: 0.6766669750213623, Loss 0.6681305354833603\n",
      "RNN, rep: 0, epoch: 228, acc: 0.6900001764297485, Loss 0.6629606580734253\n",
      "RNN, rep: 0, epoch: 229, acc: 0.6766670346260071, Loss 0.6700028324127197\n",
      "RNN, rep: 0, epoch: 230, acc: 0.6933335065841675, Loss 0.6641559755802154\n",
      "RNN, rep: 0, epoch: 231, acc: 0.6300001740455627, Loss 0.6788439416885376\n",
      "RNN, rep: 0, epoch: 232, acc: 0.666666567325592, Loss 0.6708967810869217\n",
      "RNN, rep: 0, epoch: 233, acc: 0.6766664981842041, Loss 0.66870485663414\n",
      "RNN, rep: 0, epoch: 234, acc: 0.6800000071525574, Loss 0.6793056625127792\n",
      "RNN, rep: 0, epoch: 235, acc: 0.6700000166893005, Loss 0.6639083206653595\n",
      "RNN, rep: 0, epoch: 236, acc: 0.6299998760223389, Loss 0.6814601272344589\n",
      "RNN, rep: 0, epoch: 237, acc: 0.6866668462753296, Loss 0.6640284597873688\n",
      "RNN, rep: 0, epoch: 238, acc: 0.6833333373069763, Loss 0.6643489402532577\n",
      "RNN, rep: 0, epoch: 239, acc: 0.7099999189376831, Loss 0.6977060291171074\n",
      "RNN, rep: 0, epoch: 240, acc: 0.6366667747497559, Loss 0.7015415194630623\n",
      "RNN, rep: 0, epoch: 241, acc: 0.6466668844223022, Loss 0.7268182361125946\n",
      "RNN, rep: 0, epoch: 242, acc: 0.6666668653488159, Loss 0.6972188776731492\n",
      "RNN, rep: 0, epoch: 243, acc: 0.6900001764297485, Loss 0.6649489164352417\n",
      "RNN, rep: 0, epoch: 244, acc: 0.6733336448669434, Loss 0.6692375594377518\n",
      "RNN, rep: 0, epoch: 245, acc: 0.6700000762939453, Loss 0.6738827466964722\n",
      "RNN, rep: 0, epoch: 246, acc: 0.6499999761581421, Loss 0.6691112703084946\n",
      "RNN, rep: 0, epoch: 247, acc: 0.6466667056083679, Loss 0.6700821620225906\n",
      "RNN, rep: 0, epoch: 248, acc: 0.7066667079925537, Loss 0.6640313351154328\n",
      "RNN, rep: 0, epoch: 249, acc: 0.6833334565162659, Loss 0.6600727039575577\n",
      "RNN, rep: 0, epoch: 250, acc: 0.6899999976158142, Loss 0.6782994067668915\n",
      "RNN, rep: 0, epoch: 251, acc: 0.6933332681655884, Loss 0.6704474955797195\n",
      "RNN, rep: 0, epoch: 252, acc: 0.6833332777023315, Loss 0.6567464652657509\n",
      "RNN, rep: 0, epoch: 253, acc: 0.7133333683013916, Loss 0.6492767626047135\n",
      "RNN, rep: 0, epoch: 254, acc: 0.6733333468437195, Loss 0.6754361698031426\n",
      "RNN, rep: 0, epoch: 255, acc: 0.6933335065841675, Loss 0.700468377172947\n",
      "RNN, rep: 0, epoch: 256, acc: 0.7233335375785828, Loss 0.6482008510828018\n",
      "RNN, rep: 0, epoch: 257, acc: 0.7166666388511658, Loss 0.6554661831259727\n",
      "RNN, rep: 0, epoch: 258, acc: 0.6333333849906921, Loss 0.6995025372505188\n",
      "RNN, rep: 0, epoch: 259, acc: 0.7066667675971985, Loss 0.6644059005379677\n",
      "RNN, rep: 0, epoch: 260, acc: 0.7266666293144226, Loss 0.6548117393255234\n",
      "RNN, rep: 0, epoch: 261, acc: 0.6666667461395264, Loss 0.6956987795233727\n",
      "RNN, rep: 0, epoch: 262, acc: 0.6233335733413696, Loss 0.6952539247274399\n",
      "RNN, rep: 0, epoch: 263, acc: 0.6566668152809143, Loss 0.6929370474815368\n",
      "RNN, rep: 0, epoch: 264, acc: 0.6466667056083679, Loss 0.6808411449193954\n",
      "RNN, rep: 0, epoch: 265, acc: 0.6733334064483643, Loss 0.6654329544305801\n",
      "RNN, rep: 0, epoch: 266, acc: 0.6733333468437195, Loss 0.6663050833344459\n",
      "RNN, rep: 0, epoch: 267, acc: 0.68666672706604, Loss 0.6643013352155686\n",
      "RNN, rep: 0, epoch: 268, acc: 0.6766667366027832, Loss 0.6649275469779968\n",
      "RNN, rep: 0, epoch: 269, acc: 0.6366670727729797, Loss 0.6748859024047852\n",
      "RNN, rep: 0, epoch: 270, acc: 0.7066666483879089, Loss 0.6585570386052132\n",
      "RNN, rep: 0, epoch: 271, acc: 0.6633333563804626, Loss 0.6707191318273544\n",
      "RNN, rep: 0, epoch: 272, acc: 0.7033331394195557, Loss 0.6798518931865692\n",
      "RNN, rep: 0, epoch: 273, acc: 0.5799998641014099, Loss 1.0700314149260521\n",
      "RNN, rep: 0, epoch: 274, acc: 0.7033334374427795, Loss 0.6621965193748474\n",
      "RNN, rep: 0, epoch: 275, acc: 0.6800001263618469, Loss 0.6692144280672073\n",
      "RNN, rep: 0, epoch: 276, acc: 0.6466667056083679, Loss 0.6726810082793235\n",
      "RNN, rep: 0, epoch: 277, acc: 0.6533331274986267, Loss 0.661409977376461\n",
      "RNN, rep: 0, epoch: 278, acc: 0.6866666674613953, Loss 0.6582703140377998\n",
      "RNN, rep: 0, epoch: 279, acc: 0.6866666674613953, Loss 0.6664126914739609\n",
      "RNN, rep: 0, epoch: 280, acc: 0.6466667652130127, Loss 0.6734474435448646\n",
      "RNN, rep: 0, epoch: 281, acc: 0.6633333563804626, Loss 0.6716761374473572\n",
      "RNN, rep: 0, epoch: 282, acc: 0.6699998378753662, Loss 0.6639623972773552\n",
      "RNN, rep: 0, epoch: 283, acc: 0.6399998664855957, Loss 0.6789350438117981\n",
      "RNN, rep: 0, epoch: 284, acc: 0.7099999189376831, Loss 0.644528201520443\n",
      "RNN, rep: 0, epoch: 285, acc: 0.6800000071525574, Loss 0.6593436470627785\n",
      "RNN, rep: 0, epoch: 286, acc: 0.6700000762939453, Loss 0.6728288960456849\n",
      "RNN, rep: 0, epoch: 287, acc: 0.6766668558120728, Loss 0.6645081067085266\n",
      "RNN, rep: 0, epoch: 288, acc: 0.6533333659172058, Loss 0.6710276272892952\n",
      "RNN, rep: 0, epoch: 289, acc: 0.7300000786781311, Loss 0.6462042573094368\n",
      "RNN, rep: 0, epoch: 290, acc: 0.6866666674613953, Loss 0.7530190256237984\n",
      "RNN, rep: 0, epoch: 291, acc: 0.6866666674613953, Loss 0.7948730066418648\n",
      "RNN, rep: 0, epoch: 292, acc: 0.6966668963432312, Loss 0.7516562259197235\n",
      "RNN, rep: 0, epoch: 293, acc: 0.669999897480011, Loss 0.6852976679801941\n",
      "RNN, rep: 0, epoch: 294, acc: 0.7066666483879089, Loss 0.657379368841648\n",
      "RNN, rep: 0, epoch: 295, acc: 0.690000057220459, Loss 0.7373942500352859\n",
      "RNN, rep: 0, epoch: 296, acc: 0.7433333396911621, Loss 0.6345226573944092\n",
      "RNN, rep: 0, epoch: 297, acc: 0.6700001358985901, Loss 0.6690762788057327\n",
      "RNN, rep: 0, epoch: 298, acc: 0.7599999904632568, Loss 0.6229656738042831\n",
      "RNN, rep: 0, epoch: 299, acc: 0.6533331274986267, Loss 0.6838520362973213\n",
      "RNN, rep: 0, epoch: 300, acc: 0.6733332276344299, Loss 0.6865812620520592\n",
      "RNN, rep: 0, epoch: 301, acc: 0.7333331108093262, Loss 0.6286599087715149\n",
      "RNN, rep: 0, epoch: 302, acc: 0.7233332991600037, Loss 0.7098133915662765\n",
      "RNN, rep: 0, epoch: 303, acc: 0.6700000762939453, Loss 0.6946862265467644\n",
      "RNN, rep: 0, epoch: 304, acc: 0.6866666674613953, Loss 0.665862282216549\n",
      "RNN, rep: 0, epoch: 305, acc: 0.6266667246818542, Loss 0.6737981507182121\n",
      "RNN, rep: 0, epoch: 306, acc: 0.6800001263618469, Loss 0.6669186413288116\n",
      "RNN, rep: 0, epoch: 307, acc: 0.6966667175292969, Loss 0.6484287047386169\n",
      "RNN, rep: 0, epoch: 308, acc: 0.6433334946632385, Loss 0.6806032288074494\n",
      "RNN, rep: 0, epoch: 309, acc: 0.6866667866706848, Loss 0.7390522247552872\n",
      "RNN, rep: 0, epoch: 310, acc: 0.65666663646698, Loss 0.6721375066041947\n",
      "RNN, rep: 0, epoch: 311, acc: 0.6900002360343933, Loss 0.6549730309844017\n",
      "RNN, rep: 0, epoch: 312, acc: 0.7133331298828125, Loss 0.6542724134027957\n",
      "RNN, rep: 0, epoch: 313, acc: 0.7099999785423279, Loss 0.660686560869217\n",
      "RNN, rep: 0, epoch: 314, acc: 0.6566665172576904, Loss 0.7167816907167435\n",
      "RNN, rep: 0, epoch: 315, acc: 0.6866666674613953, Loss 0.6613163709640503\n",
      "RNN, rep: 0, epoch: 316, acc: 0.65666663646698, Loss 0.6728263562917709\n",
      "RNN, rep: 0, epoch: 317, acc: 0.6400001645088196, Loss 0.6694761475920677\n",
      "RNN, rep: 0, epoch: 318, acc: 0.6700001358985901, Loss 0.6706321209669113\n",
      "RNN, rep: 0, epoch: 319, acc: 0.6666667461395264, Loss 0.665920299589634\n",
      "RNN, rep: 0, epoch: 320, acc: 0.7000000476837158, Loss 0.6620401096343994\n",
      "RNN, rep: 0, epoch: 321, acc: 0.6900002360343933, Loss 0.6594430023431778\n",
      "RNN, rep: 0, epoch: 322, acc: 0.6933332681655884, Loss 0.6618632933497429\n",
      "RNN, rep: 0, epoch: 323, acc: 0.6600000262260437, Loss 0.6729184579849243\n",
      "RNN, rep: 0, epoch: 324, acc: 0.6966667175292969, Loss 0.6584938481450081\n",
      "RNN, rep: 0, epoch: 325, acc: 0.6833332180976868, Loss 0.6628212693333626\n",
      "RNN, rep: 0, epoch: 326, acc: 0.6333330869674683, Loss 0.6685310834646225\n",
      "RNN, rep: 0, epoch: 327, acc: 0.7000000476837158, Loss 0.6586427760124206\n",
      "RNN, rep: 0, epoch: 328, acc: 0.6700000762939453, Loss 0.6564940722286701\n",
      "RNN, rep: 0, epoch: 329, acc: 0.7166664600372314, Loss 0.6420304360985756\n",
      "RNN, rep: 0, epoch: 330, acc: 0.6699998378753662, Loss 0.6688172852993012\n",
      "RNN, rep: 0, epoch: 331, acc: 0.73333340883255, Loss 0.6266212487220764\n",
      "RNN, rep: 0, epoch: 332, acc: 0.7099999785423279, Loss 0.6531255900859833\n",
      "RNN, rep: 0, epoch: 333, acc: 0.6966665387153625, Loss 0.653430660367012\n",
      "RNN, rep: 0, epoch: 334, acc: 0.7000000476837158, Loss 0.6596550950407982\n",
      "RNN, rep: 0, epoch: 335, acc: 0.75, Loss 0.6364599120616913\n",
      "RNN, rep: 0, epoch: 336, acc: 0.73333340883255, Loss 0.6378896842896938\n",
      "RNN, rep: 0, epoch: 337, acc: 0.7266665697097778, Loss 0.6385311655700207\n",
      "RNN, rep: 0, epoch: 338, acc: 0.6899999976158142, Loss 0.6410927273333072\n",
      "RNN, rep: 0, epoch: 339, acc: 0.7100000977516174, Loss 0.6378026176989079\n",
      "RNN, rep: 0, epoch: 340, acc: 0.6700000166893005, Loss 0.6652597051858902\n",
      "RNN, rep: 0, epoch: 341, acc: 0.6666665077209473, Loss 0.6753100453317166\n",
      "RNN, rep: 0, epoch: 342, acc: 0.6433334350585938, Loss 0.668688233345747\n",
      "RNN, rep: 0, epoch: 343, acc: 0.7066667675971985, Loss 0.6611345311999322\n",
      "RNN, rep: 0, epoch: 344, acc: 0.6866664886474609, Loss 0.6581853429973126\n",
      "RNN, rep: 0, epoch: 345, acc: 0.7300000190734863, Loss 0.6423608912527561\n",
      "RNN, rep: 0, epoch: 346, acc: 0.6133331060409546, Loss 0.6865406975150108\n",
      "RNN, rep: 0, epoch: 347, acc: 0.6666666269302368, Loss 0.6635801461338997\n",
      "RNN, rep: 0, epoch: 348, acc: 0.6899999380111694, Loss 0.6732195377349853\n",
      "RNN, rep: 0, epoch: 349, acc: 0.7000001668930054, Loss 0.6784735722839832\n",
      "RNN, rep: 0, epoch: 350, acc: 0.6699998378753662, Loss 0.6682128438353538\n",
      "RNN, rep: 0, epoch: 351, acc: 0.6500000953674316, Loss 0.7160965298116208\n",
      "RNN, rep: 0, epoch: 352, acc: 0.65666663646698, Loss 0.8038708655536175\n",
      "RNN, rep: 0, epoch: 353, acc: 0.6266664862632751, Loss 0.8283380550146103\n",
      "RNN, rep: 0, epoch: 354, acc: 0.7033333778381348, Loss 0.7537677183747291\n",
      "RNN, rep: 0, epoch: 355, acc: 0.6533331871032715, Loss 0.8040362814068794\n",
      "RNN, rep: 0, epoch: 356, acc: 0.7100000977516174, Loss 0.6916338382661342\n",
      "RNN, rep: 0, epoch: 357, acc: 0.6999999284744263, Loss 0.6586473113298417\n",
      "RNN, rep: 0, epoch: 358, acc: 0.6666668653488159, Loss 0.7543041297793388\n",
      "RNN, rep: 0, epoch: 359, acc: 0.6433331370353699, Loss 0.7907166297733784\n",
      "RNN, rep: 0, epoch: 360, acc: 0.6499998569488525, Loss 0.7884417712688446\n",
      "RNN, rep: 0, epoch: 361, acc: 0.7233333587646484, Loss 0.6994419156014919\n",
      "RNN, rep: 0, epoch: 362, acc: 0.6533334255218506, Loss 0.8504510356485844\n",
      "RNN, rep: 0, epoch: 363, acc: 0.6066666841506958, Loss 0.9782275059819221\n",
      "RNN, rep: 0, epoch: 364, acc: 0.6700001358985901, Loss 0.6843576131761074\n",
      "RNN, rep: 0, epoch: 365, acc: 0.7066667675971985, Loss 0.6384847357869148\n",
      "RNN, rep: 0, epoch: 366, acc: 0.7033332586288452, Loss 0.6486635343730449\n",
      "RNN, rep: 0, epoch: 367, acc: 0.7300000786781311, Loss 0.6224829870462417\n",
      "RNN, rep: 0, epoch: 368, acc: 0.6899999380111694, Loss 0.6417375481128693\n",
      "RNN, rep: 0, epoch: 369, acc: 0.6900001764297485, Loss 0.634246211796999\n",
      "RNN, rep: 0, epoch: 370, acc: 0.7366668581962585, Loss 0.61187524035573\n",
      "RNN, rep: 0, epoch: 371, acc: 0.753333330154419, Loss 0.6202204123139381\n",
      "RNN, rep: 0, epoch: 372, acc: 0.7466664910316467, Loss 0.6099495968222618\n",
      "RNN, rep: 0, epoch: 373, acc: 0.7633333802223206, Loss 0.567914929240942\n",
      "RNN, rep: 0, epoch: 374, acc: 0.6966665387153625, Loss 0.6157725214958191\n",
      "RNN, rep: 0, epoch: 375, acc: 0.7433334589004517, Loss 0.6150114472210407\n",
      "RNN, rep: 0, epoch: 376, acc: 0.7533332109451294, Loss 0.6056592011451721\n",
      "RNN, rep: 0, epoch: 377, acc: 0.7700001001358032, Loss 0.6048570378124714\n",
      "RNN, rep: 0, epoch: 378, acc: 0.7833333015441895, Loss 0.5913607235252857\n",
      "RNN, rep: 0, epoch: 379, acc: 0.7433334589004517, Loss 0.6385572466254235\n",
      "RNN, rep: 0, epoch: 380, acc: 0.7666668891906738, Loss 0.5978406834602356\n",
      "RNN, rep: 0, epoch: 381, acc: 0.7966667413711548, Loss 0.5593565680086613\n",
      "RNN, rep: 0, epoch: 382, acc: 0.7099999785423279, Loss 0.629042099416256\n",
      "RNN, rep: 0, epoch: 383, acc: 0.7766667008399963, Loss 0.5610153114795685\n",
      "RNN, rep: 0, epoch: 384, acc: 0.7933333516120911, Loss 0.5609844282269478\n",
      "RNN, rep: 0, epoch: 385, acc: 0.7066665887832642, Loss 0.688119964748621\n",
      "RNN, rep: 0, epoch: 386, acc: 0.699999988079071, Loss 0.7318081185221672\n",
      "RNN, rep: 0, epoch: 387, acc: 0.7099998593330383, Loss 0.665763439387083\n",
      "RNN, rep: 0, epoch: 388, acc: 0.676666796207428, Loss 0.6860857467353344\n",
      "RNN, rep: 0, epoch: 389, acc: 0.6599999070167542, Loss 0.6957389476895333\n",
      "RNN, rep: 0, epoch: 390, acc: 0.6966666579246521, Loss 0.6608325427770615\n",
      "RNN, rep: 0, epoch: 391, acc: 0.7199998497962952, Loss 0.650345468968153\n",
      "RNN, rep: 0, epoch: 392, acc: 0.6300000548362732, Loss 0.722024620771408\n",
      "RNN, rep: 0, epoch: 393, acc: 0.6866667866706848, Loss 0.6728784084320069\n",
      "RNN, rep: 0, epoch: 394, acc: 0.6966667771339417, Loss 0.655344423353672\n",
      "RNN, rep: 0, epoch: 395, acc: 0.6400002837181091, Loss 0.7048151896893978\n",
      "RNN, rep: 0, epoch: 396, acc: 0.6633335947990417, Loss 0.6724769802391529\n",
      "RNN, rep: 0, epoch: 397, acc: 0.7000002861022949, Loss 0.6656367759406566\n",
      "RNN, rep: 0, epoch: 398, acc: 0.7366665601730347, Loss 0.6347261394560337\n",
      "RNN, rep: 0, epoch: 399, acc: 0.7066665887832642, Loss 0.6494096313416958\n",
      "RNN, rep: 0, epoch: 400, acc: 0.690000057220459, Loss 0.7082370075583458\n",
      "RNN, rep: 0, epoch: 401, acc: 0.7166666984558105, Loss 0.668344202786684\n",
      "RNN, rep: 0, epoch: 402, acc: 0.68666672706604, Loss 0.6665154863893986\n",
      "RNN, rep: 0, epoch: 403, acc: 0.6933332085609436, Loss 0.6579410248994827\n",
      "RNN, rep: 0, epoch: 404, acc: 0.7133333683013916, Loss 0.6412467011809349\n",
      "RNN, rep: 0, epoch: 405, acc: 0.7966667413711548, Loss 0.6001586213707923\n",
      "RNN, rep: 0, epoch: 406, acc: 0.7966666221618652, Loss 0.5974258537590503\n",
      "RNN, rep: 0, epoch: 407, acc: 0.7833333611488342, Loss 0.6008422033488751\n",
      "RNN, rep: 0, epoch: 408, acc: 0.7499999403953552, Loss 0.7067062832415104\n",
      "RNN, rep: 0, epoch: 409, acc: 0.7366665005683899, Loss 0.7284730120003223\n",
      "RNN, rep: 0, epoch: 410, acc: 0.736666738986969, Loss 0.6967580763995648\n",
      "RNN, rep: 0, epoch: 411, acc: 0.8166667819023132, Loss 0.6227735197544098\n",
      "RNN, rep: 0, epoch: 412, acc: 0.7599999308586121, Loss 0.6877332003414631\n",
      "RNN, rep: 0, epoch: 413, acc: 0.6899999380111694, Loss 0.7677351635694504\n",
      "RNN, rep: 0, epoch: 414, acc: 0.7533332705497742, Loss 0.6797392401099205\n",
      "RNN, rep: 0, epoch: 415, acc: 0.7866665720939636, Loss 0.6491499383747578\n",
      "RNN, rep: 0, epoch: 416, acc: 0.7833335399627686, Loss 0.6204734785854816\n",
      "RNN, rep: 0, epoch: 417, acc: 0.7566667199134827, Loss 0.6968768875300885\n",
      "RNN, rep: 0, epoch: 418, acc: 0.7333333492279053, Loss 0.6883537094295025\n",
      "RNN, rep: 0, epoch: 419, acc: 0.7399999499320984, Loss 0.7034082792699337\n",
      "RNN, rep: 0, epoch: 420, acc: 0.8100000023841858, Loss 0.5907042247056961\n",
      "RNN, rep: 0, epoch: 421, acc: 0.7866666913032532, Loss 0.6094189900159835\n",
      "RNN, rep: 0, epoch: 422, acc: 0.8333333730697632, Loss 0.5070315578579903\n",
      "RNN, rep: 0, epoch: 423, acc: 0.7699999809265137, Loss 0.5709645214676857\n",
      "RNN, rep: 0, epoch: 424, acc: 0.8500000834465027, Loss 0.4902942131459713\n",
      "RNN, rep: 0, epoch: 425, acc: 0.8666664361953735, Loss 0.4627976393699646\n",
      "RNN, rep: 0, epoch: 426, acc: 0.7933335304260254, Loss 0.5155545919388532\n",
      "RNN, rep: 0, epoch: 427, acc: 0.8299999833106995, Loss 0.5075071997195483\n",
      "RNN, rep: 0, epoch: 428, acc: 0.8899999856948853, Loss 0.42726376667618754\n",
      "RNN, rep: 0, epoch: 429, acc: 0.8433333039283752, Loss 0.46404888920485976\n",
      "RNN, rep: 0, epoch: 430, acc: 0.7933332920074463, Loss 0.5405412036925554\n",
      "RNN, rep: 0, epoch: 431, acc: 0.8066667914390564, Loss 0.5531270199269056\n",
      "RNN, rep: 0, epoch: 432, acc: 0.9066666960716248, Loss 0.38718711577355863\n",
      "RNN, rep: 0, epoch: 433, acc: 0.8366667032241821, Loss 0.48748615249991417\n",
      "RNN, rep: 0, epoch: 434, acc: 0.7833333611488342, Loss 0.5693886943906545\n",
      "RNN, rep: 0, epoch: 435, acc: 0.8333334922790527, Loss 0.4724508960545063\n",
      "RNN, rep: 0, epoch: 436, acc: 0.8066665530204773, Loss 0.5199712123349309\n",
      "RNN, rep: 0, epoch: 437, acc: 0.8266666531562805, Loss 0.5101081543415785\n",
      "RNN, rep: 0, epoch: 438, acc: 0.8333332538604736, Loss 0.487695594355464\n",
      "RNN, rep: 0, epoch: 439, acc: 0.8333333730697632, Loss 0.47440436042845247\n",
      "RNN, rep: 0, epoch: 440, acc: 0.7933332920074463, Loss 0.5039854220300913\n",
      "RNN, rep: 0, epoch: 441, acc: 0.8266667723655701, Loss 0.4871587835624814\n",
      "RNN, rep: 0, epoch: 442, acc: 0.7099998593330383, Loss 0.7138207332789898\n",
      "RNN, rep: 0, epoch: 443, acc: 0.7866666913032532, Loss 0.6400454622134566\n",
      "RNN, rep: 0, epoch: 444, acc: 0.7933332920074463, Loss 0.5373491707071661\n",
      "RNN, rep: 0, epoch: 445, acc: 0.8533334136009216, Loss 0.4695451870188117\n",
      "RNN, rep: 0, epoch: 446, acc: 0.8366668224334717, Loss 0.472850000038743\n",
      "RNN, rep: 0, epoch: 447, acc: 0.8733334541320801, Loss 0.4320304375886917\n",
      "RNN, rep: 0, epoch: 448, acc: 0.8600000739097595, Loss 0.4560264377295971\n",
      "RNN, rep: 0, epoch: 449, acc: 0.833333432674408, Loss 0.4629056328907609\n",
      "RNN, rep: 0, epoch: 450, acc: 0.8333334922790527, Loss 0.47042240627110005\n",
      "RNN, rep: 0, epoch: 451, acc: 0.7900001406669617, Loss 0.5306122399121523\n",
      "RNN, rep: 0, epoch: 452, acc: 0.8333333730697632, Loss 0.47874155439436433\n",
      "RNN, rep: 0, epoch: 453, acc: 0.8233334422111511, Loss 0.4886304695159197\n",
      "RNN, rep: 0, epoch: 454, acc: 0.8299999833106995, Loss 0.47739144928753374\n",
      "RNN, rep: 0, epoch: 455, acc: 0.8766668438911438, Loss 0.4291516846977174\n",
      "RNN, rep: 0, epoch: 456, acc: 0.7633333802223206, Loss 0.5641295344568789\n",
      "RNN, rep: 0, epoch: 457, acc: 0.8100000023841858, Loss 0.5154464733414352\n",
      "RNN, rep: 0, epoch: 458, acc: 0.8366667032241821, Loss 0.4580317695438862\n",
      "RNN, rep: 0, epoch: 459, acc: 0.7900000214576721, Loss 0.530680640283972\n",
      "RNN, rep: 0, epoch: 460, acc: 0.8366667032241821, Loss 0.5001568573340773\n",
      "RNN, rep: 0, epoch: 461, acc: 0.8699999451637268, Loss 0.434893770813942\n",
      "RNN, rep: 0, epoch: 462, acc: 0.8399999737739563, Loss 0.4701265203766525\n",
      "RNN, rep: 0, epoch: 463, acc: 0.830000102519989, Loss 0.48187580993399026\n",
      "RNN, rep: 0, epoch: 464, acc: 0.8966667056083679, Loss 0.3804903222434223\n",
      "RNN, rep: 0, epoch: 465, acc: 0.8366668224334717, Loss 0.4770792842656374\n",
      "RNN, rep: 0, epoch: 466, acc: 0.8700000643730164, Loss 0.4206793424300849\n",
      "RNN, rep: 0, epoch: 467, acc: 0.8033332824707031, Loss 0.5228397875651717\n",
      "RNN, rep: 0, epoch: 468, acc: 0.8366666436195374, Loss 0.4404173816367984\n",
      "RNN, rep: 0, epoch: 469, acc: 0.7733333706855774, Loss 0.5341843070089817\n",
      "RNN, rep: 0, epoch: 470, acc: 0.7966665625572205, Loss 0.5292537790723145\n",
      "RNN, rep: 0, epoch: 471, acc: 0.8133334517478943, Loss 0.4961847281642258\n",
      "RNN, rep: 0, epoch: 472, acc: 0.8166666626930237, Loss 0.5144494418054819\n",
      "RNN, rep: 0, epoch: 473, acc: 0.8400000929832458, Loss 0.48739341393113134\n",
      "RNN, rep: 0, epoch: 474, acc: 0.8666666150093079, Loss 0.4401398593559861\n",
      "RNN, rep: 0, epoch: 475, acc: 0.8666666150093079, Loss 0.4141777738556266\n",
      "RNN, rep: 0, epoch: 476, acc: 0.8166666626930237, Loss 0.49895884638652205\n",
      "RNN, rep: 0, epoch: 477, acc: 0.876666784286499, Loss 0.3894300806336105\n",
      "RNN, rep: 0, epoch: 478, acc: 0.8866667151451111, Loss 0.37680736104026435\n",
      "RNN, rep: 0, epoch: 479, acc: 0.8433334231376648, Loss 0.4700341942161322\n",
      "RNN, rep: 0, epoch: 480, acc: 0.7933333516120911, Loss 0.5357746016792952\n",
      "RNN, rep: 0, epoch: 481, acc: 0.8133335113525391, Loss 0.48616851690225305\n",
      "RNN, rep: 0, epoch: 482, acc: 0.8399999141693115, Loss 0.46504729480482637\n",
      "RNN, rep: 0, epoch: 483, acc: 0.8533334136009216, Loss 0.4410197358764708\n",
      "RNN, rep: 0, epoch: 484, acc: 0.8600000739097595, Loss 0.4047364285681397\n",
      "RNN, rep: 0, epoch: 485, acc: 0.8533334136009216, Loss 0.42528603069484233\n",
      "RNN, rep: 0, epoch: 486, acc: 0.8266667127609253, Loss 0.4726927116885781\n",
      "RNN, rep: 0, epoch: 487, acc: 0.8833334445953369, Loss 0.36783723093569276\n",
      "RNN, rep: 0, epoch: 488, acc: 0.816666841506958, Loss 0.5017658691294491\n",
      "RNN, rep: 0, epoch: 489, acc: 0.8266666531562805, Loss 0.47019485528580846\n",
      "RNN, rep: 0, epoch: 490, acc: 0.8666667342185974, Loss 0.4236201138608158\n",
      "RNN, rep: 0, epoch: 491, acc: 0.8933332562446594, Loss 0.3581021852884442\n",
      "RNN, rep: 0, epoch: 492, acc: 0.8333333730697632, Loss 0.4703280400764197\n",
      "RNN, rep: 0, epoch: 493, acc: 0.84333336353302, Loss 0.42661438826471565\n",
      "RNN, rep: 0, epoch: 494, acc: 0.90666663646698, Loss 0.3289670364186168\n",
      "RNN, rep: 0, epoch: 495, acc: 0.8633334636688232, Loss 0.3996801791526377\n",
      "RNN, rep: 0, epoch: 496, acc: 0.8299999833106995, Loss 0.4625657337717712\n",
      "RNN, rep: 0, epoch: 497, acc: 0.820000171661377, Loss 0.5144527802616358\n",
      "RNN, rep: 0, epoch: 498, acc: 0.8800000548362732, Loss 0.40092473207972945\n",
      "RNN, rep: 0, epoch: 499, acc: 0.8499999046325684, Loss 0.44046744728460907\n",
      "RNN, rep: 0, epoch: 500, acc: 0.8633332848548889, Loss 0.4109479217417538\n",
      "RNN, rep: 0, epoch: 501, acc: 0.8633334636688232, Loss 0.420894081313163\n",
      "RNN, rep: 0, epoch: 502, acc: 0.8700000643730164, Loss 0.4170125555526465\n",
      "RNN, rep: 0, epoch: 503, acc: 0.8600001335144043, Loss 0.42888383826240895\n",
      "RNN, rep: 0, epoch: 504, acc: 0.6866665482521057, Loss 0.9113105703145266\n",
      "RNN, rep: 0, epoch: 505, acc: 0.7099999189376831, Loss 0.7512925699166954\n",
      "RNN, rep: 0, epoch: 506, acc: 0.7800000905990601, Loss 0.5333319970965386\n",
      "RNN, rep: 0, epoch: 507, acc: 0.7266665697097778, Loss 0.670846706405282\n",
      "RNN, rep: 0, epoch: 508, acc: 0.7366668581962585, Loss 0.5943053823709488\n",
      "RNN, rep: 0, epoch: 509, acc: 0.8199999332427979, Loss 0.49360386474989354\n",
      "RNN, rep: 0, epoch: 510, acc: 0.8133335113525391, Loss 0.522259512199089\n",
      "RNN, rep: 0, epoch: 511, acc: 0.8166667222976685, Loss 0.5242559425160289\n",
      "RNN, rep: 0, epoch: 512, acc: 0.7566667199134827, Loss 0.6040061072213575\n",
      "RNN, rep: 0, epoch: 513, acc: 0.7833333015441895, Loss 0.5515850902907551\n",
      "RNN, rep: 0, epoch: 514, acc: 0.800000011920929, Loss 0.5415763783920556\n",
      "RNN, rep: 0, epoch: 515, acc: 0.8733333349227905, Loss 0.4241078770207241\n",
      "RNN, rep: 0, epoch: 516, acc: 0.7933332324028015, Loss 0.548966734833084\n",
      "RNN, rep: 0, epoch: 517, acc: 0.8400000929832458, Loss 0.4639515287755057\n",
      "RNN, rep: 0, epoch: 518, acc: 0.8200000524520874, Loss 0.47701465875841675\n",
      "RNN, rep: 0, epoch: 519, acc: 0.8533332943916321, Loss 0.4368118351791054\n",
      "RNN, rep: 0, epoch: 520, acc: 0.8766667246818542, Loss 0.39825151169206946\n",
      "RNN, rep: 0, epoch: 521, acc: 0.8266667127609253, Loss 0.46706896969582884\n",
      "RNN, rep: 0, epoch: 522, acc: 0.8766667246818542, Loss 0.40782835728954525\n",
      "RNN, rep: 0, epoch: 523, acc: 0.8666667938232422, Loss 0.4069535611383617\n",
      "RNN, rep: 0, epoch: 524, acc: 0.8333332538604736, Loss 0.4601155226537958\n",
      "RNN, rep: 0, epoch: 525, acc: 0.8300001621246338, Loss 0.4716683051176369\n",
      "RNN, rep: 0, epoch: 526, acc: 0.8300001621246338, Loss 0.48719420468900354\n",
      "RNN, rep: 0, epoch: 527, acc: 0.8233333826065063, Loss 0.47145783046726136\n",
      "RNN, rep: 0, epoch: 528, acc: 0.8333334922790527, Loss 0.4788215280789882\n",
      "RNN, rep: 0, epoch: 529, acc: 0.8233334422111511, Loss 0.496696918355301\n",
      "RNN, rep: 0, epoch: 530, acc: 0.8466668128967285, Loss 0.43000291477423164\n",
      "RNN, rep: 0, epoch: 531, acc: 0.8866666555404663, Loss 0.3885845799464732\n",
      "RNN, rep: 0, epoch: 532, acc: 0.8666666150093079, Loss 0.41872402347624305\n",
      "RNN, rep: 0, epoch: 533, acc: 0.8433333039283752, Loss 0.4271030291682109\n",
      "RNN, rep: 0, epoch: 534, acc: 0.8399999737739563, Loss 0.44625345184002074\n",
      "RNN, rep: 0, epoch: 535, acc: 0.8533333539962769, Loss 0.41925062381662426\n",
      "RNN, rep: 0, epoch: 536, acc: 0.8533332943916321, Loss 0.46587768287397924\n",
      "RNN, rep: 0, epoch: 537, acc: 0.8400001525878906, Loss 0.43612087791319937\n",
      "RNN, rep: 0, epoch: 538, acc: 0.8200000524520874, Loss 0.4690303365280852\n",
      "RNN, rep: 0, epoch: 539, acc: 0.8733334541320801, Loss 0.38909920731559394\n",
      "RNN, rep: 0, epoch: 540, acc: 0.8866667747497559, Loss 0.37002303516492246\n",
      "RNN, rep: 0, epoch: 541, acc: 0.8666667342185974, Loss 0.37669668543618173\n",
      "RNN, rep: 0, epoch: 542, acc: 0.8299999237060547, Loss 0.4857046628650278\n",
      "RNN, rep: 0, epoch: 543, acc: 0.8266666531562805, Loss 0.44338915980421006\n",
      "RNN, rep: 0, epoch: 544, acc: 0.8033333420753479, Loss 0.5081734938872978\n",
      "RNN, rep: 0, epoch: 545, acc: 0.8466668128967285, Loss 0.4099882460036315\n",
      "RNN, rep: 0, epoch: 546, acc: 0.8800000548362732, Loss 0.39447959410957995\n",
      "RNN, rep: 0, epoch: 547, acc: 0.8233334422111511, Loss 0.4597708394844085\n",
      "RNN, rep: 0, epoch: 548, acc: 0.8733334541320801, Loss 0.40363375572487714\n",
      "RNN, rep: 0, epoch: 549, acc: 0.8400000929832458, Loss 0.4532571742963046\n",
      "RNN, rep: 0, epoch: 550, acc: 0.8500001430511475, Loss 0.43847422984894363\n",
      "RNN, rep: 0, epoch: 551, acc: 0.8666666150093079, Loss 0.3991407571174204\n",
      "RNN, rep: 0, epoch: 552, acc: 0.8466668128967285, Loss 0.4455069221090525\n",
      "RNN, rep: 0, epoch: 553, acc: 0.8533334136009216, Loss 0.425105294524692\n",
      "RNN, rep: 0, epoch: 554, acc: 0.8466666340827942, Loss 0.4302417933382094\n",
      "RNN, rep: 0, epoch: 555, acc: 0.8733332753181458, Loss 0.400821463977918\n",
      "RNN, rep: 0, epoch: 556, acc: 0.8533333539962769, Loss 0.4314662781031802\n",
      "RNN, rep: 0, epoch: 557, acc: 0.8266667127609253, Loss 0.4718730263528414\n",
      "RNN, rep: 0, epoch: 558, acc: 0.8133333325386047, Loss 0.4902687311731279\n",
      "RNN, rep: 0, epoch: 559, acc: 0.8733334541320801, Loss 0.3980785215529613\n",
      "RNN, rep: 0, epoch: 560, acc: 0.8800002336502075, Loss 0.37706902135163545\n",
      "RNN, rep: 0, epoch: 561, acc: 0.8400001525878906, Loss 0.4300027078343555\n",
      "RNN, rep: 0, epoch: 562, acc: 0.8400000929832458, Loss 0.4637513072229922\n",
      "RNN, rep: 0, epoch: 563, acc: 0.8466666340827942, Loss 0.4306128606107086\n",
      "RNN, rep: 0, epoch: 564, acc: 0.8400001525878906, Loss 0.4491975888772868\n",
      "RNN, rep: 0, epoch: 565, acc: 0.8066666126251221, Loss 0.476586040193215\n",
      "RNN, rep: 0, epoch: 566, acc: 0.8600000143051147, Loss 0.37336627709912135\n",
      "RNN, rep: 0, epoch: 567, acc: 0.8933334350585938, Loss 0.3624820474232547\n",
      "RNN, rep: 0, epoch: 568, acc: 0.8099998235702515, Loss 0.49988391622900963\n",
      "RNN, rep: 0, epoch: 569, acc: 0.8733333349227905, Loss 0.385688100932166\n",
      "RNN, rep: 0, epoch: 570, acc: 0.8566665649414062, Loss 0.4021930269245058\n",
      "RNN, rep: 0, epoch: 571, acc: 0.8466666340827942, Loss 0.4333894785679877\n",
      "RNN, rep: 0, epoch: 572, acc: 0.846666693687439, Loss 0.4340947665646672\n",
      "RNN, rep: 0, epoch: 573, acc: 0.8433333039283752, Loss 0.41644806870026513\n",
      "RNN, rep: 0, epoch: 574, acc: 0.8433331847190857, Loss 0.41244523170869796\n",
      "RNN, rep: 0, epoch: 575, acc: 0.8933334350585938, Loss 0.3422777950158343\n",
      "RNN, rep: 0, epoch: 576, acc: 0.8266666531562805, Loss 0.4517674099188298\n",
      "RNN, rep: 0, epoch: 577, acc: 0.8733333349227905, Loss 0.38394946143962444\n",
      "RNN, rep: 0, epoch: 578, acc: 0.8366668224334717, Loss 0.442930283122696\n",
      "RNN, rep: 0, epoch: 579, acc: 0.7900000810623169, Loss 0.5324468913348391\n",
      "RNN, rep: 0, epoch: 580, acc: 0.8199999928474426, Loss 0.4827911823242903\n",
      "RNN, rep: 0, epoch: 581, acc: 0.8066666126251221, Loss 0.49828219194430856\n",
      "RNN, rep: 0, epoch: 582, acc: 0.8233331441879272, Loss 0.4935457774344832\n",
      "RNN, rep: 0, epoch: 583, acc: 0.9100000262260437, Loss 0.3739702755212784\n",
      "RNN, rep: 0, epoch: 584, acc: 0.816666841506958, Loss 0.5092922091344372\n",
      "RNN, rep: 0, epoch: 585, acc: 0.8733334541320801, Loss 0.36378717493265866\n",
      "RNN, rep: 0, epoch: 586, acc: 0.9000002145767212, Loss 0.3157884802762419\n",
      "RNN, rep: 0, epoch: 587, acc: 0.8899999856948853, Loss 0.34477548421360554\n",
      "RNN, rep: 0, epoch: 588, acc: 0.903333306312561, Loss 0.3067540257191286\n",
      "RNN, rep: 0, epoch: 589, acc: 0.8833335041999817, Loss 0.3434178189467639\n",
      "RNN, rep: 0, epoch: 590, acc: 0.8766666650772095, Loss 0.3866862717736512\n",
      "RNN, rep: 0, epoch: 591, acc: 0.9166667461395264, Loss 0.2924571636039764\n",
      "RNN, rep: 0, epoch: 592, acc: 0.9100000858306885, Loss 0.339370719990693\n",
      "RNN, rep: 0, epoch: 593, acc: 0.893333375453949, Loss 0.32761171149089935\n",
      "RNN, rep: 0, epoch: 594, acc: 0.8833333849906921, Loss 0.36165103999432174\n",
      "RNN, rep: 0, epoch: 595, acc: 0.9066666960716248, Loss 0.3125752997631207\n",
      "RNN, rep: 0, epoch: 596, acc: 0.8166665434837341, Loss 0.5086572435707786\n",
      "RNN, rep: 0, epoch: 597, acc: 0.8866667747497559, Loss 0.3322577742161229\n",
      "RNN, rep: 0, epoch: 598, acc: 0.8633333444595337, Loss 0.4266660000290722\n",
      "RNN, rep: 0, epoch: 599, acc: 0.8200000524520874, Loss 0.4880149884428829\n",
      "RNN, rep: 0, epoch: 600, acc: 0.8100001811981201, Loss 0.4263224804867059\n",
      "RNN, rep: 0, epoch: 601, acc: 0.8733333349227905, Loss 0.34583167496137324\n",
      "RNN, rep: 0, epoch: 602, acc: 0.8633331060409546, Loss 0.3597796979639679\n",
      "RNN, rep: 0, epoch: 603, acc: 0.9266666173934937, Loss 0.2327911328524351\n",
      "RNN, rep: 0, epoch: 604, acc: 0.9033333659172058, Loss 0.31396317057311535\n",
      "RNN, rep: 0, epoch: 605, acc: 0.9133334159851074, Loss 0.26456555424723777\n",
      "RNN, rep: 0, epoch: 606, acc: 0.903333306312561, Loss 0.3178606237564236\n",
      "RNN, rep: 0, epoch: 607, acc: 0.9233335256576538, Loss 0.2952410667482763\n",
      "RNN, rep: 0, epoch: 608, acc: 0.9500000476837158, Loss 0.21114529167301954\n",
      "RNN, rep: 0, epoch: 609, acc: 0.9433333873748779, Loss 0.21963617226108909\n",
      "RNN, rep: 0, epoch: 610, acc: 0.9266667366027832, Loss 0.2430698833009228\n",
      "RNN, rep: 0, epoch: 611, acc: 0.9066668152809143, Loss 0.29762923150788995\n",
      "RNN, rep: 0, epoch: 612, acc: 0.8866667151451111, Loss 0.3529706665221602\n",
      "RNN, rep: 0, epoch: 613, acc: 0.9166666269302368, Loss 0.27467738366685807\n",
      "RNN, rep: 0, epoch: 614, acc: 0.9000002145767212, Loss 0.3366229712869972\n",
      "RNN, rep: 0, epoch: 615, acc: 0.9200000762939453, Loss 0.25715054011903704\n",
      "RNN, rep: 0, epoch: 616, acc: 0.919999897480011, Loss 0.27101546571124346\n",
      "RNN, rep: 0, epoch: 617, acc: 0.8933334946632385, Loss 0.3279949453799054\n",
      "RNN, rep: 0, epoch: 618, acc: 0.7966668009757996, Loss 0.5024899434624239\n",
      "RNN, rep: 0, epoch: 619, acc: 0.8166666626930237, Loss 0.41765200819354503\n",
      "RNN, rep: 0, epoch: 620, acc: 0.820000171661377, Loss 0.3674941777717322\n",
      "RNN, rep: 0, epoch: 621, acc: 0.8833332061767578, Loss 0.30433147203177213\n",
      "RNN, rep: 0, epoch: 622, acc: 0.8866667151451111, Loss 0.29208093353081493\n",
      "RNN, rep: 0, epoch: 623, acc: 0.856666624546051, Loss 0.35802888620644807\n",
      "RNN, rep: 0, epoch: 624, acc: 0.863333523273468, Loss 0.3438238841947168\n",
      "RNN, rep: 0, epoch: 625, acc: 0.8566667437553406, Loss 0.3916312069259584\n",
      "RNN, rep: 0, epoch: 626, acc: 0.8433334827423096, Loss 0.40625022786669435\n",
      "RNN, rep: 0, epoch: 627, acc: 0.9000000953674316, Loss 0.2946921071596444\n",
      "RNN, rep: 0, epoch: 628, acc: 0.9533334374427795, Loss 0.1894831825606525\n",
      "RNN, rep: 0, epoch: 629, acc: 0.9166667461395264, Loss 0.23265357836615294\n",
      "RNN, rep: 0, epoch: 630, acc: 0.8400000929832458, Loss 0.4507506627077237\n",
      "RNN, rep: 0, epoch: 631, acc: 0.9066666960716248, Loss 0.3286549900099635\n",
      "RNN, rep: 0, epoch: 632, acc: 0.949999988079071, Loss 0.21371247570728882\n",
      "RNN, rep: 0, epoch: 633, acc: 0.9133333563804626, Loss 0.31274285572813826\n",
      "RNN, rep: 0, epoch: 634, acc: 0.9300000071525574, Loss 0.2600552906980738\n",
      "RNN, rep: 0, epoch: 635, acc: 0.9133333563804626, Loss 0.32188356822472997\n",
      "RNN, rep: 0, epoch: 636, acc: 0.8999999761581421, Loss 0.34015980607480745\n",
      "RNN, rep: 0, epoch: 637, acc: 0.9133334159851074, Loss 0.2978792740113568\n",
      "RNN, rep: 0, epoch: 638, acc: 0.9100000262260437, Loss 0.29872475666925313\n",
      "RNN, rep: 0, epoch: 639, acc: 0.9133334159851074, Loss 0.28200051687890665\n",
      "RNN, rep: 0, epoch: 640, acc: 0.9133334159851074, Loss 0.2816406001977157\n",
      "RNN, rep: 0, epoch: 641, acc: 0.9266667366027832, Loss 0.2699351396074053\n",
      "RNN, rep: 0, epoch: 642, acc: 0.926666796207428, Loss 0.2488967990456149\n",
      "RNN, rep: 0, epoch: 643, acc: 0.9599999785423279, Loss 0.17562611515983007\n",
      "RNN, rep: 0, epoch: 644, acc: 0.8800000548362732, Loss 0.3397727590368595\n",
      "RNN, rep: 0, epoch: 645, acc: 0.9533333778381348, Loss 0.178506317025749\n",
      "RNN, rep: 0, epoch: 646, acc: 0.9433333873748779, Loss 0.20397761864354835\n",
      "RNN, rep: 0, epoch: 647, acc: 0.9333333373069763, Loss 0.21498711624648423\n",
      "RNN, rep: 0, epoch: 648, acc: 0.9133333563804626, Loss 0.26481952175381596\n",
      "RNN, rep: 0, epoch: 649, acc: 0.9366666674613953, Loss 0.20815704545006156\n",
      "RNN, rep: 0, epoch: 650, acc: 0.9466667771339417, Loss 0.2087543078616727\n",
      "RNN, rep: 0, epoch: 651, acc: 0.949999988079071, Loss 0.16900078361737542\n",
      "RNN, rep: 0, epoch: 652, acc: 0.9733333587646484, Loss 0.13101839612529148\n",
      "RNN                  Rep: 0   Epoch: 1     Acc: 0.9733 Params: min_length: 40, max_length: 45, fill: 0, value_1: -1, value_2: 1 Time: 309.17 sec\n",
      "NetRNNWithAttention, rep: 0, epoch: 1, acc: 0.5233333110809326, Loss 0.9977927941083908\n",
      "NetRNNWithAttention, rep: 0, epoch: 2, acc: 0.5766666531562805, Loss 0.9723963701725006\n",
      "NetRNNWithAttention, rep: 0, epoch: 3, acc: 0.5400000214576721, Loss 0.9981456249952316\n",
      "NetRNNWithAttention, rep: 0, epoch: 4, acc: 0.5166667103767395, Loss 1.004560297727585\n",
      "NetRNNWithAttention, rep: 0, epoch: 5, acc: 0.5799999833106995, Loss 0.9668733167648316\n",
      "NetRNNWithAttention, rep: 0, epoch: 6, acc: 0.5533334612846375, Loss 0.995754423737526\n",
      "NetRNNWithAttention, rep: 0, epoch: 7, acc: 0.5699999928474426, Loss 0.9599699753522873\n",
      "NetRNNWithAttention, rep: 0, epoch: 8, acc: 0.550000011920929, Loss 0.9551896521449089\n",
      "NetRNNWithAttention, rep: 0, epoch: 9, acc: 0.513333261013031, Loss 0.9702852708101273\n",
      "NetRNNWithAttention, rep: 0, epoch: 10, acc: 0.6000000834465027, Loss 0.8907850879430771\n",
      "NetRNNWithAttention, rep: 0, epoch: 11, acc: 0.6166666150093079, Loss 0.747028824687004\n",
      "NetRNNWithAttention, rep: 0, epoch: 12, acc: 0.6766665577888489, Loss 0.6887044525146484\n",
      "NetRNNWithAttention, rep: 0, epoch: 13, acc: 0.6600000262260437, Loss 0.6842501342296601\n",
      "NetRNNWithAttention, rep: 0, epoch: 14, acc: 0.6500001549720764, Loss 0.6808581042289734\n",
      "NetRNNWithAttention, rep: 0, epoch: 15, acc: 0.6899999976158142, Loss 0.6711270374059677\n",
      "NetRNNWithAttention, rep: 0, epoch: 16, acc: 0.646666944026947, Loss 0.6971801793575287\n",
      "NetRNNWithAttention, rep: 0, epoch: 17, acc: 0.6666667461395264, Loss 0.6877411592006684\n",
      "NetRNNWithAttention, rep: 0, epoch: 18, acc: 0.6799999475479126, Loss 0.6661899816989899\n",
      "NetRNNWithAttention, rep: 0, epoch: 19, acc: 0.690000057220459, Loss 0.6695920413732529\n",
      "NetRNNWithAttention, rep: 0, epoch: 20, acc: 0.6533334255218506, Loss 0.7239745056629181\n",
      "NetRNNWithAttention, rep: 0, epoch: 21, acc: 0.7000001668930054, Loss 0.6718647941946984\n",
      "NetRNNWithAttention, rep: 0, epoch: 22, acc: 0.6800000667572021, Loss 0.7236972486972809\n",
      "NetRNNWithAttention, rep: 0, epoch: 23, acc: 0.6733333468437195, Loss 0.6732677239179611\n",
      "NetRNNWithAttention, rep: 0, epoch: 24, acc: 0.6833335161209106, Loss 0.6690618324279786\n",
      "NetRNNWithAttention, rep: 0, epoch: 25, acc: 0.6700002551078796, Loss 0.6708462196588516\n",
      "NetRNNWithAttention, rep: 0, epoch: 26, acc: 0.7066667079925537, Loss 0.6836359173059463\n",
      "NetRNNWithAttention, rep: 0, epoch: 27, acc: 0.6633333563804626, Loss 0.6764159613847732\n",
      "NetRNNWithAttention, rep: 0, epoch: 28, acc: 0.6766666173934937, Loss 0.6696227043867111\n",
      "NetRNNWithAttention, rep: 0, epoch: 29, acc: 0.6733332872390747, Loss 0.6692140483856202\n",
      "NetRNNWithAttention, rep: 0, epoch: 30, acc: 0.699999988079071, Loss 0.6630837416648865\n",
      "NetRNNWithAttention, rep: 0, epoch: 31, acc: 0.68666672706604, Loss 0.6892892330884933\n",
      "NetRNNWithAttention, rep: 0, epoch: 32, acc: 0.5100001096725464, Loss 1.28783338367939\n",
      "NetRNNWithAttention, rep: 0, epoch: 33, acc: 0.4866666793823242, Loss 1.2954666084051132\n",
      "NetRNNWithAttention, rep: 0, epoch: 34, acc: 0.47333332896232605, Loss 1.115779877305031\n",
      "NetRNNWithAttention, rep: 0, epoch: 35, acc: 0.5566666126251221, Loss 0.9710150480270385\n",
      "NetRNNWithAttention, rep: 0, epoch: 36, acc: 0.5466667413711548, Loss 0.9870186448097229\n",
      "NetRNNWithAttention, rep: 0, epoch: 37, acc: 0.5299999117851257, Loss 0.9778676575422287\n",
      "NetRNNWithAttention, rep: 0, epoch: 38, acc: 0.5499998927116394, Loss 0.9691875195503235\n",
      "NetRNNWithAttention, rep: 0, epoch: 39, acc: 0.6966667175292969, Loss 0.963302253484726\n",
      "NetRNNWithAttention, rep: 0, epoch: 40, acc: 0.6866667866706848, Loss 0.9567188739776611\n",
      "NetRNNWithAttention, rep: 0, epoch: 41, acc: 0.6533333659172058, Loss 0.963610013127327\n",
      "NetRNNWithAttention, rep: 0, epoch: 42, acc: 0.6399999856948853, Loss 0.9525288337469101\n",
      "NetRNNWithAttention, rep: 0, epoch: 43, acc: 0.6766667366027832, Loss 0.9405900901556015\n",
      "NetRNNWithAttention, rep: 0, epoch: 44, acc: 0.5833336114883423, Loss 0.950556926727295\n",
      "NetRNNWithAttention, rep: 0, epoch: 45, acc: 0.6500000953674316, Loss 0.9227126830816269\n",
      "NetRNNWithAttention, rep: 0, epoch: 46, acc: 0.6233335733413696, Loss 0.9397519534826279\n",
      "NetRNNWithAttention, rep: 0, epoch: 47, acc: 0.7099999785423279, Loss 0.9116382801532745\n",
      "NetRNNWithAttention, rep: 0, epoch: 48, acc: 0.6866669654846191, Loss 0.9177349388599396\n",
      "NetRNNWithAttention, rep: 0, epoch: 49, acc: 0.6466667652130127, Loss 0.8962307161092758\n",
      "NetRNNWithAttention, rep: 0, epoch: 50, acc: 0.6266666650772095, Loss 0.9123196804523468\n",
      "NetRNNWithAttention, rep: 0, epoch: 51, acc: 0.6266666650772095, Loss 0.9039086759090423\n",
      "NetRNNWithAttention, rep: 0, epoch: 52, acc: 0.6700000762939453, Loss 0.8911405766010284\n",
      "NetRNNWithAttention, rep: 0, epoch: 53, acc: 0.6766668558120728, Loss 0.856982770562172\n",
      "NetRNNWithAttention, rep: 0, epoch: 54, acc: 0.6599999070167542, Loss 0.8603936946392059\n",
      "NetRNNWithAttention, rep: 0, epoch: 55, acc: 0.6433331966400146, Loss 0.8650980913639068\n",
      "NetRNNWithAttention, rep: 0, epoch: 56, acc: 0.6566668748855591, Loss 0.860643954873085\n",
      "NetRNNWithAttention, rep: 0, epoch: 57, acc: 0.6700001358985901, Loss 0.8519023895263672\n",
      "NetRNNWithAttention, rep: 0, epoch: 58, acc: 0.6899999976158142, Loss 0.821859073638916\n",
      "NetRNNWithAttention, rep: 0, epoch: 59, acc: 0.6899999380111694, Loss 0.8243353328108788\n",
      "NetRNNWithAttention, rep: 0, epoch: 60, acc: 0.6800001263618469, Loss 0.8169387939572335\n",
      "NetRNNWithAttention, rep: 0, epoch: 61, acc: 0.6633332967758179, Loss 0.8044047051668167\n",
      "NetRNNWithAttention, rep: 0, epoch: 62, acc: 0.6666667461395264, Loss 0.7988629984855652\n",
      "NetRNNWithAttention, rep: 0, epoch: 63, acc: 0.6766666173934937, Loss 0.7859000459313392\n",
      "NetRNNWithAttention, rep: 0, epoch: 64, acc: 0.6133334040641785, Loss 0.7908326601982116\n",
      "NetRNNWithAttention, rep: 0, epoch: 65, acc: 0.6166667342185974, Loss 0.7852881932258606\n",
      "NetRNNWithAttention, rep: 0, epoch: 66, acc: 0.720000147819519, Loss 0.7363261157274246\n",
      "NetRNNWithAttention, rep: 0, epoch: 67, acc: 0.6933333873748779, Loss 0.69854128241539\n",
      "NetRNNWithAttention, rep: 0, epoch: 68, acc: 0.6966667175292969, Loss 0.6875803247094154\n",
      "NetRNNWithAttention, rep: 0, epoch: 69, acc: 0.6866668462753296, Loss 0.7253723302483559\n",
      "NetRNNWithAttention, rep: 0, epoch: 70, acc: 0.6666667461395264, Loss 0.7068484398722649\n",
      "NetRNNWithAttention, rep: 0, epoch: 71, acc: 0.7400000691413879, Loss 0.6852679750323296\n",
      "NetRNNWithAttention, rep: 0, epoch: 72, acc: 0.7633334994316101, Loss 0.6374827685952187\n",
      "NetRNNWithAttention, rep: 0, epoch: 73, acc: 0.7166666984558105, Loss 0.6663160002231598\n",
      "NetRNNWithAttention, rep: 0, epoch: 74, acc: 0.7499998211860657, Loss 0.6572559842467308\n",
      "NetRNNWithAttention, rep: 0, epoch: 75, acc: 0.7599997520446777, Loss 0.660246886909008\n",
      "NetRNNWithAttention, rep: 0, epoch: 76, acc: 0.6966667175292969, Loss 0.6096400168538093\n",
      "NetRNNWithAttention, rep: 0, epoch: 77, acc: 0.720000147819519, Loss 0.6857480821013451\n",
      "NetRNNWithAttention, rep: 0, epoch: 78, acc: 0.7300000786781311, Loss 0.6265974324941636\n",
      "NetRNNWithAttention, rep: 0, epoch: 79, acc: 0.7400000691413879, Loss 0.6028489415347577\n",
      "NetRNNWithAttention, rep: 0, epoch: 80, acc: 0.7433333396911621, Loss 0.6171857292950154\n",
      "NetRNNWithAttention, rep: 0, epoch: 81, acc: 0.7499999403953552, Loss 0.5783795469999313\n",
      "NetRNNWithAttention, rep: 0, epoch: 82, acc: 0.7866666316986084, Loss 0.5614258912205696\n",
      "NetRNNWithAttention, rep: 0, epoch: 83, acc: 0.8033332824707031, Loss 0.5565944121778011\n",
      "NetRNNWithAttention, rep: 0, epoch: 84, acc: 0.7166666984558105, Loss 0.5934178198873997\n",
      "NetRNNWithAttention, rep: 0, epoch: 85, acc: 0.7433334589004517, Loss 0.5778860962390899\n",
      "NetRNNWithAttention, rep: 0, epoch: 86, acc: 0.7266666889190674, Loss 0.5443232668936253\n",
      "NetRNNWithAttention, rep: 0, epoch: 87, acc: 0.7233334183692932, Loss 0.5323818860948086\n",
      "NetRNNWithAttention, rep: 0, epoch: 88, acc: 0.7566666603088379, Loss 0.5003760811686516\n",
      "NetRNNWithAttention, rep: 0, epoch: 89, acc: 0.7100001573562622, Loss 0.5133167606592178\n",
      "NetRNNWithAttention, rep: 0, epoch: 90, acc: 0.6533334255218506, Loss 0.5816319267451763\n",
      "NetRNNWithAttention, rep: 0, epoch: 91, acc: 0.7466665506362915, Loss 0.5923265607655048\n",
      "NetRNNWithAttention, rep: 0, epoch: 92, acc: 0.7366666197776794, Loss 0.5291077890992164\n",
      "NetRNNWithAttention, rep: 0, epoch: 93, acc: 0.7566665410995483, Loss 0.5149837644398212\n",
      "NetRNNWithAttention, rep: 0, epoch: 94, acc: 0.690000057220459, Loss 0.5439812158048153\n",
      "NetRNNWithAttention, rep: 0, epoch: 95, acc: 0.7133333086967468, Loss 0.5149322532117366\n",
      "NetRNNWithAttention, rep: 0, epoch: 96, acc: 0.7066667675971985, Loss 0.5066858772933484\n",
      "NetRNNWithAttention, rep: 0, epoch: 97, acc: 0.7600001692771912, Loss 0.49391973540186884\n",
      "NetRNNWithAttention, rep: 0, epoch: 98, acc: 0.746666669845581, Loss 0.47896352723240854\n",
      "NetRNNWithAttention, rep: 0, epoch: 99, acc: 0.7566665410995483, Loss 0.4922517465054989\n",
      "NetRNNWithAttention, rep: 0, epoch: 100, acc: 0.7500001788139343, Loss 0.47134217962622643\n",
      "NetRNNWithAttention, rep: 0, epoch: 101, acc: 0.7733333110809326, Loss 0.46886242993175986\n",
      "NetRNNWithAttention, rep: 0, epoch: 102, acc: 0.7333333492279053, Loss 0.49149259850382804\n",
      "NetRNNWithAttention, rep: 0, epoch: 103, acc: 0.7600000500679016, Loss 0.46808354280889036\n",
      "NetRNNWithAttention, rep: 0, epoch: 104, acc: 0.7633334398269653, Loss 0.5114404863119125\n",
      "NetRNNWithAttention, rep: 0, epoch: 105, acc: 0.7366667985916138, Loss 0.4796093900501728\n",
      "NetRNNWithAttention, rep: 0, epoch: 106, acc: 0.7466665506362915, Loss 0.5115115831047297\n",
      "NetRNNWithAttention, rep: 0, epoch: 107, acc: 0.7433333396911621, Loss 0.48850683607161044\n",
      "NetRNNWithAttention, rep: 0, epoch: 108, acc: 0.8033332824707031, Loss 0.4406160093843937\n",
      "NetRNNWithAttention, rep: 0, epoch: 109, acc: 0.81333327293396, Loss 0.45678540572524073\n",
      "NetRNNWithAttention, rep: 0, epoch: 110, acc: 0.7066667079925537, Loss 0.52229980237782\n",
      "NetRNNWithAttention, rep: 0, epoch: 111, acc: 0.763333261013031, Loss 0.437884933501482\n",
      "NetRNNWithAttention, rep: 0, epoch: 112, acc: 0.7600000500679016, Loss 0.46576200738549234\n",
      "NetRNNWithAttention, rep: 0, epoch: 113, acc: 0.8333331942558289, Loss 0.3668600056320429\n",
      "NetRNNWithAttention, rep: 0, epoch: 114, acc: 0.7933332324028015, Loss 0.4130884052067995\n",
      "NetRNNWithAttention, rep: 0, epoch: 115, acc: 0.8366667032241821, Loss 0.379122027233243\n",
      "NetRNNWithAttention, rep: 0, epoch: 116, acc: 0.8766664266586304, Loss 0.30254936318844555\n",
      "NetRNNWithAttention, rep: 0, epoch: 117, acc: 0.873333215713501, Loss 0.3192801222577691\n",
      "NetRNNWithAttention, rep: 0, epoch: 118, acc: 0.893333375453949, Loss 0.3140241710096598\n",
      "NetRNNWithAttention, rep: 0, epoch: 119, acc: 0.9299999475479126, Loss 0.2941138581931591\n",
      "NetRNNWithAttention, rep: 0, epoch: 120, acc: 0.909999668598175, Loss 0.3151022519171238\n",
      "NetRNNWithAttention, rep: 0, epoch: 121, acc: 0.9299997687339783, Loss 0.2574702842719853\n",
      "NetRNNWithAttention, rep: 0, epoch: 122, acc: 0.9533331990242004, Loss 0.2405791840143502\n",
      "NetRNNWithAttention, rep: 0, epoch: 123, acc: 0.9399999380111694, Loss 0.24013074029237033\n",
      "NetRNNWithAttention, rep: 0, epoch: 124, acc: 0.9466666579246521, Loss 0.21943940514698623\n",
      "NetRNNWithAttention, rep: 0, epoch: 125, acc: 0.9566666483879089, Loss 0.21810851700603962\n",
      "NetRNNWithAttention, rep: 0, epoch: 126, acc: 0.9599999189376831, Loss 0.22512337561696769\n",
      "NetRNNWithAttention, rep: 0, epoch: 127, acc: 0.9666666388511658, Loss 0.20221489997580647\n",
      "NetRNNWithAttention, rep: 0, epoch: 128, acc: 0.9333332777023315, Loss 0.2260109182074666\n",
      "NetRNNWithAttention, rep: 0, epoch: 129, acc: 0.9600000977516174, Loss 0.18615882257930935\n",
      "NetRNNWithAttention, rep: 0, epoch: 130, acc: 0.9599999189376831, Loss 0.18305126787163317\n",
      "NetRNNWithAttention, rep: 0, epoch: 131, acc: 0.9566665887832642, Loss 0.17993673854507505\n",
      "NetRNNWithAttention, rep: 0, epoch: 132, acc: 0.949999988079071, Loss 0.18668476408347487\n",
      "NetRNNWithAttention, rep: 0, epoch: 133, acc: 0.9733332991600037, Loss 0.16667892259545625\n",
      "NetRNNWithAttention  Rep: 0   Epoch: 1     Acc: 0.9733 Params: min_length: 40, max_length: 45, fill: 0, value_1: -1, value_2: 1 Time: 78.19 sec\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 1, acc: 0.44333338737487793, Loss 1.015768437385559\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 2, acc: 0.5166666507720947, Loss 1.0043872129917144\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 3, acc: 0.4933333694934845, Loss 1.0051056659221649\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 4, acc: 0.5133334994316101, Loss 1.000886396765709\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 5, acc: 0.5066666603088379, Loss 1.003387957215309\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 6, acc: 0.7166668176651001, Loss 0.8686844635009766\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 7, acc: 0.8299997448921204, Loss 0.578460986316204\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 8, acc: 0.8700000643730164, Loss 0.41195496201515197\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 9, acc: 0.8299999833106995, Loss 0.402835248708725\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 10, acc: 0.8466665744781494, Loss 0.3604403924942017\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 11, acc: 0.856666624546051, Loss 0.3476782165467739\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 12, acc: 0.8933331966400146, Loss 0.3323083344101906\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 13, acc: 0.873333215713501, Loss 0.32456488236784936\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 14, acc: 0.883333146572113, Loss 0.3016985110938549\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 15, acc: 0.9066665768623352, Loss 0.2852859836816788\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 16, acc: 0.9066665172576904, Loss 0.267959865629673\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 17, acc: 0.9199997782707214, Loss 0.24896322149783373\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 18, acc: 0.8999999165534973, Loss 0.23770550459623338\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 19, acc: 0.8999999165534973, Loss 0.22658335337415336\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 20, acc: 0.9133331179618835, Loss 0.20174874495714903\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 21, acc: 0.9033333659172058, Loss 0.19082583341747522\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 22, acc: 0.9266667366027832, Loss 0.18511926292441785\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 23, acc: 0.9366665482521057, Loss 0.18067991473712028\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 24, acc: 0.916666567325592, Loss 0.18061028143391014\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 25, acc: 0.9166665077209473, Loss 0.17733404451981186\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 26, acc: 0.9199997782707214, Loss 0.18083864281885326\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 27, acc: 0.9133332967758179, Loss 0.17198069548234343\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 28, acc: 0.9033331274986267, Loss 0.20891258765012025\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 29, acc: 0.9133331179618835, Loss 0.1805131325032562\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 30, acc: 0.8966666460037231, Loss 0.20731851348653435\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 31, acc: 0.9199998378753662, Loss 0.18271520875860006\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 32, acc: 0.9133332967758179, Loss 0.16350856475066394\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 33, acc: 0.9366664886474609, Loss 0.15513013689778746\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 34, acc: 0.9233333468437195, Loss 0.17109875896945595\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 35, acc: 0.9033331871032715, Loss 0.191497483830899\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 36, acc: 0.9266664981842041, Loss 0.16371176614426075\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 37, acc: 0.9099998474121094, Loss 0.17884377881418914\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 38, acc: 0.9033331274986267, Loss 0.20160417878534645\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 39, acc: 0.8966665863990784, Loss 0.18253807070432232\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 40, acc: 0.9266666173934937, Loss 0.14802376651437954\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 41, acc: 0.9366665482521057, Loss 0.149746253087651\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 42, acc: 0.8933332562446594, Loss 0.31257542396197097\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 43, acc: 0.9233332276344299, Loss 0.18787294655805453\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 44, acc: 0.933333158493042, Loss 0.15799032673705368\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 45, acc: 0.9333333373069763, Loss 0.1585407082620077\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 46, acc: 0.9133332967758179, Loss 0.15531692903488875\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 47, acc: 0.9399999976158142, Loss 0.1431570093310438\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 48, acc: 0.9366666674613953, Loss 0.1652979925321415\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 49, acc: 0.9033333659172058, Loss 0.18308747110422702\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 50, acc: 0.8866666555404663, Loss 0.19096592112910002\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 51, acc: 0.9066665172576904, Loss 0.16292178643052466\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 52, acc: 0.9300000071525574, Loss 0.19301495043444447\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 53, acc: 0.90666663646698, Loss 0.18431748771923595\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 54, acc: 0.9399999380111694, Loss 0.14890759029542097\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 55, acc: 0.9133331775665283, Loss 0.1501268006232567\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 56, acc: 0.9099998474121094, Loss 0.1813199302344583\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 57, acc: 0.9133331775665283, Loss 0.16256322725093925\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 58, acc: 0.9099999070167542, Loss 0.19020435951766557\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 59, acc: 0.9233332872390747, Loss 0.16218147643026895\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 60, acc: 0.8866666555404663, Loss 0.18254951720940882\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 61, acc: 0.9166663885116577, Loss 0.17220361427520403\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 62, acc: 0.9099998474121094, Loss 0.17101628320873716\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 63, acc: 0.8933331370353699, Loss 0.19324164430378005\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 64, acc: 0.9233332872390747, Loss 0.16207457275362686\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 65, acc: 0.9133330583572388, Loss 0.18955245309509336\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 66, acc: 0.909999668598175, Loss 0.18109465449117124\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 67, acc: 0.9200000166893005, Loss 0.15846437337226235\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 68, acc: 0.9233332872390747, Loss 0.1800400142883882\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 69, acc: 0.9266665577888489, Loss 0.15942233085224872\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 70, acc: 0.9266666173934937, Loss 0.1686675324168755\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 71, acc: 0.90666663646698, Loss 0.1581698137044441\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 72, acc: 0.8799999356269836, Loss 0.19809900263033342\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 73, acc: 0.9133331775665283, Loss 0.18231839627143928\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 74, acc: 0.9499999284744263, Loss 0.16036675897892563\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 75, acc: 0.8999998569488525, Loss 0.1910044526658021\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 76, acc: 0.8233333826065063, Loss 0.4396650184341706\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 77, acc: 0.8633332848548889, Loss 0.42571905950200745\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 78, acc: 0.8733333349227905, Loss 0.2774164057930466\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 79, acc: 0.8866665363311768, Loss 0.26088242818485013\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 80, acc: 0.8499997854232788, Loss 0.2947597273113206\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 81, acc: 0.8700000643730164, Loss 0.3176988905807957\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 82, acc: 0.8699998259544373, Loss 0.30151418087305504\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 83, acc: 0.8899997472763062, Loss 0.2716104186302982\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 84, acc: 0.8499997854232788, Loss 0.2941891131072771\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 85, acc: 0.8899999260902405, Loss 0.23344366777455433\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 86, acc: 0.90666663646698, Loss 0.2150175678159576\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 87, acc: 0.9066663980484009, Loss 0.2064722294860985\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 88, acc: 0.9233332276344299, Loss 0.18821604863391259\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 89, acc: 0.8866663575172424, Loss 0.2535387278953567\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 90, acc: 0.896666407585144, Loss 0.21718457017675974\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 91, acc: 0.8933331966400146, Loss 0.2240521680645179\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 92, acc: 0.9333332180976868, Loss 0.16096703319228253\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 93, acc: 0.8966664671897888, Loss 0.20320209267665632\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 94, acc: 0.9133331179618835, Loss 0.19724979832419196\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 95, acc: 0.9133331775665283, Loss 0.17977066710591316\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 96, acc: 0.9399999380111694, Loss 0.1453872249496635\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 97, acc: 0.9099999070167542, Loss 0.1892048812750727\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 98, acc: 0.9300000071525574, Loss 0.16483439592062496\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 99, acc: 0.916666567325592, Loss 0.165431487938622\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 100, acc: 0.8833332657814026, Loss 0.18435758524457924\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 101, acc: 0.9033331871032715, Loss 0.1973403237015009\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 102, acc: 0.90666663646698, Loss 0.17277001507463866\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 103, acc: 0.9366666674613953, Loss 0.14503859060583635\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 104, acc: 0.8999999165534973, Loss 0.18033637271262706\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 105, acc: 0.9033331871032715, Loss 0.20379089043592102\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 106, acc: 0.9099999070167542, Loss 0.15805060426704587\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 107, acc: 0.9133332967758179, Loss 0.16648495792178436\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 108, acc: 0.9166666269302368, Loss 0.19143293346685822\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 109, acc: 0.893333375453949, Loss 0.1869597888883436\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 110, acc: 0.9099997878074646, Loss 0.15607781832339243\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 111, acc: 0.9233332276344299, Loss 0.1776775390910916\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 112, acc: 0.916666567325592, Loss 0.1686515273799887\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 113, acc: 0.9233332872390747, Loss 0.16056309181556572\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 114, acc: 0.8999999165534973, Loss 0.1843217070796527\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 115, acc: 0.9433332085609436, Loss 0.11653571607719641\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 116, acc: 0.8966665863990784, Loss 0.19117468431591988\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 117, acc: 0.9133331775665283, Loss 0.20157241634267847\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 118, acc: 0.8899999260902405, Loss 0.18285065611300524\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 119, acc: 0.9333332777023315, Loss 0.1576515424210811\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 120, acc: 0.9099997878074646, Loss 0.1556810418254463\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 121, acc: 0.9399999380111694, Loss 0.15090475392702501\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 122, acc: 0.8899999856948853, Loss 0.19729443582356906\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 123, acc: 0.9233332276344299, Loss 0.17733432198059745\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 124, acc: 0.9333333373069763, Loss 0.15295426000549925\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 125, acc: 0.9033333659172058, Loss 0.18806453249999322\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 126, acc: 0.9200000166893005, Loss 0.15713888123631478\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 127, acc: 0.9166665077209473, Loss 0.17843382656690665\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 128, acc: 0.9233332872390747, Loss 0.16538176456233486\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 129, acc: 0.9233332276344299, Loss 0.16258362628985196\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 130, acc: 0.8899998664855957, Loss 0.19087334916985127\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 131, acc: 0.9333332777023315, Loss 0.14828046844631898\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 132, acc: 0.9166663885116577, Loss 0.19068745674041565\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 133, acc: 0.9166665077209473, Loss 0.1515790619578911\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 134, acc: 0.9266664981842041, Loss 0.16216047054389493\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 135, acc: 0.9233332276344299, Loss 0.1649354790180223\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 136, acc: 0.929999828338623, Loss 0.17683778520149646\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 137, acc: 0.8833332657814026, Loss 0.22593027493800036\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 138, acc: 0.8399999141693115, Loss 0.49377592780132545\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 139, acc: 0.7933332920074463, Loss 0.5012106129107997\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 140, acc: 0.8000001311302185, Loss 0.47716188764898104\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 141, acc: 0.7866666913032532, Loss 0.4347713407853735\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 142, acc: 0.7699999213218689, Loss 0.4921914218022721\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 143, acc: 0.770000159740448, Loss 0.48604350932640955\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 144, acc: 0.7900000810623169, Loss 0.4768967839173274\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 145, acc: 0.7666666507720947, Loss 0.4330772060249001\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 146, acc: 0.7633333802223206, Loss 0.4584006715938449\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 147, acc: 0.7799999117851257, Loss 0.44231921522004997\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 148, acc: 0.81333327293396, Loss 0.38092914897366426\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 149, acc: 0.7833331227302551, Loss 0.4552304737456143\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 150, acc: 0.8166665434837341, Loss 0.4224287200102117\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 151, acc: 0.7666666507720947, Loss 0.4811849773675203\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 152, acc: 0.8033332824707031, Loss 0.4132004344818415\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 153, acc: 0.7900000214576721, Loss 0.4421997108432697\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 154, acc: 0.8299998641014099, Loss 0.38810585433850064\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 155, acc: 0.8100000023841858, Loss 0.44348355670983436\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 156, acc: 0.8466665744781494, Loss 0.3725635927869007\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 157, acc: 0.846666693687439, Loss 0.4763122255145572\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 158, acc: 0.916666567325592, Loss 0.25408847733517176\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 159, acc: 0.9266666173934937, Loss 0.21908275984693318\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 160, acc: 0.8966666460037231, Loss 0.2582626875658752\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 161, acc: 0.9333332777023315, Loss 0.17571461332554464\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 162, acc: 0.9166668057441711, Loss 0.20725981713447253\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 163, acc: 0.9133334159851074, Loss 0.22491753509093543\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 164, acc: 0.9199998378753662, Loss 0.19046133699943313\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 165, acc: 0.9399999380111694, Loss 0.1716922393545974\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 166, acc: 0.9366664886474609, Loss 0.14982713734614664\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 167, acc: 0.929999828338623, Loss 0.12191474599589128\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 168, acc: 0.9366666674613953, Loss 0.12302345765870996\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 169, acc: 0.9233332872390747, Loss 0.13152700380422175\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 170, acc: 0.9366666674613953, Loss 0.10354978393763303\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 171, acc: 0.9099998474121094, Loss 0.13525179081421812\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 172, acc: 0.9099997878074646, Loss 0.11912782253464684\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 173, acc: 0.9699999094009399, Loss 0.07988916517875623\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 174, acc: 0.9533331990242004, Loss 0.11134056600159965\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 175, acc: 0.9633331894874573, Loss 0.0931702624791069\n",
      "NetRNNWithAttentionExpFirst, rep: 0, epoch: 176, acc: 0.9700000286102295, Loss 0.1021538565022638\n",
      "NetRNNWithAttentionExpFirst Rep: 0   Epoch: 1     Acc: 0.9700 Params: min_length: 40, max_length: 45, fill: 0, value_1: -1, value_2: 1 Time: 98.84 sec\n",
      "LSTM, rep: 0, epoch: 1, acc: 0.47999998927116394, Loss 1.063616686463356\n",
      "LSTM, rep: 0, epoch: 2, acc: 0.5533331036567688, Loss 0.9965331596136093\n",
      "LSTM, rep: 0, epoch: 3, acc: 0.5399997234344482, Loss 0.9959240353107452\n",
      "LSTM, rep: 0, epoch: 4, acc: 0.45333313941955566, Loss 1.0285832726955413\n",
      "LSTM, rep: 0, epoch: 5, acc: 0.5400002002716064, Loss 0.9867546260356903\n",
      "LSTM, rep: 0, epoch: 6, acc: 0.5300000905990601, Loss 0.9956353348493576\n",
      "LSTM, rep: 0, epoch: 7, acc: 0.5300002098083496, Loss 0.9853161662817002\n",
      "LSTM, rep: 0, epoch: 8, acc: 0.4433334469795227, Loss 1.0236748105287552\n",
      "LSTM, rep: 0, epoch: 9, acc: 0.5600000619888306, Loss 0.9891704630851745\n",
      "LSTM, rep: 0, epoch: 10, acc: 0.49000003933906555, Loss 1.0123342227935792\n",
      "LSTM, rep: 0, epoch: 11, acc: 0.5066667795181274, Loss 1.008186289668083\n",
      "LSTM, rep: 0, epoch: 12, acc: 0.5000002384185791, Loss 0.9971895009279251\n",
      "LSTM, rep: 0, epoch: 13, acc: 0.49666666984558105, Loss 0.9979442518949508\n",
      "LSTM, rep: 0, epoch: 14, acc: 0.5200002193450928, Loss 0.9995787382125855\n",
      "LSTM, rep: 0, epoch: 15, acc: 0.49999988079071045, Loss 1.0062577241659165\n",
      "LSTM, rep: 0, epoch: 16, acc: 0.5066667199134827, Loss 1.004838491678238\n",
      "LSTM, rep: 0, epoch: 17, acc: 0.43666672706604004, Loss 1.0055793857574462\n",
      "LSTM, rep: 0, epoch: 18, acc: 0.4666666090488434, Loss 1.0016616874933242\n",
      "LSTM, rep: 0, epoch: 19, acc: 0.5166663527488708, Loss 0.9984038925170898\n",
      "LSTM, rep: 0, epoch: 20, acc: 0.5299999117851257, Loss 0.9969373971223832\n",
      "LSTM, rep: 0, epoch: 21, acc: 0.5333330035209656, Loss 0.9923024719953537\n",
      "LSTM, rep: 0, epoch: 22, acc: 0.536666750907898, Loss 0.9898593068122864\n",
      "LSTM, rep: 0, epoch: 23, acc: 0.5233330726623535, Loss 0.9858804148435593\n",
      "LSTM, rep: 0, epoch: 24, acc: 0.5300001502037048, Loss 0.9887763297557831\n",
      "LSTM, rep: 0, epoch: 25, acc: 0.5266667604446411, Loss 0.9962250709533691\n",
      "LSTM, rep: 0, epoch: 26, acc: 0.5133332014083862, Loss 1.0000625920295716\n",
      "LSTM, rep: 0, epoch: 27, acc: 0.4933331310749054, Loss 1.0026803082227707\n",
      "LSTM, rep: 0, epoch: 28, acc: 0.5533334016799927, Loss 0.9780792385339737\n",
      "LSTM, rep: 0, epoch: 29, acc: 0.5466669201850891, Loss 1.004447021484375\n",
      "LSTM, rep: 0, epoch: 30, acc: 0.5033331513404846, Loss 1.0044021451473235\n",
      "LSTM, rep: 0, epoch: 31, acc: 0.5000001788139343, Loss 0.9968936705589294\n",
      "LSTM, rep: 0, epoch: 32, acc: 0.4833333194255829, Loss 1.0045448118448257\n",
      "LSTM, rep: 0, epoch: 33, acc: 0.5133332014083862, Loss 0.9910003882646561\n",
      "LSTM, rep: 0, epoch: 34, acc: 0.4899997413158417, Loss 0.9952597916126251\n",
      "LSTM, rep: 0, epoch: 35, acc: 0.5466666221618652, Loss 0.9727177339792251\n",
      "LSTM, rep: 0, epoch: 36, acc: 0.6899999380111694, Loss 0.8588592326641082\n",
      "LSTM, rep: 0, epoch: 37, acc: 0.643333375453949, Loss 0.8863614591956138\n",
      "LSTM, rep: 0, epoch: 38, acc: 0.6866668462753296, Loss 0.8128386637568474\n",
      "LSTM, rep: 0, epoch: 39, acc: 0.6300002932548523, Loss 0.8337622520327568\n",
      "LSTM, rep: 0, epoch: 40, acc: 0.7033334970474243, Loss 0.7730793604254722\n",
      "LSTM, rep: 0, epoch: 41, acc: 0.6066666841506958, Loss 0.9323314377665519\n",
      "LSTM, rep: 0, epoch: 42, acc: 0.6700002551078796, Loss 0.7972274586558342\n",
      "LSTM, rep: 0, epoch: 43, acc: 0.7166666388511658, Loss 0.7387641444802284\n",
      "LSTM, rep: 0, epoch: 44, acc: 0.7266666889190674, Loss 0.7316399756073951\n",
      "LSTM, rep: 0, epoch: 45, acc: 0.6933334469795227, Loss 0.7281358551979065\n",
      "LSTM, rep: 0, epoch: 46, acc: 0.6600000858306885, Loss 0.7279883924126626\n",
      "LSTM, rep: 0, epoch: 47, acc: 0.7166666984558105, Loss 0.7085685387253762\n",
      "LSTM, rep: 0, epoch: 48, acc: 0.6466670036315918, Loss 0.7749582719802857\n",
      "LSTM, rep: 0, epoch: 49, acc: 0.7033334970474243, Loss 0.7373501464724541\n",
      "LSTM, rep: 0, epoch: 50, acc: 0.6800002455711365, Loss 0.6968058916926384\n",
      "LSTM, rep: 0, epoch: 51, acc: 0.7000002861022949, Loss 0.7257630467414856\n",
      "LSTM, rep: 0, epoch: 52, acc: 0.7400000095367432, Loss 0.6748578059673309\n",
      "LSTM, rep: 0, epoch: 53, acc: 0.753333330154419, Loss 0.6369686445593834\n",
      "LSTM, rep: 0, epoch: 54, acc: 0.7300002574920654, Loss 0.7126097106933593\n",
      "LSTM, rep: 0, epoch: 55, acc: 0.7866666316986084, Loss 0.6416708731651306\n",
      "LSTM, rep: 0, epoch: 56, acc: 0.7600000500679016, Loss 0.6526537099480629\n",
      "LSTM, rep: 0, epoch: 57, acc: 0.76666659116745, Loss 0.6189889821410179\n",
      "LSTM, rep: 0, epoch: 58, acc: 0.7499999403953552, Loss 0.6415071600675583\n",
      "LSTM, rep: 0, epoch: 59, acc: 0.7066669464111328, Loss 0.7338871634006501\n",
      "LSTM, rep: 0, epoch: 60, acc: 0.7333333492279053, Loss 0.6230649152398109\n",
      "LSTM, rep: 0, epoch: 61, acc: 0.736666738986969, Loss 0.5856325834989548\n",
      "LSTM, rep: 0, epoch: 62, acc: 0.7333333492279053, Loss 0.6537588453292846\n",
      "LSTM, rep: 0, epoch: 63, acc: 0.7000002264976501, Loss 0.6294205263257027\n",
      "LSTM, rep: 0, epoch: 64, acc: 0.7300000786781311, Loss 0.6190068140625954\n",
      "LSTM, rep: 0, epoch: 65, acc: 0.7300000786781311, Loss 0.5794391040503979\n",
      "LSTM, rep: 0, epoch: 66, acc: 0.7633333802223206, Loss 0.5733023558557033\n",
      "LSTM, rep: 0, epoch: 67, acc: 0.8199999928474426, Loss 0.5393951439857483\n",
      "LSTM, rep: 0, epoch: 68, acc: 0.7866666913032532, Loss 0.5616594171524047\n",
      "LSTM, rep: 0, epoch: 69, acc: 0.8066664934158325, Loss 0.5417351086437702\n",
      "LSTM, rep: 0, epoch: 70, acc: 0.8033332228660583, Loss 0.5652609895169735\n",
      "LSTM, rep: 0, epoch: 71, acc: 0.7966663241386414, Loss 0.5472105948626995\n",
      "LSTM, rep: 0, epoch: 72, acc: 0.8066665530204773, Loss 0.579967943429947\n",
      "LSTM, rep: 0, epoch: 73, acc: 0.566666841506958, Loss 1.151426863670349\n",
      "LSTM, rep: 0, epoch: 74, acc: 0.7500000596046448, Loss 0.7765319195389747\n",
      "LSTM, rep: 0, epoch: 75, acc: 0.7433334589004517, Loss 0.7499804416298866\n",
      "LSTM, rep: 0, epoch: 76, acc: 0.7333333492279053, Loss 0.8044226105511189\n",
      "LSTM, rep: 0, epoch: 77, acc: 0.7533334493637085, Loss 0.6992574419081211\n",
      "LSTM, rep: 0, epoch: 78, acc: 0.7699998617172241, Loss 0.6947214904427529\n",
      "LSTM, rep: 0, epoch: 79, acc: 0.763333261013031, Loss 0.6993389439582824\n",
      "LSTM, rep: 0, epoch: 80, acc: 0.7433333396911621, Loss 0.6774982728064061\n",
      "LSTM, rep: 0, epoch: 81, acc: 0.753333330154419, Loss 0.6712314686179162\n",
      "LSTM, rep: 0, epoch: 82, acc: 0.8066665530204773, Loss 0.5570380798727274\n",
      "LSTM, rep: 0, epoch: 83, acc: 0.8133332133293152, Loss 0.5234294681251049\n",
      "LSTM, rep: 0, epoch: 84, acc: 0.8633332252502441, Loss 0.45067988507449624\n",
      "LSTM, rep: 0, epoch: 85, acc: 0.8266666531562805, Loss 0.47814134895801547\n",
      "LSTM, rep: 0, epoch: 86, acc: 0.84333336353302, Loss 0.427099139764905\n",
      "LSTM, rep: 0, epoch: 87, acc: 0.8366667032241821, Loss 0.41245443277060984\n",
      "LSTM, rep: 0, epoch: 88, acc: 0.8499999046325684, Loss 0.4087695833295584\n",
      "LSTM, rep: 0, epoch: 89, acc: 0.7999999523162842, Loss 0.5336962540447712\n",
      "LSTM, rep: 0, epoch: 90, acc: 0.8200000524520874, Loss 0.46050928834825755\n",
      "LSTM, rep: 0, epoch: 91, acc: 0.8166664838790894, Loss 0.43923093341290953\n",
      "LSTM, rep: 0, epoch: 92, acc: 0.7999995946884155, Loss 0.48898417718708515\n",
      "LSTM, rep: 0, epoch: 93, acc: 0.836666464805603, Loss 0.41501073688268664\n",
      "LSTM, rep: 0, epoch: 94, acc: 0.8299999833106995, Loss 0.44314880777150395\n",
      "LSTM, rep: 0, epoch: 95, acc: 0.8600000143051147, Loss 0.37443556495010855\n",
      "LSTM, rep: 0, epoch: 96, acc: 0.8566665649414062, Loss 0.3679908468946815\n",
      "LSTM, rep: 0, epoch: 97, acc: 0.8633333444595337, Loss 0.35210586942732336\n",
      "LSTM, rep: 0, epoch: 98, acc: 0.8399998545646667, Loss 0.3926646341755986\n",
      "LSTM, rep: 0, epoch: 99, acc: 0.8233333826065063, Loss 0.4039658426865935\n",
      "LSTM, rep: 0, epoch: 100, acc: 0.8333331346511841, Loss 0.37493929170072077\n",
      "LSTM, rep: 0, epoch: 101, acc: 0.8499999046325684, Loss 0.38205978121608497\n",
      "LSTM, rep: 0, epoch: 102, acc: 0.8433331847190857, Loss 0.39641767360270025\n",
      "LSTM, rep: 0, epoch: 103, acc: 0.8366665840148926, Loss 0.37583171751350164\n",
      "LSTM, rep: 0, epoch: 104, acc: 0.8466665148735046, Loss 0.3505120203830302\n",
      "LSTM, rep: 0, epoch: 105, acc: 0.8466665744781494, Loss 0.37105004884302617\n",
      "LSTM, rep: 0, epoch: 106, acc: 0.8366664052009583, Loss 0.36548040457069875\n",
      "LSTM, rep: 0, epoch: 107, acc: 0.8166666626930237, Loss 0.4571142791584134\n",
      "LSTM, rep: 0, epoch: 108, acc: 0.8399999737739563, Loss 0.3573015210777521\n",
      "LSTM, rep: 0, epoch: 109, acc: 0.8199997544288635, Loss 0.3918357258476317\n",
      "LSTM, rep: 0, epoch: 110, acc: 0.8266667127609253, Loss 0.4169312099739909\n",
      "LSTM, rep: 0, epoch: 111, acc: 0.8699997067451477, Loss 0.30306180665269494\n",
      "LSTM, rep: 0, epoch: 112, acc: 0.823333203792572, Loss 0.3670746909081936\n",
      "LSTM, rep: 0, epoch: 113, acc: 0.8099998235702515, Loss 0.386377917341888\n",
      "LSTM, rep: 0, epoch: 114, acc: 0.8366665840148926, Loss 0.32991514217108486\n",
      "LSTM, rep: 0, epoch: 115, acc: 0.8266666531562805, Loss 0.3548809218406677\n",
      "LSTM, rep: 0, epoch: 116, acc: 0.8399999737739563, Loss 0.35740324007347224\n",
      "LSTM, rep: 0, epoch: 117, acc: 0.8199998736381531, Loss 0.3857230047136545\n",
      "LSTM, rep: 0, epoch: 118, acc: 0.8266666531562805, Loss 0.35197653707116844\n",
      "LSTM, rep: 0, epoch: 119, acc: 0.8399998545646667, Loss 0.33821399971842764\n",
      "LSTM, rep: 0, epoch: 120, acc: 0.836666464805603, Loss 0.3326772480458021\n",
      "LSTM, rep: 0, epoch: 121, acc: 0.8366666436195374, Loss 0.33116879910230634\n",
      "LSTM, rep: 0, epoch: 122, acc: 0.8499997854232788, Loss 0.350394630022347\n",
      "LSTM, rep: 0, epoch: 123, acc: 0.8766665458679199, Loss 0.2685889332741499\n",
      "LSTM, rep: 0, epoch: 124, acc: 0.8333332538604736, Loss 0.3333310459367931\n",
      "LSTM, rep: 0, epoch: 125, acc: 0.8066665530204773, Loss 0.4388307969458401\n",
      "LSTM, rep: 0, epoch: 126, acc: 0.793333113193512, Loss 0.5753046089131385\n",
      "LSTM, rep: 0, epoch: 127, acc: 0.8299999833106995, Loss 0.3741486075334251\n",
      "LSTM, rep: 0, epoch: 128, acc: 0.8366665840148926, Loss 0.327110262773931\n",
      "LSTM, rep: 0, epoch: 129, acc: 0.8733332753181458, Loss 0.3135166764818132\n",
      "LSTM, rep: 0, epoch: 130, acc: 0.8666664361953735, Loss 0.33958921922370794\n",
      "LSTM, rep: 0, epoch: 131, acc: 0.8733332753181458, Loss 0.3269066602177918\n",
      "LSTM, rep: 0, epoch: 132, acc: 0.8599997758865356, Loss 0.34200923817232254\n",
      "LSTM, rep: 0, epoch: 133, acc: 0.8933331966400146, Loss 0.2816782120242715\n",
      "LSTM, rep: 0, epoch: 134, acc: 0.8866666555404663, Loss 0.28971745694056156\n",
      "LSTM, rep: 0, epoch: 135, acc: 0.8433334827423096, Loss 0.44127568224444985\n",
      "LSTM, rep: 0, epoch: 136, acc: 0.9133331775665283, Loss 0.2723091155849397\n",
      "LSTM, rep: 0, epoch: 137, acc: 0.9233332872390747, Loss 0.2586940197274089\n",
      "LSTM, rep: 0, epoch: 138, acc: 0.8999999165534973, Loss 0.28176647262647747\n",
      "LSTM, rep: 0, epoch: 139, acc: 0.8999998569488525, Loss 0.31239003894850614\n",
      "LSTM, rep: 0, epoch: 140, acc: 0.9133331179618835, Loss 0.2684074773266911\n",
      "LSTM, rep: 0, epoch: 141, acc: 0.9100000262260437, Loss 0.2659362250752747\n",
      "LSTM, rep: 0, epoch: 142, acc: 0.9033333659172058, Loss 0.27268596418201924\n",
      "LSTM, rep: 0, epoch: 143, acc: 0.9200000166893005, Loss 0.23177197586745024\n",
      "LSTM, rep: 0, epoch: 144, acc: 0.8166666626930237, Loss 0.47955970011651516\n",
      "LSTM, rep: 0, epoch: 145, acc: 0.8533332943916321, Loss 0.3722608236037195\n",
      "LSTM, rep: 0, epoch: 146, acc: 0.8999998569488525, Loss 0.26381504705175757\n",
      "LSTM, rep: 0, epoch: 147, acc: 0.9299999475479126, Loss 0.2245618744753301\n",
      "LSTM, rep: 0, epoch: 148, acc: 0.9066665768623352, Loss 0.2426036830432713\n",
      "LSTM, rep: 0, epoch: 149, acc: 0.8766664862632751, Loss 0.2780286478437483\n",
      "LSTM, rep: 0, epoch: 150, acc: 0.9399998188018799, Loss 0.21576738515868782\n",
      "LSTM, rep: 0, epoch: 151, acc: 0.9266666173934937, Loss 0.21323355477303266\n",
      "LSTM, rep: 0, epoch: 152, acc: 0.9633333086967468, Loss 0.1815675821341574\n",
      "LSTM, rep: 0, epoch: 153, acc: 0.9466664791107178, Loss 0.17838983984664084\n",
      "LSTM, rep: 0, epoch: 154, acc: 0.9533331990242004, Loss 0.20170048305764796\n",
      "LSTM, rep: 0, epoch: 155, acc: 0.9100001454353333, Loss 0.3769845779053867\n",
      "LSTM, rep: 0, epoch: 156, acc: 0.9466665387153625, Loss 0.2288099415227771\n",
      "LSTM, rep: 0, epoch: 157, acc: 0.9799998998641968, Loss 0.16615054754540323\n",
      "LSTM                 Rep: 0   Epoch: 1     Acc: 0.9800 Params: min_length: 40, max_length: 45, fill: 0, value_1: -1, value_2: 1 Time: 43.97 sec\n",
      "NetLSTMWithAttention, rep: 0, epoch: 1, acc: 0.49000003933906555, Loss 1.0971712911128997\n",
      "NetLSTMWithAttention, rep: 0, epoch: 2, acc: 0.46000000834465027, Loss 1.0570241683721542\n",
      "NetLSTMWithAttention, rep: 0, epoch: 3, acc: 0.5433334708213806, Loss 0.9969478106498718\n",
      "NetLSTMWithAttention, rep: 0, epoch: 4, acc: 0.5166666507720947, Loss 1.0076695013046264\n",
      "NetLSTMWithAttention, rep: 0, epoch: 5, acc: 0.4466666877269745, Loss 1.0112525182962417\n",
      "NetLSTMWithAttention, rep: 0, epoch: 6, acc: 0.4866667091846466, Loss 1.0047391659021379\n",
      "NetLSTMWithAttention, rep: 0, epoch: 7, acc: 0.5033332705497742, Loss 1.0017814463377\n",
      "NetLSTMWithAttention, rep: 0, epoch: 8, acc: 0.4899999499320984, Loss 1.0077908462285996\n",
      "NetLSTMWithAttention, rep: 0, epoch: 9, acc: 0.49666664004325867, Loss 1.000049661397934\n",
      "NetLSTMWithAttention, rep: 0, epoch: 10, acc: 0.5033331513404846, Loss 0.9973912405967712\n",
      "NetLSTMWithAttention, rep: 0, epoch: 11, acc: 0.5066666603088379, Loss 0.9965507423877716\n",
      "NetLSTMWithAttention, rep: 0, epoch: 12, acc: 0.6033334732055664, Loss 0.9934150093793869\n",
      "NetLSTMWithAttention, rep: 0, epoch: 13, acc: 0.5733336210250854, Loss 0.9725363498926163\n",
      "NetLSTMWithAttention, rep: 0, epoch: 14, acc: 0.6400004029273987, Loss 0.8293823331594468\n",
      "NetLSTMWithAttention, rep: 0, epoch: 15, acc: 0.6566666960716248, Loss 0.6960218811035156\n",
      "NetLSTMWithAttention, rep: 0, epoch: 16, acc: 0.7000001668930054, Loss 0.6787747180461884\n",
      "NetLSTMWithAttention, rep: 0, epoch: 17, acc: 0.6800000667572021, Loss 0.6702283704280854\n",
      "NetLSTMWithAttention, rep: 0, epoch: 18, acc: 0.6899999976158142, Loss 0.6741655743122101\n",
      "NetLSTMWithAttention, rep: 0, epoch: 19, acc: 0.7233333587646484, Loss 0.6523696917295456\n",
      "NetLSTMWithAttention, rep: 0, epoch: 20, acc: 0.7666667103767395, Loss 0.6037969601154327\n",
      "NetLSTMWithAttention, rep: 0, epoch: 21, acc: 0.8033332824707031, Loss 0.5695074006915093\n",
      "NetLSTMWithAttention, rep: 0, epoch: 22, acc: 0.7300000190734863, Loss 0.8240294247865677\n",
      "NetLSTMWithAttention, rep: 0, epoch: 23, acc: 0.8033333420753479, Loss 0.673391809463501\n",
      "NetLSTMWithAttention, rep: 0, epoch: 24, acc: 0.8299997448921204, Loss 0.5215335653722286\n",
      "NetLSTMWithAttention, rep: 0, epoch: 25, acc: 0.9299999475479126, Loss 0.3632755897939205\n",
      "NetLSTMWithAttention, rep: 0, epoch: 26, acc: 0.940000057220459, Loss 0.3069596865773201\n",
      "NetLSTMWithAttention, rep: 0, epoch: 27, acc: 0.9466667175292969, Loss 0.3015876804292202\n",
      "NetLSTMWithAttention, rep: 0, epoch: 28, acc: 0.9533333778381348, Loss 0.25529006764292717\n",
      "NetLSTMWithAttention, rep: 0, epoch: 29, acc: 0.9799998998641968, Loss 0.22815356478095056\n",
      "NetLSTMWithAttention Rep: 0   Epoch: 1     Acc: 0.9800 Params: min_length: 40, max_length: 45, fill: 0, value_1: -1, value_2: 1 Time: 12.07 sec\n",
      "GRU, rep: 0, epoch: 1, acc: 0.48000025749206543, Loss 1.0455686938762665\n",
      "GRU, rep: 0, epoch: 2, acc: 0.4566665291786194, Loss 1.024442386031151\n",
      "GRU, rep: 0, epoch: 3, acc: 0.5166664123535156, Loss 1.0022657257318497\n",
      "GRU, rep: 0, epoch: 4, acc: 0.5233333706855774, Loss 0.9999149924516678\n",
      "GRU, rep: 0, epoch: 5, acc: 0.49999985098838806, Loss 1.0104133915901183\n",
      "GRU, rep: 0, epoch: 6, acc: 0.5800001621246338, Loss 0.988601855635643\n",
      "GRU, rep: 0, epoch: 7, acc: 0.5500000715255737, Loss 0.9973445630073547\n",
      "GRU, rep: 0, epoch: 8, acc: 0.5166667103767395, Loss 1.0006680804491044\n",
      "GRU, rep: 0, epoch: 9, acc: 0.49333301186561584, Loss 1.0073437744379043\n",
      "GRU, rep: 0, epoch: 10, acc: 0.4866667687892914, Loss 1.0119645649194717\n",
      "GRU, rep: 0, epoch: 11, acc: 0.556666374206543, Loss 0.9917865300178528\n",
      "GRU, rep: 0, epoch: 12, acc: 0.5066667795181274, Loss 1.0088222354650498\n",
      "GRU, rep: 0, epoch: 13, acc: 0.5266664624214172, Loss 0.9916476678848266\n",
      "GRU, rep: 0, epoch: 14, acc: 0.5166667103767395, Loss 0.9926036465167999\n",
      "GRU, rep: 0, epoch: 15, acc: 0.4666666090488434, Loss 1.0140288656949996\n",
      "GRU, rep: 0, epoch: 16, acc: 0.5233333110809326, Loss 1.0028356206417084\n",
      "GRU, rep: 0, epoch: 17, acc: 0.4966665506362915, Loss 1.0048387730121613\n",
      "GRU, rep: 0, epoch: 18, acc: 0.5233330726623535, Loss 0.9987908017635345\n",
      "GRU, rep: 0, epoch: 19, acc: 0.5266666412353516, Loss 0.9956484699249267\n",
      "GRU, rep: 0, epoch: 20, acc: 0.5233333110809326, Loss 0.9939555257558823\n",
      "GRU, rep: 0, epoch: 21, acc: 0.536666750907898, Loss 0.9995234793424607\n",
      "GRU, rep: 0, epoch: 22, acc: 0.5266665816307068, Loss 0.9951381742954254\n",
      "GRU, rep: 0, epoch: 23, acc: 0.5166664719581604, Loss 1.0008597677946092\n",
      "GRU, rep: 0, epoch: 24, acc: 0.5166666507720947, Loss 0.9932452023029328\n",
      "GRU, rep: 0, epoch: 25, acc: 0.6133333444595337, Loss 0.9583696782588959\n",
      "GRU, rep: 0, epoch: 26, acc: 0.6100001335144043, Loss 0.8391509795188904\n",
      "GRU, rep: 0, epoch: 27, acc: 0.6800003051757812, Loss 0.6952281087636948\n",
      "GRU, rep: 0, epoch: 28, acc: 0.6499998569488525, Loss 0.6830401527881622\n",
      "GRU, rep: 0, epoch: 29, acc: 0.6299998760223389, Loss 0.687196778357029\n",
      "GRU, rep: 0, epoch: 30, acc: 0.68666672706604, Loss 0.6731247252225876\n",
      "GRU, rep: 0, epoch: 31, acc: 0.6566668152809143, Loss 0.6778538197278976\n",
      "GRU, rep: 0, epoch: 32, acc: 0.699999988079071, Loss 0.6708822065591812\n",
      "GRU, rep: 0, epoch: 33, acc: 0.6666667461395264, Loss 0.676602703332901\n",
      "GRU, rep: 0, epoch: 34, acc: 0.6600000858306885, Loss 0.6716534000635147\n",
      "GRU, rep: 0, epoch: 35, acc: 0.7033332586288452, Loss 0.6605754292011261\n",
      "GRU, rep: 0, epoch: 36, acc: 0.6500001549720764, Loss 0.6735703507065773\n",
      "GRU, rep: 0, epoch: 37, acc: 0.7099998593330383, Loss 0.65115587413311\n",
      "GRU, rep: 0, epoch: 38, acc: 0.6833333373069763, Loss 0.6631645411252975\n",
      "GRU, rep: 0, epoch: 39, acc: 0.6699998378753662, Loss 0.6759959888458252\n",
      "GRU, rep: 0, epoch: 40, acc: 0.6233333945274353, Loss 0.6808222818374634\n",
      "GRU, rep: 0, epoch: 41, acc: 0.68666672706604, Loss 0.6653384196758271\n",
      "GRU, rep: 0, epoch: 42, acc: 0.6499998569488525, Loss 0.6745767611265182\n",
      "GRU, rep: 0, epoch: 43, acc: 0.6866667866706848, Loss 0.672627626657486\n",
      "GRU, rep: 0, epoch: 44, acc: 0.6566665172576904, Loss 0.668926237821579\n",
      "GRU, rep: 0, epoch: 45, acc: 0.6866667866706848, Loss 0.6669929778575897\n",
      "GRU, rep: 0, epoch: 46, acc: 0.7133333086967468, Loss 0.6552808791399002\n",
      "GRU, rep: 0, epoch: 47, acc: 0.7333332896232605, Loss 0.652084943652153\n",
      "GRU, rep: 0, epoch: 48, acc: 0.7000001668930054, Loss 0.6555435991287232\n",
      "GRU, rep: 0, epoch: 49, acc: 0.7233332991600037, Loss 0.6470110109448433\n",
      "GRU, rep: 0, epoch: 50, acc: 0.8266664743423462, Loss 0.5874944162368775\n",
      "GRU, rep: 0, epoch: 51, acc: 0.8499999046325684, Loss 0.5217581650614739\n",
      "GRU, rep: 0, epoch: 52, acc: 0.8333332538604736, Loss 0.47221434861421585\n",
      "GRU, rep: 0, epoch: 53, acc: 0.8499998450279236, Loss 0.39196503221988677\n",
      "GRU, rep: 0, epoch: 54, acc: 0.8799998760223389, Loss 0.34564646989107134\n",
      "GRU, rep: 0, epoch: 55, acc: 0.8433336019515991, Loss 0.5009868754446507\n",
      "GRU, rep: 0, epoch: 56, acc: 0.6333332657814026, Loss 1.062588566839695\n",
      "GRU, rep: 0, epoch: 57, acc: 0.7166666388511658, Loss 0.8023101007938385\n",
      "GRU, rep: 0, epoch: 58, acc: 0.8999997973442078, Loss 0.33961960837244987\n",
      "GRU, rep: 0, epoch: 59, acc: 0.9266665577888489, Loss 0.32469701766967773\n",
      "GRU, rep: 0, epoch: 60, acc: 0.9433331489562988, Loss 0.27658823803067206\n",
      "GRU, rep: 0, epoch: 61, acc: 0.9366664886474609, Loss 0.2607458682358265\n",
      "GRU, rep: 0, epoch: 62, acc: 0.9600000977516174, Loss 0.2315032547712326\n",
      "GRU, rep: 0, epoch: 63, acc: 0.9699999094009399, Loss 0.21191581696271897\n",
      "GRU, rep: 0, epoch: 64, acc: 0.9699999094009399, Loss 0.19580006808042527\n",
      "GRU, rep: 0, epoch: 65, acc: 1.0, Loss 0.16410405054688454\n",
      "GRU                  Rep: 0   Epoch: 1     Acc: 1.0000 Params: min_length: 40, max_length: 45, fill: 0, value_1: -1, value_2: 1 Time: 66.40 sec\n",
      "NetGRUWithAttention, rep: 0, epoch: 1, acc: 0.48333320021629333, Loss 1.0373797261714934\n",
      "NetGRUWithAttention, rep: 0, epoch: 2, acc: 0.5133334398269653, Loss 1.008633667230606\n",
      "NetGRUWithAttention, rep: 0, epoch: 3, acc: 0.48333340883255005, Loss 1.0043031919002532\n",
      "NetGRUWithAttention, rep: 0, epoch: 4, acc: 0.5333333611488342, Loss 1.0003385907411575\n",
      "NetGRUWithAttention, rep: 0, epoch: 5, acc: 0.5366666913032532, Loss 0.9941910123825073\n",
      "NetGRUWithAttention, rep: 0, epoch: 6, acc: 0.49000003933906555, Loss 1.0082349944114686\n",
      "NetGRUWithAttention, rep: 0, epoch: 7, acc: 0.5366666913032532, Loss 0.9963028138875961\n",
      "NetGRUWithAttention, rep: 0, epoch: 8, acc: 0.47333332896232605, Loss 1.0088644140958787\n",
      "NetGRUWithAttention, rep: 0, epoch: 9, acc: 0.5066666603088379, Loss 1.0021223229169847\n",
      "NetGRUWithAttention, rep: 0, epoch: 10, acc: 0.5200000405311584, Loss 0.9911626529693603\n",
      "NetGRUWithAttention, rep: 0, epoch: 11, acc: 0.6033333539962769, Loss 0.9579451978206635\n",
      "NetGRUWithAttention, rep: 0, epoch: 12, acc: 0.489999920129776, Loss 0.9895026385784149\n",
      "NetGRUWithAttention, rep: 0, epoch: 13, acc: 0.6433334350585938, Loss 0.9292132622003555\n",
      "NetGRUWithAttention, rep: 0, epoch: 14, acc: 0.6766666173934937, Loss 0.7470249226689338\n",
      "NetGRUWithAttention, rep: 0, epoch: 15, acc: 0.6400001645088196, Loss 0.692817752957344\n",
      "NetGRUWithAttention, rep: 0, epoch: 16, acc: 0.7233333587646484, Loss 0.6569667211174965\n",
      "NetGRUWithAttention, rep: 0, epoch: 17, acc: 0.6366665959358215, Loss 0.6823015308380127\n",
      "NetGRUWithAttention, rep: 0, epoch: 18, acc: 0.7100000977516174, Loss 0.6564791697263718\n",
      "NetGRUWithAttention, rep: 0, epoch: 19, acc: 0.7733331322669983, Loss 0.6229676190018654\n",
      "NetGRUWithAttention, rep: 0, epoch: 20, acc: 0.8433331251144409, Loss 0.5333494347333908\n",
      "NetGRUWithAttention, rep: 0, epoch: 21, acc: 0.8766665458679199, Loss 0.3664008195698261\n",
      "NetGRUWithAttention, rep: 0, epoch: 22, acc: 0.8733333349227905, Loss 0.33991088449954987\n",
      "NetGRUWithAttention, rep: 0, epoch: 23, acc: 0.9266665577888489, Loss 0.28810664132237435\n",
      "NetGRUWithAttention, rep: 0, epoch: 24, acc: 0.916666567325592, Loss 0.2775133521854877\n",
      "NetGRUWithAttention, rep: 0, epoch: 25, acc: 0.9533332586288452, Loss 0.25568003550171853\n",
      "NetGRUWithAttention, rep: 0, epoch: 26, acc: 0.9699999094009399, Loss 0.22170558117330075\n",
      "NetGRUWithAttention, rep: 0, epoch: 27, acc: 0.9633331894874573, Loss 0.20968831203877925\n",
      "NetGRUWithAttention, rep: 0, epoch: 28, acc: 0.966666579246521, Loss 0.18686874486505986\n",
      "NetGRUWithAttention, rep: 0, epoch: 29, acc: 0.9633333086967468, Loss 0.17911079220473766\n",
      "NetGRUWithAttention, rep: 0, epoch: 30, acc: 0.9599999785423279, Loss 0.17141532242298127\n",
      "NetGRUWithAttention, rep: 0, epoch: 31, acc: 0.9700000286102295, Loss 0.1657758029550314\n",
      "NetGRUWithAttention  Rep: 0   Epoch: 1     Acc: 0.9700 Params: min_length: 40, max_length: 45, fill: 0, value_1: -1, value_2: 1 Time: 35.19 sec\n"
     ]
    }
   ],
   "source": [
    "collectorA = dict()\n",
    "num_samples = 100\n",
    "for rep in range(1):  # Number of repetitions\n",
    "    for params in parameters_list:\n",
    "        params_str = \", \".join([f\"{key}: {value}\" for key, value in params.items()])\n",
    "\n",
    "        # Select the model based on 'kind'\n",
    "        for kind in [\"RNN\", \"NetRNNWithAttention\", \"NetRNNWithAttentionExpFirst\",\"LSTM\", \"NetLSTMWithAttention\", \"GRU\", \"NetGRUWithAttention\"]:\n",
    "            if kind == \"RNN\":\n",
    "                model = NetRNN(hidden_dim=12, inp=3)\n",
    "            elif kind == \"NetRNNWithAttention\":\n",
    "                model = NetRNNWithAttention(hidden_dim=12, inp=3)\n",
    "            elif kind == \"NetRNNWithAttentionExpFirst\":\n",
    "                model = NetRNNWithAttentionExpFirst(hidden_dim=12, inp=3)\n",
    "            elif kind == \"LSTM\":\n",
    "                model = NetLSTM(hidden_dim=12, inp=3)\n",
    "            elif kind == \"NetLSTMWithAttention\":\n",
    "                model = NetLSTMWithAttention(hidden_dim=12, inp=3)\n",
    "            elif kind == \"GRU\":\n",
    "                model = NetGRU(hidden_dim=12, inp=3)\n",
    "            elif kind == \"NetGRUWithAttention\":\n",
    "                model = NetGRUMWithAttention(hidden_dim=12, inp=3)\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters())\n",
    "            criterion = nn.MSELoss()\n",
    "            acc = 0.0\n",
    "            W = []\n",
    "            AC = []\n",
    "            start_time = time.time()  # Start time of the epoch\n",
    "\n",
    "            while True:\n",
    "                sequences, targets = generateTrainData(num_samples, params)\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                count = 0\n",
    "\n",
    "                for seq, target in zip(sequences, targets):\n",
    "                    optimizer.zero_grad()\n",
    "                    seq_tensor = torch.Tensor([seq])  # Add an extra dimension for batch\n",
    "                    target_tensor = torch.Tensor([target])\n",
    "\n",
    "                    output = model(seq_tensor)\n",
    "                    loss = criterion(output, target_tensor)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    A = 1.0 * (target_tensor.detach() > 0.0)\n",
    "                    B = 1.0 * (output.detach() > 0.0)\n",
    "                    acc = (1.0 * (A.flatten() == B.flatten())).mean()\n",
    "                    total_acc += acc\n",
    "                    count += 1\n",
    "\n",
    "                avg_loss = total_loss / count\n",
    "                avg_acc = total_acc / count\n",
    "                W.append(avg_loss)\n",
    "                AC.append(avg_acc)\n",
    "                print(f\"{kind}, rep: {rep}, epoch: {len(AC)}, acc: {avg_acc}, Loss {avg_loss}\")\n",
    "\n",
    "                # Check for stopping condition\n",
    "                if avg_acc >= 0.97:\n",
    "                    break\n",
    "\n",
    "            end_time = time.time()  # End time of the epoch\n",
    "            epoch_duration = end_time - start_time  # Calculate duration\n",
    "            collectorA[f\"{kind} {rep}\"] = A\n",
    "            params_save_str = \" \".join([f\"{key}_{value}\" for key, value in params.items()])\n",
    "            torch.save(model, f'models/model_{kind}_{params_save_str}.model')\n",
    "            print(f\"{kind:<20} Rep: {rep:<3} Epoch: {len(A):<5} Acc: {avg_acc:.4f} \" f\"Params: {params_str:<40} Time: {epoch_duration:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:01:12.582743400Z",
     "start_time": "2023-12-06T17:01:12.545745700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy: 97.03%\n",
      "Average Test Loss: 0.2337\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./models/model_RNN_min_length_5 max_length_5 fill_0 value_1_-1 value_2_1.model\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "params = {\n",
    "    'min_length': 5, \n",
    "    'max_length': 5, \n",
    "    'fill': 0, \n",
    "    'value_1': -1, \n",
    "    'value_2': 1\n",
    "}\n",
    "\n",
    "# Assuming generateTrainData is defined and returns sequences and targets\n",
    "sequences, targets = generateTrainData(1000, params)\n",
    "\n",
    "# Convert sequences and targets to tensors and pad sequences\n",
    "seq_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "padded_seq_tensors = pad_sequence(seq_tensors, batch_first=True)\n",
    "target_tensors = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Evaluate the model on test data\n",
    "with torch.no_grad():\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for seq_tensor, target_tensor in zip(padded_seq_tensors, target_tensors):\n",
    "        # Add batch dimension\n",
    "        seq_tensor = seq_tensor.unsqueeze(0)\n",
    "        target_tensor = target_tensor.unsqueeze(0)\n",
    "\n",
    "        output = model(seq_tensor)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target_tensor)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        A = 1.0 * (target_tensor.detach() > 0.0)\n",
    "        B = 1.0 * (output.detach() > 0.0)\n",
    "        acc = (1.0 * (A.flatten() == B.flatten())).mean()\n",
    "        total_acc += acc.item()\n",
    "    # Calculate average accuracy and loss\n",
    "    avg_acc = total_acc / len(padded_seq_tensors)\n",
    "    avg_loss = total_loss / len(padded_seq_tensors)\n",
    "    print(f\"Average Test Accuracy: {avg_acc * 100:.2f}%\")\n",
    "    print(f\"Average Test Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 3])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Accuracy: 100.00%\n",
      "Average Test Loss: 0.1706\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./models/model_NetRNNWithAttention_min_length_5 max_length_5 fill_0 value_1_-1 value_2_1.model\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "params = {\n",
    "    'min_length': 5, \n",
    "    'max_length': 5, \n",
    "    'fill': 0, \n",
    "    'value_1': -1, \n",
    "    'value_2': 1\n",
    "}\n",
    "\n",
    "# Assuming generateTrainData is defined and returns sequences and targets\n",
    "sequences, targets = generateTrainData(1000, params)\n",
    "\n",
    "# Convert sequences and targets to tensors and pad sequences\n",
    "seq_tensors = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "padded_seq_tensors = pad_sequence(seq_tensors, batch_first=True)\n",
    "target_tensors = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Evaluate the model on test data\n",
    "with torch.no_grad():\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for seq_tensor, target_tensor in zip(padded_seq_tensors, target_tensors):\n",
    "        # Add batch dimension\n",
    "        seq_tensor = seq_tensor.unsqueeze(0)\n",
    "        target_tensor = target_tensor.unsqueeze(0)\n",
    "\n",
    "        output = model(seq_tensor)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target_tensor)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        A = (target_tensor > 0.0).float()\n",
    "        B = (output > 0.0).float()\n",
    "        acc = (A.flatten() == B.flatten()).float().mean()\n",
    "        total_acc += acc.item()\n",
    "    # Calculate average accuracy and loss\n",
    "    avg_acc = total_acc / len(padded_seq_tensors)\n",
    "    avg_loss = total_loss / len(padded_seq_tensors)\n",
    "    print(f\"Average Test Accuracy: {avg_acc * 100:.2f}%\")\n",
    "    print(f\"Average Test Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T17:50:29.630985800Z",
     "start_time": "2023-12-06T17:50:29.440984500Z"
    }
   },
   "outputs": [],
   "source": [
    "def symbolEntropy(D,base=2):\n",
    "    value,counts = numpy.unique(D, return_counts=True)\n",
    "    return entropy(counts,base=base)\n",
    "\n",
    "def computeTransmissionHfast(I,H,O,maskC,maskNC,iMult=2,oMult=2):\n",
    "    #print(\"I H O\",I.shape,H.shape,O.shape)\n",
    "    B=numpy.bitwise_and(H,maskNC)\n",
    "    IB=(B*iMult)+I\n",
    "    AB=H#numpy.bitwise_and(H,maskC+maskNC)\n",
    "    BO=(B*oMult)+O\n",
    "    IAB=(AB*iMult)+I\n",
    "    IBO=(B*(iMult*oMult))+(I*oMult)+O\n",
    "    ABO=(AB*oMult)+O\n",
    "    IABO=(AB*(iMult*oMult))+(I*oMult)+O\n",
    "    hB=symbolEntropy(B, base=2)\n",
    "    hIB=symbolEntropy(IB, base=2)\n",
    "    hAB=symbolEntropy(AB, base=2)\n",
    "    hBO=symbolEntropy(BO, base=2)\n",
    "    hIAB=symbolEntropy(IAB, base=2)\n",
    "    hIBO=symbolEntropy(IBO, base=2)\n",
    "    hABO=symbolEntropy(ABO, base=2)\n",
    "    hIABO=symbolEntropy(IABO, base=2)\n",
    "    #-H(B)+H(IB)+H(AB)+H(BO)-H(IAB)-H(IBO)-H(ABO)+H(IABO)\n",
    "    #print(hB,hIB,hAB,hBO,hIAB,hIBO,hABO,hIABO)\n",
    "    return-hB+hIB+hAB+hBO-hIAB-hIBO-hABO+hIABO\n",
    "\n",
    "def singleShrinkingDecompositionInformation(I,H,O,width,iMult=2,oMult=2):\n",
    "    nodes=list(range(width))\n",
    "    cols=[]\n",
    "    colh=[]\n",
    "    while len(nodes)>0:\n",
    "        infos=[]\n",
    "        for node in nodes:\n",
    "            subset=copy.deepcopy(nodes)\n",
    "            subset.remove(node)\n",
    "            maskA=0\n",
    "            for s in subset:\n",
    "                maskA+=1*(2**s)\n",
    "            maskA=int(maskA)\n",
    "            maskB=numpy.bitwise_and(numpy.bitwise_not(maskA),((2**width)-1))\n",
    "            h=computeTransmissionHfast(I,H,O,maskA,maskB,iMult=iMult,oMult=oMult)\n",
    "            infos.append(h)\n",
    "        nodeToDrop=nodes[infos.index(max(infos))]\n",
    "        nodes.remove(nodeToDrop)\n",
    "        cols.append(copy.deepcopy(nodes))\n",
    "        colh.append(max(infos))\n",
    "    return cols,colh\n",
    "\n",
    "def getOutTaH(model,dataSet):\n",
    "    O,H=model.step(torch.Tensor(dataSet))\n",
    "    #print(H.shape,H.min(),H.max())\n",
    "    #figure()\n",
    "    #hist(H.flatten())\n",
    "    H=H.transpose()\n",
    "    O=O.transpose()\n",
    "    B=numpy.zeros(H.shape)\n",
    "    clusterNr=2\n",
    "    for i in range(B.shape[0]):\n",
    "        a=H[i].reshape(-1, 1)\n",
    "        if len(numpy.unique(a))==1:\n",
    "            who=numpy.random.randint(len(a))\n",
    "            a[who]=1-a[who]\n",
    "        kmeans = KMeans(n_clusters=clusterNr,n_init=10).fit(a)\n",
    "        B[i]=kmeans.labels_\n",
    "        #B[i]=1.0*(H[i]>numpy.median(H[i]))\n",
    "\n",
    "\n",
    "    H=numpy.zeros((H.shape))\n",
    "    for i in range(12):\n",
    "        H+=B[i]*(clusterNr**i)\n",
    "    H=H.astype((int))\n",
    "    return O,H\n",
    "\n",
    "def shrinkingDecompositionInformation(model,width,dataSet,target,numbers=[0,1,2],whichTS=5,dsLength=8):\n",
    "    output,H=getOutTaH(model,dataSet)\n",
    "    output=output.transpose()[whichTS::dsLength].transpose()\n",
    "    #print(\"target.shape\",target.shape,\"output.shape\",output.shape,\"H.shape\",H.shape,\"dataset.shape\",dataSet.shape)\n",
    "    H=H.transpose()[whichTS::dsLength].transpose()\n",
    "    #target=target.transpose()[whichTS::dsLength].transpose()\n",
    "    #print(H.shape,target.shape,numpy.array(range(512))[whichTS::dsLength])\n",
    "    collectorSet=dict()\n",
    "    collectorH=dict()\n",
    "    for number in numbers:\n",
    "        I=target[number].astype(int)\n",
    "        O=(1.0*(output[number]>0.5)).astype(int)\n",
    "        #print(\"O\",O,\"T\",target[number])\n",
    "        #print(number,\"I.shape\",I.shape,\"O.shape\",O.shape,\"H.shape\",H.shape)\n",
    "        s,h=singleShrinkingDecompositionInformation(I,H,O,width)\n",
    "        collectorSet[number]=s\n",
    "        collectorH[number]=h\n",
    "    return collectorSet,collectorH\n",
    "\n",
    "def removalIntoVec(res,width,H):\n",
    "    V=numpy.zeros(width)\n",
    "    #for i,r in enumerate(res):\n",
    "    #    for e in r:\n",
    "    #        V[e]+=H[0]-H[i]\n",
    "    fullSet=list(range(width))\n",
    "    nRes=copy.deepcopy(res)\n",
    "    nRes.insert(0,fullSet)\n",
    "    nodeList=[]\n",
    "    for i in range(width):\n",
    "        removedNode=list(set(nRes[i])-set(nRes[i+1]))[0]\n",
    "        nodeList.append(removedNode)\n",
    "    for i,node in enumerate(nodeList):\n",
    "        V[node]=H[0]-H[i]\n",
    "    #V=sqrt(V)\n",
    "    if V.sum()==0:\n",
    "        return V\n",
    "    return V#/V.max()\n",
    "\n",
    "def removalIntoMatrix(res,width,H):\n",
    "    M=[]\n",
    "    for i in range(len(res)):\n",
    "        M.append(removalIntoVec(res[i],width,H[i]))\n",
    "    return numpy.array(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-06T18:20:00.458639400Z",
     "start_time": "2023-12-06T18:19:58.361140500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.1996837037414183\n",
      "-0.010227906162740386 0.27401191478877385\n",
      "-0.03387456693415647 0.3101703481363338\n",
      "-0.034449928879149105 0.3027195803740068\n",
      "-0.06044889514663332 0.46799333191829895\n",
      "-0.047962147985288084 0.532495674081594\n",
      "-0.08606339109642125 0.5122820279838765\n",
      "-0.10148531475270861 0.8334788959098267\n",
      "-0.09330900603545889 0.9268970254188686\n",
      "-0.09701613240532314 0.9973494794585269\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./Models/model_RNN_min_length_10 max_length_10 fill_0 value_1_-1 value_2_1.model\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "params = {\n",
    "            'min_length': 10, \n",
    "            'max_length': 10, \n",
    "            'fill': 0, \n",
    "            'value_1': -1, \n",
    "            'value_2': 1\n",
    "        }\n",
    "\n",
    "# Run the analysis\n",
    "for i in range(10):\n",
    "    s, t = generateTrainData(100, params)  # Use your generateTrainData function\n",
    "    # Visualize the results\n",
    "    S,H=shrinkingDecompositionInformation(model,12,s,t.transpose(),whichTS=i,dsLength=10)\n",
    "    subplot(6,2,i+1)\n",
    "    M=removalIntoMatrix(S,12,H)\n",
    "    imshow(M)\n",
    "    print(M.min(),M.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.005005569718913527 0.10451317507833213\n",
      "-0.016705511471859502 0.24766057434100608\n",
      "-0.01740010700227934 0.24279715545657687\n",
      "-0.02124260972961345 0.2714334951014621\n",
      "-0.013770207998019757 0.4445178750577816\n",
      "-0.030045908877506022 0.5306334155256032\n",
      "-0.04749923992525229 0.7551629182619899\n",
      "-0.04128537548354405 0.8338865832689835\n",
      "-0.09416811377902246 0.6449099512361229\n",
      "-0.030574047140864202 0.36213921219646794\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./Models/model_NetRNNWithAttention_min_length_10 max_length_10 fill_0 value_1_-1 value_2_1.model\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "params = {\n",
    "            'min_length': 10, \n",
    "            'max_length': 10, \n",
    "            'fill': 0, \n",
    "            'value_1': -1, \n",
    "            'value_2': 1\n",
    "        }\n",
    "\n",
    "# Run the analysis\n",
    "for i in range(10):\n",
    "    s, t = generateTrainData(100, params)  # Use your generateTrainData function\n",
    "    # Visualize the results\n",
    "    S,H=shrinkingDecompositionInformation(model,12,s,t.transpose(),whichTS=i,dsLength=10)\n",
    "    subplot(6,2,i+1)\n",
    "    M=removalIntoMatrix(S,12,H)\n",
    "    imshow(M)\n",
    "    print(M.min(),M.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.220446049250313e-16 0.10643036859026012\n",
      "-0.017728645892067796 0.19635295245046347\n",
      "-0.021942195578577017 0.23031631998798385\n",
      "-0.011628797747730513 0.12204351281426451\n",
      "-0.029110565412027167 0.11361146601742078\n",
      "-0.01636582997727043 0.11309075508112798\n",
      "-0.007030435210046093 0.09538638915630848\n",
      "-0.017584250798138967 0.07104846712201773\n",
      "-0.03615844123948442 0.06281016307897813\n",
      "-0.004073121839491822 0.06986978610778394\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./Models/model_NetRNNWithAttentionExpFirst_min_length_10 max_length_10 fill_0 value_1_-1 value_2_1.model\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "params = {\n",
    "            'min_length': 10, \n",
    "            'max_length': 10, \n",
    "            'fill': 0, \n",
    "            'value_1': -1, \n",
    "            'value_2': 1\n",
    "        }\n",
    "\n",
    "# Run the analysis\n",
    "for i in range(10):\n",
    "    s, t = generateTrainData(100, params)  # Use your generateTrainData function\n",
    "    # Visualize the results\n",
    "    S,H=shrinkingDecompositionInformation(model,12,s,t.transpose(),whichTS=i,dsLength=10)\n",
    "    subplot(6,2,i+1)\n",
    "    M=removalIntoMatrix(S,12,H)\n",
    "    imshow(M)\n",
    "    print(M.min(),M.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x265085fc550>"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "modelRNNWithAttention=torch.load(\"models/model_RNN_min_length_10 max_length_10 fill_0 value_1_-1 value_2_1.model\")\n",
    "modelRNN=torch.load(\"models/model_NetRNNWithAttention_min_length_10 max_length_10 fill_0 value_1_-1 value_2_1.model\")\n",
    "modelRNNWithAttentionExp =torch.load(\"models/model_NetRNNWithAttentionExpFirst_min_length_10 max_length_10 fill_0 value_1_-1 value_2_1.model\")\n",
    "params = {\n",
    "            'min_length': 10, \n",
    "            'max_length': 10, \n",
    "            'fill': 0, \n",
    "            'value_1': -1, \n",
    "            'value_2': 1\n",
    "        }\n",
    "s, t = generateTrainData(100, params)  \n",
    "S,H=shrinkingDecompositionInformation(modelRNN,12,s,t.transpose(),whichTS=8,dsLength=10)\n",
    "figure()\n",
    "imshow(removalIntoMatrix(S,12,H))\n",
    "S,H=shrinkingDecompositionInformation(modelRNNWithAttention,12,s,t.transpose(),whichTS=8,dsLength=10)\n",
    "figure()\n",
    "imshow(removalIntoMatrix(S,12,H))\n",
    "S,H=shrinkingDecompositionInformation(modelRNNWithAttentionExp,12,s,t.transpose(),whichTS=8,dsLength=10)\n",
    "figure()\n",
    "imshow(removalIntoMatrix(S,12,H))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
